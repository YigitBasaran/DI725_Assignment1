{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DI725 Assignment 1 - Preprocessing - Modeling - Evaluation\n",
        "\n",
        "Ali Yiğit Başaran - 2231355"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_jayPLY5tIK",
        "outputId": "b31910ea-8ce3-4a21-83ff-4c34bf53abad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: networkx in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.2.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: transformers in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.0)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: wandb in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.9)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: requests in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: pandas in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: pydantic<3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.11.1)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.25.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (65.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (2.33.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (0.14.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (2.2.4)\n",
            "Requirement already satisfied: packaging>17.1 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2024.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.0)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (2.2.4)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.15.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nlpaug in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.11)\n",
            "Requirement already satisfied: numpy in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: nltk in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: joblib in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: click in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.18.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.13.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbformat in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.10.4)\n",
            "Requirement already satisfied: traitlets>=5.1 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from nbformat) (5.14.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbformat) (2.21.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from nbformat) (5.7.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbformat) (4.23.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.7)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (310)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.13.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.9)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.25.0)\n",
            "Requirement already satisfied: pydantic<3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.11.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (65.5.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (4.13.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (2.33.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Installing dependencies for training and augmentation, preprocessing\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
        "!pip install torchmetrics\n",
        "!pip install imbalanced-learn\n",
        "!pip install nlpaug numpy nltk\n",
        "!pip install nbformat\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "!pip install -U wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERlsOGYx1zvl",
        "outputId": "f7dbeea1-5f9a-4a20-969c-cf6dd59d01e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DI725' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/caglarmert/DI725.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K_BTLxD12Q5",
        "outputId": "40d45891-8f3e-4e04-cc97-c8dd3c079d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd DI725/assignment_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# General\n",
        "import os, time, math, pickle, io, inspect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from contextlib import nullcontext\n",
        "from dataclasses import dataclass\n",
        "import wandb\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics import MeanMetric\n",
        "\n",
        "# HuggingFace and SMOTE\n",
        "from transformers import GPT2Tokenizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "# Custom model and dataset\n",
        "from model import GPT, GPTConfig\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing and Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c3_KsHth1MuX"
      },
      "outputs": [],
      "source": [
        "# Creating GPT sentiment class to convert decoder only GPT2 model to sentiment analysis model\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from model import GPT\n",
        "class GPTSentiment(GPT):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        # We will add 3 classes for positive,neural and negative\n",
        "        self.sentiment_head = nn.Linear(config.n_embd, 3)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        # Assertion for block size (exists in original repo)\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        x = x.mean(dim=1)  # Take the mean embedding of the tokens\n",
        "        logits = self.sentiment_head(x)  # Geting logits for 3-class classification\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, targets)\n",
        "\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6wWP5Ke1PG7",
        "outputId": "14bbda0a-15a6-40a1-fc0d-7e1b4c0321d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import nlpaug.augmenter.word as naw\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import RegexpTokenizer, stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "# Creating dataloader for our dataset\n",
        "# Dataloader contains preprocess, augment, padding all these\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length, padding=None, augment=False,take_only_customer=False,if_preprocess=True):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label_to_int = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "        if padding is None:\n",
        "            self.padding = \"max_length\"\n",
        "        else:\n",
        "            self.padding = False\n",
        "        self.augment = augment\n",
        "        self.if_take_customer = take_only_customer\n",
        "        self.if_preprocess = if_preprocess\n",
        "        if augment:\n",
        "            self.aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "        if self.if_preprocess:\n",
        "          self.df = self.preprocess(self.df)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    # Remove Punctuation\n",
        "    def remove_punctuation(self,text):\n",
        "        no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
        "        return no_punct\n",
        "\n",
        "    # Remove Stopwords\n",
        "    def remove_stopwords(self,text):\n",
        "        words = [w for w in text if w not in stopwords.words('english')]\n",
        "        return words\n",
        "\n",
        "    def word_lemmatizer(self,text):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        return [lemmatizer.lemmatize(x) for x in text]\n",
        "\n",
        "    def remove_numbers(self,text):\n",
        "        return [x for x in text if not x.isdigit()]\n",
        "\n",
        "\n",
        "    def take_only_customer(self,text):\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        customer_lines = [line.replace(\"Customer: \", \"\") for line in lines if line.startswith(\"Customer:\")]\n",
        "        customer_conversation = \"\\n\".join(customer_lines)\n",
        "        return customer_conversation\n",
        "\n",
        "    def preprocess(self,df):\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.remove_punctuation(x))\n",
        "        # Tokenize the text\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.remove_stopwords(x))\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.word_lemmatizer(x))\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.remove_numbers(x))\n",
        "\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: \" \".join(x))\n",
        "        return df\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        row = self.df.iloc[idx]\n",
        "        text = row[\"conversation\"]\n",
        "        if self.if_take_customer:\n",
        "           text = self.take_only_customer(text)\n",
        "        if self.augment:\n",
        "           text = self.aug.augment(text)[0]\n",
        "        sentiment_label = self.label_to_int[row[\"customer_sentiment\"]]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_length,\n",
        "          return_token_type_ids=False,\n",
        "          padding=self.padding,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt',\n",
        "          truncation=True\n",
        "        )\n",
        "        if len(encoding['input_ids'].flatten()) == 0:\n",
        "          return {\n",
        "          'input_ids': torch.tensor([0]),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'labels': torch.tensor(sentiment_label, dtype=torch.long)\n",
        "          }\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(sentiment_label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training And Evaluation (From Scratch And Pretrained GPT-2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "out_dir = 'out'\n",
        "eval_interval = 20\n",
        "eval_test_interval = 100\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "if_SMOTE = True\n",
        "eval_only = False\n",
        "always_save_checkpoint = True\n",
        "wandb_log = True\n",
        "wandb_project = 'DI725-Asg1'\n",
        "\n",
        "gradient_accumulation_steps = 5 * 8\n",
        "batch_size = 1\n",
        "block_size = 1024\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0\n",
        "bias = False\n",
        "learning_rate = 3e-5\n",
        "max_iters = 500\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "decay_lr = False\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = 600000\n",
        "min_lr = 6e-5\n",
        "backend = 'nccl'\n",
        "device = \"cuda\"\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = False\n",
        "\n",
        "# Collect config for wandb logging\n",
        "config_keys = [k for k, v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def configure_optimizers(model, weight_decay, learning_rate, betas, device_type):\n",
        "    param_dict = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
        "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "    optim_groups = [\n",
        "        {'params': decay_params, 'weight_decay': weight_decay},\n",
        "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "    use_fused = fused_available and device_type == 'cuda'\n",
        "    extra_args = dict(fused=True) if use_fused else dict()\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "    return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_model_and_data(init_from, wandb_run_name):\n",
        "    ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "    if ddp:\n",
        "        init_process_group(backend=backend)\n",
        "        ddp_rank = int(os.environ['RANK'])\n",
        "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "        device_local = f'cuda:{ddp_local_rank}'\n",
        "        torch.cuda.set_device(device_local)\n",
        "        master_process = ddp_rank == 0\n",
        "        seed_offset = ddp_rank\n",
        "        assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    else:\n",
        "        device_local = device\n",
        "        master_process = True\n",
        "        seed_offset = 0\n",
        "        ddp_world_size = 1\n",
        "\n",
        "    torch.manual_seed(1337 + seed_offset)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    device_type_local = 'cuda' if 'cuda' in device_local else 'cpu'\n",
        "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "    ctx = nullcontext() if device_type_local == 'cpu' else torch.amp.autocast(device_type=device_type_local, dtype=ptdtype)\n",
        "\n",
        "    data_dir = os.path.join('data', \"random\")\n",
        "    meta_vocab_size = None\n",
        "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "    if os.path.exists(meta_path):\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f)\n",
        "        meta_vocab_size = meta['vocab_size']\n",
        "\n",
        "    model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                      bias=bias, vocab_size=None, dropout=dropout)\n",
        "\n",
        "    if init_from == 'scratch':\n",
        "        print(\"Initializing from scratch\")\n",
        "        model_args['vocab_size'] = meta_vocab_size if meta_vocab_size else 50304\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model_gpt = GPT(gptconf)\n",
        "        model = GPTSentiment(gptconf)\n",
        "    elif init_from == 'resume':\n",
        "        ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "        checkpoint = torch.load(ckpt_path, map_location=device_local)\n",
        "        checkpoint_model_args = checkpoint['model_args']\n",
        "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "            model_args[k] = checkpoint_model_args[k]\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model_gpt = GPT(gptconf)\n",
        "        model_gpt.load_state_dict(checkpoint['model'])\n",
        "        model = GPTSentiment(model_gpt)\n",
        "    elif init_from.startswith('gpt2'):\n",
        "        print(f\"Initializing from GPT2: {init_from}\")\n",
        "        override_args = dict(dropout=dropout)\n",
        "        model_gpt = GPT.from_pretrained(init_from, override_args)\n",
        "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "            model_args[k] = getattr(model_gpt.config, k)\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model = GPTSentiment(gptconf)\n",
        "        model.load_state_dict(model_gpt.state_dict(), strict=False)\n",
        "\n",
        "    model.to(device_local)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "    optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type_local)\n",
        "\n",
        "    if compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    if ddp:\n",
        "        model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "    if wandb_log and master_process:\n",
        "        import wandb\n",
        "        wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    train_df = pd.read_csv('./data/customer_service/train.csv')\n",
        "    test_df = pd.read_csv('./data/customer_service/test.csv')\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    if if_SMOTE:\n",
        "        sm = RandomOverSampler(random_state=42)\n",
        "        X_train_smote, y_train_smote = sm.fit_resample(train_df[[\"conversation\"]], train_df[\"customer_sentiment\"])\n",
        "        train_df_smoted = pd.concat([X_train_smote, y_train_smote.reset_index(drop=True)], axis=1)\n",
        "    else:\n",
        "        train_df_smoted = train_df\n",
        "\n",
        "    train_dataset = SentimentDataset(train_df_smoted, tokenizer, max_length=512, padding=False, augment=True, take_only_customer=False, if_preprocess=True)\n",
        "    val_dataset = SentimentDataset(val_df, tokenizer, max_length=512, padding=False, take_only_customer=False, if_preprocess=True)\n",
        "    test_dataset = SentimentDataset(test_df, tokenizer, max_length=512, padding=False, take_only_customer=False, if_preprocess=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    return model, model_args, optimizer, scaler, ctx, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset, device_local\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimate_loss(model, ctx, train_loader_iter, val_loader_iter,\n",
        "                  train_loader, val_loader, num_train, num_val, device):\n",
        "    out = {\"train\": {}, \"val\": {}}\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        loader_iter = train_loader_iter if split == \"train\" else val_loader_iter\n",
        "        loader = train_loader if split == \"train\" else val_loader\n",
        "        dataset_len = num_train if split == \"train\" else num_val\n",
        "\n",
        "        iter_num = dataset_len if loader.batch_size == 1 else min(dataset_len // loader.batch_size + 1, eval_iters // loader.batch_size + 1)\n",
        "        losses = torch.zeros(iter_num)\n",
        "\n",
        "        for k in range(iter_num):\n",
        "            try:\n",
        "                batch = next(loader_iter)\n",
        "                X = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                Y = batch['labels'].to(device)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(Y.cpu().numpy())\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        out[split] = {\n",
        "            \"loss\": losses.mean(),\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"conf_matrix\": conf_matrix\n",
        "        }\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def estimate_loss_test(model, ctx, test_loader, num_samples, device):\n",
        "    out = {\"test\": {}}\n",
        "    test_loader_iter = iter(test_loader)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    iter_num = num_samples if test_loader.batch_size == 1 else min(num_samples // test_loader.batch_size + 1, eval_iters // test_loader.batch_size + 1)\n",
        "    losses = torch.zeros(iter_num)\n",
        "\n",
        "    for k in range(iter_num):\n",
        "        try:\n",
        "            batch = next(test_loader_iter)\n",
        "            X = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            Y = batch['labels'].to(device)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(Y.cpu().numpy())\n",
        "        losses[k] = loss.item()\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    out[\"test\"] = {\n",
        "        \"loss\": losses.mean(),\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"conf_matrix\": conf_matrix\n",
        "    }\n",
        "\n",
        "    model.train()\n",
        "    return out, all_labels, all_preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(init_from):\n",
        "    global config\n",
        "    wandb_run_name = 'gpt2_' + init_from\n",
        "    model, model_args, optimizer, scaler, ctx, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset, device_used = setup_model_and_data(init_from, wandb_run_name)\n",
        "\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    val_loader_iter = iter(val_loader)\n",
        "    test_loader_iter = iter(test_loader)\n",
        "\n",
        "    t0 = time.time()\n",
        "    local_iter_num = 0\n",
        "    iter_num = 0\n",
        "    best_val_loss = 1e9\n",
        "    train_loss_metric = MeanMetric()\n",
        "    val_loss_metric = MeanMetric()\n",
        "    raw_model = model.module if isinstance(model, DDP) else model\n",
        "\n",
        "    batch = next(train_loader_iter)\n",
        "    X = batch['input_ids'].to(device_used)\n",
        "    attention_mask = batch['attention_mask'].to(device_used)\n",
        "    Y = batch['labels'].to(device_used)\n",
        "\n",
        "    while True:\n",
        "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        if iter_num % eval_interval == 0:\n",
        "            out_res = estimate_loss(\n",
        "                model, ctx,\n",
        "                train_loader_iter, val_loader_iter,\n",
        "                train_loader, val_loader,\n",
        "                len(train_dataset), len(val_dataset),\n",
        "                device_used\n",
        "            )\n",
        "            print(f\"step {iter_num}: train loss {train_loss_metric.compute():.4f}, val loss {val_loss_metric.compute():.4f}\")\n",
        "            print(f\"step val accuracy {out_res['val']['accuracy']:.4f}\")\n",
        "\n",
        "            if wandb_log:\n",
        "                wandb.log({\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train/loss\": train_loss_metric.compute(),\n",
        "                    \"val/loss\": val_loss_metric.compute(),\n",
        "                    \"lr\": lr,\n",
        "                    \"val/acc\": out_res['val']['accuracy'],\n",
        "                })\n",
        "\n",
        "            if out_res['val'][\"loss\"] < best_val_loss or always_save_checkpoint:\n",
        "                best_val_loss = out_res['val'][\"loss\"]\n",
        "                if iter_num > 0:\n",
        "                    checkpoint = {\n",
        "                        'model': raw_model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'model_args': model_args,\n",
        "                        'iter_num': iter_num,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'config': config,\n",
        "                    }\n",
        "                    os.makedirs(out_dir, exist_ok=True)\n",
        "                    print(f\"saving checkpoint to {out_dir}\")\n",
        "                    torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "        if iter_num % eval_test_interval == 0:\n",
        "            out_res, all_labels, all_preds = estimate_loss_test(model, ctx, test_loader, len(test_dataset), device_used)\n",
        "            test_acc = out_res['test']['accuracy']\n",
        "            test_precision = out_res['test']['precision']\n",
        "            test_recall = out_res['test']['recall']\n",
        "            test_f1 = out_res['test']['f1']\n",
        "            print(f\"Test accuracy {test_acc:.4f}\")\n",
        "            print(f\"Test precision {test_precision:.4f}\")\n",
        "            print(f\"Test recall {test_recall:.4f}\")\n",
        "            print(f\"Test F1 {test_f1:.4f}\")\n",
        "            print(\"Confusion Matrix\")\n",
        "            print(\"------------------\")\n",
        "            print(out_res['test']['conf_matrix'])\n",
        "\n",
        "            if wandb_log:\n",
        "                wandb.log({\n",
        "                    \"Test_Accuracy\": test_acc,\n",
        "                    \"Test_Precision\": test_precision,\n",
        "                    \"Test_Recall\": test_recall,\n",
        "                    \"Test_F1_Score\": test_f1\n",
        "                })\n",
        "                x_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "                wandb.log({\n",
        "                    \"Confusion Matrix\": wandb.plot.confusion_matrix(\n",
        "                        probs=None,\n",
        "                        y_true=all_labels,\n",
        "                        preds=all_preds,\n",
        "                        class_names=x_labels\n",
        "                    )\n",
        "                })\n",
        "\n",
        "        if iter_num == 0 and eval_only:\n",
        "            break\n",
        "\n",
        "        for micro_step in range(gradient_accumulation_steps):\n",
        "            if isinstance(model, DDP):\n",
        "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "                train_loss_metric.update(loss.item())\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            try:\n",
        "                batch = next(train_loader_iter)\n",
        "            except StopIteration:\n",
        "                train_loader_iter = iter(train_loader)\n",
        "                batch = next(train_loader_iter)\n",
        "\n",
        "            X = batch['input_ids'].to(device_used)\n",
        "            attention_mask = batch['attention_mask'].to(device_used)\n",
        "            Y = batch['labels'].to(device_used)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "        if grad_clip != 0.0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                batch_val = next(val_loader_iter)\n",
        "            except StopIteration:\n",
        "                val_loader_iter = iter(val_loader)\n",
        "                batch_val = next(val_loader_iter)\n",
        "            X_val = batch_val['input_ids'].to(device_used)\n",
        "            Y_val = batch_val['labels'].to(device_used)\n",
        "            logits_val, loss_val = model(X_val, Y_val)\n",
        "            val_loss_metric.update(loss_val.item())\n",
        "        model.train()\n",
        "\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        t0 = t1\n",
        "        if iter_num % log_interval == 0:\n",
        "            lossf = loss.item() * gradient_accumulation_steps\n",
        "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "\n",
        "        iter_num += 1\n",
        "        local_iter_num += 1\n",
        "\n",
        "        if iter_num > max_iters:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XQ-m1Bvq1Q8e",
        "outputId": "f7955db8-8037-413a-9c36-2badcef64309"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 123.59M\n",
            "number of parameters: 123.59M\n",
            "num decayed parameter tensors: 51, with 124,356,864 parameters\n",
            "num non-decayed parameter tensors: 26, with 19,203 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-763ebe6df08c>:243: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maliyigitbasaran\u001b[0m (\u001b[33maliyigitbasaran-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/DI725/assignment_1/wandb/run-20250331_212539-r4b3zxht</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1/runs/r4b3zxht' target=\"_blank\">gpt2_scratch</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1/runs/r4b3zxht' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1/runs/r4b3zxht</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.2943\n",
            "Number of Samples Test =  30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "Test precision 0.1149\n",
            "Test recall 0.3333\n",
            "Test F1 0.1709\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 0  0 10]\n",
            " [ 1  0  9]\n",
            " [ 0  0 10]]\n",
            "iter 0: loss 1.3125, time 69357.87ms\n",
            "iter 1: loss 11.3125, time 2794.59ms\n",
            "iter 2: loss 0.2080, time 2814.23ms\n",
            "iter 3: loss 2.8594, time 3199.42ms\n",
            "iter 4: loss 0.7461, time 3235.71ms\n",
            "iter 5: loss 0.7891, time 3001.80ms\n",
            "iter 6: loss 1.7734, time 2991.91ms\n",
            "iter 7: loss 1.3359, time 2973.93ms\n",
            "iter 8: loss 1.5078, time 3199.47ms\n",
            "iter 9: loss 1.1563, time 3029.41ms\n",
            "iter 10: loss 1.7031, time 2839.33ms\n",
            "iter 11: loss 1.7656, time 2972.85ms\n",
            "iter 12: loss 1.3281, time 3122.76ms\n",
            "iter 13: loss 1.3984, time 2880.09ms\n",
            "iter 14: loss 0.4785, time 2829.61ms\n",
            "iter 15: loss 1.8594, time 2891.76ms\n",
            "iter 16: loss 1.4766, time 3030.59ms\n",
            "iter 17: loss 0.7773, time 3346.64ms\n",
            "iter 18: loss 1.9766, time 3533.66ms\n",
            "iter 19: loss 1.5156, time 3111.10ms\n",
            "step 20: train loss 1.5760, val loss 1.2840\n",
            "step val accuracy 0.5089\n",
            "saving checkpoint to out\n",
            "iter 20: loss 1.3281, time 30995.47ms\n",
            "iter 21: loss 0.9375, time 3285.81ms\n",
            "iter 22: loss 0.0299, time 3958.34ms\n",
            "iter 23: loss 0.0101, time 3511.08ms\n",
            "iter 24: loss 1.8125, time 4116.53ms\n",
            "iter 25: loss 0.2051, time 5127.61ms\n",
            "iter 26: loss 1.6563, time 3126.20ms\n",
            "iter 27: loss 1.2734, time 2947.62ms\n",
            "iter 28: loss 0.9609, time 3238.46ms\n",
            "iter 29: loss 1.3672, time 3192.38ms\n",
            "iter 30: loss 0.2656, time 2887.57ms\n",
            "iter 31: loss 0.4297, time 3031.72ms\n",
            "iter 32: loss 0.1553, time 2948.71ms\n",
            "iter 33: loss 0.7773, time 3239.18ms\n",
            "iter 34: loss 0.3340, time 2957.33ms\n",
            "iter 35: loss 0.0002, time 2850.65ms\n",
            "iter 36: loss 0.7422, time 2795.91ms\n",
            "iter 37: loss 1.0859, time 3091.62ms\n",
            "iter 38: loss 0.0649, time 3070.63ms\n",
            "iter 39: loss 0.2354, time 2933.42ms\n",
            "step 40: train loss 1.0946, val loss 1.0226\n",
            "step val accuracy 0.7027\n",
            "saving checkpoint to out\n",
            "iter 40: loss 2.3281, time 32238.17ms\n",
            "iter 41: loss 0.0820, time 3068.50ms\n",
            "iter 42: loss 0.0014, time 3278.58ms\n",
            "iter 43: loss 0.0393, time 2953.21ms\n",
            "iter 44: loss 0.4746, time 2931.00ms\n",
            "iter 45: loss 0.1875, time 2987.80ms\n",
            "iter 46: loss 0.0762, time 3188.60ms\n",
            "iter 47: loss 0.0106, time 3038.14ms\n",
            "iter 48: loss 0.0574, time 2811.55ms\n",
            "iter 49: loss 0.0674, time 2873.51ms\n",
            "iter 50: loss 0.0214, time 3122.50ms\n",
            "iter 51: loss 0.0061, time 3188.28ms\n",
            "iter 52: loss 0.0154, time 3018.38ms\n",
            "iter 53: loss 0.1162, time 3008.30ms\n",
            "iter 54: loss 0.6719, time 2948.01ms\n",
            "iter 55: loss 0.0101, time 3226.59ms\n",
            "iter 56: loss 0.0439, time 3251.59ms\n",
            "iter 57: loss 0.0889, time 2999.92ms\n",
            "iter 58: loss 0.0012, time 2990.55ms\n",
            "iter 59: loss 1.2422, time 2992.92ms\n",
            "step 60: train loss 0.8633, val loss 0.9506\n",
            "step val accuracy 0.8107\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0013, time 29941.38ms\n",
            "iter 61: loss 0.0007, time 2984.17ms\n",
            "iter 62: loss 0.0005, time 2908.54ms\n",
            "iter 63: loss 0.8086, time 2902.72ms\n",
            "iter 64: loss 0.0032, time 3866.69ms\n",
            "iter 65: loss 0.0125, time 3078.71ms\n",
            "iter 66: loss 0.0767, time 2911.62ms\n",
            "iter 67: loss 0.0334, time 2961.92ms\n",
            "iter 68: loss 0.0908, time 3100.54ms\n",
            "iter 69: loss 0.0007, time 3409.12ms\n",
            "iter 70: loss 0.0141, time 2992.83ms\n",
            "iter 71: loss 0.0099, time 2967.51ms\n",
            "iter 72: loss 0.8789, time 2865.30ms\n",
            "iter 73: loss 0.0052, time 3127.57ms\n",
            "iter 74: loss 0.0004, time 3031.87ms\n",
            "iter 75: loss 0.0125, time 2983.06ms\n",
            "iter 76: loss 0.2617, time 2930.92ms\n",
            "iter 77: loss 0.0077, time 3215.19ms\n",
            "iter 78: loss 0.0908, time 3183.65ms\n",
            "iter 79: loss 4.4375, time 2948.45ms\n",
            "step 80: train loss 0.7110, val loss 0.8357\n",
            "step val accuracy 0.8624\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0156, time 30964.57ms\n",
            "iter 81: loss 0.1719, time 3078.95ms\n",
            "iter 82: loss 0.0957, time 3158.14ms\n",
            "iter 83: loss 0.1631, time 2929.76ms\n",
            "iter 84: loss 0.0815, time 2935.97ms\n",
            "iter 85: loss 0.0593, time 3065.33ms\n",
            "iter 86: loss 0.0017, time 3191.08ms\n",
            "iter 87: loss 0.0208, time 3067.53ms\n",
            "iter 88: loss 0.1226, time 2959.30ms\n",
            "iter 89: loss 0.0004, time 2862.84ms\n",
            "iter 90: loss 1.0859, time 3042.04ms\n",
            "iter 91: loss 0.0001, time 3305.18ms\n",
            "iter 92: loss 3.5469, time 2880.20ms\n",
            "iter 93: loss 0.0004, time 2996.66ms\n",
            "iter 94: loss 0.0010, time 2989.83ms\n",
            "iter 95: loss 0.0610, time 3322.51ms\n",
            "iter 96: loss 0.0009, time 3114.39ms\n",
            "iter 97: loss 0.0094, time 2876.55ms\n",
            "iter 98: loss 0.2129, time 2991.35ms\n",
            "iter 99: loss 0.6250, time 2975.22ms\n",
            "step 100: train loss 0.6051, val loss 0.7672\n",
            "step val accuracy 0.9127\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5667\n",
            "Test precision 0.5782\n",
            "Test recall 0.5667\n",
            "Test F1 0.5510\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [3 5 2]\n",
            " [0 7 3]]\n",
            "iter 100: loss 0.0002, time 33231.79ms\n",
            "iter 101: loss 0.0008, time 2931.19ms\n",
            "iter 102: loss 0.0038, time 2844.62ms\n",
            "iter 103: loss 0.0007, time 3309.73ms\n",
            "iter 104: loss 0.0014, time 3146.53ms\n",
            "iter 105: loss 0.0037, time 2974.50ms\n",
            "iter 106: loss 0.0006, time 3031.04ms\n",
            "iter 107: loss 0.0005, time 2924.26ms\n",
            "iter 108: loss 0.0874, time 3316.54ms\n",
            "iter 109: loss 0.0001, time 3002.42ms\n",
            "iter 110: loss 0.0009, time 3026.70ms\n",
            "iter 111: loss 0.5234, time 2956.80ms\n",
            "iter 112: loss 5.9688, time 3144.97ms\n",
            "iter 113: loss 0.0035, time 3076.60ms\n",
            "iter 114: loss 0.0085, time 2955.05ms\n",
            "iter 115: loss 1.7266, time 2949.46ms\n",
            "iter 116: loss 0.0197, time 3013.03ms\n",
            "iter 117: loss 0.0010, time 3272.71ms\n",
            "iter 118: loss 0.1138, time 2909.23ms\n",
            "iter 119: loss 0.0074, time 3084.06ms\n",
            "step 120: train loss 0.5487, val loss 0.7965\n",
            "step val accuracy 0.9201\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0001, time 35844.17ms\n",
            "iter 121: loss 0.0104, time 2903.25ms\n",
            "iter 122: loss 0.0010, time 2921.24ms\n",
            "iter 123: loss 0.0010, time 3021.23ms\n",
            "iter 124: loss 0.0322, time 3208.28ms\n",
            "iter 125: loss 0.0011, time 2924.35ms\n",
            "iter 126: loss 0.0454, time 2871.67ms\n",
            "iter 127: loss 0.0038, time 2947.63ms\n",
            "iter 128: loss 0.0031, time 3293.71ms\n",
            "iter 129: loss 0.0008, time 3073.39ms\n",
            "iter 130: loss 0.0019, time 3036.54ms\n",
            "iter 131: loss 0.0009, time 2928.47ms\n",
            "iter 132: loss 0.0006, time 3157.63ms\n",
            "iter 133: loss 0.0005, time 3177.69ms\n",
            "iter 134: loss 0.0006, time 3037.66ms\n",
            "iter 135: loss 0.0544, time 3037.05ms\n",
            "iter 136: loss 0.0005, time 2994.86ms\n",
            "iter 137: loss 0.6758, time 3312.64ms\n",
            "iter 138: loss 0.0004, time 3049.18ms\n",
            "iter 139: loss 0.0069, time 2912.78ms\n",
            "step 140: train loss 0.4895, val loss 0.7766\n",
            "step val accuracy 0.9334\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0259, time 29353.66ms\n",
            "iter 141: loss 0.0016, time 2829.36ms\n",
            "iter 142: loss 0.0461, time 3148.44ms\n",
            "iter 143: loss 0.0022, time 3025.36ms\n",
            "iter 144: loss 0.0020, time 2890.18ms\n",
            "iter 145: loss 0.0021, time 2914.82ms\n",
            "iter 146: loss 0.0054, time 3018.09ms\n",
            "iter 147: loss 0.0023, time 3216.08ms\n",
            "iter 148: loss 0.0115, time 3047.70ms\n",
            "iter 149: loss 0.0047, time 2904.33ms\n",
            "iter 150: loss 0.0092, time 2974.41ms\n",
            "iter 151: loss 0.0044, time 3166.92ms\n",
            "iter 152: loss 0.0435, time 3064.42ms\n",
            "iter 153: loss 0.0000, time 3032.07ms\n",
            "iter 154: loss 0.0040, time 3097.12ms\n",
            "iter 155: loss 0.0009, time 2994.69ms\n",
            "iter 156: loss 0.0001, time 3205.28ms\n",
            "iter 157: loss 0.0000, time 3030.53ms\n",
            "iter 158: loss 0.0001, time 2950.95ms\n",
            "iter 159: loss 0.0022, time 3033.38ms\n",
            "step 160: train loss 0.4420, val loss 0.7634\n",
            "step val accuracy 0.9201\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 31795.07ms\n",
            "iter 161: loss 0.0000, time 2903.67ms\n",
            "iter 162: loss 0.0007, time 2961.27ms\n",
            "iter 163: loss 0.9844, time 2847.59ms\n",
            "iter 164: loss 0.1416, time 3280.88ms\n",
            "iter 165: loss 0.0001, time 3144.06ms\n",
            "iter 166: loss 0.0001, time 3075.67ms\n",
            "iter 167: loss 0.0001, time 2954.26ms\n",
            "iter 168: loss 0.0005, time 3135.31ms\n",
            "iter 169: loss 0.0000, time 3120.63ms\n",
            "iter 170: loss 0.0000, time 2893.75ms\n",
            "iter 171: loss 0.0129, time 2988.93ms\n",
            "iter 172: loss 0.0001, time 3028.71ms\n",
            "iter 173: loss 0.0000, time 3238.73ms\n",
            "iter 174: loss 0.2256, time 2942.75ms\n",
            "iter 175: loss 0.0004, time 2883.25ms\n",
            "iter 176: loss 0.0000, time 3019.04ms\n",
            "iter 177: loss 0.1147, time 2992.71ms\n",
            "iter 178: loss 0.0001, time 3161.16ms\n",
            "iter 179: loss 0.5625, time 3095.36ms\n",
            "step 180: train loss 0.4069, val loss 0.7577\n",
            "step val accuracy 0.9527\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0007, time 32667.89ms\n",
            "iter 181: loss 0.0001, time 3138.10ms\n",
            "iter 182: loss 0.0065, time 2994.60ms\n",
            "iter 183: loss 0.0009, time 2867.05ms\n",
            "iter 184: loss 0.0002, time 2961.20ms\n",
            "iter 185: loss 0.0513, time 3049.03ms\n",
            "iter 186: loss 0.0001, time 3303.35ms\n",
            "iter 187: loss 0.0017, time 2956.04ms\n",
            "iter 188: loss 0.0000, time 3095.92ms\n",
            "iter 189: loss 0.0001, time 2989.96ms\n",
            "iter 190: loss 0.0000, time 3059.15ms\n",
            "iter 191: loss 0.0000, time 3113.96ms\n",
            "iter 192: loss 0.0001, time 2916.60ms\n",
            "iter 193: loss 0.0101, time 3033.15ms\n",
            "iter 194: loss 0.0001, time 3018.74ms\n",
            "iter 195: loss 0.0001, time 3257.64ms\n",
            "iter 196: loss 0.0000, time 2975.29ms\n",
            "iter 197: loss 0.0001, time 2929.27ms\n",
            "iter 198: loss 0.0035, time 3063.27ms\n",
            "iter 199: loss 0.0001, time 3038.95ms\n",
            "step 200: train loss 0.3736, val loss 0.7541\n",
            "step val accuracy 0.9127\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.5720\n",
            "Test recall 0.5333\n",
            "Test F1 0.5023\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [4 5 1]\n",
            " [0 8 2]]\n",
            "iter 200: loss 0.0002, time 36796.94ms\n",
            "iter 201: loss 0.0001, time 3252.08ms\n",
            "iter 202: loss 0.0000, time 3066.41ms\n",
            "iter 203: loss 0.0007, time 2991.93ms\n",
            "iter 204: loss 0.0001, time 2850.60ms\n",
            "iter 205: loss 0.0000, time 2960.51ms\n",
            "iter 206: loss 3.0469, time 3353.93ms\n",
            "iter 207: loss 0.0001, time 3037.81ms\n",
            "iter 208: loss 0.0000, time 2993.51ms\n",
            "iter 209: loss 0.0109, time 3029.96ms\n",
            "iter 210: loss 0.0001, time 3189.54ms\n",
            "iter 211: loss 0.0000, time 3067.14ms\n",
            "iter 212: loss 0.0000, time 3102.99ms\n",
            "iter 213: loss 0.0000, time 2955.49ms\n",
            "iter 214: loss 0.0006, time 2992.62ms\n",
            "iter 215: loss 0.0000, time 3094.77ms\n",
            "iter 216: loss 1.5625, time 3074.11ms\n",
            "iter 217: loss 0.0019, time 2998.29ms\n",
            "iter 218: loss 0.0056, time 2993.18ms\n",
            "iter 219: loss 0.0025, time 3220.45ms\n",
            "step 220: train loss 0.3589, val loss 0.7318\n",
            "step val accuracy 0.9453\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0003, time 31743.05ms\n",
            "iter 221: loss 0.0008, time 3037.97ms\n",
            "iter 222: loss 0.0119, time 3021.67ms\n",
            "iter 223: loss 0.0019, time 3332.39ms\n",
            "iter 224: loss 0.0007, time 3044.35ms\n",
            "iter 225: loss 0.0027, time 2936.01ms\n",
            "iter 226: loss 0.0087, time 3018.80ms\n",
            "iter 227: loss 0.0186, time 3062.53ms\n",
            "iter 228: loss 0.0004, time 3089.78ms\n",
            "iter 229: loss 0.0070, time 2973.97ms\n",
            "iter 230: loss 0.0005, time 2896.34ms\n",
            "iter 231: loss 0.0771, time 3059.80ms\n",
            "iter 232: loss 0.7656, time 3214.03ms\n",
            "iter 233: loss 0.0033, time 3052.14ms\n",
            "iter 234: loss 0.0015, time 2997.84ms\n",
            "iter 235: loss 0.0001, time 3114.99ms\n",
            "iter 236: loss 0.0684, time 3128.86ms\n",
            "iter 237: loss 0.0003, time 3081.44ms\n",
            "iter 238: loss 0.0001, time 2984.57ms\n",
            "iter 239: loss 0.0001, time 2941.22ms\n",
            "step 240: train loss 0.3349, val loss 0.7058\n",
            "step val accuracy 0.9408\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 35979.92ms\n",
            "iter 241: loss 0.0001, time 2873.57ms\n",
            "iter 242: loss 0.0309, time 2973.22ms\n",
            "iter 243: loss 0.0000, time 3189.10ms\n",
            "iter 244: loss 0.0001, time 3030.96ms\n",
            "iter 245: loss 0.0000, time 3074.74ms\n",
            "iter 246: loss 0.0002, time 2947.88ms\n",
            "iter 247: loss 0.0001, time 3165.43ms\n",
            "iter 248: loss 0.0001, time 3118.97ms\n",
            "iter 249: loss 0.0000, time 3019.17ms\n",
            "iter 250: loss 0.0000, time 3052.14ms\n",
            "iter 251: loss 0.0002, time 3072.43ms\n",
            "iter 252: loss 0.0001, time 3207.49ms\n",
            "iter 253: loss 0.0001, time 3026.02ms\n",
            "iter 254: loss 0.0000, time 2981.08ms\n",
            "iter 255: loss 0.0005, time 3055.40ms\n",
            "iter 256: loss 0.0000, time 3225.40ms\n",
            "iter 257: loss 0.0012, time 3021.22ms\n",
            "iter 258: loss 0.0000, time 3019.94ms\n",
            "iter 259: loss 0.0000, time 2875.08ms\n",
            "step 260: train loss 0.3185, val loss 0.7041\n",
            "step val accuracy 0.8743\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0020, time 30575.77ms\n",
            "iter 261: loss 0.0000, time 3161.89ms\n",
            "iter 262: loss 0.0000, time 2916.96ms\n",
            "iter 263: loss 0.0005, time 2860.36ms\n",
            "iter 264: loss 0.0011, time 2868.33ms\n",
            "iter 265: loss 0.0000, time 3171.05ms\n",
            "iter 266: loss 0.0003, time 3163.39ms\n",
            "iter 267: loss 0.0000, time 3043.52ms\n",
            "iter 268: loss 0.0002, time 2983.36ms\n",
            "iter 269: loss 0.0011, time 3107.56ms\n",
            "iter 270: loss 0.0005, time 3206.91ms\n",
            "iter 271: loss 0.0004, time 3040.49ms\n",
            "iter 272: loss 0.0006, time 3100.95ms\n",
            "iter 273: loss 0.0004, time 2951.04ms\n",
            "iter 274: loss 0.0000, time 3129.73ms\n",
            "iter 275: loss 0.0002, time 3024.25ms\n",
            "iter 276: loss 0.0164, time 3035.57ms\n",
            "iter 277: loss 0.0008, time 3029.22ms\n",
            "iter 278: loss 0.0000, time 3173.76ms\n",
            "iter 279: loss 0.0002, time 3194.77ms\n",
            "step 280: train loss 0.3027, val loss 0.6592\n",
            "step val accuracy 0.9586\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 36046.64ms\n",
            "iter 281: loss 0.0006, time 3023.80ms\n",
            "iter 282: loss 0.0003, time 2833.99ms\n",
            "iter 283: loss 0.0000, time 2942.18ms\n",
            "iter 284: loss 0.0000, time 2849.41ms\n",
            "iter 285: loss 0.0003, time 3198.55ms\n",
            "iter 286: loss 0.0000, time 3023.39ms\n",
            "iter 287: loss 0.0000, time 3108.33ms\n",
            "iter 288: loss 0.0000, time 2986.23ms\n",
            "iter 289: loss 0.0000, time 3085.70ms\n",
            "iter 290: loss 0.0003, time 3358.49ms\n",
            "iter 291: loss 0.0017, time 2923.94ms\n",
            "iter 292: loss 1.9609, time 3017.22ms\n",
            "iter 293: loss 0.0001, time 2948.05ms\n",
            "iter 294: loss 0.0004, time 3268.83ms\n",
            "iter 295: loss 0.0001, time 3013.23ms\n",
            "iter 296: loss 0.0002, time 2966.08ms\n",
            "iter 297: loss 0.0298, time 2907.69ms\n",
            "iter 298: loss 0.0000, time 3222.66ms\n",
            "iter 299: loss 0.0002, time 3975.82ms\n",
            "step 300: train loss 0.2856, val loss 0.6517\n",
            "step val accuracy 0.9601\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5667\n",
            "Test precision 0.5727\n",
            "Test recall 0.5667\n",
            "Test F1 0.5410\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 6 2]\n",
            " [0 8 2]]\n",
            "iter 300: loss 0.0001, time 30879.13ms\n",
            "iter 301: loss 0.0001, time 2955.63ms\n",
            "iter 302: loss 0.0016, time 3053.20ms\n",
            "iter 303: loss 0.0002, time 3160.90ms\n",
            "iter 304: loss 0.0001, time 2910.97ms\n",
            "iter 305: loss 0.0000, time 2951.34ms\n",
            "iter 306: loss 0.0000, time 2909.43ms\n",
            "iter 307: loss 0.0593, time 3257.04ms\n",
            "iter 308: loss 0.0003, time 3064.49ms\n",
            "iter 309: loss 0.0009, time 2933.84ms\n",
            "iter 310: loss 0.0005, time 3055.37ms\n",
            "iter 311: loss 0.0002, time 3067.43ms\n",
            "iter 312: loss 0.0013, time 3195.03ms\n",
            "iter 313: loss 0.8086, time 3011.22ms\n",
            "iter 314: loss 0.0000, time 2935.24ms\n",
            "iter 315: loss 0.0001, time 2951.14ms\n",
            "iter 316: loss 0.0001, time 3138.58ms\n",
            "iter 317: loss 0.0068, time 3025.39ms\n",
            "iter 318: loss 0.0000, time 3139.25ms\n",
            "iter 319: loss 0.0006, time 2972.41ms\n",
            "step 320: train loss 0.2700, val loss 0.6516\n",
            "step val accuracy 0.9275\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 37572.76ms\n",
            "iter 321: loss 0.0004, time 3085.41ms\n",
            "iter 322: loss 0.0117, time 3207.53ms\n",
            "iter 323: loss 0.0000, time 3110.22ms\n",
            "iter 324: loss 0.0046, time 2940.46ms\n",
            "iter 325: loss 0.0004, time 3043.06ms\n",
            "iter 326: loss 0.0002, time 2951.98ms\n",
            "iter 327: loss 0.0000, time 3338.21ms\n",
            "iter 328: loss 0.0041, time 2957.05ms\n",
            "iter 329: loss 0.0000, time 3063.76ms\n",
            "iter 330: loss 0.0000, time 2950.00ms\n",
            "iter 331: loss 0.0001, time 3205.54ms\n",
            "iter 332: loss 0.0000, time 3180.89ms\n",
            "iter 333: loss 0.0000, time 2857.39ms\n",
            "iter 334: loss 0.0001, time 3155.40ms\n",
            "iter 335: loss 0.0001, time 3045.10ms\n",
            "iter 336: loss 0.0000, time 3290.36ms\n",
            "iter 337: loss 0.0000, time 2955.53ms\n",
            "iter 338: loss 0.0000, time 3037.81ms\n",
            "iter 339: loss 0.0004, time 2924.16ms\n",
            "step 340: train loss 0.2573, val loss 0.6573\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 32652.44ms\n",
            "iter 341: loss 0.0000, time 2989.24ms\n",
            "iter 342: loss 0.0002, time 2988.58ms\n",
            "iter 343: loss 0.0000, time 2958.33ms\n",
            "iter 344: loss 0.0000, time 3212.00ms\n",
            "iter 345: loss 0.0000, time 3092.92ms\n",
            "iter 346: loss 0.0000, time 3101.47ms\n",
            "iter 347: loss 0.0000, time 3068.54ms\n",
            "iter 348: loss 0.0001, time 3323.87ms\n",
            "iter 349: loss 0.0000, time 3177.53ms\n",
            "iter 350: loss 0.0003, time 3004.30ms\n",
            "iter 351: loss 0.0000, time 3035.98ms\n",
            "iter 352: loss 0.0000, time 3127.02ms\n",
            "iter 353: loss 0.0000, time 3347.54ms\n",
            "iter 354: loss 0.0000, time 3072.39ms\n",
            "iter 355: loss 0.0000, time 3015.75ms\n",
            "iter 356: loss 0.0000, time 3088.02ms\n",
            "iter 357: loss 0.0008, time 3265.96ms\n",
            "iter 358: loss 0.0000, time 3230.17ms\n",
            "iter 359: loss 0.0000, time 2953.61ms\n",
            "step 360: train loss 0.2448, val loss 0.6635\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 36784.61ms\n",
            "iter 361: loss 0.0001, time 2892.40ms\n",
            "iter 362: loss 0.0000, time 2998.91ms\n",
            "iter 363: loss 0.0000, time 2961.24ms\n",
            "iter 364: loss 0.0001, time 3376.70ms\n",
            "iter 365: loss 0.0003, time 3060.76ms\n",
            "iter 366: loss 0.0000, time 3055.68ms\n",
            "iter 367: loss 0.0000, time 2933.39ms\n",
            "iter 368: loss 0.0000, time 3154.87ms\n",
            "iter 369: loss 0.0031, time 3151.17ms\n",
            "iter 370: loss 0.0000, time 3065.10ms\n",
            "iter 371: loss 0.0002, time 3058.30ms\n",
            "iter 372: loss 0.0016, time 2975.59ms\n",
            "iter 373: loss 0.0000, time 3144.60ms\n",
            "iter 374: loss 0.0000, time 2885.56ms\n",
            "iter 375: loss 0.0001, time 3011.55ms\n",
            "iter 376: loss 0.0001, time 2959.03ms\n",
            "iter 377: loss 0.0000, time 3111.53ms\n",
            "iter 378: loss 0.0000, time 3247.30ms\n",
            "iter 379: loss 0.0000, time 3055.21ms\n",
            "step 380: train loss 0.2328, val loss 0.6715\n",
            "step val accuracy 0.9586\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 37220.20ms\n",
            "iter 381: loss 0.0000, time 2805.64ms\n",
            "iter 382: loss 0.0003, time 2935.36ms\n",
            "iter 383: loss 0.0000, time 3024.89ms\n",
            "iter 384: loss 0.0000, time 3299.96ms\n",
            "iter 385: loss 0.0000, time 3090.11ms\n",
            "iter 386: loss 0.0000, time 2946.78ms\n",
            "iter 387: loss 0.0000, time 2979.44ms\n",
            "iter 388: loss 0.0000, time 2932.93ms\n",
            "iter 389: loss 0.0000, time 3395.04ms\n",
            "iter 390: loss 0.0008, time 3079.33ms\n",
            "iter 391: loss 0.0000, time 3143.58ms\n",
            "iter 392: loss 0.0000, time 3052.50ms\n",
            "iter 393: loss 0.0000, time 3281.81ms\n",
            "iter 394: loss 0.0000, time 2941.22ms\n",
            "iter 395: loss 0.0000, time 2905.40ms\n",
            "iter 396: loss 0.0000, time 2920.96ms\n",
            "iter 397: loss 0.0000, time 3202.78ms\n",
            "iter 398: loss 0.0000, time 3116.73ms\n",
            "iter 399: loss 0.0007, time 2999.94ms\n",
            "step 400: train loss 0.2220, val loss 0.6740\n",
            "step val accuracy 0.9527\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.4024\n",
            "Test recall 0.5333\n",
            "Test F1 0.4524\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 2  7  1]\n",
            " [ 0 10  0]]\n",
            "iter 400: loss 0.0001, time 30790.42ms\n",
            "iter 401: loss 0.0000, time 2988.81ms\n",
            "iter 402: loss 0.0008, time 3260.28ms\n",
            "iter 403: loss 0.0001, time 2936.27ms\n",
            "iter 404: loss 0.0000, time 2992.87ms\n",
            "iter 405: loss 0.0002, time 2983.20ms\n",
            "iter 406: loss 0.0000, time 3428.92ms\n",
            "iter 407: loss 0.0001, time 3052.22ms\n",
            "iter 408: loss 0.0000, time 3090.16ms\n",
            "iter 409: loss 0.0001, time 2888.58ms\n",
            "iter 410: loss 0.0000, time 3291.92ms\n",
            "iter 411: loss 0.0001, time 3104.91ms\n",
            "iter 412: loss 0.0000, time 2899.59ms\n",
            "iter 413: loss 0.0000, time 3038.09ms\n",
            "iter 414: loss 0.0000, time 2982.89ms\n",
            "iter 415: loss 0.0000, time 3156.59ms\n",
            "iter 416: loss 0.0006, time 2989.81ms\n",
            "iter 417: loss 0.0001, time 2932.71ms\n",
            "iter 418: loss 0.0001, time 3549.01ms\n",
            "iter 419: loss 0.0000, time 3160.09ms\n",
            "step 420: train loss 0.2121, val loss 0.6810\n",
            "step val accuracy 0.9527\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0002, time 34750.09ms\n",
            "iter 421: loss 0.0000, time 2978.04ms\n",
            "iter 422: loss 0.0001, time 3243.82ms\n",
            "iter 423: loss 0.0011, time 3177.48ms\n",
            "iter 424: loss 0.0003, time 2994.57ms\n",
            "iter 425: loss 0.0001, time 3063.87ms\n",
            "iter 426: loss 0.0295, time 2996.36ms\n",
            "iter 427: loss 0.0000, time 3310.31ms\n",
            "iter 428: loss 0.0000, time 2865.30ms\n",
            "iter 429: loss 0.0000, time 2985.12ms\n",
            "iter 430: loss 0.0000, time 3142.98ms\n",
            "iter 431: loss 0.0003, time 3226.79ms\n",
            "iter 432: loss 0.0010, time 2976.83ms\n",
            "iter 433: loss 0.0000, time 3139.46ms\n",
            "iter 434: loss 0.0017, time 3055.64ms\n",
            "iter 435: loss 0.0000, time 3091.15ms\n",
            "iter 436: loss 0.0000, time 3188.56ms\n",
            "iter 437: loss 0.0000, time 2992.41ms\n",
            "iter 438: loss 0.0369, time 2939.86ms\n",
            "iter 439: loss 0.0007, time 2791.52ms\n",
            "step 440: train loss 0.2050, val loss 0.6887\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0007, time 31420.23ms\n",
            "iter 441: loss 0.0000, time 2921.16ms\n",
            "iter 442: loss 0.0000, time 2841.58ms\n",
            "iter 443: loss 0.0000, time 2978.65ms\n",
            "iter 444: loss 0.0000, time 3212.09ms\n",
            "iter 445: loss 0.0001, time 3080.18ms\n",
            "iter 446: loss 0.0000, time 3012.98ms\n",
            "iter 447: loss 0.0000, time 2950.80ms\n",
            "iter 448: loss 0.0000, time 2956.51ms\n",
            "iter 449: loss 0.0000, time 3322.66ms\n",
            "iter 450: loss 0.0056, time 2975.37ms\n",
            "iter 451: loss 0.0001, time 3207.71ms\n",
            "iter 452: loss 0.0001, time 3057.55ms\n",
            "iter 453: loss 0.0000, time 3147.39ms\n",
            "iter 454: loss 0.0000, time 3084.46ms\n",
            "iter 455: loss 0.0000, time 2966.21ms\n",
            "iter 456: loss 0.0002, time 2941.05ms\n",
            "iter 457: loss 0.0000, time 3025.55ms\n",
            "iter 458: loss 0.0001, time 3253.57ms\n",
            "iter 459: loss 0.0000, time 3002.16ms\n",
            "step 460: train loss 0.1968, val loss 0.6948\n",
            "step val accuracy 0.9571\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0002, time 38740.06ms\n",
            "iter 461: loss 0.0000, time 2921.77ms\n",
            "iter 462: loss 0.0000, time 2983.77ms\n",
            "iter 463: loss 0.0001, time 3043.68ms\n",
            "iter 464: loss 0.0000, time 3127.35ms\n",
            "iter 465: loss 0.0059, time 3055.53ms\n",
            "iter 466: loss 0.0000, time 2964.62ms\n",
            "iter 467: loss 0.0000, time 2949.39ms\n",
            "iter 468: loss 0.0000, time 2998.37ms\n",
            "iter 469: loss 0.0001, time 3157.73ms\n",
            "iter 470: loss 0.0000, time 3008.82ms\n",
            "iter 471: loss 0.0002, time 2994.26ms\n",
            "iter 472: loss 0.0000, time 3046.63ms\n",
            "iter 473: loss 0.0000, time 3163.14ms\n",
            "iter 474: loss 0.0000, time 3208.33ms\n",
            "iter 475: loss 0.0000, time 3072.56ms\n",
            "iter 476: loss 0.0000, time 3049.24ms\n",
            "iter 477: loss 0.0000, time 3001.83ms\n",
            "iter 478: loss 0.0000, time 3234.72ms\n",
            "iter 479: loss 0.0000, time 3002.07ms\n",
            "step 480: train loss 0.1896, val loss 0.6935\n",
            "step val accuracy 0.9660\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0001, time 30256.53ms\n",
            "iter 481: loss 0.0000, time 2968.97ms\n",
            "iter 482: loss 0.0000, time 3237.35ms\n",
            "iter 483: loss 0.0000, time 3069.42ms\n",
            "iter 484: loss 0.0000, time 3072.10ms\n",
            "iter 485: loss 0.0000, time 2957.04ms\n",
            "iter 486: loss 0.0117, time 3090.77ms\n",
            "iter 487: loss 0.0000, time 3098.51ms\n",
            "iter 488: loss 0.0000, time 2931.10ms\n",
            "iter 489: loss 0.0000, time 3007.28ms\n",
            "iter 490: loss 0.0000, time 2847.99ms\n",
            "iter 491: loss 0.0000, time 3003.22ms\n",
            "iter 492: loss 0.0000, time 2965.08ms\n",
            "iter 493: loss 0.0000, time 2920.70ms\n",
            "iter 494: loss 0.0000, time 2910.22ms\n",
            "iter 495: loss 0.0000, time 3078.53ms\n",
            "iter 496: loss 0.0000, time 3171.30ms\n",
            "iter 497: loss 0.0000, time 2988.62ms\n",
            "iter 498: loss 0.0000, time 3147.98ms\n",
            "iter 499: loss 0.0000, time 2920.23ms\n",
            "step 500: train loss 0.1820, val loss 0.6914\n",
            "step val accuracy 0.9689\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.4296\n",
            "Test recall 0.5333\n",
            "Test F1 0.4667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 1  7  2]\n",
            " [ 0 10  0]]\n",
            "iter 500: loss 0.0000, time 39295.67ms\n"
          ]
        }
      ],
      "source": [
        "# First run: From Scratch\n",
        "train_model(\"scratch\")\n",
        "# Second run: From GPT-2 pretrained\n",
        "train_model(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning for Pretrianed GPT-2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rebuild datasets with hyper config for augment and preprocessing.\n",
        "# (We assume that SentimentDataset accepts 'augment' and 'if_preprocess' as parameters.)\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "def setup_model_and_data_hyper(init_from, wandb_run_name, if_SMOTE_override=None, augment_override=None, if_preprocess_override=None):\n",
        "    \"\"\"\n",
        "    This is a wrapper around your existing setup_model_and_data\n",
        "    that overrides some data parameters based on wandb hyperconfig.\n",
        "    \"\"\"\n",
        "    # Override global data parameters if provided\n",
        "    global if_SMOTE\n",
        "    if if_SMOTE_override is not None:\n",
        "        if_SMOTE = if_SMOTE_override  # use the hyper parameter value\n",
        "    \n",
        "    # Call the original setup (which is assumed to use globals for SMOTE, augment, and preprocessing)\n",
        "    model, model_args, optimizer, scaler, ctx, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset, device_used = setup_model_and_data(init_from, wandb_run_name)\n",
        "\n",
        "    train_df = pd.read_csv('./data/customer_service/train.csv')\n",
        "    test_df = pd.read_csv('./data/customer_service/test.csv')\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "    \n",
        "    if if_SMOTE:\n",
        "        from imblearn.over_sampling import RandomOverSampler\n",
        "        sm = RandomOverSampler(random_state=42)\n",
        "        X_train_smote, y_train_smote = sm.fit_resample(train_df[[\"conversation\"]], train_df[\"customer_sentiment\"])\n",
        "        train_df_smoted = pd.concat([X_train_smote, y_train_smote.reset_index(drop=True)], axis=1)\n",
        "    else:\n",
        "        train_df_smoted = train_df\n",
        "\n",
        "    # Use wandb config overrides for augmentation and preprocessing; default to True if not provided.\n",
        "    train_dataset = SentimentDataset(\n",
        "        train_df_smoted, tokenizer, max_length=512, padding=False,\n",
        "        augment=augment_override if augment_override is not None else True,\n",
        "        take_only_customer=False,\n",
        "        if_preprocess=if_preprocess_override if if_preprocess_override is not None else True\n",
        "    )\n",
        "    val_dataset = SentimentDataset(\n",
        "        val_df, tokenizer, max_length=512, padding=False,\n",
        "        take_only_customer=False,\n",
        "        if_preprocess=if_preprocess_override if if_preprocess_override is not None else True\n",
        "    )\n",
        "    test_dataset = SentimentDataset(\n",
        "        test_df, tokenizer, max_length=512, padding=False,\n",
        "        take_only_customer=False,\n",
        "        if_preprocess=if_preprocess_override if if_preprocess_override is not None else True\n",
        "    )\n",
        "    from torch.utils.data import DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "    \n",
        "    return model, model_args, optimizer, scaler, ctx, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset, device_used\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Modularized Hyperparameter Tuning Function ===\n",
        "def hyper_train_model():\n",
        "    \"\"\"\n",
        "    This function is designed for hyperparameter tuning runs using GPT-2 weights.\n",
        "    It is invoked via wandb sweep and reuses your earlier setup and evaluation functions.\n",
        "    \"\"\"\n",
        "    import wandb, time, math, os\n",
        "    from datetime import datetime\n",
        "    with wandb.init() as run:\n",
        "        wandb_config = wandb.config\n",
        "        # Use wandb sweep parameters for data augmentation, SMOTE, and preprocessing\n",
        "        if_smote = wandb_config.get(\"if_smote\", True)\n",
        "        augment_flag = wandb_config.get(\"augment\", True)\n",
        "        if_preprocess_flag = wandb_config.get(\"if_preprocess\", True)\n",
        "        \n",
        "        # Set up parameters for this hyper run (only using GPT2 weights)\n",
        "        init_from = \"gpt2\"  \n",
        "        now = datetime.now()\n",
        "        formatted_time = now.strftime(\"%H%M\")\n",
        "        print(formatted_time)\n",
        "        wandb_run_name = wandb_config.get(\"run_name\", f\"gpt2_hyper_{formatted_time}\")\n",
        "        \n",
        "        # Call modified setup to get the model, optimizer, data loaders, etc.\n",
        "        model, model_args, optimizer, scaler, ctx, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset, device_used = setup_model_and_data_hyper(\n",
        "            init_from, wandb_run_name, \n",
        "            if_SMOTE_override=if_smote, \n",
        "            augment_override=augment_flag, \n",
        "            if_preprocess_override=if_preprocess_flag\n",
        "        )\n",
        "        \n",
        "        # Define a learning rate scheduler\n",
        "        def get_lr(it):\n",
        "            warmup_iters = wandb_config.get(\"warmup_iters\", 2000)\n",
        "            lr_decay_iters = wandb_config.get(\"lr_decay_iters\", 600000)\n",
        "            base_lr = wandb_config[\"learning_rate\"]\n",
        "            min_lr = wandb_config.get(\"min_lr\", 6e-5)\n",
        "            if it < warmup_iters:\n",
        "                return base_lr * it / warmup_iters\n",
        "            if it > lr_decay_iters:\n",
        "                return min_lr\n",
        "            decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "            coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "            return min_lr + coeff * (base_lr - min_lr)\n",
        "        \n",
        "        # Initialize iterators and metrics\n",
        "        from torchmetrics import MeanMetric\n",
        "        train_loader_iter = iter(train_loader)\n",
        "        val_loader_iter = iter(val_loader)\n",
        "        t0 = time.time()\n",
        "        iter_num = 0\n",
        "        best_val_loss = float(\"inf\")\n",
        "        train_loss_metric = MeanMetric()\n",
        "        val_loss_metric = MeanMetric()\n",
        "        raw_model = model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model\n",
        "\n",
        "        # Grab an initial training batch\n",
        "        batch = next(train_loader_iter)\n",
        "        X = batch['input_ids'].to(device_used)\n",
        "        Y = batch['labels'].to(device_used)\n",
        "\n",
        "        max_iters = wandb_config.get(\"max_iters\", 500)\n",
        "        eval_interval = wandb_config.get(\"eval_interval\", 20)\n",
        "        eval_test_interval = wandb_config.get(\"eval_test_interval\", 100)\n",
        "        gradient_accumulation_steps = wandb_config.get(\"gradient_accumulation_steps\", 40)\n",
        "        grad_clip = wandb_config.get(\"grad_clip\", 1.0)\n",
        "        log_interval = wandb_config.get(\"log_interval\", 1)\n",
        "\n",
        "        # Main training loop\n",
        "        while iter_num <= max_iters:\n",
        "            lr = get_lr(iter_num) if wandb_config.get(\"decay_lr\", False) else wandb_config[\"learning_rate\"]\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "            # Evaluation on train and validation sets at intervals\n",
        "            if iter_num % eval_interval == 0:\n",
        "                # Use your previously defined estimate_loss function\n",
        "                out_res = estimate_loss(model, ctx, train_loader_iter, val_loader_iter, train_loader, val_loader, len(train_dataset), len(val_dataset), device_used)\n",
        "                print(f\"Step {iter_num}: train loss {train_loss_metric.compute():.4f}, val loss {val_loss_metric.compute():.4f}\")\n",
        "                print(f\"Validation accuracy: {out_res['val']['accuracy']:.4f}\")\n",
        "                wandb.log({\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train/loss\": train_loss_metric.compute(),\n",
        "                    \"val/loss\": val_loss_metric.compute(),\n",
        "                    \"lr\": lr,\n",
        "                    \"val/acc\": out_res['val']['accuracy']\n",
        "                })\n",
        "                # Save checkpoint if validation loss improves\n",
        "                if out_res['val']['loss'] < best_val_loss:\n",
        "                    best_val_loss = out_res['val']['loss']\n",
        "                    if iter_num > 0:\n",
        "                        checkpoint = {\n",
        "                            'model': raw_model.state_dict(),\n",
        "                            'optimizer': optimizer.state_dict(),\n",
        "                            'model_args': model_args,\n",
        "                            'iter_num': iter_num,\n",
        "                            'best_val_loss': best_val_loss,\n",
        "                            'config': wandb_config,\n",
        "                        }\n",
        "                        os.makedirs(\"out\", exist_ok=True)\n",
        "                        print(f\"Saving checkpoint to out/ckpt.pt\")\n",
        "                        torch.save(checkpoint, os.path.join(\"out\", \"ckpt.pt\"))\n",
        "            \n",
        "            # Evaluation on the test set at intervals\n",
        "            if iter_num % eval_test_interval == 0:\n",
        "                out_res, all_labels, all_preds = estimate_loss_test(model, ctx, test_loader, len(test_dataset), device_used)\n",
        "                print(f\"Test accuracy {out_res['test']['accuracy']:.4f}\")\n",
        "                wandb.log({\n",
        "                    \"Test_Accuracy\": out_res['test']['accuracy'],\n",
        "                    \"Test_Precision\": out_res['test']['precision'],\n",
        "                    \"Test_Recall\": out_res['test']['recall'],\n",
        "                    \"Test_F1_Score\": out_res['test']['f1'],\n",
        "                    \"Confusion Matrix\": wandb.plot.confusion_matrix(\n",
        "                        probs=None,\n",
        "                        y_true=all_labels,\n",
        "                        preds=all_preds,\n",
        "                        class_names=[\"negative\", \"neutral\", \"positive\"]\n",
        "                    )\n",
        "                })\n",
        "            \n",
        "            # Forward/backward pass with gradient accumulation\n",
        "            for micro_step in range(gradient_accumulation_steps):\n",
        "                if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "                    model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                    train_loss_metric.update(loss.item())\n",
        "                    loss = loss / gradient_accumulation_steps\n",
        "                try:\n",
        "                    batch = next(train_loader_iter)\n",
        "                except StopIteration:\n",
        "                    train_loader_iter = iter(train_loader)\n",
        "                    batch = next(train_loader_iter)\n",
        "                X = batch['input_ids'].to(device_used)\n",
        "                Y = batch['labels'].to(device_used)\n",
        "                scaler.scale(loss).backward()\n",
        "            \n",
        "            # Gradient clipping if enabled\n",
        "            if grad_clip != 0.0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            \n",
        "            # Do a quick validation forward pass for metric update\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                try:\n",
        "                    batch_val = next(val_loader_iter)\n",
        "                except StopIteration:\n",
        "                    val_loader_iter = iter(val_loader)\n",
        "                    batch_val = next(val_loader_iter)\n",
        "                X_val = batch_val['input_ids'].to(device_used)\n",
        "                Y_val = batch_val['labels'].to(device_used)\n",
        "                logits_val, loss_val = model(X_val, Y_val)\n",
        "                val_loss_metric.update(loss_val.item())\n",
        "            model.train()\n",
        "            \n",
        "            # Logging timing and loss info\n",
        "            t1 = time.time()\n",
        "            dt = t1 - t0\n",
        "            t0 = t1\n",
        "            if iter_num % log_interval == 0:\n",
        "                lossf = loss.item() * gradient_accumulation_steps\n",
        "                print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "            iter_num += 1\n",
        "        wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Sweep configuration and agent setup ===\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'Test_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'if_smote': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'if_preprocess': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [5e-5, 1e-4]\n",
        "        },\n",
        "        'augment': {\n",
        "            'values': [True, False]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: zm57at65\n",
            "Sweep URL: https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1qn18iv0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maliyigitbasaran\u001b[0m (\u001b[33maliyigitbasaran-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_180318-1qn18iv0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0' target=\"_blank\">effortless-sweep-1</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1803\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">effortless-sweep-1</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_180318-1qn18iv0\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_180328-1qn18iv0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0' target=\"_blank\">gpt2_hyper_1803</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n",
            "iter 0: loss 0.6172, time 37472.38ms\n",
            "iter 1: loss 4.9375, time 1459.28ms\n",
            "iter 2: loss 0.9492, time 1405.75ms\n",
            "iter 3: loss 1.5781, time 1413.67ms\n",
            "iter 4: loss 1.0078, time 1526.88ms\n",
            "iter 5: loss 0.8125, time 1460.68ms\n",
            "iter 6: loss 1.0859, time 1449.37ms\n",
            "iter 7: loss 1.1016, time 1511.75ms\n",
            "iter 8: loss 0.7070, time 1544.58ms\n",
            "iter 9: loss 1.0000, time 1532.75ms\n",
            "iter 10: loss 0.6367, time 1456.19ms\n",
            "iter 11: loss 1.1953, time 1476.17ms\n",
            "iter 12: loss 0.5977, time 1479.79ms\n",
            "iter 13: loss 0.5937, time 1426.82ms\n",
            "iter 14: loss 1.5000, time 1400.52ms\n",
            "iter 15: loss 0.5273, time 1453.56ms\n",
            "iter 16: loss 0.7695, time 1388.45ms\n",
            "iter 17: loss 0.6055, time 1446.83ms\n",
            "iter 18: loss 1.1797, time 1534.03ms\n",
            "iter 19: loss 1.1563, time 1444.51ms\n",
            "Step 20: train loss 1.1641, val loss 0.7810\n",
            "Validation accuracy: 0.8166\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.1533, time 14863.09ms\n",
            "iter 21: loss 0.3848, time 1493.14ms\n",
            "iter 22: loss 0.5117, time 1537.51ms\n",
            "iter 23: loss 0.6914, time 1621.29ms\n",
            "iter 24: loss 0.7266, time 1605.85ms\n",
            "iter 25: loss 0.0425, time 1769.08ms\n",
            "iter 26: loss 0.1582, time 1603.73ms\n",
            "iter 27: loss 0.8125, time 1693.66ms\n",
            "iter 28: loss 0.0129, time 1795.02ms\n",
            "iter 29: loss 0.4551, time 1709.19ms\n",
            "iter 30: loss 0.2080, time 1572.59ms\n",
            "iter 31: loss 0.0952, time 1573.17ms\n",
            "iter 32: loss 0.0075, time 1545.32ms\n",
            "iter 33: loss 0.0175, time 1500.83ms\n",
            "iter 34: loss 0.4375, time 1573.74ms\n",
            "iter 35: loss 0.0203, time 1606.71ms\n",
            "iter 36: loss 0.0023, time 1562.63ms\n",
            "iter 37: loss 0.1216, time 1626.18ms\n",
            "iter 38: loss 0.0289, time 1529.01ms\n",
            "iter 39: loss 1.1484, time 1510.73ms\n",
            "Step 40: train loss 0.7487, val loss 0.6381\n",
            "Validation accuracy: 0.8447\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.0515, time 15510.48ms\n",
            "iter 41: loss 0.0190, time 1753.38ms\n",
            "iter 42: loss 0.1807, time 1689.82ms\n",
            "iter 43: loss 0.0007, time 1477.29ms\n",
            "iter 44: loss 0.0135, time 1478.58ms\n",
            "iter 45: loss 0.1719, time 1449.35ms\n",
            "iter 46: loss 0.0398, time 1453.26ms\n",
            "iter 47: loss 0.0133, time 1437.80ms\n",
            "iter 48: loss 0.0894, time 1403.50ms\n",
            "iter 49: loss 0.0206, time 1504.26ms\n",
            "iter 50: loss 0.0009, time 1411.85ms\n",
            "iter 51: loss 0.0398, time 1514.99ms\n",
            "iter 52: loss 0.0141, time 1496.47ms\n",
            "iter 53: loss 0.0014, time 1447.12ms\n",
            "iter 54: loss 0.0349, time 1475.27ms\n",
            "iter 55: loss 0.0018, time 1505.03ms\n",
            "iter 56: loss 0.0038, time 1451.05ms\n",
            "iter 57: loss 0.0003, time 1477.78ms\n",
            "iter 58: loss 0.2441, time 1454.18ms\n",
            "iter 59: loss 0.7031, time 1477.17ms\n",
            "Step 60: train loss 0.5767, val loss 0.6076\n",
            "Validation accuracy: 0.9216\n",
            "iter 60: loss 2.0156, time 13586.15ms\n",
            "iter 61: loss 0.0173, time 1473.37ms\n",
            "iter 62: loss 0.0090, time 1474.68ms\n",
            "iter 63: loss 0.0005, time 1539.34ms\n",
            "iter 64: loss 0.0654, time 1462.63ms\n",
            "iter 65: loss 2.5156, time 1480.63ms\n",
            "iter 66: loss 2.7969, time 1496.86ms\n",
            "iter 67: loss 0.0003, time 1510.95ms\n",
            "iter 68: loss 0.0027, time 1551.89ms\n",
            "iter 69: loss 0.0063, time 1527.28ms\n",
            "iter 70: loss 0.0267, time 1476.91ms\n",
            "iter 71: loss 0.0173, time 1497.94ms\n",
            "iter 72: loss 0.0118, time 1527.80ms\n",
            "iter 73: loss 0.0000, time 1488.54ms\n",
            "iter 74: loss 0.0179, time 1434.20ms\n",
            "iter 75: loss 0.0074, time 1492.52ms\n",
            "iter 76: loss 0.2070, time 1460.55ms\n",
            "iter 77: loss 0.0063, time 1439.19ms\n",
            "iter 78: loss 0.0097, time 1501.04ms\n",
            "iter 79: loss 0.0009, time 1421.68ms\n",
            "Step 80: train loss 0.4736, val loss 0.5374\n",
            "Validation accuracy: 0.9186\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0026, time 15461.71ms\n",
            "iter 81: loss 0.0077, time 1494.57ms\n",
            "iter 82: loss 1.1016, time 1493.41ms\n",
            "iter 83: loss 0.0160, time 1526.27ms\n",
            "iter 84: loss 0.0022, time 1550.61ms\n",
            "iter 85: loss 0.0002, time 1605.74ms\n",
            "iter 86: loss 0.3828, time 1480.57ms\n",
            "iter 87: loss 0.0023, time 1492.78ms\n",
            "iter 88: loss 0.0011, time 1508.50ms\n",
            "iter 89: loss 0.0003, time 1612.68ms\n",
            "iter 90: loss 0.0014, time 1546.36ms\n",
            "iter 91: loss 0.0021, time 1643.07ms\n",
            "iter 92: loss 0.0001, time 1533.24ms\n",
            "iter 93: loss 0.0032, time 1550.79ms\n",
            "iter 94: loss 0.0008, time 1503.15ms\n",
            "iter 95: loss 0.0006, time 1596.72ms\n",
            "iter 96: loss 0.0011, time 1550.12ms\n",
            "iter 97: loss 0.0032, time 1638.95ms\n",
            "iter 98: loss 0.0003, time 2226.66ms\n",
            "iter 99: loss 0.0040, time 2127.38ms\n",
            "Step 100: train loss 0.3933, val loss 0.5674\n",
            "Validation accuracy: 0.9379\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.6333\n",
            "iter 100: loss 0.0240, time 23296.53ms\n",
            "iter 101: loss 0.0001, time 2160.01ms\n",
            "iter 102: loss 0.0010, time 2118.78ms\n",
            "iter 103: loss 0.0001, time 2101.09ms\n",
            "iter 104: loss 0.0059, time 2128.68ms\n",
            "iter 105: loss 0.0005, time 2126.38ms\n",
            "iter 106: loss 0.0006, time 2078.86ms\n",
            "iter 107: loss 0.0791, time 2090.43ms\n",
            "iter 108: loss 0.0000, time 2130.28ms\n",
            "iter 109: loss 0.0053, time 2134.53ms\n",
            "iter 110: loss 2.1719, time 2171.22ms\n",
            "iter 111: loss 0.0006, time 2139.56ms\n",
            "iter 112: loss 0.0025, time 2112.81ms\n",
            "iter 113: loss 0.0013, time 2141.80ms\n",
            "iter 114: loss 0.0020, time 2311.17ms\n",
            "iter 115: loss 0.0015, time 2445.74ms\n",
            "iter 116: loss 4.2500, time 2436.72ms\n",
            "iter 117: loss 0.0024, time 2115.95ms\n",
            "iter 118: loss 0.0022, time 2209.90ms\n",
            "iter 119: loss 0.0055, time 2192.06ms\n",
            "Step 120: train loss 0.3659, val loss 0.5473\n",
            "Validation accuracy: 0.9615\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 120: loss 0.0019, time 19666.87ms\n",
            "iter 121: loss 0.0439, time 1564.64ms\n",
            "iter 122: loss 0.0057, time 1564.41ms\n",
            "iter 123: loss 0.0130, time 2102.25ms\n",
            "iter 124: loss 0.0086, time 2133.31ms\n",
            "iter 125: loss 0.0104, time 2162.55ms\n",
            "iter 126: loss 0.0098, time 2152.17ms\n",
            "iter 127: loss 0.0020, time 2183.10ms\n",
            "iter 128: loss 0.0036, time 2227.86ms\n",
            "iter 129: loss 0.0004, time 2130.61ms\n",
            "iter 130: loss 0.0040, time 2128.80ms\n",
            "iter 131: loss 0.0010, time 2137.56ms\n",
            "iter 132: loss 0.0155, time 2204.99ms\n",
            "iter 133: loss 0.0006, time 2300.17ms\n",
            "iter 134: loss 0.0002, time 2219.85ms\n",
            "iter 135: loss 0.0005, time 2172.25ms\n",
            "iter 136: loss 0.0007, time 2114.97ms\n",
            "iter 137: loss 0.1914, time 2140.23ms\n",
            "iter 138: loss 0.0508, time 2158.64ms\n",
            "iter 139: loss 0.0520, time 2142.02ms\n",
            "Step 140: train loss 0.3226, val loss 0.5312\n",
            "Validation accuracy: 0.9645\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 140: loss 0.0007, time 21928.34ms\n",
            "iter 141: loss 0.0015, time 2426.34ms\n",
            "iter 142: loss 0.3789, time 2242.67ms\n",
            "iter 143: loss 0.0175, time 1492.60ms\n",
            "iter 144: loss 0.0004, time 1442.17ms\n",
            "iter 145: loss 0.0003, time 1433.79ms\n",
            "iter 146: loss 0.0021, time 1442.30ms\n",
            "iter 147: loss 0.0004, time 1404.75ms\n",
            "iter 148: loss 0.0031, time 1471.26ms\n",
            "iter 149: loss 0.0447, time 1435.89ms\n",
            "iter 150: loss 0.0007, time 1555.97ms\n",
            "iter 151: loss 0.0052, time 1401.25ms\n",
            "iter 152: loss 0.0000, time 1415.43ms\n",
            "iter 153: loss 0.0004, time 1481.85ms\n",
            "iter 154: loss 0.0002, time 1501.49ms\n",
            "iter 155: loss 0.0019, time 1503.21ms\n",
            "iter 156: loss 0.0020, time 1484.55ms\n",
            "iter 157: loss 0.0002, time 1459.89ms\n",
            "iter 158: loss 0.0013, time 1415.35ms\n",
            "iter 159: loss 0.0015, time 1420.82ms\n",
            "Step 160: train loss 0.2934, val loss 0.5295\n",
            "Validation accuracy: 0.9763\n",
            "iter 160: loss 0.0074, time 12987.95ms\n",
            "iter 161: loss 0.0010, time 1420.04ms\n",
            "iter 162: loss 0.0001, time 1411.83ms\n",
            "iter 163: loss 0.3926, time 1526.41ms\n",
            "iter 164: loss 0.0000, time 1437.62ms\n",
            "iter 165: loss 0.0000, time 1412.62ms\n",
            "iter 166: loss 0.0002, time 1451.25ms\n",
            "iter 167: loss 0.0033, time 1425.75ms\n",
            "iter 168: loss 0.0000, time 1417.70ms\n",
            "iter 169: loss 0.0003, time 1349.31ms\n",
            "iter 170: loss 0.0002, time 1435.68ms\n",
            "iter 171: loss 1.9609, time 1451.83ms\n",
            "iter 172: loss 0.0000, time 1413.96ms\n",
            "iter 173: loss 0.0002, time 1381.72ms\n",
            "iter 174: loss 0.0002, time 1393.37ms\n",
            "iter 175: loss 0.3887, time 1436.38ms\n",
            "iter 176: loss 0.0003, time 1419.48ms\n",
            "iter 177: loss 0.0005, time 1421.00ms\n",
            "iter 178: loss 0.0002, time 1470.61ms\n",
            "iter 179: loss 0.0001, time 1475.16ms\n",
            "Step 180: train loss 0.2652, val loss 0.5487\n",
            "Validation accuracy: 0.9763\n",
            "iter 180: loss 0.0007, time 12528.35ms\n",
            "iter 181: loss 0.0122, time 1375.64ms\n",
            "iter 182: loss 0.0002, time 1675.04ms\n",
            "iter 183: loss 0.0004, time 1345.27ms\n",
            "iter 184: loss 0.0008, time 1384.00ms\n",
            "iter 185: loss 0.0002, time 1402.02ms\n",
            "iter 186: loss 0.0001, time 1404.76ms\n",
            "iter 187: loss 0.0001, time 1383.93ms\n",
            "iter 188: loss 0.0006, time 1447.43ms\n",
            "iter 189: loss 0.0001, time 1392.98ms\n",
            "iter 190: loss 0.0001, time 1408.15ms\n",
            "iter 191: loss 0.0007, time 1428.51ms\n",
            "iter 192: loss 0.0012, time 1400.42ms\n",
            "iter 193: loss 0.0004, time 1400.80ms\n",
            "iter 194: loss 0.0009, time 1416.83ms\n",
            "iter 195: loss 0.0110, time 1457.96ms\n",
            "iter 196: loss 0.0028, time 1443.19ms\n",
            "iter 197: loss 0.0000, time 1582.64ms\n",
            "iter 198: loss 0.0013, time 1582.59ms\n",
            "iter 199: loss 0.0001, time 1407.71ms\n",
            "Step 200: train loss 0.2400, val loss 0.5560\n",
            "Validation accuracy: 0.9763\n",
            "Test accuracy 0.7000\n",
            "iter 200: loss 0.0001, time 14711.59ms\n",
            "iter 201: loss 0.0001, time 1599.46ms\n",
            "iter 202: loss 0.0000, time 1580.89ms\n",
            "iter 203: loss 0.0001, time 1473.65ms\n",
            "iter 204: loss 0.0000, time 1470.40ms\n",
            "iter 205: loss 0.0001, time 1419.16ms\n",
            "iter 206: loss 0.0001, time 1535.54ms\n",
            "iter 207: loss 0.0001, time 2124.93ms\n",
            "iter 208: loss 0.0000, time 1812.23ms\n",
            "iter 209: loss 0.0001, time 1909.42ms\n",
            "iter 210: loss 0.0056, time 2100.30ms\n",
            "iter 211: loss 0.0021, time 2035.11ms\n",
            "iter 212: loss 0.0000, time 2062.45ms\n",
            "iter 213: loss 0.0002, time 2046.41ms\n",
            "iter 214: loss 0.0003, time 2051.04ms\n",
            "iter 215: loss 0.0004, time 2186.13ms\n",
            "iter 216: loss 0.0073, time 2109.10ms\n",
            "iter 217: loss 0.0002, time 2067.63ms\n",
            "iter 218: loss 0.0016, time 2078.03ms\n",
            "iter 219: loss 0.0038, time 2071.13ms\n",
            "Step 220: train loss 0.2202, val loss 0.5637\n",
            "Validation accuracy: 0.9808\n",
            "iter 220: loss 0.0006, time 19126.12ms\n",
            "iter 221: loss 0.0000, time 2098.65ms\n",
            "iter 222: loss 0.0000, time 2067.81ms\n",
            "iter 223: loss 0.0001, time 2141.40ms\n",
            "iter 224: loss 0.0000, time 2148.96ms\n",
            "iter 225: loss 0.0002, time 2273.34ms\n",
            "iter 226: loss 0.0000, time 2201.70ms\n",
            "iter 227: loss 0.0000, time 2469.02ms\n",
            "iter 228: loss 0.0001, time 2192.34ms\n",
            "iter 229: loss 0.0000, time 2260.56ms\n",
            "iter 230: loss 0.0000, time 2152.24ms\n",
            "iter 231: loss 0.0001, time 2117.03ms\n",
            "iter 232: loss 0.0000, time 2147.18ms\n",
            "iter 233: loss 0.0000, time 2153.85ms\n",
            "iter 234: loss 0.0000, time 2163.41ms\n",
            "iter 235: loss 0.0000, time 2232.34ms\n",
            "iter 236: loss 0.0957, time 2177.14ms\n",
            "iter 237: loss 0.0001, time 2170.26ms\n",
            "iter 238: loss 0.0008, time 2185.59ms\n",
            "iter 239: loss 0.0004, time 2146.38ms\n",
            "Step 240: train loss 0.2024, val loss 0.5706\n",
            "Validation accuracy: 0.9808\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 240: loss 0.0000, time 22015.85ms\n",
            "iter 241: loss 0.0006, time 2140.58ms\n",
            "iter 242: loss 0.0000, time 2291.43ms\n",
            "iter 243: loss 0.0000, time 2139.32ms\n",
            "iter 244: loss 0.0001, time 2142.04ms\n",
            "iter 245: loss 0.0001, time 2154.47ms\n",
            "iter 246: loss 0.0003, time 2167.98ms\n",
            "iter 247: loss 0.0000, time 2129.32ms\n",
            "iter 248: loss 0.0175, time 2143.23ms\n",
            "iter 249: loss 0.0001, time 2168.75ms\n",
            "iter 250: loss 0.0000, time 2153.75ms\n",
            "iter 251: loss 0.0010, time 2138.47ms\n",
            "iter 252: loss 0.0000, time 2159.87ms\n",
            "iter 253: loss 0.0002, time 2242.06ms\n",
            "iter 254: loss 0.0000, time 2164.16ms\n",
            "iter 255: loss 0.0000, time 2184.04ms\n",
            "iter 256: loss 0.0000, time 2229.65ms\n",
            "iter 257: loss 0.0000, time 2175.83ms\n",
            "iter 258: loss 0.0344, time 2167.39ms\n",
            "iter 259: loss 0.2451, time 2168.57ms\n",
            "Step 260: train loss 0.1870, val loss 0.5560\n",
            "Validation accuracy: 0.9660\n",
            "iter 260: loss 0.0000, time 19973.18ms\n",
            "iter 261: loss 0.0000, time 2240.87ms\n",
            "iter 262: loss 0.0003, time 2144.72ms\n",
            "iter 263: loss 0.0000, time 2159.78ms\n",
            "iter 264: loss 0.0000, time 2170.65ms\n",
            "iter 265: loss 0.0000, time 2221.40ms\n",
            "iter 266: loss 0.0001, time 2163.78ms\n",
            "iter 267: loss 0.0000, time 2223.12ms\n",
            "iter 268: loss 0.0000, time 2227.77ms\n",
            "iter 269: loss 0.0001, time 2171.97ms\n",
            "iter 270: loss 0.0000, time 2192.31ms\n",
            "iter 271: loss 0.0000, time 2535.07ms\n",
            "iter 272: loss 0.0000, time 2327.09ms\n",
            "iter 273: loss 0.0000, time 1754.87ms\n",
            "iter 274: loss 0.0033, time 1516.47ms\n",
            "iter 275: loss 0.0000, time 1796.00ms\n",
            "iter 276: loss 0.0000, time 1569.40ms\n",
            "iter 277: loss 0.0000, time 1591.38ms\n",
            "iter 278: loss 0.0000, time 1591.71ms\n",
            "iter 279: loss 0.0000, time 1549.78ms\n",
            "Step 280: train loss 0.1783, val loss 0.5696\n",
            "Validation accuracy: 0.9749\n",
            "iter 280: loss 0.0000, time 14460.29ms\n",
            "iter 281: loss 0.0000, time 1628.39ms\n",
            "iter 282: loss 0.0001, time 1510.71ms\n",
            "iter 283: loss 0.0001, time 1510.58ms\n",
            "iter 284: loss 0.0003, time 1501.55ms\n",
            "iter 285: loss 0.0000, time 1494.02ms\n",
            "iter 286: loss 0.0001, time 1501.47ms\n",
            "iter 287: loss 0.0010, time 1623.42ms\n",
            "iter 288: loss 0.0000, time 1692.67ms\n",
            "iter 289: loss 0.0002, time 1614.55ms\n",
            "iter 290: loss 0.0000, time 1648.40ms\n",
            "iter 291: loss 0.0003, time 1570.00ms\n",
            "iter 292: loss 0.0000, time 1595.44ms\n",
            "iter 293: loss 0.0000, time 1722.07ms\n",
            "iter 294: loss 0.0001, time 1698.37ms\n",
            "iter 295: loss 0.0000, time 2418.62ms\n",
            "iter 296: loss 0.0000, time 2282.35ms\n",
            "iter 297: loss 0.0000, time 2276.70ms\n",
            "iter 298: loss 0.0002, time 2502.71ms\n",
            "iter 299: loss 0.0001, time 2370.64ms\n",
            "Step 300: train loss 0.1676, val loss 0.5829\n",
            "Validation accuracy: 0.9142\n",
            "Test accuracy 0.7000\n",
            "iter 300: loss 0.0000, time 23013.62ms\n",
            "iter 301: loss 0.0001, time 2303.01ms\n",
            "iter 302: loss 0.0000, time 2351.11ms\n",
            "iter 303: loss 0.0000, time 2218.23ms\n",
            "iter 304: loss 0.0001, time 1761.40ms\n",
            "iter 305: loss 0.0000, time 1560.18ms\n",
            "iter 306: loss 0.0001, time 1496.99ms\n",
            "iter 307: loss 0.0000, time 1449.19ms\n",
            "iter 308: loss 0.0002, time 1463.26ms\n",
            "iter 309: loss 0.0001, time 1465.73ms\n",
            "iter 310: loss 0.0008, time 1485.00ms\n",
            "iter 311: loss 0.0000, time 1504.26ms\n",
            "iter 312: loss 0.0007, time 1670.73ms\n",
            "iter 313: loss 0.0000, time 1590.23ms\n",
            "iter 314: loss 0.0001, time 1595.75ms\n",
            "iter 315: loss 0.0001, time 1499.13ms\n",
            "iter 316: loss 0.0001, time 1528.78ms\n",
            "iter 317: loss 0.0000, time 1675.00ms\n",
            "iter 318: loss 0.0000, time 1599.67ms\n",
            "iter 319: loss 0.0007, time 1525.22ms\n",
            "Step 320: train loss 0.1590, val loss 0.5473\n",
            "Validation accuracy: 0.9689\n",
            "iter 320: loss 0.0001, time 15649.38ms\n",
            "iter 321: loss 0.0001, time 2194.32ms\n",
            "iter 322: loss 0.0000, time 2127.23ms\n",
            "iter 323: loss 0.0000, time 2107.84ms\n",
            "iter 324: loss 0.0000, time 2114.65ms\n",
            "iter 325: loss 0.0000, time 2092.90ms\n",
            "iter 326: loss 0.0000, time 2170.49ms\n",
            "iter 327: loss 0.0000, time 2241.48ms\n",
            "iter 328: loss 0.0000, time 2197.76ms\n",
            "iter 329: loss 0.0002, time 2154.16ms\n",
            "iter 330: loss 0.0000, time 2137.21ms\n",
            "iter 331: loss 0.0000, time 2114.70ms\n",
            "iter 332: loss 0.0000, time 2064.76ms\n",
            "iter 333: loss 0.0004, time 2139.73ms\n",
            "iter 334: loss 0.0000, time 2115.66ms\n",
            "iter 335: loss 0.0000, time 2112.60ms\n",
            "iter 336: loss 0.0000, time 2128.01ms\n",
            "iter 337: loss 0.0018, time 2089.13ms\n",
            "iter 338: loss 0.0000, time 2114.03ms\n",
            "iter 339: loss 0.0000, time 2106.31ms\n",
            "Step 340: train loss 0.1498, val loss 0.5362\n",
            "Validation accuracy: 0.9778\n",
            "iter 340: loss 0.0000, time 20135.31ms\n",
            "iter 341: loss 0.0000, time 2133.17ms\n",
            "iter 342: loss 0.0000, time 2118.54ms\n",
            "iter 343: loss 0.0000, time 2063.19ms\n",
            "iter 344: loss 0.0000, time 2077.56ms\n",
            "iter 345: loss 0.0000, time 2099.64ms\n",
            "iter 346: loss 0.0000, time 2070.06ms\n",
            "iter 347: loss 0.0000, time 2146.87ms\n",
            "iter 348: loss 0.0000, time 2128.88ms\n",
            "iter 349: loss 0.0000, time 2105.32ms\n",
            "iter 350: loss 0.0000, time 2102.92ms\n",
            "iter 351: loss 0.0000, time 2081.38ms\n",
            "iter 352: loss 0.0001, time 2088.84ms\n",
            "iter 353: loss 0.0048, time 2087.16ms\n",
            "iter 354: loss 0.0000, time 2115.47ms\n",
            "iter 355: loss 0.0000, time 2188.66ms\n",
            "iter 356: loss 0.0006, time 2102.28ms\n",
            "iter 357: loss 0.0001, time 2207.23ms\n",
            "iter 358: loss 0.0000, time 2200.66ms\n",
            "iter 359: loss 0.0000, time 2279.28ms\n",
            "Step 360: train loss 0.1419, val loss 0.5617\n",
            "Validation accuracy: 0.9822\n",
            "iter 360: loss 0.0000, time 19638.99ms\n",
            "iter 361: loss 0.0000, time 2091.80ms\n",
            "iter 362: loss 0.0000, time 2297.59ms\n",
            "iter 363: loss 0.0000, time 2159.72ms\n",
            "iter 364: loss 0.0000, time 2201.52ms\n",
            "iter 365: loss 0.0000, time 2288.55ms\n",
            "iter 366: loss 0.0000, time 2238.30ms\n",
            "iter 367: loss 0.0000, time 2206.85ms\n",
            "iter 368: loss 0.0000, time 2257.26ms\n",
            "iter 369: loss 0.0000, time 2175.81ms\n",
            "iter 370: loss 0.0000, time 2158.18ms\n",
            "iter 371: loss 0.0000, time 2192.62ms\n",
            "iter 372: loss 0.0000, time 2138.49ms\n",
            "iter 373: loss 0.0001, time 2086.25ms\n",
            "iter 374: loss 0.0000, time 2107.02ms\n",
            "iter 375: loss 0.0000, time 2104.34ms\n",
            "iter 376: loss 0.0000, time 2114.14ms\n",
            "iter 377: loss 0.0000, time 2057.67ms\n",
            "iter 378: loss 0.0000, time 2211.66ms\n",
            "iter 379: loss 0.0000, time 2170.33ms\n",
            "Step 380: train loss 0.1346, val loss 0.5738\n",
            "Validation accuracy: 0.9793\n",
            "iter 380: loss 0.0003, time 19796.21ms\n",
            "iter 381: loss 0.0000, time 2221.64ms\n",
            "iter 382: loss 0.0001, time 2208.49ms\n",
            "iter 383: loss 0.0000, time 2206.20ms\n",
            "iter 384: loss 0.0017, time 2176.71ms\n",
            "iter 385: loss 0.0000, time 2167.24ms\n",
            "iter 386: loss 0.0000, time 2161.93ms\n",
            "iter 387: loss 0.0000, time 2319.03ms\n",
            "iter 388: loss 0.0000, time 2274.69ms\n",
            "iter 389: loss 0.0000, time 2195.18ms\n",
            "iter 390: loss 0.0000, time 2229.52ms\n",
            "iter 391: loss 0.0000, time 2187.57ms\n",
            "iter 392: loss 0.0004, time 2100.89ms\n",
            "iter 393: loss 0.0001, time 2195.48ms\n",
            "iter 394: loss 0.0000, time 2121.76ms\n",
            "iter 395: loss 0.0000, time 2161.20ms\n",
            "iter 396: loss 0.0000, time 2175.03ms\n",
            "iter 397: loss 0.0000, time 2245.99ms\n",
            "iter 398: loss 0.0001, time 2089.57ms\n",
            "iter 399: loss 0.0028, time 2125.71ms\n",
            "Step 400: train loss 0.1280, val loss 0.5780\n",
            "Validation accuracy: 0.9837\n",
            "Test accuracy 0.7333\n",
            "iter 400: loss 0.0000, time 21252.83ms\n",
            "iter 401: loss 0.0000, time 2133.88ms\n",
            "iter 402: loss 0.0000, time 2187.27ms\n",
            "iter 403: loss 0.0000, time 2164.49ms\n",
            "iter 404: loss 0.0000, time 2244.65ms\n",
            "iter 405: loss 0.0000, time 2084.17ms\n",
            "iter 406: loss 0.0000, time 2478.69ms\n",
            "iter 407: loss 0.0000, time 2132.40ms\n",
            "iter 408: loss 1.0625, time 2087.84ms\n",
            "iter 409: loss 0.0001, time 2080.99ms\n",
            "iter 410: loss 0.0000, time 2059.91ms\n",
            "iter 411: loss 0.0000, time 2061.19ms\n",
            "iter 412: loss 0.0002, time 2155.98ms\n",
            "iter 413: loss 0.0000, time 2137.22ms\n",
            "iter 414: loss 0.0000, time 2248.17ms\n",
            "iter 415: loss 0.0002, time 2297.93ms\n",
            "iter 416: loss 0.0000, time 2453.67ms\n",
            "iter 417: loss 0.0000, time 2166.66ms\n",
            "iter 418: loss 0.0000, time 2189.28ms\n",
            "iter 419: loss 0.0000, time 1650.63ms\n",
            "Step 420: train loss 0.1229, val loss 0.6301\n",
            "Validation accuracy: 0.9645\n",
            "iter 420: loss 0.0000, time 14326.63ms\n",
            "iter 421: loss 0.0001, time 1585.91ms\n",
            "iter 422: loss 0.0006, time 1470.72ms\n",
            "iter 423: loss 0.0000, time 1601.66ms\n",
            "iter 424: loss 0.0000, time 1477.01ms\n",
            "iter 425: loss 0.0000, time 1522.96ms\n",
            "iter 426: loss 0.0000, time 1530.49ms\n",
            "iter 427: loss 0.0000, time 1587.43ms\n",
            "iter 428: loss 0.0000, time 1556.82ms\n",
            "iter 429: loss 0.0000, time 1525.52ms\n",
            "iter 430: loss 0.0000, time 1612.94ms\n",
            "iter 431: loss 0.0000, time 1593.87ms\n",
            "iter 432: loss 0.0000, time 1622.00ms\n",
            "iter 433: loss 0.0000, time 1629.65ms\n",
            "iter 434: loss 0.0000, time 1633.10ms\n",
            "iter 435: loss 0.0000, time 1573.17ms\n",
            "iter 436: loss 0.0000, time 1507.17ms\n",
            "iter 437: loss 0.0000, time 1603.84ms\n",
            "iter 438: loss 0.0001, time 1532.36ms\n",
            "iter 439: loss 0.0000, time 1521.45ms\n",
            "Step 440: train loss 0.1174, val loss 0.6494\n",
            "Validation accuracy: 0.9778\n",
            "iter 440: loss 0.0000, time 14048.44ms\n",
            "iter 441: loss 0.0000, time 1529.60ms\n",
            "iter 442: loss 0.0000, time 1552.64ms\n",
            "iter 443: loss 0.0000, time 1553.36ms\n",
            "iter 444: loss 0.0000, time 1529.28ms\n",
            "iter 445: loss 0.0000, time 1517.26ms\n",
            "iter 446: loss 0.0000, time 1576.47ms\n",
            "iter 447: loss 0.0000, time 1650.11ms\n",
            "iter 448: loss 0.0000, time 1535.71ms\n",
            "iter 449: loss 0.0000, time 1564.83ms\n",
            "iter 450: loss 0.0000, time 1492.07ms\n",
            "iter 451: loss 0.0000, time 1533.83ms\n",
            "iter 452: loss 0.0017, time 1505.01ms\n",
            "iter 453: loss 0.0001, time 1514.79ms\n",
            "iter 454: loss 0.0000, time 1576.39ms\n",
            "iter 455: loss 0.0000, time 1920.38ms\n",
            "iter 456: loss 0.0000, time 1499.50ms\n",
            "iter 457: loss 0.0000, time 1475.17ms\n",
            "iter 458: loss 0.0003, time 1511.84ms\n",
            "iter 459: loss 0.0002, time 1565.22ms\n",
            "Step 460: train loss 0.1129, val loss 0.6599\n",
            "Validation accuracy: 0.9675\n",
            "iter 460: loss 0.0000, time 13921.19ms\n",
            "iter 461: loss 7.7500, time 1510.23ms\n",
            "iter 462: loss 0.0000, time 1536.46ms\n",
            "iter 463: loss 0.0001, time 1561.87ms\n",
            "iter 464: loss 0.0000, time 1516.27ms\n",
            "iter 465: loss 0.0004, time 1538.99ms\n",
            "iter 466: loss 0.0000, time 1487.47ms\n",
            "iter 467: loss 0.0000, time 1526.32ms\n",
            "iter 468: loss 0.0000, time 1546.68ms\n",
            "iter 469: loss 0.0001, time 1517.08ms\n",
            "iter 470: loss 0.1484, time 1487.10ms\n",
            "iter 471: loss 0.0001, time 1498.34ms\n",
            "iter 472: loss 0.0001, time 1522.02ms\n",
            "iter 473: loss 0.0000, time 1490.27ms\n",
            "iter 474: loss 0.0001, time 1502.56ms\n",
            "iter 475: loss 0.0001, time 1599.75ms\n",
            "iter 476: loss 0.0001, time 1490.26ms\n",
            "iter 477: loss 0.0001, time 1545.54ms\n",
            "iter 478: loss 0.0000, time 1582.66ms\n",
            "iter 479: loss 0.0000, time 1531.01ms\n",
            "Step 480: train loss 0.1091, val loss 0.6585\n",
            "Validation accuracy: 0.9822\n",
            "iter 480: loss 0.0000, time 13838.92ms\n",
            "iter 481: loss 0.0002, time 1469.57ms\n",
            "iter 482: loss 0.0000, time 1490.05ms\n",
            "iter 483: loss 0.0000, time 1553.95ms\n",
            "iter 484: loss 0.0000, time 1516.29ms\n",
            "iter 485: loss 0.0000, time 1567.41ms\n",
            "iter 486: loss 0.0001, time 1595.71ms\n",
            "iter 487: loss 0.0000, time 1556.15ms\n",
            "iter 488: loss 0.0001, time 1537.28ms\n",
            "iter 489: loss 0.0000, time 1572.88ms\n",
            "iter 490: loss 0.0000, time 1581.12ms\n",
            "iter 491: loss 0.0000, time 1504.29ms\n",
            "iter 492: loss 0.0000, time 1551.73ms\n",
            "iter 493: loss 0.0000, time 1544.42ms\n",
            "iter 494: loss 0.0000, time 1558.61ms\n",
            "iter 495: loss 0.0000, time 1546.14ms\n",
            "iter 496: loss 0.0000, time 1562.31ms\n",
            "iter 497: loss 0.0000, time 1494.14ms\n",
            "iter 498: loss 0.0000, time 1519.54ms\n",
            "iter 499: loss 0.0000, time 1527.46ms\n",
            "Step 500: train loss 0.1047, val loss 0.6635\n",
            "Validation accuracy: 0.9749\n",
            "Test accuracy 0.8000\n",
            "iter 500: loss 0.0001, time 15312.49ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▄▆▆▆█</td></tr><tr><td>Test_F1_Score</td><td>▁▅▆▆▇█</td></tr><tr><td>Test_Precision</td><td>▁▅█▆▇█</td></tr><tr><td>Test_Recall</td><td>▁▄▆▆▆█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▆▇▇▇█████████▇██████████</td></tr><tr><td>val/loss</td><td> █▄▃▁▂▁▁▁▂▂▂▂▂▂▂▁▁▂▂▂▄▄▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.8</td></tr><tr><td>Test_F1_Score</td><td>0.78022</td></tr><tr><td>Test_Precision</td><td>0.875</td></tr><tr><td>Test_Recall</td><td>0.8</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.10473</td></tr><tr><td>val/acc</td><td>0.97485</td></tr><tr><td>val/loss</td><td>0.66348</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_1803</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qn18iv0</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_180328-1qn18iv0\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 28gch4k3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_183045-28gch4k3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3' target=\"_blank\">dark-sweep-2</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1830\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dark-sweep-2</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_183045-28gch4k3\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_183051-28gch4k3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3' target=\"_blank\">gpt2_hyper_1830</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n",
            "iter 0: loss 0.5898, time 51636.10ms\n",
            "iter 1: loss 3.0625, time 2193.10ms\n",
            "iter 2: loss 2.2969, time 2198.77ms\n",
            "iter 3: loss 0.9805, time 2233.89ms\n",
            "iter 4: loss 0.8125, time 2279.24ms\n",
            "iter 5: loss 0.7344, time 2281.07ms\n",
            "iter 6: loss 1.0625, time 2240.04ms\n",
            "iter 7: loss 1.2734, time 2275.62ms\n",
            "iter 8: loss 0.6484, time 2231.06ms\n",
            "iter 9: loss 0.7617, time 2260.72ms\n",
            "iter 10: loss 0.7383, time 2230.32ms\n",
            "iter 11: loss 1.0781, time 2261.28ms\n",
            "iter 12: loss 1.3984, time 2223.62ms\n",
            "iter 13: loss 0.4336, time 2229.21ms\n",
            "iter 14: loss 1.0703, time 2223.80ms\n",
            "iter 15: loss 0.4316, time 2241.10ms\n",
            "iter 16: loss 0.7617, time 2210.40ms\n",
            "iter 17: loss 1.1875, time 2225.44ms\n",
            "iter 18: loss 1.5469, time 2241.03ms\n",
            "iter 19: loss 0.5391, time 2268.49ms\n",
            "Step 20: train loss 1.1441, val loss 1.0356\n",
            "Validation accuracy: 0.8299\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.2871, time 22432.10ms\n",
            "iter 21: loss 0.1475, time 2178.19ms\n",
            "iter 22: loss 1.4609, time 2171.61ms\n",
            "iter 23: loss 0.1118, time 2209.61ms\n",
            "iter 24: loss 0.3242, time 2238.39ms\n",
            "iter 25: loss 0.0317, time 2198.83ms\n",
            "iter 26: loss 0.7422, time 2238.62ms\n",
            "iter 27: loss 1.0078, time 2181.00ms\n",
            "iter 28: loss 0.0131, time 2191.65ms\n",
            "iter 29: loss 0.0801, time 2220.78ms\n",
            "iter 30: loss 0.0313, time 2163.07ms\n",
            "iter 31: loss 0.1191, time 2170.25ms\n",
            "iter 32: loss 0.0425, time 2220.99ms\n",
            "iter 33: loss 0.0347, time 2176.02ms\n",
            "iter 34: loss 0.0610, time 2173.45ms\n",
            "iter 35: loss 0.0045, time 2231.23ms\n",
            "iter 36: loss 0.0056, time 2165.03ms\n",
            "iter 37: loss 0.0149, time 2185.66ms\n",
            "iter 38: loss 0.0176, time 2230.81ms\n",
            "iter 39: loss 3.7969, time 2185.89ms\n",
            "Step 40: train loss 0.7889, val loss 0.8277\n",
            "Validation accuracy: 0.8772\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.1504, time 22344.54ms\n",
            "iter 41: loss 0.0125, time 2214.70ms\n",
            "iter 42: loss 0.0297, time 2175.73ms\n",
            "iter 43: loss 0.0082, time 2219.21ms\n",
            "iter 44: loss 0.0058, time 2149.91ms\n",
            "iter 45: loss 0.0991, time 2180.66ms\n",
            "iter 46: loss 0.0067, time 2126.10ms\n",
            "iter 47: loss 0.0050, time 2189.08ms\n",
            "iter 48: loss 0.0515, time 2166.79ms\n",
            "iter 49: loss 0.0056, time 2184.45ms\n",
            "iter 50: loss 0.0004, time 2272.71ms\n",
            "iter 51: loss 0.0062, time 2234.01ms\n",
            "iter 52: loss 0.0023, time 2411.26ms\n",
            "iter 53: loss 0.0002, time 2150.00ms\n",
            "iter 54: loss 0.0159, time 2211.50ms\n",
            "iter 55: loss 0.0066, time 2218.46ms\n",
            "iter 56: loss 0.0016, time 2185.42ms\n",
            "iter 57: loss 0.0001, time 2220.00ms\n",
            "iter 58: loss 0.1289, time 2199.72ms\n",
            "iter 59: loss 0.0170, time 2249.34ms\n",
            "Step 60: train loss 0.6046, val loss 0.7136\n",
            "Validation accuracy: 0.9393\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 3.5469, time 22682.30ms\n",
            "iter 61: loss 0.1943, time 2196.71ms\n",
            "iter 62: loss 0.0048, time 2199.07ms\n",
            "iter 63: loss 0.0356, time 2554.33ms\n",
            "iter 64: loss 0.0095, time 2183.61ms\n",
            "iter 65: loss 4.3125, time 2231.35ms\n",
            "iter 66: loss 2.7031, time 2230.87ms\n",
            "iter 67: loss 0.0046, time 2191.42ms\n",
            "iter 68: loss 0.0009, time 2223.93ms\n",
            "iter 69: loss 0.0086, time 2190.12ms\n",
            "iter 70: loss 0.0027, time 2174.50ms\n",
            "iter 71: loss 0.0041, time 2190.18ms\n",
            "iter 72: loss 0.0008, time 2226.34ms\n",
            "iter 73: loss 0.0017, time 2183.04ms\n",
            "iter 74: loss 0.0011, time 2198.10ms\n",
            "iter 75: loss 0.0008, time 2223.37ms\n",
            "iter 76: loss 0.0248, time 2188.97ms\n",
            "iter 77: loss 0.0002, time 2180.16ms\n",
            "iter 78: loss 0.0005, time 2175.08ms\n",
            "iter 79: loss 0.0001, time 2292.88ms\n",
            "Step 80: train loss 0.4962, val loss 0.6386\n",
            "Validation accuracy: 0.9497\n",
            "iter 80: loss 0.0016, time 19576.72ms\n",
            "iter 81: loss 0.0017, time 2092.24ms\n",
            "iter 82: loss 0.0188, time 2135.31ms\n",
            "iter 83: loss 0.0060, time 2135.32ms\n",
            "iter 84: loss 0.0010, time 2172.08ms\n",
            "iter 85: loss 0.0001, time 2251.25ms\n",
            "iter 86: loss 0.5508, time 2137.70ms\n",
            "iter 87: loss 0.0002, time 2103.60ms\n",
            "iter 88: loss 0.0001, time 2132.51ms\n",
            "iter 89: loss 0.0000, time 2095.27ms\n",
            "iter 90: loss 0.0028, time 2162.02ms\n",
            "iter 91: loss 0.0003, time 2122.87ms\n",
            "iter 92: loss 0.0000, time 2119.48ms\n",
            "iter 93: loss 0.0080, time 2105.30ms\n",
            "iter 94: loss 0.0045, time 2108.10ms\n",
            "iter 95: loss 0.0050, time 2099.59ms\n",
            "iter 96: loss 0.0016, time 2111.07ms\n",
            "iter 97: loss 0.0065, time 2149.77ms\n",
            "iter 98: loss 0.0013, time 2170.56ms\n",
            "iter 99: loss 0.0194, time 2107.63ms\n",
            "Step 100: train loss 0.4234, val loss 0.6746\n",
            "Validation accuracy: 0.9467\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.6667\n",
            "iter 100: loss 0.0103, time 28619.85ms\n",
            "iter 101: loss 0.0186, time 2903.53ms\n",
            "iter 102: loss 0.0002, time 2838.67ms\n",
            "iter 103: loss 0.0004, time 2851.92ms\n",
            "iter 104: loss 0.0007, time 2846.00ms\n",
            "iter 105: loss 0.0033, time 2873.19ms\n",
            "iter 106: loss 0.0020, time 2800.96ms\n",
            "iter 107: loss 0.0093, time 2981.16ms\n",
            "iter 108: loss 0.0007, time 2840.51ms\n",
            "iter 109: loss 3.1563, time 2849.33ms\n",
            "iter 110: loss 0.0010, time 2876.43ms\n",
            "iter 111: loss 0.0001, time 3242.24ms\n",
            "iter 112: loss 0.0002, time 2815.29ms\n",
            "iter 113: loss 0.0000, time 2803.06ms\n",
            "iter 114: loss 0.0001, time 2801.01ms\n",
            "iter 115: loss 0.0386, time 2803.96ms\n",
            "iter 116: loss 0.0776, time 2113.99ms\n",
            "iter 117: loss 0.0023, time 2048.57ms\n",
            "iter 118: loss 0.0042, time 2174.82ms\n",
            "iter 119: loss 0.0124, time 2255.09ms\n",
            "Step 120: train loss 0.3666, val loss 0.6088\n",
            "Validation accuracy: 0.9423\n",
            "iter 120: loss 0.0113, time 16640.77ms\n",
            "iter 121: loss 0.0010, time 1434.05ms\n",
            "iter 122: loss 0.1211, time 1507.15ms\n",
            "iter 123: loss 0.0001, time 1519.24ms\n",
            "iter 124: loss 0.0001, time 1475.23ms\n",
            "iter 125: loss 0.0000, time 1582.72ms\n",
            "iter 126: loss 0.0002, time 1508.39ms\n",
            "iter 127: loss 0.0000, time 1604.71ms\n",
            "iter 128: loss 0.0000, time 1625.31ms\n",
            "iter 129: loss 0.0002, time 1447.05ms\n",
            "iter 130: loss 0.0004, time 1468.03ms\n",
            "iter 131: loss 0.0001, time 1448.40ms\n",
            "iter 132: loss 0.0090, time 1535.31ms\n",
            "iter 133: loss 0.0001, time 1508.39ms\n",
            "iter 134: loss 0.0049, time 1507.90ms\n",
            "iter 135: loss 0.0003, time 1462.39ms\n",
            "iter 136: loss 0.0081, time 1406.61ms\n",
            "iter 137: loss 0.1377, time 1400.74ms\n",
            "iter 138: loss 0.3457, time 1480.72ms\n",
            "iter 139: loss 0.0000, time 1509.86ms\n",
            "Step 140: train loss 0.3251, val loss 0.5992\n",
            "Validation accuracy: 0.9541\n",
            "iter 140: loss 0.0001, time 12848.01ms\n",
            "iter 141: loss 0.0001, time 1495.01ms\n",
            "iter 142: loss 1.9609, time 1445.73ms\n",
            "iter 143: loss 0.0012, time 1415.50ms\n",
            "iter 144: loss 0.0001, time 1509.35ms\n",
            "iter 145: loss 0.0000, time 1516.93ms\n",
            "iter 146: loss 0.0134, time 1488.13ms\n",
            "iter 147: loss 0.0001, time 1444.79ms\n",
            "iter 148: loss 0.0001, time 1405.32ms\n",
            "iter 149: loss 0.0000, time 1474.20ms\n",
            "iter 150: loss 0.0001, time 1440.37ms\n",
            "iter 151: loss 0.0001, time 1401.17ms\n",
            "iter 152: loss 0.0000, time 1476.12ms\n",
            "iter 153: loss 0.0003, time 1385.96ms\n",
            "iter 154: loss 0.0001, time 1412.41ms\n",
            "iter 155: loss 0.0371, time 1397.61ms\n",
            "iter 156: loss 0.0009, time 1460.20ms\n",
            "iter 157: loss 0.0001, time 1454.66ms\n",
            "iter 158: loss 0.0002, time 1533.39ms\n",
            "iter 159: loss 0.0085, time 1481.53ms\n",
            "Step 160: train loss 0.3000, val loss 0.6266\n",
            "Validation accuracy: 0.9216\n",
            "iter 160: loss 0.0004, time 13114.44ms\n",
            "iter 161: loss 0.0000, time 1438.24ms\n",
            "iter 162: loss 0.0669, time 1446.94ms\n",
            "iter 163: loss 0.0000, time 1473.02ms\n",
            "iter 164: loss 0.0000, time 1437.36ms\n",
            "iter 165: loss 0.0000, time 1487.90ms\n",
            "iter 166: loss 0.0006, time 1499.47ms\n",
            "iter 167: loss 0.0005, time 1394.78ms\n",
            "iter 168: loss 0.0000, time 1423.90ms\n",
            "iter 169: loss 0.0000, time 1444.47ms\n",
            "iter 170: loss 0.0002, time 1440.39ms\n",
            "iter 171: loss 0.0140, time 1449.90ms\n",
            "iter 172: loss 0.0000, time 1417.14ms\n",
            "iter 173: loss 0.0000, time 1454.36ms\n",
            "iter 174: loss 0.0035, time 1364.07ms\n",
            "iter 175: loss 0.0566, time 1430.43ms\n",
            "iter 176: loss 0.0001, time 1386.93ms\n",
            "iter 177: loss 0.0000, time 1434.41ms\n",
            "iter 178: loss 0.0009, time 1360.48ms\n",
            "iter 179: loss 0.0000, time 1456.99ms\n",
            "Step 180: train loss 0.2733, val loss 0.6209\n",
            "Validation accuracy: 0.9556\n",
            "iter 180: loss 0.0006, time 12656.80ms\n",
            "iter 181: loss 0.0001, time 1378.85ms\n",
            "iter 182: loss 0.0615, time 1438.24ms\n",
            "iter 183: loss 0.0000, time 1392.65ms\n",
            "iter 184: loss 0.0000, time 1439.16ms\n",
            "iter 185: loss 0.0000, time 1435.67ms\n",
            "iter 186: loss 0.0000, time 1361.85ms\n",
            "iter 187: loss 0.0000, time 1450.84ms\n",
            "iter 188: loss 0.0000, time 1462.87ms\n",
            "iter 189: loss 0.0000, time 1395.07ms\n",
            "iter 190: loss 0.0000, time 1437.58ms\n",
            "iter 191: loss 0.0000, time 1400.63ms\n",
            "iter 192: loss 0.0006, time 1543.11ms\n",
            "iter 193: loss 0.0000, time 1415.95ms\n",
            "iter 194: loss 0.0000, time 1460.76ms\n",
            "iter 195: loss 0.0006, time 1387.83ms\n",
            "iter 196: loss 0.0000, time 1395.54ms\n",
            "iter 197: loss 0.0000, time 1411.22ms\n",
            "iter 198: loss 0.0000, time 1516.70ms\n",
            "iter 199: loss 0.0000, time 1444.01ms\n",
            "Step 200: train loss 0.2482, val loss 0.6187\n",
            "Validation accuracy: 0.9763\n",
            "Test accuracy 0.7333\n",
            "iter 200: loss 0.0001, time 13295.37ms\n",
            "iter 201: loss 0.0000, time 1445.87ms\n",
            "iter 202: loss 0.0000, time 1498.35ms\n",
            "iter 203: loss 0.0000, time 1445.62ms\n",
            "iter 204: loss 0.0000, time 1447.39ms\n",
            "iter 205: loss 0.0000, time 1474.33ms\n",
            "iter 206: loss 0.0000, time 1460.05ms\n",
            "iter 207: loss 0.0000, time 1427.93ms\n",
            "iter 208: loss 0.0001, time 1423.25ms\n",
            "iter 209: loss 0.0000, time 1402.41ms\n",
            "iter 210: loss 0.0000, time 1383.36ms\n",
            "iter 211: loss 0.0003, time 1418.41ms\n",
            "iter 212: loss 0.0000, time 1488.89ms\n",
            "iter 213: loss 0.0006, time 1465.21ms\n",
            "iter 214: loss 0.0000, time 1441.77ms\n",
            "iter 215: loss 0.0000, time 1467.50ms\n",
            "iter 216: loss 0.0000, time 1466.10ms\n",
            "iter 217: loss 0.0000, time 1376.95ms\n",
            "iter 218: loss 0.0001, time 1461.13ms\n",
            "iter 219: loss 0.0000, time 1394.22ms\n",
            "Step 220: train loss 0.2298, val loss 0.6334\n",
            "Validation accuracy: 0.9645\n",
            "iter 220: loss 0.0000, time 12489.98ms\n",
            "iter 221: loss 0.0000, time 1392.94ms\n",
            "iter 222: loss 0.0000, time 1455.31ms\n",
            "iter 223: loss 0.0000, time 1396.83ms\n",
            "iter 224: loss 0.0010, time 1439.48ms\n",
            "iter 225: loss 0.0000, time 1513.27ms\n",
            "iter 226: loss 0.0000, time 1505.42ms\n",
            "iter 227: loss 0.0001, time 1375.69ms\n",
            "iter 228: loss 0.0008, time 1492.26ms\n",
            "iter 229: loss 0.0001, time 1422.27ms\n",
            "iter 230: loss 0.0000, time 1436.71ms\n",
            "iter 231: loss 0.0001, time 1421.75ms\n",
            "iter 232: loss 0.0000, time 1496.75ms\n",
            "iter 233: loss 0.0002, time 1451.39ms\n",
            "iter 234: loss 0.0000, time 1451.16ms\n",
            "iter 235: loss 0.0000, time 1368.84ms\n",
            "iter 236: loss 0.0299, time 1422.23ms\n",
            "iter 237: loss 0.0003, time 1407.40ms\n",
            "iter 238: loss 0.1445, time 1402.60ms\n",
            "iter 239: loss 0.0000, time 1455.06ms\n",
            "Step 240: train loss 0.2118, val loss 0.5898\n",
            "Validation accuracy: 0.9749\n",
            "iter 240: loss 0.0000, time 12580.12ms\n",
            "iter 241: loss 0.0119, time 1431.58ms\n",
            "iter 242: loss 0.0001, time 1406.28ms\n",
            "iter 243: loss 0.0000, time 1722.13ms\n",
            "iter 244: loss 0.0000, time 1396.88ms\n",
            "iter 245: loss 0.0001, time 1461.95ms\n",
            "iter 246: loss 0.0000, time 1501.36ms\n",
            "iter 247: loss 0.0000, time 1435.47ms\n",
            "iter 248: loss 0.0000, time 1406.49ms\n",
            "iter 249: loss 0.0000, time 1423.15ms\n",
            "iter 250: loss 0.0001, time 1446.64ms\n",
            "iter 251: loss 0.3457, time 1449.76ms\n",
            "iter 252: loss 0.0001, time 1427.46ms\n",
            "iter 253: loss 0.0001, time 1556.45ms\n",
            "iter 254: loss 0.0000, time 1473.78ms\n",
            "iter 255: loss 0.0000, time 1460.17ms\n",
            "iter 256: loss 0.0242, time 1491.24ms\n",
            "iter 257: loss 0.0000, time 1488.70ms\n",
            "iter 258: loss 0.0000, time 1479.77ms\n",
            "iter 259: loss 0.0001, time 1462.20ms\n",
            "Step 260: train loss 0.1967, val loss 0.5652\n",
            "Validation accuracy: 0.9778\n",
            "iter 260: loss 0.0002, time 12555.27ms\n",
            "iter 261: loss 0.0000, time 1455.10ms\n",
            "iter 262: loss 0.0001, time 1406.98ms\n",
            "iter 263: loss 0.0334, time 1397.41ms\n",
            "iter 264: loss 0.0000, time 1438.99ms\n",
            "iter 265: loss 0.2061, time 1481.84ms\n",
            "iter 266: loss 0.0000, time 1473.67ms\n",
            "iter 267: loss 0.0000, time 1352.21ms\n",
            "iter 268: loss 0.0000, time 1401.36ms\n",
            "iter 269: loss 0.0000, time 1473.91ms\n",
            "iter 270: loss 0.0000, time 1384.23ms\n",
            "iter 271: loss 0.0001, time 1471.68ms\n",
            "iter 272: loss 0.0000, time 1472.30ms\n",
            "iter 273: loss 0.0000, time 1444.82ms\n",
            "iter 274: loss 0.0002, time 1441.25ms\n",
            "iter 275: loss 0.0001, time 1439.83ms\n",
            "iter 276: loss 0.0001, time 1421.65ms\n",
            "iter 277: loss 0.0000, time 1422.89ms\n",
            "iter 278: loss 0.0000, time 1415.18ms\n",
            "iter 279: loss 0.0001, time 1408.73ms\n",
            "Step 280: train loss 0.1841, val loss 0.5692\n",
            "Validation accuracy: 0.9630\n",
            "iter 280: loss 0.0000, time 12646.97ms\n",
            "iter 281: loss 0.0000, time 1436.09ms\n",
            "iter 282: loss 0.0000, time 1400.21ms\n",
            "iter 283: loss 0.0001, time 1446.23ms\n",
            "iter 284: loss 0.0000, time 1434.94ms\n",
            "iter 285: loss 0.0000, time 1462.20ms\n",
            "iter 286: loss 0.0000, time 1469.59ms\n",
            "iter 287: loss 0.0000, time 1444.85ms\n",
            "iter 288: loss 0.0000, time 1474.29ms\n",
            "iter 289: loss 0.0005, time 1455.66ms\n",
            "iter 290: loss 0.0000, time 1425.33ms\n",
            "iter 291: loss 0.0000, time 1385.63ms\n",
            "iter 292: loss 0.0000, time 1428.95ms\n",
            "iter 293: loss 0.0000, time 1706.54ms\n",
            "iter 294: loss 0.0000, time 1403.93ms\n",
            "iter 295: loss 0.0000, time 1392.58ms\n",
            "iter 296: loss 0.0000, time 1402.73ms\n",
            "iter 297: loss 0.0000, time 1417.41ms\n",
            "iter 298: loss 0.0000, time 1445.64ms\n",
            "iter 299: loss 0.0000, time 1440.27ms\n",
            "Step 300: train loss 0.1738, val loss 0.5614\n",
            "Validation accuracy: 0.9749\n",
            "Test accuracy 0.6667\n",
            "iter 300: loss 0.0000, time 13455.44ms\n",
            "iter 301: loss 0.0000, time 1398.15ms\n",
            "iter 302: loss 0.0000, time 1409.00ms\n",
            "iter 303: loss 0.0000, time 1395.61ms\n",
            "iter 304: loss 0.0000, time 1363.09ms\n",
            "iter 305: loss 0.0000, time 1448.97ms\n",
            "iter 306: loss 0.0000, time 1363.89ms\n",
            "iter 307: loss 0.0000, time 1424.47ms\n",
            "iter 308: loss 0.0000, time 1400.82ms\n",
            "iter 309: loss 0.0000, time 1373.09ms\n",
            "iter 310: loss 0.0000, time 1427.99ms\n",
            "iter 311: loss 0.0007, time 1480.35ms\n",
            "iter 312: loss 0.0003, time 1407.80ms\n",
            "iter 313: loss 0.0000, time 1441.46ms\n",
            "iter 314: loss 0.0000, time 1443.16ms\n",
            "iter 315: loss 0.0000, time 1367.02ms\n",
            "iter 316: loss 0.0000, time 1374.95ms\n",
            "iter 317: loss 0.0025, time 1368.23ms\n",
            "iter 318: loss 0.0000, time 1407.92ms\n",
            "iter 319: loss 0.0000, time 1432.69ms\n",
            "Step 320: train loss 0.1639, val loss 0.5434\n",
            "Validation accuracy: 0.9719\n",
            "iter 320: loss 0.0000, time 12255.87ms\n",
            "iter 321: loss 0.0000, time 1351.53ms\n",
            "iter 322: loss 0.0000, time 1357.88ms\n",
            "iter 323: loss 0.0000, time 1414.28ms\n",
            "iter 324: loss 0.0000, time 1356.99ms\n",
            "iter 325: loss 0.0000, time 1450.30ms\n",
            "iter 326: loss 0.0000, time 1432.44ms\n",
            "iter 327: loss 0.2520, time 1428.67ms\n",
            "iter 328: loss 0.0000, time 1383.95ms\n",
            "iter 329: loss 0.0000, time 1367.25ms\n",
            "iter 330: loss 0.0000, time 1426.12ms\n",
            "iter 331: loss 0.0000, time 1407.42ms\n",
            "iter 332: loss 0.0000, time 1392.69ms\n",
            "iter 333: loss 0.0000, time 1354.21ms\n",
            "iter 334: loss 0.0000, time 1379.75ms\n",
            "iter 335: loss 0.0000, time 1374.10ms\n",
            "iter 336: loss 0.0000, time 1385.28ms\n",
            "iter 337: loss 0.0000, time 1452.99ms\n",
            "iter 338: loss 0.0082, time 1440.32ms\n",
            "iter 339: loss 0.0002, time 1391.40ms\n",
            "Step 340: train loss 0.1545, val loss 0.5320\n",
            "Validation accuracy: 0.9763\n",
            "iter 340: loss 0.0000, time 12697.73ms\n",
            "iter 341: loss 0.0000, time 1410.33ms\n",
            "iter 342: loss 0.0000, time 1397.71ms\n",
            "iter 343: loss 0.0000, time 1382.08ms\n",
            "iter 344: loss 0.0000, time 1411.56ms\n",
            "iter 345: loss 0.0000, time 1397.02ms\n",
            "iter 346: loss 0.0000, time 1410.76ms\n",
            "iter 347: loss 0.0000, time 1445.46ms\n",
            "iter 348: loss 0.0000, time 1341.44ms\n",
            "iter 349: loss 0.0000, time 1374.93ms\n",
            "iter 350: loss 0.0000, time 1375.72ms\n",
            "iter 351: loss 0.0001, time 1402.49ms\n",
            "iter 352: loss 0.0001, time 1391.54ms\n",
            "iter 353: loss 0.0000, time 1365.42ms\n",
            "iter 354: loss 0.0000, time 1410.00ms\n",
            "iter 355: loss 0.0000, time 1417.05ms\n",
            "iter 356: loss 0.0040, time 1383.94ms\n",
            "iter 357: loss 0.0000, time 1410.34ms\n",
            "iter 358: loss 0.0000, time 1384.42ms\n",
            "iter 359: loss 0.0000, time 1380.01ms\n",
            "Step 360: train loss 0.1464, val loss 0.5473\n",
            "Validation accuracy: 0.9822\n",
            "iter 360: loss 0.0000, time 12815.35ms\n",
            "iter 361: loss 0.0000, time 1383.86ms\n",
            "iter 362: loss 0.0000, time 1397.54ms\n",
            "iter 363: loss 0.0000, time 1386.64ms\n",
            "iter 364: loss 0.0000, time 1365.45ms\n",
            "iter 365: loss 0.0000, time 1429.32ms\n",
            "iter 366: loss 0.0000, time 1430.65ms\n",
            "iter 367: loss 0.0000, time 1366.08ms\n",
            "iter 368: loss 0.0000, time 1439.36ms\n",
            "iter 369: loss 0.0000, time 1361.87ms\n",
            "iter 370: loss 0.0000, time 1359.16ms\n",
            "iter 371: loss 0.0000, time 1367.47ms\n",
            "iter 372: loss 0.0000, time 1453.64ms\n",
            "iter 373: loss 0.0000, time 1459.36ms\n",
            "iter 374: loss 0.0000, time 1503.53ms\n",
            "iter 375: loss 0.0000, time 1378.41ms\n",
            "iter 376: loss 0.0000, time 1412.57ms\n",
            "iter 377: loss 0.0006, time 1426.58ms\n",
            "iter 378: loss 0.0000, time 1394.48ms\n",
            "iter 379: loss 0.0000, time 1379.16ms\n",
            "Step 380: train loss 0.1393, val loss 0.5559\n",
            "Validation accuracy: 0.9719\n",
            "iter 380: loss 0.0000, time 12724.57ms\n",
            "iter 381: loss 0.0000, time 1445.06ms\n",
            "iter 382: loss 0.0000, time 1408.97ms\n",
            "iter 383: loss 0.0000, time 1399.08ms\n",
            "iter 384: loss 0.0013, time 1377.64ms\n",
            "iter 385: loss 0.0000, time 1349.10ms\n",
            "iter 386: loss 0.0000, time 1403.40ms\n",
            "iter 387: loss 0.0000, time 1419.25ms\n",
            "iter 388: loss 0.0000, time 1454.80ms\n",
            "iter 389: loss 0.0000, time 1360.77ms\n",
            "iter 390: loss 0.0002, time 1392.61ms\n",
            "iter 391: loss 0.0000, time 1367.89ms\n",
            "iter 392: loss 0.0001, time 1364.93ms\n",
            "iter 393: loss 0.0000, time 1458.60ms\n",
            "iter 394: loss 0.0000, time 1465.22ms\n",
            "iter 395: loss 0.0000, time 1471.80ms\n",
            "iter 396: loss 0.0000, time 1409.10ms\n",
            "iter 397: loss 0.0000, time 1407.29ms\n",
            "iter 398: loss 0.0000, time 1398.11ms\n",
            "iter 399: loss 0.0000, time 1439.68ms\n",
            "Step 400: train loss 0.1330, val loss 0.5546\n",
            "Validation accuracy: 0.9704\n",
            "Test accuracy 0.7000\n",
            "iter 400: loss 0.0007, time 13198.51ms\n",
            "iter 401: loss 0.0000, time 1417.53ms\n",
            "iter 402: loss 0.0000, time 1385.65ms\n",
            "iter 403: loss 0.0000, time 1397.01ms\n",
            "iter 404: loss 0.0000, time 1390.74ms\n",
            "iter 405: loss 0.0000, time 1382.86ms\n",
            "iter 406: loss 0.0000, time 1409.33ms\n",
            "iter 407: loss 0.0000, time 1452.35ms\n",
            "iter 408: loss 0.0000, time 1430.18ms\n",
            "iter 409: loss 0.0000, time 1355.46ms\n",
            "iter 410: loss 0.0000, time 1389.62ms\n",
            "iter 411: loss 0.0000, time 1402.13ms\n",
            "iter 412: loss 0.0000, time 1463.84ms\n",
            "iter 413: loss 0.0000, time 1431.78ms\n",
            "iter 414: loss 0.0001, time 1486.80ms\n",
            "iter 415: loss 0.0000, time 1446.74ms\n",
            "iter 416: loss 0.0000, time 1450.20ms\n",
            "iter 417: loss 0.0000, time 1344.28ms\n",
            "iter 418: loss 0.0000, time 1426.22ms\n",
            "iter 419: loss 0.0000, time 1397.13ms\n",
            "Step 420: train loss 0.1268, val loss 0.5668\n",
            "Validation accuracy: 0.9822\n",
            "iter 420: loss 0.0000, time 12553.10ms\n",
            "iter 421: loss 0.0000, time 1467.15ms\n",
            "iter 422: loss 0.0000, time 1424.54ms\n",
            "iter 423: loss 0.0000, time 1401.55ms\n",
            "iter 424: loss 0.0000, time 1334.22ms\n",
            "iter 425: loss 0.0000, time 1370.60ms\n",
            "iter 426: loss 0.0000, time 1447.82ms\n",
            "iter 427: loss 0.0000, time 1690.48ms\n",
            "iter 428: loss 0.0000, time 1418.65ms\n",
            "iter 429: loss 0.0001, time 1445.82ms\n",
            "iter 430: loss 0.0000, time 1424.08ms\n",
            "iter 431: loss 0.0001, time 1425.97ms\n",
            "iter 432: loss 0.0000, time 1389.50ms\n",
            "iter 433: loss 0.0000, time 1428.88ms\n",
            "iter 434: loss 0.0000, time 1477.50ms\n",
            "iter 435: loss 0.0000, time 1478.00ms\n",
            "iter 436: loss 0.0005, time 1355.69ms\n",
            "iter 437: loss 0.0000, time 1475.88ms\n",
            "iter 438: loss 0.0002, time 1416.83ms\n",
            "iter 439: loss 0.0000, time 1352.78ms\n",
            "Step 440: train loss 0.1215, val loss 0.5695\n",
            "Validation accuracy: 0.9749\n",
            "iter 440: loss 0.0000, time 12391.35ms\n",
            "iter 441: loss 0.0000, time 1441.26ms\n",
            "iter 442: loss 0.0000, time 1386.49ms\n",
            "iter 443: loss 0.0000, time 1514.95ms\n",
            "iter 444: loss 0.0000, time 1376.80ms\n",
            "iter 445: loss 0.0000, time 1374.43ms\n",
            "iter 446: loss 0.0000, time 1448.11ms\n",
            "iter 447: loss 0.0000, time 1495.76ms\n",
            "iter 448: loss 0.0000, time 1448.86ms\n",
            "iter 449: loss 0.0000, time 1420.63ms\n",
            "iter 450: loss 0.0000, time 1484.70ms\n",
            "iter 451: loss 0.0000, time 1432.67ms\n",
            "iter 452: loss 0.0003, time 1434.11ms\n",
            "iter 453: loss 0.0000, time 1488.43ms\n",
            "iter 454: loss 0.0001, time 1416.07ms\n",
            "iter 455: loss 0.0000, time 1520.00ms\n",
            "iter 456: loss 0.0000, time 1509.26ms\n",
            "iter 457: loss 0.0000, time 1439.30ms\n",
            "iter 458: loss 0.0004, time 1501.01ms\n",
            "iter 459: loss 0.0000, time 1403.85ms\n",
            "Step 460: train loss 0.1167, val loss 0.5532\n",
            "Validation accuracy: 0.9778\n",
            "iter 460: loss 0.0000, time 12393.79ms\n",
            "iter 461: loss 0.0003, time 1462.87ms\n",
            "iter 462: loss 0.0000, time 1531.44ms\n",
            "iter 463: loss 0.0000, time 1382.48ms\n",
            "iter 464: loss 0.0000, time 1418.30ms\n",
            "iter 465: loss 0.0000, time 1416.65ms\n",
            "iter 466: loss 0.0000, time 1427.61ms\n",
            "iter 467: loss 0.0000, time 1504.95ms\n",
            "iter 468: loss 0.0000, time 1439.53ms\n",
            "iter 469: loss 0.0000, time 1447.41ms\n",
            "iter 470: loss 0.0000, time 1446.93ms\n",
            "iter 471: loss 0.0000, time 1377.42ms\n",
            "iter 472: loss 0.0000, time 1429.62ms\n",
            "iter 473: loss 0.0000, time 1423.37ms\n",
            "iter 474: loss 0.0000, time 1412.19ms\n",
            "iter 475: loss 0.0000, time 1748.61ms\n",
            "iter 476: loss 0.0000, time 1433.85ms\n",
            "iter 477: loss 0.0000, time 1408.79ms\n",
            "iter 478: loss 0.0000, time 1441.65ms\n",
            "iter 479: loss 0.0000, time 1408.30ms\n",
            "Step 480: train loss 0.1126, val loss 0.5458\n",
            "Validation accuracy: 0.9793\n",
            "iter 480: loss 0.0000, time 12499.85ms\n",
            "iter 481: loss 0.0000, time 1454.69ms\n",
            "iter 482: loss 0.0001, time 1367.16ms\n",
            "iter 483: loss 0.0000, time 1396.28ms\n",
            "iter 484: loss 0.0000, time 1395.01ms\n",
            "iter 485: loss 0.0000, time 1392.72ms\n",
            "iter 486: loss 0.0000, time 1419.26ms\n",
            "iter 487: loss 0.0000, time 1468.92ms\n",
            "iter 488: loss 0.0000, time 1414.26ms\n",
            "iter 489: loss 0.0000, time 1436.71ms\n",
            "iter 490: loss 0.0031, time 1489.35ms\n",
            "iter 491: loss 0.0000, time 1414.92ms\n",
            "iter 492: loss 0.0000, time 1433.89ms\n",
            "iter 493: loss 0.0000, time 1429.67ms\n",
            "iter 494: loss 0.0000, time 1448.01ms\n",
            "iter 495: loss 0.0000, time 1542.86ms\n",
            "iter 496: loss 0.0000, time 1451.42ms\n",
            "iter 497: loss 0.0000, time 1386.27ms\n",
            "iter 498: loss 0.0000, time 1387.95ms\n",
            "iter 499: loss 0.0000, time 1430.50ms\n",
            "Step 500: train loss 0.1084, val loss 0.5490\n",
            "Validation accuracy: 0.9719\n",
            "Test accuracy 0.6667\n",
            "iter 500: loss 0.0000, time 13330.14ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▆█▆▇▆</td></tr><tr><td>Test_F1_Score</td><td>▁▆█▆▇▆</td></tr><tr><td>Test_Precision</td><td>▁██▇▇▇</td></tr><tr><td>Test_Recall</td><td>▁▆█▆▇▆</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇█████▇█████████████████</td></tr><tr><td>val/loss</td><td> █▅▄▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.66667</td></tr><tr><td>Test_F1_Score</td><td>0.65344</td></tr><tr><td>Test_Precision</td><td>0.75</td></tr><tr><td>Test_Recall</td><td>0.66667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.10842</td></tr><tr><td>val/acc</td><td>0.97189</td></tr><tr><td>val/loss</td><td>0.54896</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_1830</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/28gch4k3</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_183051-28gch4k3\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rm4ii3ki with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_185630-rm4ii3ki</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki' target=\"_blank\">good-sweep-3</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1856\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">good-sweep-3</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_185630-rm4ii3ki\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_185638-rm4ii3ki</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki' target=\"_blank\">gpt2_hyper_1856</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.2353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n",
            "iter 0: loss 8.0625, time 18458.35ms\n",
            "iter 1: loss 5.2188, time 1458.57ms\n",
            "iter 2: loss 0.5000, time 1754.90ms\n",
            "iter 3: loss 1.1953, time 1408.45ms\n",
            "iter 4: loss 3.6250, time 1488.73ms\n",
            "iter 5: loss 0.6406, time 1538.02ms\n",
            "iter 6: loss 0.8594, time 1453.31ms\n",
            "iter 7: loss 0.9961, time 1457.95ms\n",
            "iter 8: loss 0.9727, time 1479.42ms\n",
            "iter 9: loss 0.9727, time 1460.83ms\n",
            "iter 10: loss 0.6016, time 1449.32ms\n",
            "iter 11: loss 0.5078, time 1477.60ms\n",
            "iter 12: loss 0.4531, time 1537.87ms\n",
            "iter 13: loss 0.4277, time 1555.18ms\n",
            "iter 14: loss 0.6406, time 1429.92ms\n",
            "iter 15: loss 0.4336, time 1429.39ms\n",
            "iter 16: loss 0.6992, time 1431.25ms\n",
            "iter 17: loss 0.5039, time 1451.86ms\n",
            "iter 18: loss 0.7227, time 1415.92ms\n",
            "iter 19: loss 0.6289, time 1450.24ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 1.2521, val loss 0.5970\n",
            "Validation accuracy: 0.7441\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.5430, time 19053.68ms\n",
            "iter 21: loss 0.2734, time 1511.38ms\n",
            "iter 22: loss 0.1543, time 1471.04ms\n",
            "iter 23: loss 0.3066, time 1379.52ms\n",
            "iter 24: loss 0.6563, time 1540.76ms\n",
            "iter 25: loss 0.8906, time 1503.04ms\n",
            "iter 26: loss 0.1172, time 1452.81ms\n",
            "iter 27: loss 0.2988, time 1478.93ms\n",
            "iter 28: loss 0.0112, time 1466.68ms\n",
            "iter 29: loss 0.1172, time 1572.57ms\n",
            "iter 30: loss 0.0447, time 1427.69ms\n",
            "iter 31: loss 0.2227, time 1444.50ms\n",
            "iter 32: loss 0.6719, time 1448.09ms\n",
            "iter 33: loss 0.0996, time 1524.20ms\n",
            "iter 34: loss 1.6484, time 1553.67ms\n",
            "iter 35: loss 0.1011, time 1501.98ms\n",
            "iter 36: loss 4.8438, time 1472.63ms\n",
            "iter 37: loss 0.0095, time 1495.01ms\n",
            "iter 38: loss 1.4453, time 1449.75ms\n",
            "iter 39: loss 2.4063, time 1413.25ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.8608, val loss 0.5725\n",
            "Validation accuracy: 0.8661\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.3848, time 19267.78ms\n",
            "iter 41: loss 0.0366, time 1490.76ms\n",
            "iter 42: loss 0.9023, time 1560.51ms\n",
            "iter 43: loss 0.1533, time 1516.00ms\n",
            "iter 44: loss 0.0713, time 1481.64ms\n",
            "iter 45: loss 2.2969, time 1513.66ms\n",
            "iter 46: loss 0.6836, time 1415.99ms\n",
            "iter 47: loss 0.1787, time 1446.80ms\n",
            "iter 48: loss 0.0405, time 1521.58ms\n",
            "iter 49: loss 0.0261, time 1447.15ms\n",
            "iter 50: loss 0.1138, time 1401.23ms\n",
            "iter 51: loss 0.0786, time 1368.16ms\n",
            "iter 52: loss 0.0037, time 1457.94ms\n",
            "iter 53: loss 8.6250, time 1411.01ms\n",
            "iter 54: loss 0.0028, time 1460.04ms\n",
            "iter 55: loss 1.1250, time 1411.58ms\n",
            "iter 56: loss 0.5664, time 1477.47ms\n",
            "iter 57: loss 0.0008, time 1460.38ms\n",
            "iter 58: loss 0.0581, time 1489.99ms\n",
            "iter 59: loss 0.0041, time 1468.47ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.6796, val loss 0.4983\n",
            "Validation accuracy: 0.9190\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.0018, time 19114.26ms\n",
            "iter 61: loss 0.3066, time 1526.45ms\n",
            "iter 62: loss 0.0039, time 1443.83ms\n",
            "iter 63: loss 0.0129, time 1498.06ms\n",
            "iter 64: loss 0.0157, time 1510.69ms\n",
            "iter 65: loss 0.0070, time 1529.51ms\n",
            "iter 66: loss 0.0024, time 1441.71ms\n",
            "iter 67: loss 0.0020, time 1464.41ms\n",
            "iter 68: loss 0.0012, time 1430.64ms\n",
            "iter 69: loss 0.0266, time 1408.11ms\n",
            "iter 70: loss 0.0166, time 1454.19ms\n",
            "iter 71: loss 0.2412, time 1491.16ms\n",
            "iter 72: loss 2.2656, time 1520.62ms\n",
            "iter 73: loss 0.0089, time 1391.74ms\n",
            "iter 74: loss 0.0286, time 1432.96ms\n",
            "iter 75: loss 0.0923, time 1434.43ms\n",
            "iter 76: loss 0.0037, time 1398.47ms\n",
            "iter 77: loss 1.1797, time 1460.63ms\n",
            "iter 78: loss 0.1602, time 1474.54ms\n",
            "iter 79: loss 0.0010, time 1453.38ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80: train loss 0.5576, val loss 0.4569\n",
            "Validation accuracy: 0.9298\n",
            "iter 80: loss 0.0074, time 17463.95ms\n",
            "iter 81: loss 2.6406, time 1528.28ms\n",
            "iter 82: loss 0.0005, time 1472.21ms\n",
            "iter 83: loss 0.1094, time 1492.01ms\n",
            "iter 84: loss 0.0012, time 1451.80ms\n",
            "iter 85: loss 0.0028, time 1444.14ms\n",
            "iter 86: loss 0.0008, time 1503.63ms\n",
            "iter 87: loss 0.0508, time 1487.16ms\n",
            "iter 88: loss 0.0525, time 1485.53ms\n",
            "iter 89: loss 0.0608, time 1495.31ms\n",
            "iter 90: loss 0.0197, time 1461.84ms\n",
            "iter 91: loss 0.0005, time 1469.45ms\n",
            "iter 92: loss 0.0042, time 1453.56ms\n",
            "iter 93: loss 0.0060, time 1483.19ms\n",
            "iter 94: loss 0.0894, time 1420.59ms\n",
            "iter 95: loss 0.0034, time 1489.27ms\n",
            "iter 96: loss 0.0052, time 1411.04ms\n",
            "iter 97: loss 0.0435, time 1396.86ms\n",
            "iter 98: loss 0.0037, time 1428.92ms\n",
            "iter 99: loss 0.0054, time 1449.36ms\n",
            "Step 100: train loss 0.4675, val loss 0.4210\n",
            "Validation accuracy: 0.9687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5333\n",
            "iter 100: loss 0.3379, time 18136.94ms\n",
            "iter 101: loss 0.0030, time 1421.66ms\n",
            "iter 102: loss 0.0005, time 1423.41ms\n",
            "iter 103: loss 0.0001, time 1461.51ms\n",
            "iter 104: loss 0.0002, time 1377.89ms\n",
            "iter 105: loss 0.0003, time 1403.22ms\n",
            "iter 106: loss 0.0172, time 1434.12ms\n",
            "iter 107: loss 0.0250, time 1473.93ms\n",
            "iter 108: loss 0.0028, time 1381.95ms\n",
            "iter 109: loss 0.0952, time 1421.98ms\n",
            "iter 110: loss 0.7422, time 1461.49ms\n",
            "iter 111: loss 0.1768, time 1444.56ms\n",
            "iter 112: loss 0.0004, time 1755.19ms\n",
            "iter 113: loss 0.0038, time 1404.11ms\n",
            "iter 114: loss 0.0035, time 1384.02ms\n",
            "iter 115: loss 0.0118, time 1402.54ms\n",
            "iter 116: loss 0.0003, time 1405.31ms\n",
            "iter 117: loss 0.0143, time 1433.28ms\n",
            "iter 118: loss 0.0106, time 1433.99ms\n",
            "iter 119: loss 0.0014, time 1417.76ms\n",
            "Step 120: train loss 0.4052, val loss 0.4290\n",
            "Validation accuracy: 0.9719\n",
            "iter 120: loss 0.0066, time 16948.18ms\n",
            "iter 121: loss 0.0003, time 1484.32ms\n",
            "iter 122: loss 0.0630, time 1412.84ms\n",
            "iter 123: loss 0.2520, time 1413.54ms\n",
            "iter 124: loss 0.0002, time 1411.99ms\n",
            "iter 125: loss 0.0007, time 1412.18ms\n",
            "iter 126: loss 0.0020, time 1379.80ms\n",
            "iter 127: loss 0.0008, time 1367.28ms\n",
            "iter 128: loss 0.0002, time 1511.71ms\n",
            "iter 129: loss 0.0001, time 1430.75ms\n",
            "iter 130: loss 0.0000, time 1381.12ms\n",
            "iter 131: loss 0.0000, time 1410.11ms\n",
            "iter 132: loss 0.0000, time 1402.96ms\n",
            "iter 133: loss 0.0002, time 1362.37ms\n",
            "iter 134: loss 0.0001, time 1459.87ms\n",
            "iter 135: loss 0.0003, time 1393.50ms\n",
            "iter 136: loss 0.0003, time 1407.31ms\n",
            "iter 137: loss 0.0000, time 1380.69ms\n",
            "iter 138: loss 0.0688, time 1374.26ms\n",
            "iter 139: loss 0.0000, time 1387.66ms\n",
            "Step 140: train loss 0.3501, val loss 0.4059\n",
            "Validation accuracy: 0.9838\n",
            "iter 140: loss 0.0001, time 17266.96ms\n",
            "iter 141: loss 0.0277, time 1395.45ms\n",
            "iter 142: loss 0.0001, time 1393.05ms\n",
            "iter 143: loss 0.0004, time 1684.10ms\n",
            "iter 144: loss 0.0300, time 1373.97ms\n",
            "iter 145: loss 0.0000, time 1371.37ms\n",
            "iter 146: loss 0.0000, time 1392.30ms\n",
            "iter 147: loss 0.0000, time 1429.21ms\n",
            "iter 148: loss 0.0000, time 1381.47ms\n",
            "iter 149: loss 0.0001, time 1397.61ms\n",
            "iter 150: loss 0.0193, time 1390.64ms\n",
            "iter 151: loss 0.0000, time 1396.92ms\n",
            "iter 152: loss 0.0006, time 1407.69ms\n",
            "iter 153: loss 0.0000, time 1403.43ms\n",
            "iter 154: loss 0.0000, time 1457.68ms\n",
            "iter 155: loss 0.0001, time 1406.82ms\n",
            "iter 156: loss 0.0000, time 1433.38ms\n",
            "iter 157: loss 0.0075, time 1398.52ms\n",
            "iter 158: loss 0.0002, time 1397.35ms\n",
            "iter 159: loss 0.0000, time 1437.50ms\n",
            "Step 160: train loss 0.3108, val loss 0.3881\n",
            "Validation accuracy: 0.9816\n",
            "iter 160: loss 0.0000, time 17108.95ms\n",
            "iter 161: loss 0.0002, time 1392.47ms\n",
            "iter 162: loss 0.0001, time 1456.58ms\n",
            "iter 163: loss 0.0000, time 1443.20ms\n",
            "iter 164: loss 0.0000, time 1411.28ms\n",
            "iter 165: loss 0.0000, time 1441.66ms\n",
            "iter 166: loss 0.0000, time 1423.00ms\n",
            "iter 167: loss 0.0000, time 1424.47ms\n",
            "iter 168: loss 0.0000, time 1424.60ms\n",
            "iter 169: loss 0.0000, time 1543.45ms\n",
            "iter 170: loss 0.0020, time 1483.64ms\n",
            "iter 171: loss 0.0000, time 1378.29ms\n",
            "iter 172: loss 0.0000, time 1435.05ms\n",
            "iter 173: loss 0.0000, time 1373.25ms\n",
            "iter 174: loss 0.0001, time 1442.11ms\n",
            "iter 175: loss 0.0000, time 1451.51ms\n",
            "iter 176: loss 0.0000, time 1427.76ms\n",
            "iter 177: loss 0.0000, time 1417.69ms\n",
            "iter 178: loss 0.0479, time 1466.13ms\n",
            "iter 179: loss 0.0000, time 1407.28ms\n",
            "Step 180: train loss 0.2766, val loss 0.4097\n",
            "Validation accuracy: 0.9806\n",
            "iter 180: loss 0.0000, time 17563.91ms\n",
            "iter 181: loss 0.0000, time 1431.06ms\n",
            "iter 182: loss 0.0000, time 1417.44ms\n",
            "iter 183: loss 5.4375, time 1370.50ms\n",
            "iter 184: loss 0.0087, time 1392.62ms\n",
            "iter 185: loss 0.0000, time 1426.05ms\n",
            "iter 186: loss 0.0001, time 1433.09ms\n",
            "iter 187: loss 0.0000, time 1418.40ms\n",
            "iter 188: loss 0.0000, time 1386.44ms\n",
            "iter 189: loss 0.0000, time 1409.05ms\n",
            "iter 190: loss 0.0001, time 1410.32ms\n",
            "iter 191: loss 0.0000, time 1427.36ms\n",
            "iter 192: loss 0.0000, time 1459.71ms\n",
            "iter 193: loss 0.0000, time 1419.27ms\n",
            "iter 194: loss 0.0000, time 1491.44ms\n",
            "iter 195: loss 0.0000, time 1429.86ms\n",
            "iter 196: loss 0.0000, time 1436.61ms\n",
            "iter 197: loss 0.0000, time 1466.95ms\n",
            "iter 198: loss 0.0000, time 1428.72ms\n",
            "iter 199: loss 0.0002, time 1383.21ms\n",
            "Step 200: train loss 0.2503, val loss 0.4157\n",
            "Validation accuracy: 0.9719\n",
            "Test accuracy 0.6667\n",
            "iter 200: loss 0.0000, time 17916.60ms\n",
            "iter 201: loss 0.0007, time 1405.68ms\n",
            "iter 202: loss 0.0000, time 1454.80ms\n",
            "iter 203: loss 0.0001, time 1455.63ms\n",
            "iter 204: loss 0.0000, time 1409.01ms\n",
            "iter 205: loss 0.0000, time 1464.77ms\n",
            "iter 206: loss 0.0000, time 1445.38ms\n",
            "iter 207: loss 0.0000, time 1440.15ms\n",
            "iter 208: loss 0.0000, time 1395.44ms\n",
            "iter 209: loss 0.0002, time 1483.90ms\n",
            "iter 210: loss 0.0000, time 1439.97ms\n",
            "iter 211: loss 0.0000, time 1458.00ms\n",
            "iter 212: loss 0.0000, time 1430.82ms\n",
            "iter 213: loss 0.0000, time 1401.72ms\n",
            "iter 214: loss 0.0000, time 1436.29ms\n",
            "iter 215: loss 0.0000, time 1397.57ms\n",
            "iter 216: loss 0.0007, time 1368.70ms\n",
            "iter 217: loss 0.0000, time 1399.35ms\n",
            "iter 218: loss 0.0010, time 1478.41ms\n",
            "iter 219: loss 0.0001, time 1430.77ms\n",
            "Step 220: train loss 0.2286, val loss 0.4162\n",
            "Validation accuracy: 0.9752\n",
            "iter 220: loss 0.0008, time 17679.42ms\n",
            "iter 221: loss 0.0000, time 1448.90ms\n",
            "iter 222: loss 0.0001, time 1417.52ms\n",
            "iter 223: loss 0.0422, time 1436.09ms\n",
            "iter 224: loss 0.0000, time 1414.24ms\n",
            "iter 225: loss 0.0000, time 1453.96ms\n",
            "iter 226: loss 0.0000, time 1419.95ms\n",
            "iter 227: loss 0.0000, time 1408.36ms\n",
            "iter 228: loss 0.0000, time 1407.38ms\n",
            "iter 229: loss 0.0000, time 1387.39ms\n",
            "iter 230: loss 0.0008, time 1393.84ms\n",
            "iter 231: loss 0.0001, time 1424.68ms\n",
            "iter 232: loss 0.0000, time 1435.09ms\n",
            "iter 233: loss 0.0000, time 1503.09ms\n",
            "iter 234: loss 0.0003, time 1459.37ms\n",
            "iter 235: loss 0.0000, time 1384.53ms\n",
            "iter 236: loss 0.0000, time 1423.66ms\n",
            "iter 237: loss 0.0000, time 1390.52ms\n",
            "iter 238: loss 0.0000, time 1396.28ms\n",
            "iter 239: loss 0.0010, time 1424.60ms\n",
            "Step 240: train loss 0.2102, val loss 0.4209\n",
            "Validation accuracy: 0.9827\n",
            "iter 240: loss 0.0000, time 17195.42ms\n",
            "iter 241: loss 0.0000, time 1439.08ms\n",
            "iter 242: loss 0.0011, time 1358.03ms\n",
            "iter 243: loss 0.0000, time 1409.32ms\n",
            "iter 244: loss 0.0001, time 1413.90ms\n",
            "iter 245: loss 0.0000, time 1475.58ms\n",
            "iter 246: loss 0.0000, time 1391.11ms\n",
            "iter 247: loss 0.0000, time 1457.24ms\n",
            "iter 248: loss 0.0177, time 1422.80ms\n",
            "iter 249: loss 0.0000, time 1378.64ms\n",
            "iter 250: loss 0.0000, time 1447.92ms\n",
            "iter 251: loss 0.0000, time 1462.47ms\n",
            "iter 252: loss 0.0000, time 1439.88ms\n",
            "iter 253: loss 0.0000, time 1391.64ms\n",
            "iter 254: loss 0.0000, time 1381.61ms\n",
            "iter 255: loss 0.0066, time 1677.09ms\n",
            "iter 256: loss 0.0001, time 1443.71ms\n",
            "iter 257: loss 0.0000, time 1394.39ms\n",
            "iter 258: loss 0.0000, time 1427.62ms\n",
            "iter 259: loss 0.0000, time 1386.85ms\n",
            "Step 260: train loss 0.1950, val loss 0.4827\n",
            "Validation accuracy: 0.8920\n",
            "iter 260: loss 0.0000, time 17161.75ms\n",
            "iter 261: loss 0.0000, time 1440.68ms\n",
            "iter 262: loss 0.0000, time 1414.22ms\n",
            "iter 263: loss 0.0001, time 1383.57ms\n",
            "iter 264: loss 0.0000, time 1418.49ms\n",
            "iter 265: loss 0.0000, time 1443.89ms\n",
            "iter 266: loss 0.0000, time 1408.21ms\n",
            "iter 267: loss 0.0000, time 1370.87ms\n",
            "iter 268: loss 0.0000, time 1388.16ms\n",
            "iter 269: loss 0.0009, time 1444.67ms\n",
            "iter 270: loss 0.0000, time 1371.15ms\n",
            "iter 271: loss 0.0000, time 1388.65ms\n",
            "iter 272: loss 0.0000, time 1461.34ms\n",
            "iter 273: loss 0.0000, time 1422.76ms\n",
            "iter 274: loss 0.0008, time 1376.63ms\n",
            "iter 275: loss 0.0000, time 1464.92ms\n",
            "iter 276: loss 0.0000, time 1455.93ms\n",
            "iter 277: loss 0.0000, time 1501.95ms\n",
            "iter 278: loss 0.3242, time 1379.03ms\n",
            "iter 279: loss 0.0000, time 1433.44ms\n",
            "Step 280: train loss 0.1862, val loss 0.4553\n",
            "Validation accuracy: 0.9806\n",
            "iter 280: loss 0.0000, time 17117.86ms\n",
            "iter 281: loss 0.0000, time 1439.32ms\n",
            "iter 282: loss 0.0001, time 1425.71ms\n",
            "iter 283: loss 0.0000, time 1382.35ms\n",
            "iter 284: loss 0.0000, time 1445.54ms\n",
            "iter 285: loss 0.0000, time 1411.49ms\n",
            "iter 286: loss 0.0000, time 1448.94ms\n",
            "iter 287: loss 0.0001, time 1433.22ms\n",
            "iter 288: loss 0.0003, time 1383.23ms\n",
            "iter 289: loss 0.0000, time 1721.75ms\n",
            "iter 290: loss 0.0000, time 1389.36ms\n",
            "iter 291: loss 0.0000, time 1391.75ms\n",
            "iter 292: loss 0.0000, time 1391.69ms\n",
            "iter 293: loss 0.0000, time 1456.64ms\n",
            "iter 294: loss 0.0000, time 1458.61ms\n",
            "iter 295: loss 0.0000, time 1463.57ms\n",
            "iter 296: loss 0.0000, time 1455.90ms\n",
            "iter 297: loss 0.0000, time 1452.12ms\n",
            "iter 298: loss 0.0000, time 1422.20ms\n",
            "iter 299: loss 0.0000, time 1438.51ms\n",
            "Step 300: train loss 0.1747, val loss 0.4324\n",
            "Validation accuracy: 0.9838\n",
            "Test accuracy 0.6333\n",
            "iter 300: loss 0.0001, time 18281.35ms\n",
            "iter 301: loss 0.0000, time 1506.68ms\n",
            "iter 302: loss 0.0000, time 1452.89ms\n",
            "iter 303: loss 0.0000, time 1427.39ms\n",
            "iter 304: loss 0.0002, time 1436.47ms\n",
            "iter 305: loss 0.0001, time 1447.31ms\n",
            "iter 306: loss 0.0000, time 1430.02ms\n",
            "iter 307: loss 0.0001, time 1431.99ms\n",
            "iter 308: loss 0.0000, time 1413.71ms\n",
            "iter 309: loss 0.0000, time 1428.68ms\n",
            "iter 310: loss 0.0000, time 1437.30ms\n",
            "iter 311: loss 0.0000, time 1535.03ms\n",
            "iter 312: loss 0.0425, time 1461.40ms\n",
            "iter 313: loss 0.0698, time 1444.46ms\n",
            "iter 314: loss 0.0000, time 1439.05ms\n",
            "iter 315: loss 0.0008, time 1393.02ms\n",
            "iter 316: loss 0.0004, time 1468.96ms\n",
            "iter 317: loss 0.0002, time 1431.86ms\n",
            "iter 318: loss 0.0000, time 1535.17ms\n",
            "iter 319: loss 0.1006, time 1388.20ms\n",
            "Step 320: train loss 0.1662, val loss 0.4607\n",
            "Validation accuracy: 0.9762\n",
            "iter 320: loss 0.0002, time 17113.19ms\n",
            "iter 321: loss 0.0030, time 1430.35ms\n",
            "iter 322: loss 0.0005, time 1686.81ms\n",
            "iter 323: loss 0.0000, time 1493.40ms\n",
            "iter 324: loss 0.0000, time 1520.38ms\n",
            "iter 325: loss 0.0000, time 1434.39ms\n",
            "iter 326: loss 0.0000, time 1447.01ms\n",
            "iter 327: loss 0.0000, time 1427.30ms\n",
            "iter 328: loss 0.0000, time 1412.45ms\n",
            "iter 329: loss 0.0000, time 1442.05ms\n",
            "iter 330: loss 0.0000, time 1416.45ms\n",
            "iter 331: loss 0.0000, time 1509.35ms\n",
            "iter 332: loss 0.0000, time 1497.76ms\n",
            "iter 333: loss 0.0000, time 1424.86ms\n",
            "iter 334: loss 0.0000, time 1446.61ms\n",
            "iter 335: loss 0.0001, time 1454.80ms\n",
            "iter 336: loss 0.0000, time 1416.18ms\n",
            "iter 337: loss 0.0000, time 1447.78ms\n",
            "iter 338: loss 0.0000, time 1425.00ms\n",
            "iter 339: loss 0.0000, time 1381.38ms\n",
            "Step 340: train loss 0.1565, val loss 0.4462\n",
            "Validation accuracy: 0.9816\n",
            "iter 340: loss 0.0001, time 16931.40ms\n",
            "iter 341: loss 0.0000, time 1488.84ms\n",
            "iter 342: loss 0.0000, time 1500.98ms\n",
            "iter 343: loss 0.0002, time 1428.41ms\n",
            "iter 344: loss 0.0000, time 1426.92ms\n",
            "iter 345: loss 0.0000, time 1393.61ms\n",
            "iter 346: loss 0.0000, time 1460.20ms\n",
            "iter 347: loss 0.0000, time 1356.77ms\n",
            "iter 348: loss 0.0000, time 1411.36ms\n",
            "iter 349: loss 0.0000, time 1405.84ms\n",
            "iter 350: loss 0.0000, time 1411.45ms\n",
            "iter 351: loss 0.0000, time 1419.54ms\n",
            "iter 352: loss 0.0000, time 1424.51ms\n",
            "iter 353: loss 0.0000, time 1406.93ms\n",
            "iter 354: loss 0.0000, time 1413.12ms\n",
            "iter 355: loss 0.0012, time 1579.55ms\n",
            "iter 356: loss 0.0000, time 1472.86ms\n",
            "iter 357: loss 0.0000, time 1400.80ms\n",
            "iter 358: loss 0.0000, time 1412.27ms\n",
            "iter 359: loss 0.0000, time 1370.25ms\n",
            "Step 360: train loss 0.1478, val loss 0.4485\n",
            "Validation accuracy: 0.9838\n",
            "iter 360: loss 0.0000, time 17340.79ms\n",
            "iter 361: loss 0.0000, time 1409.61ms\n",
            "iter 362: loss 0.0000, time 1391.20ms\n",
            "iter 363: loss 0.0001, time 1441.16ms\n",
            "iter 364: loss 0.0000, time 1385.36ms\n",
            "iter 365: loss 0.0000, time 1501.56ms\n",
            "iter 366: loss 0.0000, time 1486.87ms\n",
            "iter 367: loss 0.0001, time 1428.48ms\n",
            "iter 368: loss 0.0000, time 1465.87ms\n",
            "iter 369: loss 0.0000, time 1421.26ms\n",
            "iter 370: loss 0.0000, time 1419.05ms\n",
            "iter 371: loss 0.0000, time 1447.65ms\n",
            "iter 372: loss 0.0000, time 1445.47ms\n",
            "iter 373: loss 0.0000, time 1455.50ms\n",
            "iter 374: loss 0.0000, time 1442.92ms\n",
            "iter 375: loss 0.0000, time 1369.90ms\n",
            "iter 376: loss 0.0000, time 1391.44ms\n",
            "iter 377: loss 0.0000, time 1490.56ms\n",
            "iter 378: loss 0.0000, time 1366.71ms\n",
            "iter 379: loss 0.0000, time 1403.10ms\n",
            "Step 380: train loss 0.1401, val loss 0.4253\n",
            "Validation accuracy: 0.9816\n",
            "iter 380: loss 0.0000, time 17161.37ms\n",
            "iter 381: loss 0.0000, time 1417.74ms\n",
            "iter 382: loss 0.0000, time 1442.43ms\n",
            "iter 383: loss 0.0000, time 1464.33ms\n",
            "iter 384: loss 0.0000, time 1374.86ms\n",
            "iter 385: loss 0.0000, time 1455.28ms\n",
            "iter 386: loss 0.0000, time 1421.91ms\n",
            "iter 387: loss 0.0000, time 1462.67ms\n",
            "iter 388: loss 0.0000, time 1421.55ms\n",
            "iter 389: loss 0.0000, time 1424.24ms\n",
            "iter 390: loss 0.0000, time 1389.68ms\n",
            "iter 391: loss 0.0000, time 1364.76ms\n",
            "iter 392: loss 0.0046, time 1446.75ms\n",
            "iter 393: loss 0.0008, time 1363.75ms\n",
            "iter 394: loss 0.0000, time 1415.91ms\n",
            "iter 395: loss 0.0000, time 1436.87ms\n",
            "iter 396: loss 0.0000, time 1492.56ms\n",
            "iter 397: loss 0.0000, time 1448.77ms\n",
            "iter 398: loss 0.0000, time 1388.96ms\n",
            "iter 399: loss 0.0000, time 1402.18ms\n",
            "Step 400: train loss 0.1335, val loss 0.4050\n",
            "Validation accuracy: 0.9827\n",
            "Test accuracy 0.6667\n",
            "iter 400: loss 0.0000, time 18132.20ms\n",
            "iter 401: loss 0.0004, time 1431.69ms\n",
            "iter 402: loss 0.0000, time 1413.74ms\n",
            "iter 403: loss 0.0000, time 1427.22ms\n",
            "iter 404: loss 0.0752, time 1341.39ms\n",
            "iter 405: loss 0.0000, time 1391.35ms\n",
            "iter 406: loss 0.0000, time 1436.19ms\n",
            "iter 407: loss 0.0000, time 1431.68ms\n",
            "iter 408: loss 0.0000, time 1407.14ms\n",
            "iter 409: loss 0.0000, time 1347.31ms\n",
            "iter 410: loss 0.0000, time 1403.05ms\n",
            "iter 411: loss 0.0000, time 1383.82ms\n",
            "iter 412: loss 0.0000, time 1437.96ms\n",
            "iter 413: loss 0.0000, time 1452.17ms\n",
            "iter 414: loss 0.0000, time 1447.44ms\n",
            "iter 415: loss 0.0000, time 1382.15ms\n",
            "iter 416: loss 0.0000, time 1389.93ms\n",
            "iter 417: loss 0.0000, time 1386.23ms\n",
            "iter 418: loss 0.6055, time 1392.71ms\n",
            "iter 419: loss 0.0000, time 1359.82ms\n",
            "Step 420: train loss 0.1279, val loss 0.4180\n",
            "Validation accuracy: 0.9676\n",
            "iter 420: loss 0.0000, time 16868.20ms\n",
            "iter 421: loss 0.0000, time 1374.95ms\n",
            "iter 422: loss 0.0576, time 1402.93ms\n",
            "iter 423: loss 0.0000, time 1411.50ms\n",
            "iter 424: loss 0.0000, time 1414.05ms\n",
            "iter 425: loss 0.0000, time 1358.86ms\n",
            "iter 426: loss 0.0000, time 1409.21ms\n",
            "iter 427: loss 0.0000, time 1411.27ms\n",
            "iter 428: loss 0.0000, time 1426.16ms\n",
            "iter 429: loss 0.0000, time 1472.10ms\n",
            "iter 430: loss 0.0000, time 1372.09ms\n",
            "iter 431: loss 0.0000, time 1396.18ms\n",
            "iter 432: loss 0.0000, time 1422.65ms\n",
            "iter 433: loss 0.0000, time 1414.73ms\n",
            "iter 434: loss 0.0000, time 1654.17ms\n",
            "iter 435: loss 0.0000, time 1403.85ms\n",
            "iter 436: loss 0.0000, time 1388.94ms\n",
            "iter 437: loss 0.0000, time 1411.85ms\n",
            "iter 438: loss 0.0000, time 1440.91ms\n",
            "iter 439: loss 0.0000, time 1453.17ms\n",
            "Step 440: train loss 0.1237, val loss 0.4134\n",
            "Validation accuracy: 0.9838\n",
            "iter 440: loss 0.0000, time 16894.76ms\n",
            "iter 441: loss 0.0000, time 1383.73ms\n",
            "iter 442: loss 0.0000, time 1445.75ms\n",
            "iter 443: loss 0.0000, time 1422.14ms\n",
            "iter 444: loss 0.0000, time 1382.44ms\n",
            "iter 445: loss 0.0007, time 1428.16ms\n",
            "iter 446: loss 0.0021, time 1402.29ms\n",
            "iter 447: loss 0.0028, time 1431.81ms\n",
            "iter 448: loss 0.0000, time 1380.43ms\n",
            "iter 449: loss 0.0000, time 1411.54ms\n",
            "iter 450: loss 0.0000, time 1394.06ms\n",
            "iter 451: loss 0.0030, time 1428.99ms\n",
            "iter 452: loss 0.0000, time 1384.11ms\n",
            "iter 453: loss 0.0000, time 1383.86ms\n",
            "iter 454: loss 0.0000, time 1415.58ms\n",
            "iter 455: loss 0.0000, time 1374.17ms\n",
            "iter 456: loss 0.0000, time 1423.44ms\n",
            "iter 457: loss 0.0000, time 1432.20ms\n",
            "iter 458: loss 0.0000, time 1391.32ms\n",
            "iter 459: loss 0.0000, time 1461.99ms\n",
            "Step 460: train loss 0.1187, val loss 0.4120\n",
            "Validation accuracy: 0.9816\n",
            "iter 460: loss 0.0000, time 16850.12ms\n",
            "iter 461: loss 0.0000, time 1369.89ms\n",
            "iter 462: loss 0.0000, time 1447.39ms\n",
            "iter 463: loss 0.0000, time 1407.44ms\n",
            "iter 464: loss 0.0000, time 1687.92ms\n",
            "iter 465: loss 0.0000, time 1447.00ms\n",
            "iter 466: loss 0.0000, time 1364.16ms\n",
            "iter 467: loss 0.0000, time 1437.81ms\n",
            "iter 468: loss 0.0000, time 1370.25ms\n",
            "iter 469: loss 0.0000, time 1401.81ms\n",
            "iter 470: loss 0.0000, time 1485.28ms\n",
            "iter 471: loss 0.0391, time 1410.18ms\n",
            "iter 472: loss 0.0000, time 1446.90ms\n",
            "iter 473: loss 0.0000, time 1417.16ms\n",
            "iter 474: loss 0.0000, time 1382.87ms\n",
            "iter 475: loss 0.0000, time 1453.33ms\n",
            "iter 476: loss 0.0000, time 1479.14ms\n",
            "iter 477: loss 0.0000, time 1418.95ms\n",
            "iter 478: loss 0.0004, time 1395.44ms\n",
            "iter 479: loss 0.0000, time 1470.52ms\n",
            "Step 480: train loss 0.1140, val loss 0.4220\n",
            "Validation accuracy: 0.9860\n",
            "iter 480: loss 0.0000, time 16899.93ms\n",
            "iter 481: loss 0.0000, time 1395.77ms\n",
            "iter 482: loss 0.0000, time 1475.79ms\n",
            "iter 483: loss 0.0000, time 1424.32ms\n",
            "iter 484: loss 0.0000, time 1485.62ms\n",
            "iter 485: loss 0.0000, time 1410.62ms\n",
            "iter 486: loss 0.0000, time 1417.78ms\n",
            "iter 487: loss 0.0000, time 1404.62ms\n",
            "iter 488: loss 0.0000, time 1379.28ms\n",
            "iter 489: loss 0.0012, time 1425.41ms\n",
            "iter 490: loss 0.0000, time 1433.20ms\n",
            "iter 491: loss 0.0000, time 1374.38ms\n",
            "iter 492: loss 0.0000, time 1429.19ms\n",
            "iter 493: loss 0.0000, time 1411.87ms\n",
            "iter 494: loss 0.0000, time 1456.86ms\n",
            "iter 495: loss 0.0000, time 1410.18ms\n",
            "iter 496: loss 0.0000, time 1460.72ms\n",
            "iter 497: loss 0.0000, time 1481.60ms\n",
            "iter 498: loss 0.0000, time 1492.50ms\n",
            "iter 499: loss 0.0000, time 1418.79ms\n",
            "Step 500: train loss 0.1095, val loss 0.4152\n",
            "Validation accuracy: 0.9838\n",
            "Test accuracy 0.6333\n",
            "iter 500: loss 0.0000, time 17977.91ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▂█▇█▇</td></tr><tr><td>Test_F1_Score</td><td>▁▂█▆█▆</td></tr><tr><td>Test_Precision</td><td>▁▂█▇█▇</td></tr><tr><td>Test_Recall</td><td>▁▂█▇█▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▇▇████████▇████████████</td></tr><tr><td>val/loss</td><td> █▇▅▃▂▂▂▁▂▂▂▂▄▃▂▃▃▃▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.63333</td></tr><tr><td>Test_F1_Score</td><td>0.60009</td></tr><tr><td>Test_Precision</td><td>0.67908</td></tr><tr><td>Test_Recall</td><td>0.63333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.1095</td></tr><tr><td>val/acc</td><td>0.9838</td></tr><tr><td>val/loss</td><td>0.41523</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_1856</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/rm4ii3ki</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_185638-rm4ii3ki\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j1vnthpe with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_191742-j1vnthpe</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe' target=\"_blank\">ruby-sweep-4</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1917\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ruby-sweep-4</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_191742-j1vnthpe\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_191747-j1vnthpe</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe' target=\"_blank\">gpt2_hyper_1917</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.2343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n",
            "iter 0: loss 8.2500, time 18164.15ms\n",
            "iter 1: loss 3.2969, time 1460.04ms\n",
            "iter 2: loss 1.4141, time 1454.80ms\n",
            "iter 3: loss 0.4766, time 1471.21ms\n",
            "iter 4: loss 2.1250, time 1436.12ms\n",
            "iter 5: loss 0.5391, time 1439.52ms\n",
            "iter 6: loss 6.7500, time 1415.85ms\n",
            "iter 7: loss 3.9687, time 1456.33ms\n",
            "iter 8: loss 1.5391, time 1397.75ms\n",
            "iter 9: loss 0.1680, time 1404.72ms\n",
            "iter 10: loss 0.7969, time 1446.24ms\n",
            "iter 11: loss 0.5391, time 1452.13ms\n",
            "iter 12: loss 0.3555, time 1488.22ms\n",
            "iter 13: loss 0.1641, time 1467.12ms\n",
            "iter 14: loss 1.3828, time 1467.03ms\n",
            "iter 15: loss 1.1016, time 1438.13ms\n",
            "iter 16: loss 0.5469, time 1464.61ms\n",
            "iter 17: loss 0.4160, time 1495.05ms\n",
            "iter 18: loss 0.2734, time 1429.59ms\n",
            "iter 19: loss 0.2305, time 1418.58ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 1.3842, val loss 1.3089\n",
            "Validation accuracy: 0.5626\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.2236, time 18627.84ms\n",
            "iter 21: loss 0.2656, time 1426.12ms\n",
            "iter 22: loss 0.7852, time 1449.35ms\n",
            "iter 23: loss 0.0781, time 1432.93ms\n",
            "iter 24: loss 1.0859, time 1712.76ms\n",
            "iter 25: loss 0.7461, time 1423.41ms\n",
            "iter 26: loss 0.3438, time 1443.01ms\n",
            "iter 27: loss 0.3145, time 1463.16ms\n",
            "iter 28: loss 0.0693, time 1496.78ms\n",
            "iter 29: loss 0.0143, time 1522.78ms\n",
            "iter 30: loss 0.0359, time 1434.17ms\n",
            "iter 31: loss 0.1167, time 1480.93ms\n",
            "iter 32: loss 0.5742, time 1399.39ms\n",
            "iter 33: loss 0.0396, time 1439.94ms\n",
            "iter 34: loss 4.0313, time 1439.91ms\n",
            "iter 35: loss 0.0300, time 1403.11ms\n",
            "iter 36: loss 5.4375, time 1416.64ms\n",
            "iter 37: loss 0.0022, time 1420.92ms\n",
            "iter 38: loss 1.2891, time 1417.52ms\n",
            "iter 39: loss 0.8320, time 1393.54ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.9649, val loss 0.9010\n",
            "Validation accuracy: 0.8780\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.1602, time 18711.75ms\n",
            "iter 41: loss 0.0322, time 1432.19ms\n",
            "iter 42: loss 0.5898, time 1447.52ms\n",
            "iter 43: loss 0.1416, time 1429.62ms\n",
            "iter 44: loss 0.0381, time 1446.71ms\n",
            "iter 45: loss 3.8594, time 1522.89ms\n",
            "iter 46: loss 0.4473, time 1445.07ms\n",
            "iter 47: loss 0.3125, time 1411.24ms\n",
            "iter 48: loss 0.2129, time 1499.15ms\n",
            "iter 49: loss 0.1387, time 1442.52ms\n",
            "iter 50: loss 0.0625, time 1436.79ms\n",
            "iter 51: loss 0.6328, time 1435.87ms\n",
            "iter 52: loss 0.0016, time 1434.89ms\n",
            "iter 53: loss 6.4375, time 1475.76ms\n",
            "iter 54: loss 0.0007, time 1456.25ms\n",
            "iter 55: loss 0.1187, time 1476.37ms\n",
            "iter 56: loss 0.0084, time 1417.77ms\n",
            "iter 57: loss 0.0095, time 1476.80ms\n",
            "iter 58: loss 0.2441, time 1436.56ms\n",
            "iter 59: loss 0.0052, time 1443.47ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.7359, val loss 0.7286\n",
            "Validation accuracy: 0.9298\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.0010, time 19199.53ms\n",
            "iter 61: loss 0.2813, time 1420.99ms\n",
            "iter 62: loss 0.0024, time 1439.44ms\n",
            "iter 63: loss 0.0140, time 1443.15ms\n",
            "iter 64: loss 0.0106, time 1508.33ms\n",
            "iter 65: loss 0.0200, time 1449.14ms\n",
            "iter 66: loss 0.0032, time 1420.92ms\n",
            "iter 67: loss 0.0002, time 1460.52ms\n",
            "iter 68: loss 0.0001, time 1420.86ms\n",
            "iter 69: loss 0.0186, time 1467.63ms\n",
            "iter 70: loss 0.0376, time 1471.95ms\n",
            "iter 71: loss 0.6992, time 1424.37ms\n",
            "iter 72: loss 0.1455, time 1510.35ms\n",
            "iter 73: loss 0.0143, time 1454.94ms\n",
            "iter 74: loss 0.0525, time 1369.92ms\n",
            "iter 75: loss 0.0205, time 1506.94ms\n",
            "iter 76: loss 0.0125, time 1398.59ms\n",
            "iter 77: loss 3.3594, time 1412.43ms\n",
            "iter 78: loss 0.7266, time 1483.98ms\n",
            "iter 79: loss 0.0006, time 1473.83ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80: train loss 0.6043, val loss 0.6734\n",
            "Validation accuracy: 0.9093\n",
            "iter 80: loss 0.0093, time 17054.32ms\n",
            "iter 81: loss 3.1719, time 1491.55ms\n",
            "iter 82: loss 0.0121, time 1405.71ms\n",
            "iter 83: loss 0.0057, time 1492.08ms\n",
            "iter 84: loss 0.0001, time 1498.23ms\n",
            "iter 85: loss 0.0869, time 1436.70ms\n",
            "iter 86: loss 0.0000, time 1481.96ms\n",
            "iter 87: loss 0.0200, time 1461.36ms\n",
            "iter 88: loss 0.0815, time 1462.59ms\n",
            "iter 89: loss 0.0374, time 1422.77ms\n",
            "iter 90: loss 0.0081, time 1434.75ms\n",
            "iter 91: loss 0.0034, time 1435.31ms\n",
            "iter 92: loss 0.0079, time 1476.10ms\n",
            "iter 93: loss 0.0079, time 1497.43ms\n",
            "iter 94: loss 0.0248, time 1433.99ms\n",
            "iter 95: loss 0.0337, time 1481.08ms\n",
            "iter 96: loss 0.1396, time 1482.82ms\n",
            "iter 97: loss 0.2695, time 1467.52ms\n",
            "iter 98: loss 0.0062, time 1412.90ms\n",
            "iter 99: loss 0.0063, time 1497.92ms\n",
            "Step 100: train loss 0.5094, val loss 0.6766\n",
            "Validation accuracy: 0.9708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6000\n",
            "iter 100: loss 0.5820, time 17981.88ms\n",
            "iter 101: loss 0.0002, time 1357.26ms\n",
            "iter 102: loss 0.0001, time 1428.60ms\n",
            "iter 103: loss 0.0000, time 1393.83ms\n",
            "iter 104: loss 0.0000, time 1401.10ms\n",
            "iter 105: loss 0.0037, time 1348.99ms\n",
            "iter 106: loss 0.0000, time 1419.97ms\n",
            "iter 107: loss 0.0023, time 1414.36ms\n",
            "iter 108: loss 0.0019, time 1421.87ms\n",
            "iter 109: loss 0.0009, time 1421.37ms\n",
            "iter 110: loss 0.0010, time 1410.28ms\n",
            "iter 111: loss 0.1621, time 1393.44ms\n",
            "iter 112: loss 0.0000, time 1417.08ms\n",
            "iter 113: loss 0.0000, time 1411.12ms\n",
            "iter 114: loss 0.0001, time 1371.21ms\n",
            "iter 115: loss 0.0034, time 1401.11ms\n",
            "iter 116: loss 0.0002, time 1371.87ms\n",
            "iter 117: loss 0.4551, time 1351.31ms\n",
            "iter 118: loss 0.0157, time 1405.64ms\n",
            "iter 119: loss 0.0039, time 1437.76ms\n",
            "Step 120: train loss 0.4380, val loss 0.6695\n",
            "Validation accuracy: 0.9816\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 120: loss 0.0101, time 18527.46ms\n",
            "iter 121: loss 0.0003, time 1504.99ms\n",
            "iter 122: loss 0.0022, time 1429.73ms\n",
            "iter 123: loss 0.0002, time 1421.76ms\n",
            "iter 124: loss 0.0000, time 1418.40ms\n",
            "iter 125: loss 0.0001, time 1446.75ms\n",
            "iter 126: loss 0.0001, time 1410.28ms\n",
            "iter 127: loss 0.0027, time 1433.56ms\n",
            "iter 128: loss 0.0002, time 1410.97ms\n",
            "iter 129: loss 0.0000, time 1394.19ms\n",
            "iter 130: loss 0.0000, time 1512.62ms\n",
            "iter 131: loss 0.0017, time 1411.87ms\n",
            "iter 132: loss 0.0002, time 1402.64ms\n",
            "iter 133: loss 0.0000, time 1431.92ms\n",
            "iter 134: loss 0.0000, time 1465.13ms\n",
            "iter 135: loss 0.0115, time 1453.00ms\n",
            "iter 136: loss 0.0000, time 1396.77ms\n",
            "iter 137: loss 0.0007, time 1424.77ms\n",
            "iter 138: loss 0.0000, time 1747.54ms\n",
            "iter 139: loss 0.0000, time 1451.01ms\n",
            "Step 140: train loss 0.3794, val loss 0.6409\n",
            "Validation accuracy: 0.9806\n",
            "iter 140: loss 0.0001, time 16713.29ms\n",
            "iter 141: loss 0.0001, time 1401.65ms\n",
            "iter 142: loss 0.0000, time 1427.93ms\n",
            "iter 143: loss 0.0000, time 1416.68ms\n",
            "iter 144: loss 0.0002, time 1377.62ms\n",
            "iter 145: loss 0.0000, time 1376.01ms\n",
            "iter 146: loss 0.0000, time 1367.33ms\n",
            "iter 147: loss 0.0000, time 1455.11ms\n",
            "iter 148: loss 0.0000, time 1384.04ms\n",
            "iter 149: loss 0.0000, time 1354.17ms\n",
            "iter 150: loss 0.0415, time 1382.64ms\n",
            "iter 151: loss 0.0000, time 1387.99ms\n",
            "iter 152: loss 0.0000, time 1391.69ms\n",
            "iter 153: loss 0.0000, time 1366.72ms\n",
            "iter 154: loss 0.0000, time 1431.06ms\n",
            "iter 155: loss 0.0000, time 1395.23ms\n",
            "iter 156: loss 0.0000, time 1437.45ms\n",
            "iter 157: loss 0.0001, time 1436.49ms\n",
            "iter 158: loss 0.0000, time 1419.36ms\n",
            "iter 159: loss 0.0001, time 1387.87ms\n",
            "Step 160: train loss 0.3324, val loss 0.6436\n",
            "Validation accuracy: 0.9827\n",
            "iter 160: loss 0.0000, time 16846.05ms\n",
            "iter 161: loss 0.0001, time 1429.77ms\n",
            "iter 162: loss 0.0000, time 1454.55ms\n",
            "iter 163: loss 0.0000, time 1428.22ms\n",
            "iter 164: loss 0.0000, time 1429.89ms\n",
            "iter 165: loss 0.0053, time 1396.86ms\n",
            "iter 166: loss 0.0000, time 1380.74ms\n",
            "iter 167: loss 0.0000, time 1387.79ms\n",
            "iter 168: loss 0.0000, time 1397.34ms\n",
            "iter 169: loss 0.0000, time 1437.20ms\n",
            "iter 170: loss 0.0024, time 1668.66ms\n",
            "iter 171: loss 0.0000, time 1441.37ms\n",
            "iter 172: loss 0.0000, time 1372.30ms\n",
            "iter 173: loss 0.0000, time 1395.57ms\n",
            "iter 174: loss 0.0103, time 1405.21ms\n",
            "iter 175: loss 0.0000, time 1415.75ms\n",
            "iter 176: loss 0.0000, time 1452.86ms\n",
            "iter 177: loss 0.0000, time 1393.45ms\n",
            "iter 178: loss 0.0020, time 1399.92ms\n",
            "iter 179: loss 0.0000, time 1415.90ms\n",
            "Step 180: train loss 0.3007, val loss 0.6725\n",
            "Validation accuracy: 0.9600\n",
            "iter 180: loss 0.0000, time 16697.86ms\n",
            "iter 181: loss 0.0000, time 1423.57ms\n",
            "iter 182: loss 0.0000, time 1379.97ms\n",
            "iter 183: loss 0.0000, time 1426.20ms\n",
            "iter 184: loss 0.0001, time 1360.28ms\n",
            "iter 185: loss 0.0000, time 1377.30ms\n",
            "iter 186: loss 0.0003, time 1418.95ms\n",
            "iter 187: loss 0.0000, time 1430.55ms\n",
            "iter 188: loss 0.0001, time 1418.03ms\n",
            "iter 189: loss 0.0001, time 1384.73ms\n",
            "iter 190: loss 0.0000, time 1372.06ms\n",
            "iter 191: loss 0.0000, time 1429.67ms\n",
            "iter 192: loss 0.0000, time 1406.12ms\n",
            "iter 193: loss 0.0003, time 1418.53ms\n",
            "iter 194: loss 0.0000, time 1406.67ms\n",
            "iter 195: loss 0.0000, time 1399.29ms\n",
            "iter 196: loss 0.0000, time 1367.73ms\n",
            "iter 197: loss 0.0000, time 1458.27ms\n",
            "iter 198: loss 0.0000, time 1448.04ms\n",
            "iter 199: loss 0.0017, time 1431.67ms\n",
            "Step 200: train loss 0.2753, val loss 0.6565\n",
            "Validation accuracy: 0.9806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6000\n",
            "iter 200: loss 0.0011, time 17527.38ms\n",
            "iter 201: loss 0.0000, time 1445.62ms\n",
            "iter 202: loss 0.0000, time 1752.45ms\n",
            "iter 203: loss 0.0000, time 1428.66ms\n",
            "iter 204: loss 0.0001, time 1406.16ms\n",
            "iter 205: loss 0.0000, time 1419.97ms\n",
            "iter 206: loss 0.0000, time 1435.76ms\n",
            "iter 207: loss 0.0000, time 1438.64ms\n",
            "iter 208: loss 0.0000, time 1487.05ms\n",
            "iter 209: loss 0.0000, time 1506.32ms\n",
            "iter 210: loss 0.0001, time 1439.95ms\n",
            "iter 211: loss 0.0000, time 1404.15ms\n",
            "iter 212: loss 0.0000, time 1416.23ms\n",
            "iter 213: loss 0.0000, time 1420.41ms\n",
            "iter 214: loss 0.0000, time 1387.54ms\n",
            "iter 215: loss 0.0000, time 1414.89ms\n",
            "iter 216: loss 0.1377, time 1400.65ms\n",
            "iter 217: loss 0.0000, time 1407.97ms\n",
            "iter 218: loss 0.0003, time 1468.56ms\n",
            "iter 219: loss 0.0000, time 1492.53ms\n",
            "Step 220: train loss 0.2520, val loss 0.6600\n",
            "Validation accuracy: 0.9827\n",
            "iter 220: loss 0.0000, time 16858.69ms\n",
            "iter 221: loss 0.0000, time 1460.70ms\n",
            "iter 222: loss 0.0002, time 1433.23ms\n",
            "iter 223: loss 0.0060, time 1391.11ms\n",
            "iter 224: loss 0.0001, time 1427.86ms\n",
            "iter 225: loss 0.0000, time 1444.85ms\n",
            "iter 226: loss 0.0000, time 1407.01ms\n",
            "iter 227: loss 0.0000, time 1464.63ms\n",
            "iter 228: loss 0.0000, time 1416.33ms\n",
            "iter 229: loss 0.0000, time 1452.36ms\n",
            "iter 230: loss 0.0000, time 1426.15ms\n",
            "iter 231: loss 0.0000, time 1442.14ms\n",
            "iter 232: loss 0.0000, time 1392.10ms\n",
            "iter 233: loss 0.0051, time 1385.91ms\n",
            "iter 234: loss 0.0002, time 1358.83ms\n",
            "iter 235: loss 0.0000, time 1373.71ms\n",
            "iter 236: loss 0.0000, time 1420.14ms\n",
            "iter 237: loss 0.0000, time 1393.48ms\n",
            "iter 238: loss 0.0008, time 1363.29ms\n",
            "iter 239: loss 1.0781, time 1427.68ms\n",
            "Step 240: train loss 0.2354, val loss 0.6382\n",
            "Validation accuracy: 0.9438\n",
            "iter 240: loss 0.0000, time 17011.48ms\n",
            "iter 241: loss 0.0000, time 1409.16ms\n",
            "iter 242: loss 0.0004, time 1404.83ms\n",
            "iter 243: loss 0.0003, time 1419.01ms\n",
            "iter 244: loss 0.0022, time 1342.84ms\n",
            "iter 245: loss 0.0023, time 1372.91ms\n",
            "iter 246: loss 0.0002, time 1415.08ms\n",
            "iter 247: loss 0.0000, time 1400.54ms\n",
            "iter 248: loss 0.0001, time 1436.22ms\n",
            "iter 249: loss 0.0001, time 1441.64ms\n",
            "iter 250: loss 0.0000, time 1473.39ms\n",
            "iter 251: loss 0.0000, time 1384.54ms\n",
            "iter 252: loss 0.0000, time 1399.82ms\n",
            "iter 253: loss 0.0059, time 1428.94ms\n",
            "iter 254: loss 0.0010, time 1416.23ms\n",
            "iter 255: loss 0.0195, time 1393.15ms\n",
            "iter 256: loss 0.0001, time 1404.23ms\n",
            "iter 257: loss 0.0004, time 1390.59ms\n",
            "iter 258: loss 0.0034, time 1428.84ms\n",
            "iter 259: loss 0.0001, time 1393.34ms\n",
            "Step 260: train loss 0.2204, val loss 0.6680\n",
            "Validation accuracy: 0.9784\n",
            "iter 260: loss 0.0000, time 16657.50ms\n",
            "iter 261: loss 0.0000, time 1399.50ms\n",
            "iter 262: loss 0.0000, time 1436.99ms\n",
            "iter 263: loss 0.0000, time 1373.77ms\n",
            "iter 264: loss 0.0001, time 1398.59ms\n",
            "iter 265: loss 0.0000, time 1433.69ms\n",
            "iter 266: loss 0.0002, time 1423.96ms\n",
            "iter 267: loss 0.0000, time 1419.48ms\n",
            "iter 268: loss 0.0002, time 1387.86ms\n",
            "iter 269: loss 0.0036, time 1442.00ms\n",
            "iter 270: loss 0.0000, time 1469.31ms\n",
            "iter 271: loss 0.0000, time 1532.74ms\n",
            "iter 272: loss 0.0000, time 1497.58ms\n",
            "iter 273: loss 0.0001, time 1466.02ms\n",
            "iter 274: loss 0.0000, time 1463.43ms\n",
            "iter 275: loss 0.0000, time 1420.50ms\n",
            "iter 276: loss 0.0000, time 1431.27ms\n",
            "iter 277: loss 0.0000, time 1443.96ms\n",
            "iter 278: loss 0.0031, time 1432.59ms\n",
            "iter 279: loss 0.0000, time 1737.35ms\n",
            "Step 280: train loss 0.2048, val loss 0.6641\n",
            "Validation accuracy: 0.9870\n",
            "iter 280: loss 0.0000, time 16819.75ms\n",
            "iter 281: loss 0.0000, time 1417.43ms\n",
            "iter 282: loss 0.0000, time 1477.16ms\n",
            "iter 283: loss 0.0000, time 1403.52ms\n",
            "iter 284: loss 0.0000, time 1478.12ms\n",
            "iter 285: loss 0.0000, time 1503.40ms\n",
            "iter 286: loss 0.0000, time 1384.87ms\n",
            "iter 287: loss 0.0000, time 1417.08ms\n",
            "iter 288: loss 0.0001, time 1386.01ms\n",
            "iter 289: loss 0.0000, time 1405.35ms\n",
            "iter 290: loss 0.0000, time 1446.04ms\n",
            "iter 291: loss 0.0000, time 1460.88ms\n",
            "iter 292: loss 0.0000, time 1400.66ms\n",
            "iter 293: loss 2.6562, time 1498.47ms\n",
            "iter 294: loss 0.0000, time 1416.41ms\n",
            "iter 295: loss 0.0001, time 1403.27ms\n",
            "iter 296: loss 0.0000, time 1408.47ms\n",
            "iter 297: loss 0.0000, time 1458.04ms\n",
            "iter 298: loss 0.0000, time 1377.05ms\n",
            "iter 299: loss 0.0000, time 1433.06ms\n",
            "Step 300: train loss 0.1917, val loss 0.6740\n",
            "Validation accuracy: 0.9806\n",
            "Test accuracy 0.6667\n",
            "iter 300: loss 0.0000, time 21764.07ms\n",
            "iter 301: loss 0.0000, time 1463.08ms\n",
            "iter 302: loss 0.0000, time 1436.46ms\n",
            "iter 303: loss 0.0001, time 1384.47ms\n",
            "iter 304: loss 0.0011, time 1440.52ms\n",
            "iter 305: loss 0.0000, time 1411.21ms\n",
            "iter 306: loss 0.0001, time 1408.55ms\n",
            "iter 307: loss 0.0001, time 1442.54ms\n",
            "iter 308: loss 0.0000, time 1408.73ms\n",
            "iter 309: loss 0.0000, time 1412.38ms\n",
            "iter 310: loss 0.0000, time 1399.98ms\n",
            "iter 311: loss 0.0000, time 1772.52ms\n",
            "iter 312: loss 0.0000, time 1411.42ms\n",
            "iter 313: loss 0.0000, time 1421.42ms\n",
            "iter 314: loss 0.0004, time 1439.64ms\n",
            "iter 315: loss 0.0000, time 1356.37ms\n",
            "iter 316: loss 0.0000, time 1432.19ms\n",
            "iter 317: loss 0.0000, time 1471.58ms\n",
            "iter 318: loss 0.0000, time 1469.14ms\n",
            "iter 319: loss 0.0000, time 1462.48ms\n",
            "Step 320: train loss 0.1799, val loss 0.6773\n",
            "Validation accuracy: 0.9860\n",
            "iter 320: loss 0.0000, time 16787.22ms\n",
            "iter 321: loss 0.0016, time 1400.04ms\n",
            "iter 322: loss 0.0000, time 1392.12ms\n",
            "iter 323: loss 0.0000, time 1379.64ms\n",
            "iter 324: loss 0.0000, time 1376.17ms\n",
            "iter 325: loss 0.0000, time 1409.17ms\n",
            "iter 326: loss 0.0001, time 1366.16ms\n",
            "iter 327: loss 0.0000, time 1432.01ms\n",
            "iter 328: loss 0.0000, time 1449.12ms\n",
            "iter 329: loss 0.0075, time 1371.99ms\n",
            "iter 330: loss 0.0000, time 1413.09ms\n",
            "iter 331: loss 0.0002, time 1400.65ms\n",
            "iter 332: loss 0.0000, time 1398.24ms\n",
            "iter 333: loss 0.0003, time 1440.82ms\n",
            "iter 334: loss 0.0000, time 1414.32ms\n",
            "iter 335: loss 0.0000, time 1404.19ms\n",
            "iter 336: loss 0.0000, time 1362.59ms\n",
            "iter 337: loss 0.0000, time 1449.90ms\n",
            "iter 338: loss 0.0137, time 1420.13ms\n",
            "iter 339: loss 0.0000, time 1396.26ms\n",
            "Step 340: train loss 0.1703, val loss 0.6847\n",
            "Validation accuracy: 0.9870\n",
            "iter 340: loss 0.0000, time 16730.84ms\n",
            "iter 341: loss 0.0000, time 1382.89ms\n",
            "iter 342: loss 0.0000, time 1686.36ms\n",
            "iter 343: loss 0.0009, time 1393.25ms\n",
            "iter 344: loss 0.0000, time 1398.80ms\n",
            "iter 345: loss 0.0000, time 1402.35ms\n",
            "iter 346: loss 0.0000, time 1410.78ms\n",
            "iter 347: loss 0.0004, time 1376.49ms\n",
            "iter 348: loss 0.0000, time 1371.46ms\n",
            "iter 349: loss 0.0000, time 1400.50ms\n",
            "iter 350: loss 0.0000, time 1478.31ms\n",
            "iter 351: loss 0.0000, time 1394.52ms\n",
            "iter 352: loss 0.0000, time 1411.48ms\n",
            "iter 353: loss 0.0000, time 1452.79ms\n",
            "iter 354: loss 0.0000, time 1392.40ms\n",
            "iter 355: loss 0.0023, time 1399.77ms\n",
            "iter 356: loss 0.0000, time 1454.75ms\n",
            "iter 357: loss 0.0000, time 1390.15ms\n",
            "iter 358: loss 0.0000, time 1453.77ms\n",
            "iter 359: loss 0.0000, time 1448.74ms\n",
            "Step 360: train loss 0.1611, val loss 0.6832\n",
            "Validation accuracy: 0.9838\n",
            "iter 360: loss 0.0000, time 16857.38ms\n",
            "iter 361: loss 0.0000, time 1397.51ms\n",
            "iter 362: loss 0.0000, time 1452.16ms\n",
            "iter 363: loss 0.0000, time 1442.97ms\n",
            "iter 364: loss 0.0000, time 1415.22ms\n",
            "iter 365: loss 0.0000, time 1493.12ms\n",
            "iter 366: loss 0.0000, time 1370.58ms\n",
            "iter 367: loss 0.0000, time 1376.22ms\n",
            "iter 368: loss 0.0000, time 1460.68ms\n",
            "iter 369: loss 0.0000, time 1419.65ms\n",
            "iter 370: loss 0.0000, time 1479.25ms\n",
            "iter 371: loss 0.0000, time 1438.14ms\n",
            "iter 372: loss 0.0000, time 1410.12ms\n",
            "iter 373: loss 0.0000, time 1444.89ms\n",
            "iter 374: loss 0.0000, time 1404.68ms\n",
            "iter 375: loss 0.0000, time 1485.10ms\n",
            "iter 376: loss 0.0000, time 1457.80ms\n",
            "iter 377: loss 0.0000, time 1503.28ms\n",
            "iter 378: loss 0.0000, time 1397.06ms\n",
            "iter 379: loss 0.0000, time 1334.09ms\n",
            "Step 380: train loss 0.1528, val loss 0.6831\n",
            "Validation accuracy: 0.9719\n",
            "iter 380: loss 0.0000, time 17017.08ms\n",
            "iter 381: loss 0.0000, time 1362.68ms\n",
            "iter 382: loss 0.0000, time 1390.94ms\n",
            "iter 383: loss 0.0000, time 1440.29ms\n",
            "iter 384: loss 0.0000, time 1343.00ms\n",
            "iter 385: loss 0.0000, time 1443.71ms\n",
            "iter 386: loss 0.0000, time 1347.50ms\n",
            "iter 387: loss 0.0000, time 1411.03ms\n",
            "iter 388: loss 0.0000, time 1384.84ms\n",
            "iter 389: loss 0.0000, time 1362.86ms\n",
            "iter 390: loss 0.0000, time 1425.26ms\n",
            "iter 391: loss 0.0000, time 1445.79ms\n",
            "iter 392: loss 0.0000, time 1404.11ms\n",
            "iter 393: loss 0.0000, time 1402.61ms\n",
            "iter 394: loss 0.0000, time 1458.91ms\n",
            "iter 395: loss 0.0000, time 1391.72ms\n",
            "iter 396: loss 0.0003, time 1415.50ms\n",
            "iter 397: loss 0.0000, time 1399.08ms\n",
            "iter 398: loss 0.0000, time 1351.02ms\n",
            "iter 399: loss 0.0000, time 1418.46ms\n",
            "Step 400: train loss 0.1456, val loss 0.6899\n",
            "Validation accuracy: 0.9816\n",
            "Test accuracy 0.6333\n",
            "iter 400: loss 0.0000, time 17394.89ms\n",
            "iter 401: loss 0.0000, time 1407.91ms\n",
            "iter 402: loss 0.0183, time 1461.92ms\n",
            "iter 403: loss 0.0000, time 1387.18ms\n",
            "iter 404: loss 0.0003, time 1369.23ms\n",
            "iter 405: loss 0.0000, time 1378.38ms\n",
            "iter 406: loss 0.0000, time 1396.01ms\n",
            "iter 407: loss 0.0000, time 1394.81ms\n",
            "iter 408: loss 0.0000, time 1386.50ms\n",
            "iter 409: loss 0.0000, time 1408.62ms\n",
            "iter 410: loss 0.0000, time 1427.46ms\n",
            "iter 411: loss 0.0000, time 1373.35ms\n",
            "iter 412: loss 0.0000, time 1417.99ms\n",
            "iter 413: loss 0.0000, time 1358.12ms\n",
            "iter 414: loss 0.0000, time 1389.69ms\n",
            "iter 415: loss 0.0000, time 1436.91ms\n",
            "iter 416: loss 0.0000, time 1423.09ms\n",
            "iter 417: loss 0.0000, time 1421.51ms\n",
            "iter 418: loss 0.0000, time 1383.73ms\n",
            "iter 419: loss 0.0000, time 1405.29ms\n",
            "Step 420: train loss 0.1398, val loss 0.6940\n",
            "Validation accuracy: 0.9838\n",
            "iter 420: loss 0.0000, time 17010.47ms\n",
            "iter 421: loss 0.0000, time 1365.10ms\n",
            "iter 422: loss 0.0000, time 1389.96ms\n",
            "iter 423: loss 0.0000, time 1376.73ms\n",
            "iter 424: loss 0.0000, time 1459.85ms\n",
            "iter 425: loss 0.0000, time 1359.34ms\n",
            "iter 426: loss 0.0000, time 1402.29ms\n",
            "iter 427: loss 0.0000, time 1371.87ms\n",
            "iter 428: loss 0.0000, time 1434.28ms\n",
            "iter 429: loss 0.0000, time 1374.96ms\n",
            "iter 430: loss 0.0000, time 1407.75ms\n",
            "iter 431: loss 0.0000, time 1365.65ms\n",
            "iter 432: loss 0.0000, time 1382.50ms\n",
            "iter 433: loss 0.0000, time 1371.01ms\n",
            "iter 434: loss 0.0000, time 1395.87ms\n",
            "iter 435: loss 0.0000, time 1396.93ms\n",
            "iter 436: loss 0.0000, time 1428.74ms\n",
            "iter 437: loss 0.0000, time 1378.35ms\n",
            "iter 438: loss 0.0000, time 1405.73ms\n",
            "iter 439: loss 0.0000, time 1379.55ms\n",
            "Step 440: train loss 0.1340, val loss 0.6978\n",
            "Validation accuracy: 0.9838\n",
            "iter 440: loss 0.0005, time 16619.84ms\n",
            "iter 441: loss 0.0000, time 1381.99ms\n",
            "iter 442: loss 0.0008, time 1403.14ms\n",
            "iter 443: loss 0.0000, time 1389.06ms\n",
            "iter 444: loss 0.0000, time 1382.63ms\n",
            "iter 445: loss 0.0000, time 1357.44ms\n",
            "iter 446: loss 0.0000, time 1406.77ms\n",
            "iter 447: loss 0.0005, time 1351.40ms\n",
            "iter 448: loss 0.0000, time 1368.91ms\n",
            "iter 449: loss 0.0000, time 1403.08ms\n",
            "iter 450: loss 0.0000, time 1378.69ms\n",
            "iter 451: loss 0.0000, time 1370.62ms\n",
            "iter 452: loss 0.0000, time 1749.94ms\n",
            "iter 453: loss 0.0000, time 1377.49ms\n",
            "iter 454: loss 0.0000, time 1419.37ms\n",
            "iter 455: loss 0.0000, time 1389.45ms\n",
            "iter 456: loss 0.0000, time 1369.82ms\n",
            "iter 457: loss 0.0000, time 1401.20ms\n",
            "iter 458: loss 0.0000, time 1427.40ms\n",
            "iter 459: loss 0.0000, time 1439.07ms\n",
            "Step 460: train loss 0.1286, val loss 0.7163\n",
            "Validation accuracy: 0.9816\n",
            "iter 460: loss 0.0000, time 17258.93ms\n",
            "iter 461: loss 0.0000, time 1397.09ms\n",
            "iter 462: loss 0.0000, time 1407.72ms\n",
            "iter 463: loss 0.0000, time 1397.23ms\n",
            "iter 464: loss 0.0000, time 1363.79ms\n",
            "iter 465: loss 0.0000, time 1407.37ms\n",
            "iter 466: loss 0.0000, time 1375.45ms\n",
            "iter 467: loss 0.0000, time 1435.00ms\n",
            "iter 468: loss 0.0000, time 1371.50ms\n",
            "iter 469: loss 0.0000, time 1379.78ms\n",
            "iter 470: loss 0.0000, time 1398.53ms\n",
            "iter 471: loss 0.0000, time 1371.00ms\n",
            "iter 472: loss 0.0000, time 1384.92ms\n",
            "iter 473: loss 0.0000, time 1369.70ms\n",
            "iter 474: loss 0.0000, time 1405.34ms\n",
            "iter 475: loss 0.0000, time 1378.97ms\n",
            "iter 476: loss 0.0000, time 1396.36ms\n",
            "iter 477: loss 0.0000, time 1350.15ms\n",
            "iter 478: loss 0.0000, time 1425.88ms\n",
            "iter 479: loss 0.0000, time 1389.16ms\n",
            "Step 480: train loss 0.1234, val loss 0.7254\n",
            "Validation accuracy: 0.9816\n",
            "iter 480: loss 0.0000, time 16847.74ms\n",
            "iter 481: loss 0.0000, time 1406.57ms\n",
            "iter 482: loss 0.0000, time 1697.90ms\n",
            "iter 483: loss 0.0000, time 1366.51ms\n",
            "iter 484: loss 0.0000, time 1387.93ms\n",
            "iter 485: loss 0.0000, time 1397.69ms\n",
            "iter 486: loss 0.0000, time 1386.68ms\n",
            "iter 487: loss 0.0000, time 1359.58ms\n",
            "iter 488: loss 0.0000, time 1399.09ms\n",
            "iter 489: loss 0.0050, time 1413.83ms\n",
            "iter 490: loss 0.0000, time 1394.49ms\n",
            "iter 491: loss 0.0000, time 1388.24ms\n",
            "iter 492: loss 0.0000, time 1370.81ms\n",
            "iter 493: loss 0.0000, time 1364.19ms\n",
            "iter 494: loss 0.0000, time 1414.90ms\n",
            "iter 495: loss 0.0000, time 1395.45ms\n",
            "iter 496: loss 0.0000, time 1433.74ms\n",
            "iter 497: loss 0.0000, time 1410.88ms\n",
            "iter 498: loss 0.0000, time 1357.46ms\n",
            "iter 499: loss 0.0000, time 1419.23ms\n",
            "Step 500: train loss 0.1186, val loss 0.7338\n",
            "Validation accuracy: 0.9860\n",
            "Test accuracy 0.6333\n",
            "iter 500: loss 0.0000, time 17544.15ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▅█▇▇</td></tr><tr><td>Test_F1_Score</td><td>▁▅▅█▇▇</td></tr><tr><td>Test_Precision</td><td>▁▃▃█▇▅</td></tr><tr><td>Test_Recall</td><td>▁▅▅█▇▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▄▇▇▇█████████████████████</td></tr><tr><td>val/loss</td><td> █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.63333</td></tr><tr><td>Test_F1_Score</td><td>0.57055</td></tr><tr><td>Test_Precision</td><td>0.62656</td></tr><tr><td>Test_Recall</td><td>0.63333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.11864</td></tr><tr><td>val/acc</td><td>0.98596</td></tr><tr><td>val/loss</td><td>0.7338</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_1917</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j1vnthpe</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_191747-j1vnthpe\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lz5qfzjp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_193844-lz5qfzjp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp' target=\"_blank\">scarlet-sweep-5</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1938\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">scarlet-sweep-5</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_193844-lz5qfzjp\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_193850-lz5qfzjp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp' target=\"_blank\">gpt2_hyper_1938</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 0.5039, time 34797.93ms\n",
            "iter 1: loss 3.6719, time 1637.30ms\n",
            "iter 2: loss 0.9258, time 1666.67ms\n",
            "iter 3: loss 3.3906, time 1574.41ms\n",
            "iter 4: loss 2.1406, time 1628.50ms\n",
            "iter 5: loss 1.0469, time 1666.12ms\n",
            "iter 6: loss 0.7266, time 1653.61ms\n",
            "iter 7: loss 0.6758, time 1620.50ms\n",
            "iter 8: loss 3.0469, time 1659.43ms\n",
            "iter 9: loss 2.2656, time 1729.00ms\n",
            "iter 10: loss 0.8086, time 1668.04ms\n",
            "iter 11: loss 0.8711, time 1962.27ms\n",
            "iter 12: loss 0.0913, time 1638.88ms\n",
            "iter 13: loss 1.2656, time 1701.50ms\n",
            "iter 14: loss 1.0156, time 1654.91ms\n",
            "iter 15: loss 0.5430, time 1657.40ms\n",
            "iter 16: loss 0.9141, time 1654.29ms\n",
            "iter 17: loss 0.6172, time 1632.76ms\n",
            "iter 18: loss 1.0078, time 1684.95ms\n",
            "iter 19: loss 0.7617, time 1636.02ms\n",
            "Step 20: train loss 1.3103, val loss 1.0051\n",
            "Validation accuracy: 0.7456\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.7695, time 16834.12ms\n",
            "iter 21: loss 0.8906, time 1646.47ms\n",
            "iter 22: loss 0.3105, time 1616.82ms\n",
            "iter 23: loss 0.8945, time 1612.59ms\n",
            "iter 24: loss 0.8828, time 1959.16ms\n",
            "iter 25: loss 0.5820, time 1623.39ms\n",
            "iter 26: loss 0.5664, time 1651.23ms\n",
            "iter 27: loss 1.0625, time 1632.13ms\n",
            "iter 28: loss 0.2148, time 1627.69ms\n",
            "iter 29: loss 0.5508, time 1709.29ms\n",
            "iter 30: loss 0.1357, time 1670.01ms\n",
            "iter 31: loss 0.2500, time 1621.00ms\n",
            "iter 32: loss 0.1006, time 1701.62ms\n",
            "iter 33: loss 0.0767, time 1658.27ms\n",
            "iter 34: loss 0.0669, time 1571.28ms\n",
            "iter 35: loss 0.3379, time 1730.71ms\n",
            "iter 36: loss 0.0737, time 1576.35ms\n",
            "iter 37: loss 0.8398, time 2013.56ms\n",
            "iter 38: loss 0.3086, time 1567.55ms\n",
            "iter 39: loss 3.3906, time 1654.27ms\n",
            "Step 40: train loss 0.9480, val loss 0.7903\n",
            "Validation accuracy: 0.8210\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.1138, time 17090.47ms\n",
            "iter 41: loss 0.2139, time 1616.66ms\n",
            "iter 42: loss 0.0315, time 1635.01ms\n",
            "iter 43: loss 0.0048, time 1656.77ms\n",
            "iter 44: loss 0.0703, time 1637.99ms\n",
            "iter 45: loss 0.6133, time 1665.21ms\n",
            "iter 46: loss 0.0869, time 1533.17ms\n",
            "iter 47: loss 0.0184, time 1595.27ms\n",
            "iter 48: loss 0.0786, time 1594.53ms\n",
            "iter 49: loss 0.2051, time 1704.86ms\n",
            "iter 50: loss 0.0003, time 1602.28ms\n",
            "iter 51: loss 0.0109, time 1975.92ms\n",
            "iter 52: loss 0.0046, time 1595.84ms\n",
            "iter 53: loss 0.0020, time 1638.82ms\n",
            "iter 54: loss 0.0124, time 1643.21ms\n",
            "iter 55: loss 0.0015, time 1685.44ms\n",
            "iter 56: loss 0.0275, time 1636.95ms\n",
            "iter 57: loss 0.0023, time 1636.98ms\n",
            "iter 58: loss 0.0159, time 1672.09ms\n",
            "iter 59: loss 0.0056, time 1660.14ms\n",
            "Step 60: train loss 0.7160, val loss 0.6263\n",
            "Validation accuracy: 0.9172\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.5898, time 17260.51ms\n",
            "iter 61: loss 0.0752, time 1645.28ms\n",
            "iter 62: loss 0.0405, time 1683.00ms\n",
            "iter 63: loss 0.0085, time 1950.41ms\n",
            "iter 64: loss 0.0222, time 1616.76ms\n",
            "iter 65: loss 2.1094, time 1618.40ms\n",
            "iter 66: loss 0.9063, time 1659.71ms\n",
            "iter 67: loss 0.0215, time 1632.22ms\n",
            "iter 68: loss 0.0027, time 1692.05ms\n",
            "iter 69: loss 0.2002, time 1630.97ms\n",
            "iter 70: loss 0.0103, time 1589.97ms\n",
            "iter 71: loss 0.0078, time 1577.86ms\n",
            "iter 72: loss 0.0393, time 1656.49ms\n",
            "iter 73: loss 0.0012, time 1677.29ms\n",
            "iter 74: loss 0.0090, time 1642.94ms\n",
            "iter 75: loss 0.0033, time 1674.12ms\n",
            "iter 76: loss 0.0571, time 1901.84ms\n",
            "iter 77: loss 0.0030, time 1580.76ms\n",
            "iter 78: loss 0.0039, time 1623.46ms\n",
            "iter 79: loss 0.0044, time 1589.19ms\n",
            "Step 80: train loss 0.5664, val loss 0.5054\n",
            "Validation accuracy: 0.9660\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0013, time 17070.76ms\n",
            "iter 81: loss 0.0015, time 1603.66ms\n",
            "iter 82: loss 0.2617, time 1625.08ms\n",
            "iter 83: loss 0.0075, time 1617.96ms\n",
            "iter 84: loss 0.0066, time 1695.80ms\n",
            "iter 85: loss 0.0025, time 1761.68ms\n",
            "iter 86: loss 1.4219, time 1693.41ms\n",
            "iter 87: loss 0.0013, time 1584.41ms\n",
            "iter 88: loss 0.0029, time 1671.60ms\n",
            "iter 89: loss 0.0019, time 1640.21ms\n",
            "iter 90: loss 0.0138, time 1997.44ms\n",
            "iter 91: loss 0.0037, time 1602.41ms\n",
            "iter 92: loss 0.0002, time 1585.78ms\n",
            "iter 93: loss 0.0050, time 1624.02ms\n",
            "iter 94: loss 0.0008, time 1604.18ms\n",
            "iter 95: loss 0.0021, time 1568.80ms\n",
            "iter 96: loss 0.0032, time 1606.37ms\n",
            "iter 97: loss 0.0889, time 1643.88ms\n",
            "iter 98: loss 0.0258, time 1631.91ms\n",
            "iter 99: loss 0.0021, time 1609.69ms\n",
            "Step 100: train loss 0.4665, val loss 0.4703\n",
            "Validation accuracy: 0.9689\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.7333\n",
            "iter 100: loss 0.0347, time 17933.49ms\n",
            "iter 101: loss 0.1396, time 1611.99ms\n",
            "iter 102: loss 0.0035, time 1585.29ms\n",
            "iter 103: loss 0.0001, time 1587.74ms\n",
            "iter 104: loss 0.0001, time 1591.86ms\n",
            "iter 105: loss 0.0022, time 1612.07ms\n",
            "iter 106: loss 0.0579, time 1912.94ms\n",
            "iter 107: loss 0.0058, time 1597.38ms\n",
            "iter 108: loss 0.0001, time 1640.98ms\n",
            "iter 109: loss 0.0972, time 1625.85ms\n",
            "iter 110: loss 0.0015, time 1639.67ms\n",
            "iter 111: loss 0.0002, time 1618.71ms\n",
            "iter 112: loss 0.0009, time 1582.95ms\n",
            "iter 113: loss 0.0001, time 1595.33ms\n",
            "iter 114: loss 0.0001, time 1595.71ms\n",
            "iter 115: loss 0.0232, time 1554.55ms\n",
            "iter 116: loss 0.0001, time 1592.76ms\n",
            "iter 117: loss 1.2109, time 1561.87ms\n",
            "iter 118: loss 0.0001, time 1909.46ms\n",
            "iter 119: loss 0.0004, time 1604.70ms\n",
            "Step 120: train loss 0.4036, val loss 0.4601\n",
            "Validation accuracy: 0.9704\n",
            "iter 120: loss 0.0007, time 15237.41ms\n",
            "iter 121: loss 0.0205, time 1631.25ms\n",
            "iter 122: loss 0.0004, time 1614.65ms\n",
            "iter 123: loss 0.0002, time 1609.91ms\n",
            "iter 124: loss 0.0015, time 1575.35ms\n",
            "iter 125: loss 0.0003, time 1658.37ms\n",
            "iter 126: loss 0.0005, time 1583.35ms\n",
            "iter 127: loss 0.0007, time 1625.72ms\n",
            "iter 128: loss 0.0007, time 1700.23ms\n",
            "iter 129: loss 0.0015, time 1590.55ms\n",
            "iter 130: loss 0.0021, time 1620.91ms\n",
            "iter 131: loss 0.0012, time 1622.76ms\n",
            "iter 132: loss 0.0001, time 1925.07ms\n",
            "iter 133: loss 0.0001, time 1655.65ms\n",
            "iter 134: loss 0.0000, time 1620.35ms\n",
            "iter 135: loss 0.0004, time 1586.02ms\n",
            "iter 136: loss 0.0000, time 1614.16ms\n",
            "iter 137: loss 0.0000, time 1633.42ms\n",
            "iter 138: loss 0.0003, time 1592.83ms\n",
            "iter 139: loss 0.0064, time 1567.91ms\n",
            "Step 140: train loss 0.3583, val loss 0.4638\n",
            "Validation accuracy: 0.9246\n",
            "iter 140: loss 0.0020, time 15276.61ms\n",
            "iter 141: loss 0.0037, time 1632.71ms\n",
            "iter 142: loss 0.0136, time 1637.21ms\n",
            "iter 143: loss 0.0009, time 1611.61ms\n",
            "iter 144: loss 0.0002, time 1646.32ms\n",
            "iter 145: loss 0.0001, time 1617.48ms\n",
            "iter 146: loss 0.0986, time 1925.03ms\n",
            "iter 147: loss 0.0048, time 1590.03ms\n",
            "iter 148: loss 0.0062, time 1607.38ms\n",
            "iter 149: loss 0.0009, time 1613.97ms\n",
            "iter 150: loss 0.0021, time 1595.32ms\n",
            "iter 151: loss 0.0002, time 1560.32ms\n",
            "iter 152: loss 0.0003, time 1553.96ms\n",
            "iter 153: loss 0.0003, time 1570.66ms\n",
            "iter 154: loss 0.0000, time 1610.62ms\n",
            "iter 155: loss 0.0654, time 1579.50ms\n",
            "iter 156: loss 0.0001, time 1645.65ms\n",
            "iter 157: loss 0.0104, time 1647.98ms\n",
            "iter 158: loss 0.0003, time 1619.98ms\n",
            "iter 159: loss 0.0003, time 1922.59ms\n",
            "Step 160: train loss 0.3169, val loss 0.4188\n",
            "Validation accuracy: 0.9808\n",
            "iter 160: loss 0.0038, time 15346.64ms\n",
            "iter 161: loss 0.0020, time 1660.89ms\n",
            "iter 162: loss 0.0747, time 1683.65ms\n",
            "iter 163: loss 0.0061, time 1663.87ms\n",
            "iter 164: loss 0.0001, time 1597.00ms\n",
            "iter 165: loss 0.0000, time 1632.04ms\n",
            "iter 166: loss 0.0032, time 1626.31ms\n",
            "iter 167: loss 0.0006, time 1581.29ms\n",
            "iter 168: loss 0.0000, time 1657.18ms\n",
            "iter 169: loss 0.0001, time 1557.16ms\n",
            "iter 170: loss 0.0015, time 1623.28ms\n",
            "iter 171: loss 0.0020, time 1616.07ms\n",
            "iter 172: loss 0.0217, time 1602.45ms\n",
            "iter 173: loss 0.0003, time 1914.40ms\n",
            "iter 174: loss 0.0009, time 1563.46ms\n",
            "iter 175: loss 0.0000, time 1663.30ms\n",
            "iter 176: loss 0.0001, time 1617.36ms\n",
            "iter 177: loss 0.0008, time 1633.79ms\n",
            "iter 178: loss 0.0000, time 1612.48ms\n",
            "iter 179: loss 0.0000, time 1634.00ms\n",
            "Step 180: train loss 0.2826, val loss 0.3958\n",
            "Validation accuracy: 0.9926\n",
            "iter 180: loss 0.0001, time 15284.08ms\n",
            "iter 181: loss 0.0003, time 1610.29ms\n",
            "iter 182: loss 0.0012, time 1667.42ms\n",
            "iter 183: loss 0.0003, time 1642.17ms\n",
            "iter 184: loss 0.0001, time 1631.80ms\n",
            "iter 185: loss 0.0000, time 1645.70ms\n",
            "iter 186: loss 0.0001, time 1637.88ms\n",
            "iter 187: loss 0.0001, time 1912.48ms\n",
            "iter 188: loss 0.0000, time 1659.16ms\n",
            "iter 189: loss 0.0000, time 1598.20ms\n",
            "iter 190: loss 0.0003, time 1665.24ms\n",
            "iter 191: loss 0.0000, time 1686.48ms\n",
            "iter 192: loss 0.0010, time 1638.25ms\n",
            "iter 193: loss 0.0001, time 1646.66ms\n",
            "iter 194: loss 0.0002, time 1568.97ms\n",
            "iter 195: loss 0.0000, time 1620.82ms\n",
            "iter 196: loss 0.0001, time 1604.37ms\n",
            "iter 197: loss 0.0000, time 1612.53ms\n",
            "iter 198: loss 0.0000, time 1654.24ms\n",
            "iter 199: loss 0.0000, time 1578.67ms\n",
            "Step 200: train loss 0.2548, val loss 0.3794\n",
            "Validation accuracy: 0.9808\n",
            "Test accuracy 0.7667\n",
            "iter 200: loss 0.0000, time 16089.99ms\n",
            "iter 201: loss 0.0000, time 1706.72ms\n",
            "iter 202: loss 0.0000, time 1982.97ms\n",
            "iter 203: loss 0.0001, time 1636.77ms\n",
            "iter 204: loss 0.0000, time 1622.76ms\n",
            "iter 205: loss 0.0000, time 1635.17ms\n",
            "iter 206: loss 0.0000, time 1628.63ms\n",
            "iter 207: loss 0.0001, time 1660.69ms\n",
            "iter 208: loss 0.0000, time 1633.78ms\n",
            "iter 209: loss 0.0001, time 1604.62ms\n",
            "iter 210: loss 0.0004, time 1572.23ms\n",
            "iter 211: loss 0.0002, time 1625.65ms\n",
            "iter 212: loss 0.0000, time 1668.07ms\n",
            "iter 213: loss 0.0003, time 1635.72ms\n",
            "iter 214: loss 0.0000, time 1644.00ms\n",
            "iter 215: loss 0.0000, time 1963.01ms\n",
            "iter 216: loss 0.0000, time 1674.09ms\n",
            "iter 217: loss 0.0000, time 1640.89ms\n",
            "iter 218: loss 0.0001, time 1668.01ms\n",
            "iter 219: loss 0.0000, time 1709.46ms\n",
            "Step 220: train loss 0.2329, val loss 0.3832\n",
            "Validation accuracy: 0.9867\n",
            "iter 220: loss 0.0000, time 15272.64ms\n",
            "iter 221: loss 0.0000, time 1594.85ms\n",
            "iter 222: loss 0.0000, time 1678.39ms\n",
            "iter 223: loss 0.0002, time 1617.47ms\n",
            "iter 224: loss 0.0042, time 1630.46ms\n",
            "iter 225: loss 0.0000, time 1665.63ms\n",
            "iter 226: loss 0.0000, time 1666.60ms\n",
            "iter 227: loss 0.0003, time 1630.64ms\n",
            "iter 228: loss 0.0005, time 1635.81ms\n",
            "iter 229: loss 0.0000, time 1630.73ms\n",
            "iter 230: loss 0.0001, time 1650.16ms\n",
            "iter 231: loss 0.0000, time 1923.29ms\n",
            "iter 232: loss 0.0000, time 1593.45ms\n",
            "iter 233: loss 0.0002, time 1624.41ms\n",
            "iter 234: loss 0.0000, time 1653.86ms\n",
            "iter 235: loss 0.0009, time 1661.85ms\n",
            "iter 236: loss 0.0002, time 1655.32ms\n",
            "iter 237: loss 0.0000, time 1699.70ms\n",
            "iter 238: loss 0.0000, time 1669.34ms\n",
            "iter 239: loss 0.0002, time 1639.88ms\n",
            "Step 240: train loss 0.2145, val loss 0.4190\n",
            "Validation accuracy: 0.9911\n",
            "iter 240: loss 0.0000, time 15364.23ms\n",
            "iter 241: loss 0.0028, time 1644.32ms\n",
            "iter 242: loss 0.0000, time 1610.65ms\n",
            "iter 243: loss 0.0000, time 1617.84ms\n",
            "iter 244: loss 0.0002, time 1684.87ms\n",
            "iter 245: loss 0.0000, time 1628.46ms\n",
            "iter 246: loss 0.0001, time 1969.20ms\n",
            "iter 247: loss 0.0000, time 1615.23ms\n",
            "iter 248: loss 0.0015, time 1645.31ms\n",
            "iter 249: loss 0.0000, time 1630.28ms\n",
            "iter 250: loss 0.0003, time 1618.85ms\n",
            "iter 251: loss 0.0000, time 1618.03ms\n",
            "iter 252: loss 0.0022, time 1627.46ms\n",
            "iter 253: loss 0.0002, time 1651.50ms\n",
            "iter 254: loss 0.0000, time 1601.45ms\n",
            "iter 255: loss 0.0001, time 1669.41ms\n",
            "iter 256: loss 0.0000, time 1629.76ms\n",
            "iter 257: loss 0.0015, time 1631.80ms\n",
            "iter 258: loss 0.0000, time 1634.60ms\n",
            "iter 259: loss 0.0000, time 1607.90ms\n",
            "Step 260: train loss 0.1988, val loss 0.3946\n",
            "Validation accuracy: 0.9852\n",
            "iter 260: loss 0.0000, time 15653.03ms\n",
            "iter 261: loss 0.0000, time 1707.62ms\n",
            "iter 262: loss 0.0014, time 1627.93ms\n",
            "iter 263: loss 0.0000, time 1644.80ms\n",
            "iter 264: loss 0.0001, time 1621.70ms\n",
            "iter 265: loss 0.0000, time 1714.85ms\n",
            "iter 266: loss 0.0002, time 1625.27ms\n",
            "iter 267: loss 0.0000, time 1654.21ms\n",
            "iter 268: loss 0.0000, time 1638.65ms\n",
            "iter 269: loss 0.0000, time 1655.14ms\n",
            "iter 270: loss 0.0000, time 1558.83ms\n",
            "iter 271: loss 0.0002, time 1704.14ms\n",
            "iter 272: loss 0.0002, time 1689.24ms\n",
            "iter 273: loss 0.0000, time 1610.10ms\n",
            "iter 274: loss 0.0000, time 1921.79ms\n",
            "iter 275: loss 0.0000, time 1617.05ms\n",
            "iter 276: loss 0.0001, time 1735.29ms\n",
            "iter 277: loss 0.0000, time 1705.93ms\n",
            "iter 278: loss 0.0001, time 1638.55ms\n",
            "iter 279: loss 0.0000, time 1648.36ms\n",
            "Step 280: train loss 0.1853, val loss 0.3793\n",
            "Validation accuracy: 0.9704\n",
            "iter 280: loss 0.0195, time 15507.95ms\n",
            "iter 281: loss 0.0002, time 1625.24ms\n",
            "iter 282: loss 0.0001, time 1627.33ms\n",
            "iter 283: loss 0.0004, time 1649.69ms\n",
            "iter 284: loss 0.0000, time 1677.95ms\n",
            "iter 285: loss 0.0000, time 1689.58ms\n",
            "iter 286: loss 0.0003, time 1667.97ms\n",
            "iter 287: loss 3.2344, time 1669.80ms\n",
            "iter 288: loss 0.0016, time 1623.32ms\n",
            "iter 289: loss 0.0000, time 1957.94ms\n",
            "iter 290: loss 0.0001, time 1632.08ms\n",
            "iter 291: loss 0.0000, time 1645.80ms\n",
            "iter 292: loss 0.0000, time 1685.54ms\n",
            "iter 293: loss 0.0000, time 1625.61ms\n",
            "iter 294: loss 0.0000, time 1611.80ms\n",
            "iter 295: loss 0.0000, time 1682.12ms\n",
            "iter 296: loss 0.0000, time 1592.61ms\n",
            "iter 297: loss 0.0000, time 1658.92ms\n",
            "iter 298: loss 0.0000, time 1588.09ms\n",
            "iter 299: loss 0.1309, time 1666.08ms\n",
            "Step 300: train loss 0.1736, val loss 0.4137\n",
            "Validation accuracy: 0.9822\n",
            "Test accuracy 0.8667\n",
            "iter 300: loss 0.0000, time 16501.80ms\n",
            "iter 301: loss 0.0000, time 1597.52ms\n",
            "iter 302: loss 0.0000, time 1617.49ms\n",
            "iter 303: loss 0.0000, time 1922.08ms\n",
            "iter 304: loss 0.0000, time 1606.17ms\n",
            "iter 305: loss 0.0000, time 1627.95ms\n",
            "iter 306: loss 0.0000, time 1598.12ms\n",
            "iter 307: loss 0.0000, time 1561.73ms\n",
            "iter 308: loss 0.0000, time 1635.37ms\n",
            "iter 309: loss 0.0000, time 1631.69ms\n",
            "iter 310: loss 0.0000, time 1648.67ms\n",
            "iter 311: loss 0.0001, time 1656.34ms\n",
            "iter 312: loss 0.0000, time 1597.33ms\n",
            "iter 313: loss 0.0000, time 1615.54ms\n",
            "iter 314: loss 0.0000, time 1696.61ms\n",
            "iter 315: loss 0.0000, time 1606.80ms\n",
            "iter 316: loss 0.0000, time 1876.63ms\n",
            "iter 317: loss 0.0000, time 1544.14ms\n",
            "iter 318: loss 0.1196, time 1617.37ms\n",
            "iter 319: loss 0.0000, time 1609.59ms\n",
            "Step 320: train loss 0.1637, val loss 0.3964\n",
            "Validation accuracy: 0.9882\n",
            "iter 320: loss 0.0000, time 15161.86ms\n",
            "iter 321: loss 0.0001, time 1574.59ms\n",
            "iter 322: loss 0.0000, time 1580.42ms\n",
            "iter 323: loss 0.0000, time 1623.56ms\n",
            "iter 324: loss 0.0000, time 1598.76ms\n",
            "iter 325: loss 0.0000, time 1650.21ms\n",
            "iter 326: loss 0.0000, time 1598.32ms\n",
            "iter 327: loss 0.0003, time 1602.35ms\n",
            "iter 328: loss 0.0001, time 1590.63ms\n",
            "iter 329: loss 0.0000, time 1570.52ms\n",
            "iter 330: loss 0.0000, time 1965.94ms\n",
            "iter 331: loss 0.0000, time 1614.75ms\n",
            "iter 332: loss 0.0002, time 1628.49ms\n",
            "iter 333: loss 0.0013, time 1597.55ms\n",
            "iter 334: loss 0.0000, time 1623.09ms\n",
            "iter 335: loss 0.0000, time 1625.15ms\n",
            "iter 336: loss 0.0000, time 1613.69ms\n",
            "iter 337: loss 0.0000, time 1595.96ms\n",
            "iter 338: loss 0.0000, time 1675.80ms\n",
            "iter 339: loss 0.0000, time 1635.14ms\n",
            "Step 340: train loss 0.1541, val loss 0.3810\n",
            "Validation accuracy: 0.9837\n",
            "iter 340: loss 0.0000, time 15294.11ms\n",
            "iter 341: loss 0.0000, time 1649.20ms\n",
            "iter 342: loss 0.0000, time 1624.49ms\n",
            "iter 343: loss 0.0005, time 1604.53ms\n",
            "iter 344: loss 0.0000, time 1930.70ms\n",
            "iter 345: loss 0.0000, time 1599.86ms\n",
            "iter 346: loss 0.0000, time 1622.00ms\n",
            "iter 347: loss 0.0001, time 1658.84ms\n",
            "iter 348: loss 0.0000, time 1604.28ms\n",
            "iter 349: loss 0.0000, time 1625.37ms\n",
            "iter 350: loss 0.0000, time 1646.48ms\n",
            "iter 351: loss 0.0000, time 1621.40ms\n",
            "iter 352: loss 0.0000, time 1596.99ms\n",
            "iter 353: loss 0.0037, time 1636.61ms\n",
            "iter 354: loss 0.0000, time 1673.73ms\n",
            "iter 355: loss 0.0000, time 1703.94ms\n",
            "iter 356: loss 0.0000, time 1673.95ms\n",
            "iter 357: loss 0.0000, time 1899.08ms\n",
            "iter 358: loss 0.0000, time 1596.20ms\n",
            "iter 359: loss 0.0000, time 1602.37ms\n",
            "Step 360: train loss 0.1460, val loss 0.3633\n",
            "Validation accuracy: 0.9822\n",
            "iter 360: loss 0.0000, time 15324.15ms\n",
            "iter 361: loss 0.0000, time 1628.75ms\n",
            "iter 362: loss 0.0000, time 1576.50ms\n",
            "iter 363: loss 0.0000, time 1687.04ms\n",
            "iter 364: loss 0.0000, time 1613.45ms\n",
            "iter 365: loss 0.0000, time 1627.70ms\n",
            "iter 366: loss 0.0000, time 1638.37ms\n",
            "iter 367: loss 0.0000, time 1637.15ms\n",
            "iter 368: loss 0.0001, time 1630.67ms\n",
            "iter 369: loss 0.0000, time 1656.98ms\n",
            "iter 370: loss 0.0000, time 1601.73ms\n",
            "iter 371: loss 0.0000, time 1683.64ms\n",
            "iter 372: loss 0.0000, time 1905.40ms\n",
            "iter 373: loss 0.0000, time 1667.46ms\n",
            "iter 374: loss 0.0000, time 1681.77ms\n",
            "iter 375: loss 0.0000, time 1622.19ms\n",
            "iter 376: loss 0.0000, time 1628.80ms\n",
            "iter 377: loss 0.0002, time 1625.86ms\n",
            "iter 378: loss 0.0000, time 1686.94ms\n",
            "iter 379: loss 0.0000, time 1644.64ms\n",
            "Step 380: train loss 0.1384, val loss 0.3445\n",
            "Validation accuracy: 0.9645\n",
            "iter 380: loss 0.0000, time 15381.48ms\n",
            "iter 381: loss 0.0000, time 1681.74ms\n",
            "iter 382: loss 0.0001, time 1682.89ms\n",
            "iter 383: loss 0.0000, time 1661.10ms\n",
            "iter 384: loss 0.0015, time 1649.83ms\n",
            "iter 385: loss 0.0000, time 1935.18ms\n",
            "iter 386: loss 0.0000, time 1681.12ms\n",
            "iter 387: loss 0.0000, time 1609.99ms\n",
            "iter 388: loss 0.0000, time 1688.02ms\n",
            "iter 389: loss 0.0000, time 1625.13ms\n",
            "iter 390: loss 0.0000, time 1670.70ms\n",
            "iter 391: loss 0.0000, time 1633.82ms\n",
            "iter 392: loss 0.0000, time 1623.70ms\n",
            "iter 393: loss 0.0001, time 1668.52ms\n",
            "iter 394: loss 0.0000, time 1578.52ms\n",
            "iter 395: loss 0.0000, time 1626.65ms\n",
            "iter 396: loss 0.0000, time 1591.39ms\n",
            "iter 397: loss 0.0000, time 1687.27ms\n",
            "iter 398: loss 0.0003, time 1601.74ms\n",
            "iter 399: loss 0.0000, time 1918.15ms\n",
            "Step 400: train loss 0.1317, val loss 0.3275\n",
            "Validation accuracy: 0.9808\n",
            "Test accuracy 0.8000\n",
            "iter 400: loss 0.0001, time 16143.15ms\n",
            "iter 401: loss 0.0000, time 1599.20ms\n",
            "iter 402: loss 0.0035, time 1606.83ms\n",
            "iter 403: loss 0.0000, time 1674.67ms\n",
            "iter 404: loss 0.0000, time 1685.26ms\n",
            "iter 405: loss 0.0000, time 1621.02ms\n",
            "iter 406: loss 0.0000, time 1632.66ms\n",
            "iter 407: loss 0.0000, time 1594.02ms\n",
            "iter 408: loss 0.0000, time 1623.23ms\n",
            "iter 409: loss 0.0001, time 1561.58ms\n",
            "iter 410: loss 0.0000, time 1585.47ms\n",
            "iter 411: loss 0.0000, time 1646.69ms\n",
            "iter 412: loss 0.0000, time 1617.09ms\n",
            "iter 413: loss 0.0000, time 1613.98ms\n",
            "iter 414: loss 0.0000, time 2006.73ms\n",
            "iter 415: loss 0.0000, time 1699.96ms\n",
            "iter 416: loss 0.0000, time 1691.79ms\n",
            "iter 417: loss 0.0000, time 1610.52ms\n",
            "iter 418: loss 0.0000, time 1659.28ms\n",
            "iter 419: loss 0.0000, time 1620.98ms\n",
            "Step 420: train loss 0.1257, val loss 0.3394\n",
            "Validation accuracy: 0.9778\n",
            "iter 420: loss 0.0000, time 15434.37ms\n",
            "iter 421: loss 0.0000, time 1637.06ms\n",
            "iter 422: loss 0.0000, time 1605.91ms\n",
            "iter 423: loss 0.0000, time 1665.56ms\n",
            "iter 424: loss 0.0000, time 1576.62ms\n",
            "iter 425: loss 0.0000, time 1588.98ms\n",
            "iter 426: loss 0.0000, time 1618.03ms\n",
            "iter 427: loss 0.0000, time 1669.21ms\n",
            "iter 428: loss 0.0000, time 1998.79ms\n",
            "iter 429: loss 0.0251, time 1643.33ms\n",
            "iter 430: loss 0.0000, time 1639.57ms\n",
            "iter 431: loss 0.0000, time 1614.28ms\n",
            "iter 432: loss 0.0000, time 1651.50ms\n",
            "iter 433: loss 0.0000, time 1678.11ms\n",
            "iter 434: loss 0.0000, time 1631.57ms\n",
            "iter 435: loss 0.0004, time 1569.29ms\n",
            "iter 436: loss 0.0000, time 1571.69ms\n",
            "iter 437: loss 0.0000, time 1595.09ms\n",
            "iter 438: loss 0.0001, time 1561.29ms\n",
            "iter 439: loss 0.0000, time 1611.29ms\n",
            "Step 440: train loss 0.1204, val loss 0.3505\n",
            "Validation accuracy: 0.9808\n",
            "iter 440: loss 0.0002, time 15105.67ms\n",
            "iter 441: loss 0.0000, time 1599.90ms\n",
            "iter 442: loss 0.0000, time 1638.05ms\n",
            "iter 443: loss 0.0000, time 1915.76ms\n",
            "iter 444: loss 0.0000, time 1615.65ms\n",
            "iter 445: loss 0.0001, time 1553.33ms\n",
            "iter 446: loss 0.0000, time 1575.96ms\n",
            "iter 447: loss 0.0000, time 1636.11ms\n",
            "iter 448: loss 0.0000, time 1584.40ms\n",
            "iter 449: loss 0.0000, time 1538.96ms\n",
            "iter 450: loss 0.0000, time 1660.09ms\n",
            "iter 451: loss 0.0000, time 1644.53ms\n",
            "iter 452: loss 0.0000, time 1570.37ms\n",
            "iter 453: loss 0.0001, time 1534.45ms\n",
            "iter 454: loss 0.0000, time 1652.41ms\n",
            "iter 455: loss 0.0001, time 1630.22ms\n",
            "iter 456: loss 0.0000, time 2004.75ms\n",
            "iter 457: loss 0.0001, time 1614.16ms\n",
            "iter 458: loss 0.0000, time 1660.80ms\n",
            "iter 459: loss 0.0000, time 1632.02ms\n",
            "Step 460: train loss 0.1153, val loss 0.3382\n",
            "Validation accuracy: 0.9837\n",
            "iter 460: loss 0.0000, time 15176.66ms\n",
            "iter 461: loss 0.0000, time 1692.41ms\n",
            "iter 462: loss 0.0000, time 1656.33ms\n",
            "iter 463: loss 0.0000, time 1629.73ms\n",
            "iter 464: loss 0.0000, time 1607.76ms\n",
            "iter 465: loss 0.0000, time 1632.60ms\n",
            "iter 466: loss 0.0013, time 1648.18ms\n",
            "iter 467: loss 0.0000, time 1652.65ms\n",
            "iter 468: loss 0.0000, time 1649.70ms\n",
            "iter 469: loss 0.0000, time 2032.78ms\n",
            "iter 470: loss 0.0000, time 1667.82ms\n",
            "iter 471: loss 0.0000, time 1599.13ms\n",
            "iter 472: loss 0.0000, time 1607.49ms\n",
            "iter 473: loss 0.0000, time 1678.78ms\n",
            "iter 474: loss 0.0000, time 1612.65ms\n",
            "iter 475: loss 0.0000, time 1721.89ms\n",
            "iter 476: loss 0.0000, time 1616.17ms\n",
            "iter 477: loss 0.0000, time 1616.86ms\n",
            "iter 478: loss 0.0000, time 1606.44ms\n",
            "iter 479: loss 0.0000, time 1644.43ms\n",
            "Step 480: train loss 0.1111, val loss 0.3386\n",
            "Validation accuracy: 0.9852\n",
            "iter 480: loss 0.0000, time 15190.54ms\n",
            "iter 481: loss 0.0000, time 1646.22ms\n",
            "iter 482: loss 0.0000, time 1551.78ms\n",
            "iter 483: loss 0.0000, time 1570.01ms\n",
            "iter 484: loss 0.0000, time 1885.75ms\n",
            "iter 485: loss 0.0000, time 1617.98ms\n",
            "iter 486: loss 0.0000, time 1652.10ms\n",
            "iter 487: loss 0.0000, time 1595.93ms\n",
            "iter 488: loss 0.0000, time 1599.88ms\n",
            "iter 489: loss 0.0000, time 1646.59ms\n",
            "iter 490: loss 0.0000, time 1674.40ms\n",
            "iter 491: loss 0.0000, time 1617.57ms\n",
            "iter 492: loss 0.0000, time 1568.12ms\n",
            "iter 493: loss 0.0000, time 1622.86ms\n",
            "iter 494: loss 0.0000, time 1635.95ms\n",
            "iter 495: loss 0.0000, time 1678.67ms\n",
            "iter 496: loss 0.0000, time 1593.69ms\n",
            "iter 497: loss 0.0000, time 1916.12ms\n",
            "iter 498: loss 0.0000, time 1601.52ms\n",
            "iter 499: loss 0.0001, time 1614.69ms\n",
            "Step 500: train loss 0.1068, val loss 0.3509\n",
            "Validation accuracy: 0.9882\n",
            "Test accuracy 0.7667\n",
            "iter 500: loss 0.0000, time 16169.39ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▆▇█▇▇</td></tr><tr><td>Test_F1_Score</td><td>▁▇▇█▇▇</td></tr><tr><td>Test_Precision</td><td>▁▇██▇▇</td></tr><tr><td>Test_Recall</td><td>▁▆▇█▇▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▆▇███▇██████████████████</td></tr><tr><td>val/loss</td><td> █▆▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.76667</td></tr><tr><td>Test_F1_Score</td><td>0.76055</td></tr><tr><td>Test_Precision</td><td>0.77561</td></tr><tr><td>Test_Recall</td><td>0.76667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.10678</td></tr><tr><td>val/acc</td><td>0.98817</td></tr><tr><td>val/loss</td><td>0.35089</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_1938</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/lz5qfzjp</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_193850-lz5qfzjp\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j93sni7j with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_200038-j93sni7j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j' target=\"_blank\">jumping-sweep-6</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jumping-sweep-6</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_200038-j93sni7j\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_200043-j93sni7j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j' target=\"_blank\">gpt2_hyper_2000</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 0.5313, time 34220.49ms\n",
            "iter 1: loss 1.3984, time 1646.47ms\n",
            "iter 2: loss 5.5000, time 1647.91ms\n",
            "iter 3: loss 3.1563, time 1601.02ms\n",
            "iter 4: loss 0.8750, time 1605.89ms\n",
            "iter 5: loss 0.7187, time 1921.41ms\n",
            "iter 6: loss 0.8281, time 1616.79ms\n",
            "iter 7: loss 0.8477, time 1701.44ms\n",
            "iter 8: loss 1.5469, time 1631.03ms\n",
            "iter 9: loss 1.2188, time 1621.06ms\n",
            "iter 10: loss 1.2578, time 1695.66ms\n",
            "iter 11: loss 1.3750, time 1666.27ms\n",
            "iter 12: loss 0.4668, time 1613.33ms\n",
            "iter 13: loss 1.0703, time 1703.05ms\n",
            "iter 14: loss 1.1328, time 1704.30ms\n",
            "iter 15: loss 0.5898, time 1679.54ms\n",
            "iter 16: loss 1.0078, time 1628.12ms\n",
            "iter 17: loss 0.8984, time 1896.12ms\n",
            "iter 18: loss 1.0859, time 1649.96ms\n",
            "iter 19: loss 0.8477, time 1626.19ms\n",
            "Step 20: train loss 1.2807, val loss 1.1323\n",
            "Validation accuracy: 0.7678\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.6523, time 16730.37ms\n",
            "iter 21: loss 0.8672, time 1618.70ms\n",
            "iter 22: loss 0.4395, time 1650.16ms\n",
            "iter 23: loss 0.6133, time 1570.83ms\n",
            "iter 24: loss 0.8242, time 1681.15ms\n",
            "iter 25: loss 0.3887, time 1633.99ms\n",
            "iter 26: loss 0.4512, time 1650.00ms\n",
            "iter 27: loss 0.9297, time 1586.32ms\n",
            "iter 28: loss 0.0205, time 1615.82ms\n",
            "iter 29: loss 0.1377, time 1638.68ms\n",
            "iter 30: loss 0.0038, time 1610.09ms\n",
            "iter 31: loss 0.0313, time 1657.38ms\n",
            "iter 32: loss 0.0113, time 1645.60ms\n",
            "iter 33: loss 0.0148, time 1894.34ms\n",
            "iter 34: loss 0.3184, time 1631.54ms\n",
            "iter 35: loss 0.0569, time 1671.19ms\n",
            "iter 36: loss 0.0408, time 1610.19ms\n",
            "iter 37: loss 0.0374, time 1599.78ms\n",
            "iter 38: loss 0.2813, time 1584.27ms\n",
            "iter 39: loss 2.0469, time 1704.60ms\n",
            "Step 40: train loss 0.8628, val loss 0.9008\n",
            "Validation accuracy: 0.8772\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.0315, time 16954.84ms\n",
            "iter 41: loss 0.0184, time 1659.27ms\n",
            "iter 42: loss 0.0229, time 1576.04ms\n",
            "iter 43: loss 0.0014, time 1640.13ms\n",
            "iter 44: loss 0.0500, time 1624.46ms\n",
            "iter 45: loss 0.0085, time 1629.08ms\n",
            "iter 46: loss 0.0013, time 1572.79ms\n",
            "iter 47: loss 0.0048, time 1633.71ms\n",
            "iter 48: loss 0.0518, time 1906.58ms\n",
            "iter 49: loss 0.0210, time 1626.90ms\n",
            "iter 50: loss 0.0126, time 1580.12ms\n",
            "iter 51: loss 0.0044, time 1628.12ms\n",
            "iter 52: loss 0.1357, time 1615.26ms\n",
            "iter 53: loss 0.0049, time 1610.63ms\n",
            "iter 54: loss 0.0566, time 1627.30ms\n",
            "iter 55: loss 0.0003, time 1655.46ms\n",
            "iter 56: loss 0.0114, time 1670.57ms\n",
            "iter 57: loss 0.0010, time 1843.20ms\n",
            "iter 58: loss 0.0188, time 1758.87ms\n",
            "iter 59: loss 0.0021, time 1664.08ms\n",
            "Step 60: train loss 0.6569, val loss 0.7601\n",
            "Validation accuracy: 0.9453\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.4902, time 17121.22ms\n",
            "iter 61: loss 0.0583, time 1896.65ms\n",
            "iter 62: loss 0.0104, time 1671.80ms\n",
            "iter 63: loss 0.0313, time 1646.90ms\n",
            "iter 64: loss 0.0197, time 1641.36ms\n",
            "iter 65: loss 1.7969, time 1629.95ms\n",
            "iter 66: loss 0.5508, time 1698.97ms\n",
            "iter 67: loss 0.0013, time 1585.31ms\n",
            "iter 68: loss 0.0025, time 1653.10ms\n",
            "iter 69: loss 0.0132, time 1612.53ms\n",
            "iter 70: loss 0.0009, time 1619.05ms\n",
            "iter 71: loss 0.0010, time 1572.78ms\n",
            "iter 72: loss 0.0014, time 1662.90ms\n",
            "iter 73: loss 0.0002, time 1686.75ms\n",
            "iter 74: loss 0.0371, time 1946.92ms\n",
            "iter 75: loss 0.0003, time 1657.75ms\n",
            "iter 76: loss 0.0098, time 1630.34ms\n",
            "iter 77: loss 0.0105, time 1626.84ms\n",
            "iter 78: loss 0.0020, time 1643.15ms\n",
            "iter 79: loss 0.0001, time 1601.27ms\n",
            "Step 80: train loss 0.5222, val loss 0.6909\n",
            "Validation accuracy: 0.9541\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0003, time 16932.33ms\n",
            "iter 81: loss 0.0002, time 1594.97ms\n",
            "iter 82: loss 0.0571, time 1622.68ms\n",
            "iter 83: loss 0.0481, time 1634.31ms\n",
            "iter 84: loss 0.0007, time 1640.40ms\n",
            "iter 85: loss 0.0004, time 1727.79ms\n",
            "iter 86: loss 0.2441, time 1593.18ms\n",
            "iter 87: loss 0.0001, time 1551.23ms\n",
            "iter 88: loss 0.0002, time 1637.44ms\n",
            "iter 89: loss 0.0003, time 1908.88ms\n",
            "iter 90: loss 0.0002, time 1653.72ms\n",
            "iter 91: loss 0.0007, time 1599.35ms\n",
            "iter 92: loss 0.0001, time 1611.25ms\n",
            "iter 93: loss 0.0002, time 1588.77ms\n",
            "iter 94: loss 0.0001, time 1582.08ms\n",
            "iter 95: loss 0.0001, time 1583.15ms\n",
            "iter 96: loss 0.0001, time 1600.01ms\n",
            "iter 97: loss 0.0126, time 1582.25ms\n",
            "iter 98: loss 0.0030, time 1640.50ms\n",
            "iter 99: loss 0.0007, time 1662.07ms\n",
            "Step 100: train loss 0.4360, val loss 0.7276\n",
            "Validation accuracy: 0.8935\n",
            "Test accuracy 0.7333\n",
            "iter 100: loss 0.0002, time 16249.16ms\n",
            "iter 101: loss 1.5234, time 1631.62ms\n",
            "iter 102: loss 0.0003, time 1622.14ms\n",
            "iter 103: loss 0.0030, time 1902.63ms\n",
            "iter 104: loss 0.0001, time 1606.15ms\n",
            "iter 105: loss 0.0000, time 1594.05ms\n",
            "iter 106: loss 0.0001, time 1549.82ms\n",
            "iter 107: loss 0.0072, time 1579.80ms\n",
            "iter 108: loss 0.0000, time 1630.73ms\n",
            "iter 109: loss 0.0113, time 1638.61ms\n",
            "iter 110: loss 0.0032, time 1627.38ms\n",
            "iter 111: loss 0.0000, time 1625.30ms\n",
            "iter 112: loss 0.0001, time 1588.79ms\n",
            "iter 113: loss 0.0001, time 1588.18ms\n",
            "iter 114: loss 0.0001, time 1578.88ms\n",
            "iter 115: loss 0.0052, time 1575.40ms\n",
            "iter 116: loss 0.0009, time 1582.48ms\n",
            "iter 117: loss 0.9453, time 1546.93ms\n",
            "iter 118: loss 0.0000, time 1904.05ms\n",
            "iter 119: loss 0.0000, time 1701.17ms\n",
            "Step 120: train loss 0.3785, val loss 0.6175\n",
            "Validation accuracy: 0.9630\n",
            "iter 120: loss 0.0001, time 15021.15ms\n",
            "iter 121: loss 0.0123, time 1638.19ms\n",
            "iter 122: loss 0.0193, time 1593.99ms\n",
            "iter 123: loss 0.0000, time 1608.59ms\n",
            "iter 124: loss 0.0003, time 1576.49ms\n",
            "iter 125: loss 0.0000, time 1619.70ms\n",
            "iter 126: loss 0.0003, time 1613.93ms\n",
            "iter 127: loss 0.0001, time 1647.59ms\n",
            "iter 128: loss 0.0001, time 1672.38ms\n",
            "iter 129: loss 0.0004, time 1562.80ms\n",
            "iter 130: loss 0.0008, time 1587.78ms\n",
            "iter 131: loss 0.0002, time 1588.50ms\n",
            "iter 132: loss 0.0086, time 1929.90ms\n",
            "iter 133: loss 0.0000, time 1638.66ms\n",
            "iter 134: loss 0.0000, time 1627.30ms\n",
            "iter 135: loss 0.0000, time 1565.72ms\n",
            "iter 136: loss 0.0000, time 1554.62ms\n",
            "iter 137: loss 0.0029, time 1595.77ms\n",
            "iter 138: loss 0.0009, time 1623.75ms\n",
            "iter 139: loss 0.0004, time 1585.87ms\n",
            "Step 140: train loss 0.3290, val loss 0.5901\n",
            "Validation accuracy: 0.9704\n",
            "iter 140: loss 0.0000, time 15247.81ms\n",
            "iter 141: loss 0.0018, time 1607.44ms\n",
            "iter 142: loss 0.0361, time 1618.10ms\n",
            "iter 143: loss 0.0001, time 1595.59ms\n",
            "iter 144: loss 0.0000, time 1623.47ms\n",
            "iter 145: loss 0.0000, time 1598.40ms\n",
            "iter 146: loss 0.0045, time 1652.81ms\n",
            "iter 147: loss 0.0000, time 1884.78ms\n",
            "iter 148: loss 0.0011, time 1621.26ms\n",
            "iter 149: loss 0.0002, time 1618.73ms\n",
            "iter 150: loss 0.0000, time 1556.53ms\n",
            "iter 151: loss 0.0000, time 1579.88ms\n",
            "iter 152: loss 0.0000, time 1562.56ms\n",
            "iter 153: loss 0.0000, time 1584.13ms\n",
            "iter 154: loss 0.0000, time 1606.37ms\n",
            "iter 155: loss 0.0005, time 1603.48ms\n",
            "iter 156: loss 0.0000, time 1617.52ms\n",
            "iter 157: loss 0.0000, time 1641.83ms\n",
            "iter 158: loss 0.0000, time 1622.17ms\n",
            "iter 159: loss 0.0002, time 1655.05ms\n",
            "Step 160: train loss 0.2918, val loss 0.6151\n",
            "Validation accuracy: 0.9896\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 160: loss 0.0000, time 16788.06ms\n",
            "iter 161: loss 0.0000, time 1923.61ms\n",
            "iter 162: loss 0.0000, time 1614.82ms\n",
            "iter 163: loss 0.0001, time 1659.33ms\n",
            "iter 164: loss 0.0000, time 1585.24ms\n",
            "iter 165: loss 0.0000, time 1631.78ms\n",
            "iter 166: loss 0.0000, time 1597.13ms\n",
            "iter 167: loss 0.0000, time 1642.25ms\n",
            "iter 168: loss 0.0000, time 1613.90ms\n",
            "iter 169: loss 0.0000, time 1590.05ms\n",
            "iter 170: loss 0.0171, time 1697.33ms\n",
            "iter 171: loss 0.0032, time 1705.04ms\n",
            "iter 172: loss 0.0000, time 1605.96ms\n",
            "iter 173: loss 0.0001, time 1624.88ms\n",
            "iter 174: loss 0.0001, time 1641.50ms\n",
            "iter 175: loss 0.0000, time 1960.67ms\n",
            "iter 176: loss 0.0000, time 1622.09ms\n",
            "iter 177: loss 0.0000, time 1582.65ms\n",
            "iter 178: loss 0.0000, time 1642.94ms\n",
            "iter 179: loss 0.1006, time 1618.81ms\n",
            "Step 180: train loss 0.2606, val loss 0.6203\n",
            "Validation accuracy: 0.9186\n",
            "iter 180: loss 0.0000, time 15257.54ms\n",
            "iter 181: loss 0.0000, time 1624.52ms\n",
            "iter 182: loss 0.0003, time 1689.95ms\n",
            "iter 183: loss 0.0007, time 1620.03ms\n",
            "iter 184: loss 0.0000, time 1639.29ms\n",
            "iter 185: loss 0.0001, time 1671.03ms\n",
            "iter 186: loss 0.0006, time 1763.65ms\n",
            "iter 187: loss 0.1592, time 1640.34ms\n",
            "iter 188: loss 0.0000, time 1674.49ms\n",
            "iter 189: loss 0.0000, time 1905.70ms\n",
            "iter 190: loss 0.0000, time 1660.21ms\n",
            "iter 191: loss 0.0000, time 1691.99ms\n",
            "iter 192: loss 0.0020, time 1649.31ms\n",
            "iter 193: loss 0.0000, time 1709.82ms\n",
            "iter 194: loss 0.0052, time 1631.15ms\n",
            "iter 195: loss 0.0002, time 1621.51ms\n",
            "iter 196: loss 0.0000, time 1565.02ms\n",
            "iter 197: loss 0.0000, time 1644.64ms\n",
            "iter 198: loss 0.0000, time 1611.33ms\n",
            "iter 199: loss 0.0001, time 1617.47ms\n",
            "Step 200: train loss 0.2381, val loss 0.5825\n",
            "Validation accuracy: 0.9719\n",
            "Test accuracy 0.7000\n",
            "iter 200: loss 0.0001, time 16029.00ms\n",
            "iter 201: loss 0.0001, time 1666.01ms\n",
            "iter 202: loss 0.0000, time 1949.70ms\n",
            "iter 203: loss 0.0003, time 1665.78ms\n",
            "iter 204: loss 0.0000, time 1601.83ms\n",
            "iter 205: loss 0.0000, time 1612.26ms\n",
            "iter 206: loss 0.0000, time 1645.37ms\n",
            "iter 207: loss 0.0000, time 1629.59ms\n",
            "iter 208: loss 0.0000, time 1627.72ms\n",
            "iter 209: loss 0.0000, time 1629.98ms\n",
            "iter 210: loss 0.2236, time 1547.80ms\n",
            "iter 211: loss 0.0476, time 1601.46ms\n",
            "iter 212: loss 0.0000, time 1682.49ms\n",
            "iter 213: loss 0.0000, time 1666.08ms\n",
            "iter 214: loss 0.0000, time 1618.59ms\n",
            "iter 215: loss 0.0000, time 1650.12ms\n",
            "iter 216: loss 0.0000, time 1943.56ms\n",
            "iter 217: loss 0.0000, time 1597.89ms\n",
            "iter 218: loss 0.0003, time 1619.31ms\n",
            "iter 219: loss 0.0000, time 1662.48ms\n",
            "Step 220: train loss 0.2181, val loss 0.6140\n",
            "Validation accuracy: 0.9867\n",
            "iter 220: loss 0.0000, time 15140.79ms\n",
            "iter 221: loss 0.0000, time 1579.07ms\n",
            "iter 222: loss 0.0000, time 1662.77ms\n",
            "iter 223: loss 0.0000, time 1650.42ms\n",
            "iter 224: loss 0.0056, time 1608.95ms\n",
            "iter 225: loss 0.0000, time 1666.78ms\n",
            "iter 226: loss 0.0000, time 1659.50ms\n",
            "iter 227: loss 0.0000, time 1598.90ms\n",
            "iter 228: loss 0.0000, time 1659.46ms\n",
            "iter 229: loss 0.0000, time 1957.34ms\n",
            "iter 230: loss 0.0000, time 1674.27ms\n",
            "iter 231: loss 0.0000, time 1614.00ms\n",
            "iter 232: loss 0.0000, time 1633.50ms\n",
            "iter 233: loss 0.0000, time 1618.51ms\n",
            "iter 234: loss 0.0000, time 1595.61ms\n",
            "iter 235: loss 0.0004, time 1613.15ms\n",
            "iter 236: loss 0.0000, time 1692.39ms\n",
            "iter 237: loss 0.0000, time 1648.15ms\n",
            "iter 238: loss 0.0013, time 1655.56ms\n",
            "iter 239: loss 0.0000, time 1658.05ms\n",
            "Step 240: train loss 0.2007, val loss 0.5961\n",
            "Validation accuracy: 0.9837\n",
            "iter 240: loss 0.0000, time 15228.36ms\n",
            "iter 241: loss 0.0000, time 1711.01ms\n",
            "iter 242: loss 0.0000, time 1679.37ms\n",
            "iter 243: loss 0.0000, time 1629.95ms\n",
            "iter 244: loss 0.0000, time 1694.96ms\n",
            "iter 245: loss 0.0000, time 1930.09ms\n",
            "iter 246: loss 0.0000, time 1690.25ms\n",
            "iter 247: loss 0.0000, time 1644.70ms\n",
            "iter 248: loss 0.0000, time 1666.95ms\n",
            "iter 249: loss 0.0012, time 1640.34ms\n",
            "iter 250: loss 0.0001, time 1666.03ms\n",
            "iter 251: loss 3.7188, time 1679.45ms\n",
            "iter 252: loss 0.0000, time 1624.58ms\n",
            "iter 253: loss 0.0000, time 1652.31ms\n",
            "iter 254: loss 0.0000, time 1609.50ms\n",
            "iter 255: loss 0.0000, time 1681.46ms\n",
            "iter 256: loss 0.0007, time 1630.99ms\n",
            "iter 257: loss 0.0001, time 1646.47ms\n",
            "iter 258: loss 0.0000, time 1944.09ms\n",
            "iter 259: loss 0.0000, time 1632.46ms\n",
            "Step 260: train loss 0.1857, val loss 0.6010\n",
            "Validation accuracy: 0.9808\n",
            "iter 260: loss 0.0000, time 15282.99ms\n",
            "iter 261: loss 0.0000, time 1651.74ms\n",
            "iter 262: loss 0.0001, time 1643.81ms\n",
            "iter 263: loss 0.0000, time 1609.62ms\n",
            "iter 264: loss 0.0000, time 1605.19ms\n",
            "iter 265: loss 0.0000, time 1634.37ms\n",
            "iter 266: loss 0.0000, time 1616.42ms\n",
            "iter 267: loss 0.0000, time 1617.88ms\n",
            "iter 268: loss 0.0000, time 1626.32ms\n",
            "iter 269: loss 0.0000, time 1612.03ms\n",
            "iter 270: loss 0.0000, time 1595.78ms\n",
            "iter 271: loss 0.0000, time 1644.08ms\n",
            "iter 272: loss 0.0000, time 1964.60ms\n",
            "iter 273: loss 0.0000, time 1624.95ms\n",
            "iter 274: loss 0.0003, time 1552.37ms\n",
            "iter 275: loss 0.0000, time 1639.00ms\n",
            "iter 276: loss 0.0000, time 1640.14ms\n",
            "iter 277: loss 0.0000, time 1692.29ms\n",
            "iter 278: loss 0.0000, time 1677.43ms\n",
            "iter 279: loss 0.0000, time 1559.11ms\n",
            "Step 280: train loss 0.1744, val loss 0.6010\n",
            "Validation accuracy: 0.9808\n",
            "iter 280: loss 0.0038, time 15091.60ms\n",
            "iter 281: loss 0.0000, time 1648.21ms\n",
            "iter 282: loss 0.0000, time 1613.36ms\n",
            "iter 283: loss 0.0000, time 1572.29ms\n",
            "iter 284: loss 0.0000, time 1637.05ms\n",
            "iter 285: loss 0.0001, time 1648.66ms\n",
            "iter 286: loss 0.0000, time 1679.44ms\n",
            "iter 287: loss 0.1387, time 1924.61ms\n",
            "iter 288: loss 0.0000, time 1711.33ms\n",
            "iter 289: loss 0.0000, time 1691.91ms\n",
            "iter 290: loss 0.0000, time 1611.55ms\n",
            "iter 291: loss 0.0000, time 1635.16ms\n",
            "iter 292: loss 0.0000, time 1629.42ms\n",
            "iter 293: loss 0.0000, time 1594.92ms\n",
            "iter 294: loss 0.0000, time 1595.19ms\n",
            "iter 295: loss 0.0001, time 1735.60ms\n",
            "iter 296: loss 0.0000, time 1671.09ms\n",
            "iter 297: loss 0.0000, time 1735.60ms\n",
            "iter 298: loss 0.0000, time 1627.93ms\n",
            "iter 299: loss 0.0000, time 1642.62ms\n",
            "Step 300: train loss 0.1634, val loss 0.6462\n",
            "Validation accuracy: 0.9704\n",
            "Test accuracy 0.7000\n",
            "iter 300: loss 0.0000, time 16348.87ms\n",
            "iter 301: loss 0.0000, time 1578.71ms\n",
            "iter 302: loss 0.0000, time 1584.12ms\n",
            "iter 303: loss 0.0001, time 1622.76ms\n",
            "iter 304: loss 0.0000, time 1639.33ms\n",
            "iter 305: loss 0.0002, time 1638.70ms\n",
            "iter 306: loss 0.0001, time 1600.83ms\n",
            "iter 307: loss 0.0000, time 1550.71ms\n",
            "iter 308: loss 0.0000, time 1642.80ms\n",
            "iter 309: loss 0.0000, time 1584.77ms\n",
            "iter 310: loss 0.0000, time 1629.06ms\n",
            "iter 311: loss 0.0000, time 1615.97ms\n",
            "iter 312: loss 0.0000, time 1586.75ms\n",
            "iter 313: loss 0.0000, time 1603.69ms\n",
            "iter 314: loss 0.0000, time 1901.91ms\n",
            "iter 315: loss 0.0000, time 1636.86ms\n",
            "iter 316: loss 0.0000, time 1615.42ms\n",
            "iter 317: loss 0.0000, time 1561.78ms\n",
            "iter 318: loss 0.0007, time 1614.07ms\n",
            "iter 319: loss 0.0000, time 1616.00ms\n",
            "Step 320: train loss 0.1545, val loss 0.6213\n",
            "Validation accuracy: 0.9749\n",
            "iter 320: loss 0.0000, time 15136.40ms\n",
            "iter 321: loss 0.0000, time 1556.29ms\n",
            "iter 322: loss 0.0000, time 1585.03ms\n",
            "iter 323: loss 0.0000, time 1597.15ms\n",
            "iter 324: loss 0.0000, time 1638.21ms\n",
            "iter 325: loss 0.0000, time 1593.05ms\n",
            "iter 326: loss 0.0000, time 1599.52ms\n",
            "iter 327: loss 0.0001, time 1628.95ms\n",
            "iter 328: loss 0.0000, time 1616.27ms\n",
            "iter 329: loss 0.0493, time 1908.22ms\n",
            "iter 330: loss 0.0000, time 1602.19ms\n",
            "iter 331: loss 0.0000, time 1633.00ms\n",
            "iter 332: loss 0.0000, time 1575.64ms\n",
            "iter 333: loss 0.0000, time 1574.96ms\n",
            "iter 334: loss 0.0000, time 1634.20ms\n",
            "iter 335: loss 0.0000, time 1597.20ms\n",
            "iter 336: loss 0.0000, time 1622.92ms\n",
            "iter 337: loss 0.0000, time 1607.70ms\n",
            "iter 338: loss 0.0000, time 1643.75ms\n",
            "iter 339: loss 0.0004, time 1619.14ms\n",
            "Step 340: train loss 0.1457, val loss 0.6227\n",
            "Validation accuracy: 0.9852\n",
            "iter 340: loss 0.0000, time 15235.54ms\n",
            "iter 341: loss 0.0001, time 1633.89ms\n",
            "iter 342: loss 0.0000, time 1628.70ms\n",
            "iter 343: loss 0.0010, time 1906.01ms\n",
            "iter 344: loss 0.0000, time 1619.51ms\n",
            "iter 345: loss 0.0000, time 1666.30ms\n",
            "iter 346: loss 0.0000, time 1595.07ms\n",
            "iter 347: loss 0.0000, time 1618.59ms\n",
            "iter 348: loss 0.0000, time 1612.39ms\n",
            "iter 349: loss 0.0000, time 1599.68ms\n",
            "iter 350: loss 0.0000, time 1639.23ms\n",
            "iter 351: loss 0.0000, time 1593.17ms\n",
            "iter 352: loss 0.0000, time 1651.77ms\n",
            "iter 353: loss 0.0000, time 1602.30ms\n",
            "iter 354: loss 0.0000, time 1625.68ms\n",
            "iter 355: loss 0.0000, time 1733.57ms\n",
            "iter 356: loss 0.0000, time 1926.54ms\n",
            "iter 357: loss 0.0088, time 1573.82ms\n",
            "iter 358: loss 0.0000, time 1626.58ms\n",
            "iter 359: loss 0.0000, time 1568.19ms\n",
            "Step 360: train loss 0.1378, val loss 0.6435\n",
            "Validation accuracy: 0.9837\n",
            "iter 360: loss 0.0000, time 15098.85ms\n",
            "iter 361: loss 0.0000, time 1587.39ms\n",
            "iter 362: loss 0.0000, time 1586.68ms\n",
            "iter 363: loss 0.0000, time 1564.00ms\n",
            "iter 364: loss 0.0000, time 1589.94ms\n",
            "iter 365: loss 0.0001, time 1590.96ms\n",
            "iter 366: loss 0.0000, time 1649.56ms\n",
            "iter 367: loss 0.0000, time 1649.51ms\n",
            "iter 368: loss 0.0000, time 1634.08ms\n",
            "iter 369: loss 0.0000, time 1610.53ms\n",
            "iter 370: loss 0.0000, time 1653.68ms\n",
            "iter 371: loss 0.0004, time 1894.41ms\n",
            "iter 372: loss 0.0000, time 1638.48ms\n",
            "iter 373: loss 0.0000, time 1579.42ms\n",
            "iter 374: loss 0.0014, time 1653.39ms\n",
            "iter 375: loss 0.0000, time 1660.39ms\n",
            "iter 376: loss 0.0000, time 1614.46ms\n",
            "iter 377: loss 0.0000, time 1580.01ms\n",
            "iter 378: loss 0.0000, time 1609.59ms\n",
            "iter 379: loss 0.0000, time 1606.58ms\n",
            "Step 380: train loss 0.1308, val loss 0.6359\n",
            "Validation accuracy: 0.9778\n",
            "iter 380: loss 0.0000, time 15283.28ms\n",
            "iter 381: loss 0.0000, time 1653.86ms\n",
            "iter 382: loss 0.0000, time 1635.32ms\n",
            "iter 383: loss 0.0000, time 1638.83ms\n",
            "iter 384: loss 0.0000, time 1588.84ms\n",
            "iter 385: loss 0.0000, time 1918.61ms\n",
            "iter 386: loss 0.0000, time 1642.01ms\n",
            "iter 387: loss 0.0000, time 1588.85ms\n",
            "iter 388: loss 0.0000, time 1621.94ms\n",
            "iter 389: loss 0.0000, time 1610.90ms\n",
            "iter 390: loss 0.0000, time 1664.26ms\n",
            "iter 391: loss 0.0000, time 1610.70ms\n",
            "iter 392: loss 0.0085, time 1588.51ms\n",
            "iter 393: loss 0.0000, time 1661.31ms\n",
            "iter 394: loss 0.0000, time 1609.48ms\n",
            "iter 395: loss 0.0000, time 1594.50ms\n",
            "iter 396: loss 0.0000, time 1635.94ms\n",
            "iter 397: loss 0.0000, time 1606.00ms\n",
            "iter 398: loss 0.0000, time 1642.83ms\n",
            "iter 399: loss 0.0000, time 1930.30ms\n",
            "Step 400: train loss 0.1243, val loss 0.6302\n",
            "Validation accuracy: 0.9778\n",
            "Test accuracy 0.6667\n",
            "iter 400: loss 0.0000, time 16132.02ms\n",
            "iter 401: loss 0.0000, time 1595.53ms\n",
            "iter 402: loss 0.0000, time 1630.72ms\n",
            "iter 403: loss 0.0000, time 1649.74ms\n",
            "iter 404: loss 0.0000, time 1663.08ms\n",
            "iter 405: loss 0.0000, time 1632.10ms\n",
            "iter 406: loss 0.0000, time 1609.31ms\n",
            "iter 407: loss 0.0000, time 1587.22ms\n",
            "iter 408: loss 0.0000, time 1691.13ms\n",
            "iter 409: loss 0.0000, time 1563.97ms\n",
            "iter 410: loss 0.0000, time 1604.42ms\n",
            "iter 411: loss 0.0000, time 1623.61ms\n",
            "iter 412: loss 0.0000, time 1624.23ms\n",
            "iter 413: loss 0.0000, time 1924.47ms\n",
            "iter 414: loss 0.0002, time 1614.91ms\n",
            "iter 415: loss 0.0000, time 1724.92ms\n",
            "iter 416: loss 0.0000, time 1676.03ms\n",
            "iter 417: loss 0.0000, time 1694.99ms\n",
            "iter 418: loss 0.0000, time 1652.83ms\n",
            "iter 419: loss 0.0000, time 1659.51ms\n",
            "Step 420: train loss 0.1184, val loss 0.6380\n",
            "Validation accuracy: 0.9822\n",
            "iter 420: loss 0.0000, time 15283.33ms\n",
            "iter 421: loss 0.0000, time 1626.79ms\n",
            "iter 422: loss 0.0000, time 1587.61ms\n",
            "iter 423: loss 0.0000, time 1683.55ms\n",
            "iter 424: loss 0.0000, time 1673.66ms\n",
            "iter 425: loss 0.0000, time 1679.77ms\n",
            "iter 426: loss 0.0000, time 1675.90ms\n",
            "iter 427: loss 0.0000, time 1994.15ms\n",
            "iter 428: loss 0.0000, time 1710.40ms\n",
            "iter 429: loss 0.0000, time 1632.76ms\n",
            "iter 430: loss 0.0000, time 1590.93ms\n",
            "iter 431: loss 0.0000, time 1612.03ms\n",
            "iter 432: loss 0.0000, time 1648.04ms\n",
            "iter 433: loss 0.0000, time 1619.35ms\n",
            "iter 434: loss 0.0000, time 1606.63ms\n",
            "iter 435: loss 0.0001, time 1622.71ms\n",
            "iter 436: loss 0.0000, time 1531.47ms\n",
            "iter 437: loss 0.0000, time 1625.02ms\n",
            "iter 438: loss 0.0000, time 1585.15ms\n",
            "iter 439: loss 0.0000, time 1601.44ms\n",
            "Step 440: train loss 0.1130, val loss 0.6596\n",
            "Validation accuracy: 0.9837\n",
            "iter 440: loss 0.0000, time 15121.64ms\n",
            "iter 441: loss 0.0000, time 1925.62ms\n",
            "iter 442: loss 0.0000, time 1574.15ms\n",
            "iter 443: loss 0.0000, time 1580.69ms\n",
            "iter 444: loss 0.0000, time 1653.34ms\n",
            "iter 445: loss 0.0000, time 1591.59ms\n",
            "iter 446: loss 0.0000, time 1633.84ms\n",
            "iter 447: loss 0.0000, time 1675.41ms\n",
            "iter 448: loss 0.0000, time 1643.93ms\n",
            "iter 449: loss 0.0000, time 1559.91ms\n",
            "iter 450: loss 0.0000, time 1616.22ms\n",
            "iter 451: loss 0.0000, time 1622.57ms\n",
            "iter 452: loss 0.0000, time 1590.79ms\n",
            "iter 453: loss 0.0000, time 1618.39ms\n",
            "iter 454: loss 0.0000, time 1653.20ms\n",
            "iter 455: loss 0.0000, time 1953.14ms\n",
            "iter 456: loss 0.0000, time 1656.95ms\n",
            "iter 457: loss 0.0000, time 1683.59ms\n",
            "iter 458: loss 0.0000, time 1651.58ms\n",
            "iter 459: loss 0.0001, time 1664.97ms\n",
            "Step 460: train loss 0.1102, val loss 0.6899\n",
            "Validation accuracy: 0.9852\n",
            "iter 460: loss 0.0000, time 15098.56ms\n",
            "iter 461: loss 0.0000, time 1625.74ms\n",
            "iter 462: loss 0.0000, time 1669.46ms\n",
            "iter 463: loss 0.0000, time 1639.55ms\n",
            "iter 464: loss 0.0000, time 1625.91ms\n",
            "iter 465: loss 0.0001, time 1674.77ms\n",
            "iter 466: loss 0.0000, time 1634.85ms\n",
            "iter 467: loss 0.0000, time 1627.48ms\n",
            "iter 468: loss 0.0000, time 1697.79ms\n",
            "iter 469: loss 0.0000, time 1995.40ms\n",
            "iter 470: loss 0.0009, time 1644.82ms\n",
            "iter 471: loss 0.0000, time 1600.62ms\n",
            "iter 472: loss 0.0000, time 1586.86ms\n",
            "iter 473: loss 0.0000, time 1599.45ms\n",
            "iter 474: loss 0.0000, time 1590.56ms\n",
            "iter 475: loss 0.0000, time 1686.06ms\n",
            "iter 476: loss 0.0001, time 1610.12ms\n",
            "iter 477: loss 0.0000, time 1582.85ms\n",
            "iter 478: loss 0.0000, time 1644.39ms\n",
            "iter 479: loss 0.0002, time 1649.23ms\n",
            "Step 480: train loss 0.1061, val loss 0.6794\n",
            "Validation accuracy: 0.9822\n",
            "iter 480: loss 0.0000, time 15103.30ms\n",
            "iter 481: loss 0.0000, time 1648.59ms\n",
            "iter 482: loss 0.0000, time 1620.71ms\n",
            "iter 483: loss 0.0000, time 1634.84ms\n",
            "iter 484: loss 0.0000, time 1645.10ms\n",
            "iter 485: loss 0.0000, time 1996.83ms\n",
            "iter 486: loss 0.0000, time 1629.32ms\n",
            "iter 487: loss 0.0001, time 1630.32ms\n",
            "iter 488: loss 0.0000, time 1653.13ms\n",
            "iter 489: loss 0.0000, time 1608.99ms\n",
            "iter 490: loss 0.0000, time 1689.43ms\n",
            "iter 491: loss 0.0000, time 1638.21ms\n",
            "iter 492: loss 0.0000, time 1606.91ms\n",
            "iter 493: loss 0.0000, time 1595.18ms\n",
            "iter 494: loss 0.0000, time 1649.33ms\n",
            "iter 495: loss 0.0000, time 1660.78ms\n",
            "iter 496: loss 0.0000, time 1651.97ms\n",
            "iter 497: loss 0.0000, time 1641.88ms\n",
            "iter 498: loss 0.0000, time 1983.43ms\n",
            "iter 499: loss 0.0000, time 1648.33ms\n",
            "Step 500: train loss 0.1020, val loss 0.6845\n",
            "Validation accuracy: 0.9793\n",
            "Test accuracy 0.7667\n",
            "iter 500: loss 0.0000, time 16065.97ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▇▇▇▆█</td></tr><tr><td>Test_F1_Score</td><td>▁█▇▇▇█</td></tr><tr><td>Test_Precision</td><td>▁▇█▇▇█</td></tr><tr><td>Test_Recall</td><td>▁▇▇▇▆█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇██▇███▇████████████████</td></tr><tr><td>val/loss</td><td> █▅▃▂▃▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.76667</td></tr><tr><td>Test_F1_Score</td><td>0.75318</td></tr><tr><td>Test_Precision</td><td>0.86275</td></tr><tr><td>Test_Recall</td><td>0.76667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.10203</td></tr><tr><td>val/acc</td><td>0.97929</td></tr><tr><td>val/loss</td><td>0.68451</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2000</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/j93sni7j</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_200043-j93sni7j\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a236qqsp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_202226-a236qqsp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp' target=\"_blank\">cerulean-sweep-7</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cerulean-sweep-7</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_202226-a236qqsp\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_202231-a236qqsp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp' target=\"_blank\">gpt2_hyper_2022</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.4231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 8.0000, time 22830.35ms\n",
            "iter 1: loss 4.2812, time 1666.39ms\n",
            "iter 2: loss 0.6836, time 1979.89ms\n",
            "iter 3: loss 0.1089, time 1646.77ms\n",
            "iter 4: loss 2.0781, time 1687.63ms\n",
            "iter 5: loss 1.8906, time 1645.90ms\n",
            "iter 6: loss 1.2812, time 1655.77ms\n",
            "iter 7: loss 0.8906, time 1667.40ms\n",
            "iter 8: loss 0.5703, time 1644.52ms\n",
            "iter 9: loss 0.8281, time 1705.27ms\n",
            "iter 10: loss 0.0228, time 1593.81ms\n",
            "iter 11: loss 0.0825, time 1745.85ms\n",
            "iter 12: loss 0.1377, time 1708.21ms\n",
            "iter 13: loss 0.2715, time 1680.68ms\n",
            "iter 14: loss 0.2422, time 2012.74ms\n",
            "iter 15: loss 0.4766, time 1622.41ms\n",
            "iter 16: loss 0.5078, time 1660.60ms\n",
            "iter 17: loss 0.6016, time 1668.33ms\n",
            "iter 18: loss 0.5156, time 1741.73ms\n",
            "iter 19: loss 0.2793, time 1711.13ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 1.1148, val loss 0.4880\n",
            "Validation accuracy: 0.7279\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.8945, time 23515.40ms\n",
            "iter 21: loss 0.0332, time 1690.93ms\n",
            "iter 22: loss 0.0674, time 1701.07ms\n",
            "iter 23: loss 0.4043, time 1615.10ms\n",
            "iter 24: loss 0.0649, time 1700.70ms\n",
            "iter 25: loss 0.2324, time 1653.45ms\n",
            "iter 26: loss 0.2061, time 1702.20ms\n",
            "iter 27: loss 0.0273, time 1679.13ms\n",
            "iter 28: loss 0.1118, time 1646.83ms\n",
            "iter 29: loss 0.0030, time 1737.82ms\n",
            "iter 30: loss 0.1328, time 1616.32ms\n",
            "iter 31: loss 0.9727, time 1681.32ms\n",
            "iter 32: loss 0.1338, time 1707.22ms\n",
            "iter 33: loss 0.0469, time 1968.10ms\n",
            "iter 34: loss 0.9023, time 1765.44ms\n",
            "iter 35: loss 0.0552, time 1668.37ms\n",
            "iter 36: loss 4.9062, time 1697.34ms\n",
            "iter 37: loss 0.0084, time 1662.41ms\n",
            "iter 38: loss 0.3086, time 1671.52ms\n",
            "iter 39: loss 0.5547, time 1714.36ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.7772, val loss 0.5511\n",
            "Validation accuracy: 0.9050\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.3906, time 23569.39ms\n",
            "iter 41: loss 0.1201, time 1601.62ms\n",
            "iter 42: loss 2.6250, time 1697.96ms\n",
            "iter 43: loss 0.1177, time 1691.76ms\n",
            "iter 44: loss 0.0300, time 1730.11ms\n",
            "iter 45: loss 2.6562, time 1669.96ms\n",
            "iter 46: loss 0.1504, time 1743.88ms\n",
            "iter 47: loss 0.0688, time 1700.23ms\n",
            "iter 48: loss 0.2324, time 2012.52ms\n",
            "iter 49: loss 0.4590, time 1746.65ms\n",
            "iter 50: loss 0.1133, time 1796.49ms\n",
            "iter 51: loss 0.9297, time 1656.35ms\n",
            "iter 52: loss 0.0168, time 1643.41ms\n",
            "iter 53: loss 5.0313, time 1699.82ms\n",
            "iter 54: loss 0.0084, time 1731.62ms\n",
            "iter 55: loss 0.0884, time 1667.00ms\n",
            "iter 56: loss 0.0183, time 1676.03ms\n",
            "iter 57: loss 0.1982, time 1693.43ms\n",
            "iter 58: loss 0.0452, time 1672.17ms\n",
            "iter 59: loss 0.0057, time 1711.00ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.6129, val loss 0.6159\n",
            "Validation accuracy: 0.8672\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.0050, time 23737.66ms\n",
            "iter 61: loss 0.4844, time 1658.94ms\n",
            "iter 62: loss 0.0029, time 1669.52ms\n",
            "iter 63: loss 0.0178, time 1713.65ms\n",
            "iter 64: loss 0.0010, time 1691.05ms\n",
            "iter 65: loss 0.0430, time 1959.22ms\n",
            "iter 66: loss 0.0157, time 1628.86ms\n",
            "iter 67: loss 0.0038, time 1728.19ms\n",
            "iter 68: loss 0.0045, time 1666.69ms\n",
            "iter 69: loss 0.0008, time 1713.76ms\n",
            "iter 70: loss 0.0004, time 1737.58ms\n",
            "iter 71: loss 0.0080, time 1699.95ms\n",
            "iter 72: loss 0.8320, time 1682.25ms\n",
            "iter 73: loss 0.0161, time 1628.54ms\n",
            "iter 74: loss 0.0074, time 1662.32ms\n",
            "iter 75: loss 0.0713, time 1699.36ms\n",
            "iter 76: loss 0.0066, time 1703.86ms\n",
            "iter 77: loss 0.5820, time 1721.61ms\n",
            "iter 78: loss 0.0781, time 2007.89ms\n",
            "iter 79: loss 0.0154, time 1675.37ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80: train loss 0.5034, val loss 0.6231\n",
            "Validation accuracy: 0.9406\n",
            "iter 80: loss 0.0037, time 21991.37ms\n",
            "iter 81: loss 3.7969, time 1698.82ms\n",
            "iter 82: loss 0.0070, time 1673.23ms\n",
            "iter 83: loss 0.0046, time 1630.16ms\n",
            "iter 84: loss 0.0055, time 2006.53ms\n",
            "iter 85: loss 0.0007, time 1682.01ms\n",
            "iter 86: loss 0.0033, time 1694.07ms\n",
            "iter 87: loss 0.1553, time 1682.67ms\n",
            "iter 88: loss 0.0024, time 1665.90ms\n",
            "iter 89: loss 0.0031, time 1706.35ms\n",
            "iter 90: loss 0.0115, time 1639.87ms\n",
            "iter 91: loss 0.0014, time 1642.11ms\n",
            "iter 92: loss 0.0039, time 1685.79ms\n",
            "iter 93: loss 0.0005, time 1674.90ms\n",
            "iter 94: loss 0.0376, time 1727.17ms\n",
            "iter 95: loss 0.0006, time 1734.97ms\n",
            "iter 96: loss 0.0171, time 1645.38ms\n",
            "iter 97: loss 0.0101, time 2055.17ms\n",
            "iter 98: loss 0.0033, time 1663.38ms\n",
            "iter 99: loss 0.0058, time 1667.80ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100: train loss 0.4203, val loss 0.6288\n",
            "Validation accuracy: 0.9730\n",
            "Saving checkpoint to out/ckpt.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6000\n",
            "iter 100: loss 1.2344, time 24236.53ms\n",
            "iter 101: loss 0.0027, time 1639.57ms\n",
            "iter 102: loss 0.0018, time 1950.69ms\n",
            "iter 103: loss 0.0005, time 1665.44ms\n",
            "iter 104: loss 0.0020, time 1614.51ms\n",
            "iter 105: loss 0.0001, time 1625.49ms\n",
            "iter 106: loss 0.0006, time 1652.00ms\n",
            "iter 107: loss 0.0068, time 1665.32ms\n",
            "iter 108: loss 0.0000, time 1663.36ms\n",
            "iter 109: loss 0.0001, time 1685.67ms\n",
            "iter 110: loss 0.0000, time 1600.75ms\n",
            "iter 111: loss 0.0000, time 1688.06ms\n",
            "iter 112: loss 0.0010, time 1659.92ms\n",
            "iter 113: loss 0.0003, time 1937.89ms\n",
            "iter 114: loss 0.0039, time 1651.35ms\n",
            "iter 115: loss 0.0009, time 1662.22ms\n",
            "iter 116: loss 0.0001, time 1626.24ms\n",
            "iter 117: loss 0.0080, time 1631.21ms\n",
            "iter 118: loss 0.0013, time 1606.63ms\n",
            "iter 119: loss 0.0001, time 1648.39ms\n",
            "Step 120: train loss 0.3558, val loss 0.6588\n",
            "Validation accuracy: 0.9892\n",
            "iter 120: loss 0.0003, time 22086.95ms\n",
            "iter 121: loss 0.0005, time 1619.58ms\n",
            "iter 122: loss 0.0007, time 1673.04ms\n",
            "iter 123: loss 0.0041, time 1615.32ms\n",
            "iter 124: loss 0.0000, time 1660.27ms\n",
            "iter 125: loss 0.0001, time 1650.57ms\n",
            "iter 126: loss 0.0019, time 1647.17ms\n",
            "iter 127: loss 0.0018, time 1650.31ms\n",
            "iter 128: loss 0.0001, time 1668.02ms\n",
            "iter 129: loss 0.0001, time 1634.59ms\n",
            "iter 130: loss 0.0001, time 1659.84ms\n",
            "iter 131: loss 0.0001, time 1938.64ms\n",
            "iter 132: loss 0.0000, time 1650.35ms\n",
            "iter 133: loss 0.0004, time 1681.96ms\n",
            "iter 134: loss 0.0012, time 1688.20ms\n",
            "iter 135: loss 0.0001, time 1651.19ms\n",
            "iter 136: loss 0.0003, time 1707.04ms\n",
            "iter 137: loss 0.0000, time 1633.97ms\n",
            "iter 138: loss 0.0000, time 1646.46ms\n",
            "iter 139: loss 0.0000, time 1649.04ms\n",
            "Step 140: train loss 0.3094, val loss 0.7248\n",
            "Validation accuracy: 0.9827\n",
            "iter 140: loss 0.0001, time 22012.98ms\n",
            "iter 141: loss 0.0000, time 1608.12ms\n",
            "iter 142: loss 0.0002, time 1661.94ms\n",
            "iter 143: loss 0.0000, time 1640.41ms\n",
            "iter 144: loss 0.0000, time 1696.78ms\n",
            "iter 145: loss 0.0000, time 1656.63ms\n",
            "iter 146: loss 0.0000, time 1694.13ms\n",
            "iter 147: loss 0.0000, time 1652.45ms\n",
            "iter 148: loss 0.0000, time 1734.03ms\n",
            "iter 149: loss 0.0016, time 1912.15ms\n",
            "iter 150: loss 0.0002, time 1574.45ms\n",
            "iter 151: loss 0.0087, time 1652.43ms\n",
            "iter 152: loss 0.0001, time 1615.40ms\n",
            "iter 153: loss 0.0001, time 1620.42ms\n",
            "iter 154: loss 0.0010, time 1658.65ms\n",
            "iter 155: loss 0.0068, time 1680.21ms\n",
            "iter 156: loss 0.0002, time 1692.55ms\n",
            "iter 157: loss 0.0008, time 1660.74ms\n",
            "iter 158: loss 0.0002, time 1646.34ms\n",
            "iter 159: loss 0.0001, time 1614.29ms\n",
            "Step 160: train loss 0.2738, val loss 0.7617\n",
            "Validation accuracy: 0.9870\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 160: loss 0.0000, time 23602.96ms\n",
            "iter 161: loss 0.0000, time 1652.34ms\n",
            "iter 162: loss 0.0000, time 1714.79ms\n",
            "iter 163: loss 0.0001, time 1691.20ms\n",
            "iter 164: loss 0.0000, time 1642.50ms\n",
            "iter 165: loss 0.0064, time 1695.30ms\n",
            "iter 166: loss 0.0001, time 1639.16ms\n",
            "iter 167: loss 0.0000, time 1632.16ms\n",
            "iter 168: loss 0.0000, time 1633.22ms\n",
            "iter 169: loss 0.0000, time 2000.06ms\n",
            "iter 170: loss 0.0000, time 1662.07ms\n",
            "iter 171: loss 0.0000, time 1641.23ms\n",
            "iter 172: loss 0.0000, time 1596.07ms\n",
            "iter 173: loss 0.0000, time 1596.12ms\n",
            "iter 174: loss 0.0001, time 1735.13ms\n",
            "iter 175: loss 0.0000, time 1737.12ms\n",
            "iter 176: loss 0.0001, time 1691.03ms\n",
            "iter 177: loss 0.0000, time 1627.19ms\n",
            "iter 178: loss 0.0063, time 1647.66ms\n",
            "iter 179: loss 0.0000, time 1615.43ms\n",
            "Step 180: train loss 0.2462, val loss 0.7422\n",
            "Validation accuracy: 0.9849\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 180: loss 0.0000, time 23519.98ms\n",
            "iter 181: loss 0.0000, time 1708.26ms\n",
            "iter 182: loss 0.0000, time 1711.81ms\n",
            "iter 183: loss 0.0002, time 1670.36ms\n",
            "iter 184: loss 0.0001, time 1660.04ms\n",
            "iter 185: loss 0.0000, time 1626.37ms\n",
            "iter 186: loss 0.0001, time 1632.97ms\n",
            "iter 187: loss 0.0003, time 2006.72ms\n",
            "iter 188: loss 0.0001, time 1593.55ms\n",
            "iter 189: loss 0.0011, time 1714.55ms\n",
            "iter 190: loss 0.0000, time 1670.90ms\n",
            "iter 191: loss 0.0006, time 1725.98ms\n",
            "iter 192: loss 0.0000, time 1647.67ms\n",
            "iter 193: loss 0.0000, time 1641.35ms\n",
            "iter 194: loss 0.0000, time 1691.18ms\n",
            "iter 195: loss 0.0000, time 1689.52ms\n",
            "iter 196: loss 0.0000, time 1686.45ms\n",
            "iter 197: loss 0.0000, time 1679.08ms\n",
            "iter 198: loss 0.0000, time 1701.25ms\n",
            "iter 199: loss 0.0000, time 1678.20ms\n",
            "Step 200: train loss 0.2251, val loss 0.7659\n",
            "Validation accuracy: 0.9881\n",
            "Test accuracy 0.8000\n",
            "iter 200: loss 0.0000, time 22864.05ms\n",
            "iter 201: loss 0.0006, time 1663.86ms\n",
            "iter 202: loss 0.0002, time 1742.67ms\n",
            "iter 203: loss 0.0000, time 1636.56ms\n",
            "iter 204: loss 0.0000, time 1646.52ms\n",
            "iter 205: loss 0.0000, time 1943.53ms\n",
            "iter 206: loss 0.0000, time 1629.52ms\n",
            "iter 207: loss 0.0000, time 1602.86ms\n",
            "iter 208: loss 0.0000, time 1643.97ms\n",
            "iter 209: loss 0.0004, time 1693.83ms\n",
            "iter 210: loss 0.0001, time 1689.35ms\n",
            "iter 211: loss 0.0000, time 1624.25ms\n",
            "iter 212: loss 0.0000, time 1644.50ms\n",
            "iter 213: loss 0.0000, time 1693.36ms\n",
            "iter 214: loss 0.0000, time 1633.93ms\n",
            "iter 215: loss 0.0000, time 1626.06ms\n",
            "iter 216: loss 0.0050, time 1653.53ms\n",
            "iter 217: loss 0.0000, time 2004.89ms\n",
            "iter 218: loss 0.0000, time 1702.92ms\n",
            "iter 219: loss 0.0000, time 1643.47ms\n",
            "Step 220: train loss 0.2061, val loss 0.7543\n",
            "Validation accuracy: 0.9827\n",
            "iter 220: loss 0.0000, time 21661.05ms\n",
            "iter 221: loss 0.0000, time 1685.24ms\n",
            "iter 222: loss 0.0000, time 1685.14ms\n",
            "iter 223: loss 0.0008, time 1975.25ms\n",
            "iter 224: loss 0.0000, time 1665.01ms\n",
            "iter 225: loss 0.0000, time 1705.77ms\n",
            "iter 226: loss 0.0010, time 1642.77ms\n",
            "iter 227: loss 0.0000, time 1672.86ms\n",
            "iter 228: loss 0.0000, time 1707.34ms\n",
            "iter 229: loss 0.0000, time 1650.23ms\n",
            "iter 230: loss 0.0001, time 1670.80ms\n",
            "iter 231: loss 0.0000, time 1706.53ms\n",
            "iter 232: loss 0.0000, time 1703.78ms\n",
            "iter 233: loss 0.0001, time 1680.23ms\n",
            "iter 234: loss 0.0000, time 1654.18ms\n",
            "iter 235: loss 0.0000, time 1937.88ms\n",
            "iter 236: loss 0.0001, time 1692.98ms\n",
            "iter 237: loss 0.0003, time 1667.85ms\n",
            "iter 238: loss 0.0000, time 1682.07ms\n",
            "iter 239: loss 0.0000, time 1707.88ms\n",
            "Step 240: train loss 0.1901, val loss 0.7336\n",
            "Validation accuracy: 0.9795\n",
            "iter 240: loss 0.0000, time 21638.20ms\n",
            "iter 241: loss 0.0008, time 1685.57ms\n",
            "iter 242: loss 0.0001, time 1914.98ms\n",
            "iter 243: loss 0.0000, time 1707.43ms\n",
            "iter 244: loss 0.0000, time 1601.62ms\n",
            "iter 245: loss 0.0000, time 1632.00ms\n",
            "iter 246: loss 0.0001, time 1716.80ms\n",
            "iter 247: loss 0.0000, time 1677.11ms\n",
            "iter 248: loss 0.0002, time 1689.90ms\n",
            "iter 249: loss 0.0000, time 1614.45ms\n",
            "iter 250: loss 0.0000, time 1698.17ms\n",
            "iter 251: loss 0.0000, time 1628.50ms\n",
            "iter 252: loss 0.0000, time 1655.80ms\n",
            "iter 253: loss 0.0000, time 1698.35ms\n",
            "iter 254: loss 0.0000, time 1998.29ms\n",
            "iter 255: loss 0.0010, time 1565.07ms\n",
            "iter 256: loss 0.0001, time 1649.86ms\n",
            "iter 257: loss 0.0000, time 1689.71ms\n",
            "iter 258: loss 0.0001, time 1655.41ms\n",
            "iter 259: loss 0.0006, time 1650.61ms\n",
            "Step 260: train loss 0.1755, val loss 0.7290\n",
            "Validation accuracy: 0.9838\n",
            "iter 260: loss 0.0000, time 21586.95ms\n",
            "iter 261: loss 0.0000, time 1941.60ms\n",
            "iter 262: loss 0.0001, time 1695.06ms\n",
            "iter 263: loss 0.0000, time 1635.24ms\n",
            "iter 264: loss 0.0002, time 1720.29ms\n",
            "iter 265: loss 0.0000, time 1725.33ms\n",
            "iter 266: loss 0.0000, time 1698.66ms\n",
            "iter 267: loss 0.0000, time 1594.17ms\n",
            "iter 268: loss 0.1592, time 1658.01ms\n",
            "iter 269: loss 0.0001, time 1651.78ms\n",
            "iter 270: loss 0.0000, time 1663.67ms\n",
            "iter 271: loss 0.0000, time 1604.58ms\n",
            "iter 272: loss 0.0000, time 1706.42ms\n",
            "iter 273: loss 0.0000, time 1664.84ms\n",
            "iter 274: loss 0.0000, time 1901.80ms\n",
            "iter 275: loss 0.0000, time 1685.83ms\n",
            "iter 276: loss 0.0000, time 1684.02ms\n",
            "iter 277: loss 0.0000, time 1688.13ms\n",
            "iter 278: loss 0.0016, time 1683.09ms\n",
            "iter 279: loss 0.0000, time 1659.44ms\n",
            "Step 280: train loss 0.1638, val loss 0.7367\n",
            "Validation accuracy: 0.9881\n",
            "iter 280: loss 0.0000, time 22033.67ms\n",
            "iter 281: loss 0.0000, time 1642.71ms\n",
            "iter 282: loss 0.0003, time 1683.39ms\n",
            "iter 283: loss 0.0000, time 1629.28ms\n",
            "iter 284: loss 0.0000, time 1684.06ms\n",
            "iter 285: loss 0.0000, time 1658.61ms\n",
            "iter 286: loss 0.0000, time 1629.85ms\n",
            "iter 287: loss 0.0001, time 1700.07ms\n",
            "iter 288: loss 0.4375, time 1604.47ms\n",
            "iter 289: loss 0.0000, time 1650.46ms\n",
            "iter 290: loss 0.0000, time 1668.25ms\n",
            "iter 291: loss 0.0000, time 1643.31ms\n",
            "iter 292: loss 0.0000, time 1948.08ms\n",
            "iter 293: loss 0.0000, time 1651.12ms\n",
            "iter 294: loss 0.0000, time 1684.30ms\n",
            "iter 295: loss 0.0000, time 1646.73ms\n",
            "iter 296: loss 0.0000, time 1606.72ms\n",
            "iter 297: loss 0.0000, time 1678.93ms\n",
            "iter 298: loss 0.0000, time 1640.68ms\n",
            "iter 299: loss 0.0000, time 1709.43ms\n",
            "Step 300: train loss 0.1539, val loss 0.7220\n",
            "Validation accuracy: 0.9870\n",
            "Test accuracy 0.6333\n",
            "iter 300: loss 0.0000, time 22762.56ms\n",
            "iter 301: loss 0.0000, time 1739.72ms\n",
            "iter 302: loss 0.0148, time 1612.73ms\n",
            "iter 303: loss 0.0000, time 1651.87ms\n",
            "iter 304: loss 0.0000, time 1694.19ms\n",
            "iter 305: loss 0.0000, time 1624.49ms\n",
            "iter 306: loss 0.0000, time 1683.13ms\n",
            "iter 307: loss 0.0002, time 1685.82ms\n",
            "iter 308: loss 0.0000, time 1656.68ms\n",
            "iter 309: loss 0.0000, time 1612.84ms\n",
            "iter 310: loss 0.0000, time 1676.61ms\n",
            "iter 311: loss 0.0000, time 2007.14ms\n",
            "iter 312: loss 0.0000, time 1621.18ms\n",
            "iter 313: loss 0.0009, time 1703.85ms\n",
            "iter 314: loss 0.0000, time 1653.41ms\n",
            "iter 315: loss 0.0000, time 1634.34ms\n",
            "iter 316: loss 0.0001, time 1715.27ms\n",
            "iter 317: loss 0.0000, time 1616.06ms\n",
            "iter 318: loss 0.0000, time 1695.92ms\n",
            "iter 319: loss 0.0000, time 1663.38ms\n",
            "Step 320: train loss 0.1445, val loss 0.7365\n",
            "Validation accuracy: 0.9784\n",
            "iter 320: loss 0.0000, time 22065.75ms\n",
            "iter 321: loss 0.0000, time 1693.04ms\n",
            "iter 322: loss 0.0000, time 1617.17ms\n",
            "iter 323: loss 0.0000, time 1616.68ms\n",
            "iter 324: loss 0.0000, time 1700.86ms\n",
            "iter 325: loss 0.0000, time 1621.43ms\n",
            "iter 326: loss 0.0000, time 1638.00ms\n",
            "iter 327: loss 0.0000, time 1699.69ms\n",
            "iter 328: loss 0.0000, time 1633.41ms\n",
            "iter 329: loss 0.0000, time 2006.46ms\n",
            "iter 330: loss 0.0000, time 1665.92ms\n",
            "iter 331: loss 0.0000, time 1661.42ms\n",
            "iter 332: loss 0.0000, time 1632.87ms\n",
            "iter 333: loss 0.0000, time 1654.62ms\n",
            "iter 334: loss 0.0000, time 1668.09ms\n",
            "iter 335: loss 0.0000, time 1710.38ms\n",
            "iter 336: loss 0.0000, time 1644.78ms\n",
            "iter 337: loss 0.0001, time 1665.83ms\n",
            "iter 338: loss 0.0000, time 1665.70ms\n",
            "iter 339: loss 0.0000, time 1667.98ms\n",
            "Step 340: train loss 0.1367, val loss 0.7760\n",
            "Validation accuracy: 0.9838\n",
            "iter 340: loss 0.0000, time 21743.60ms\n",
            "iter 341: loss 0.0000, time 1636.22ms\n",
            "iter 342: loss 0.0000, time 1735.79ms\n",
            "iter 343: loss 0.0008, time 1719.46ms\n",
            "iter 344: loss 0.0001, time 1626.50ms\n",
            "iter 345: loss 0.0000, time 1645.40ms\n",
            "iter 346: loss 0.0000, time 1681.70ms\n",
            "iter 347: loss 0.0000, time 1913.46ms\n",
            "iter 348: loss 0.0000, time 1690.99ms\n",
            "iter 349: loss 0.0000, time 1683.99ms\n",
            "iter 350: loss 0.0000, time 1748.32ms\n",
            "iter 351: loss 0.0000, time 1741.66ms\n",
            "iter 352: loss 0.0000, time 1670.71ms\n",
            "iter 353: loss 0.0000, time 1740.34ms\n",
            "iter 354: loss 0.0000, time 1622.35ms\n",
            "iter 355: loss 0.0000, time 1647.41ms\n",
            "iter 356: loss 0.0000, time 1702.64ms\n",
            "iter 357: loss 0.0000, time 1636.51ms\n",
            "iter 358: loss 0.0000, time 1678.33ms\n",
            "iter 359: loss 0.0000, time 1932.21ms\n",
            "Step 360: train loss 0.1293, val loss 0.7989\n",
            "Validation accuracy: 0.9816\n",
            "iter 360: loss 0.0000, time 21392.80ms\n",
            "iter 361: loss 0.0000, time 1668.93ms\n",
            "iter 362: loss 0.0000, time 1687.67ms\n",
            "iter 363: loss 0.0000, time 1686.40ms\n",
            "iter 364: loss 0.0000, time 1629.38ms\n",
            "iter 365: loss 0.0000, time 1961.24ms\n",
            "iter 366: loss 0.9180, time 1741.96ms\n",
            "iter 367: loss 0.0000, time 1644.97ms\n",
            "iter 368: loss 0.0000, time 1688.98ms\n",
            "iter 369: loss 0.0000, time 1648.38ms\n",
            "iter 370: loss 0.0000, time 1714.40ms\n",
            "iter 371: loss 0.0000, time 1686.06ms\n",
            "iter 372: loss 0.0000, time 1675.19ms\n",
            "iter 373: loss 0.0021, time 1701.77ms\n",
            "iter 374: loss 0.0000, time 1677.03ms\n",
            "iter 375: loss 0.0000, time 1667.79ms\n",
            "iter 376: loss 0.0000, time 1690.68ms\n",
            "iter 377: loss 0.0000, time 1718.30ms\n",
            "iter 378: loss 0.0000, time 1985.92ms\n",
            "iter 379: loss 0.0000, time 1623.50ms\n",
            "Step 380: train loss 0.1236, val loss 0.8214\n",
            "Validation accuracy: 0.9914\n",
            "iter 380: loss 0.0000, time 21557.12ms\n",
            "iter 381: loss 0.0000, time 1621.63ms\n",
            "iter 382: loss 0.0000, time 1680.08ms\n",
            "iter 383: loss 0.0214, time 1700.42ms\n",
            "iter 384: loss 0.0000, time 1942.68ms\n",
            "iter 385: loss 0.0000, time 1684.01ms\n",
            "iter 386: loss 0.0000, time 1609.41ms\n",
            "iter 387: loss 0.0000, time 1636.61ms\n",
            "iter 388: loss 0.0015, time 1700.21ms\n",
            "iter 389: loss 0.0000, time 1712.35ms\n",
            "iter 390: loss 0.0001, time 1638.23ms\n",
            "iter 391: loss 0.0000, time 1634.69ms\n",
            "iter 392: loss 0.0000, time 1725.63ms\n",
            "iter 393: loss 0.0000, time 1703.69ms\n",
            "iter 394: loss 0.0000, time 1715.00ms\n",
            "iter 395: loss 0.0000, time 1688.42ms\n",
            "iter 396: loss 0.0003, time 1656.22ms\n",
            "iter 397: loss 0.0000, time 1965.05ms\n",
            "iter 398: loss 0.0000, time 1625.89ms\n",
            "iter 399: loss 0.0000, time 1643.20ms\n",
            "Step 400: train loss 0.1176, val loss 0.8292\n",
            "Validation accuracy: 0.9924\n",
            "Test accuracy 0.7333\n",
            "iter 400: loss 0.0000, time 22316.03ms\n",
            "iter 401: loss 0.0000, time 1651.86ms\n",
            "iter 402: loss 0.2988, time 1649.74ms\n",
            "iter 403: loss 0.0000, time 1898.16ms\n",
            "iter 404: loss 0.0000, time 1603.45ms\n",
            "iter 405: loss 0.0000, time 1622.25ms\n",
            "iter 406: loss 0.0000, time 1655.53ms\n",
            "iter 407: loss 0.0000, time 1634.65ms\n",
            "iter 408: loss 0.0000, time 1639.38ms\n",
            "iter 409: loss 0.0000, time 1629.75ms\n",
            "iter 410: loss 0.0000, time 1649.62ms\n",
            "iter 411: loss 0.0000, time 1596.95ms\n",
            "iter 412: loss 0.0000, time 1633.78ms\n",
            "iter 413: loss 0.0000, time 1647.88ms\n",
            "iter 414: loss 0.0000, time 1667.79ms\n",
            "iter 415: loss 0.0000, time 1621.45ms\n",
            "iter 416: loss 0.0000, time 1986.19ms\n",
            "iter 417: loss 0.0000, time 1621.84ms\n",
            "iter 418: loss 0.0000, time 1611.26ms\n",
            "iter 419: loss 0.0000, time 1663.76ms\n",
            "Step 420: train loss 0.1124, val loss 0.8102\n",
            "Validation accuracy: 0.9924\n",
            "iter 420: loss 0.0000, time 21399.41ms\n",
            "iter 421: loss 0.0000, time 1658.11ms\n",
            "iter 422: loss 0.0000, time 1608.30ms\n",
            "iter 423: loss 0.0000, time 1953.67ms\n",
            "iter 424: loss 0.0002, time 1666.84ms\n",
            "iter 425: loss 0.0000, time 1625.81ms\n",
            "iter 426: loss 0.0000, time 1621.75ms\n",
            "iter 427: loss 0.0000, time 1622.76ms\n",
            "iter 428: loss 0.0000, time 1655.47ms\n",
            "iter 429: loss 0.0000, time 1672.49ms\n",
            "iter 430: loss 0.0000, time 1652.24ms\n",
            "iter 431: loss 0.0000, time 1598.88ms\n",
            "iter 432: loss 0.0000, time 1598.28ms\n",
            "iter 433: loss 0.0000, time 1576.56ms\n",
            "iter 434: loss 0.0000, time 1650.31ms\n",
            "iter 435: loss 0.0000, time 1913.62ms\n",
            "iter 436: loss 0.0000, time 1656.02ms\n",
            "iter 437: loss 0.0000, time 1604.56ms\n",
            "iter 438: loss 0.0000, time 1678.78ms\n",
            "iter 439: loss 0.0000, time 1658.85ms\n",
            "Step 440: train loss 0.1077, val loss 0.7928\n",
            "Validation accuracy: 0.9924\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 440: loss 0.0000, time 23026.16ms\n",
            "iter 441: loss 0.0000, time 1608.17ms\n",
            "iter 442: loss 0.0000, time 1954.50ms\n",
            "iter 443: loss 0.0000, time 1694.02ms\n",
            "iter 444: loss 0.0000, time 1641.72ms\n",
            "iter 445: loss 0.0000, time 1640.08ms\n",
            "iter 446: loss 0.0001, time 1630.18ms\n",
            "iter 447: loss 0.0008, time 1627.93ms\n",
            "iter 448: loss 0.0003, time 1654.90ms\n",
            "iter 449: loss 0.0000, time 1628.89ms\n",
            "iter 450: loss 0.0000, time 1644.39ms\n",
            "iter 451: loss 0.0000, time 1769.13ms\n",
            "iter 452: loss 0.0000, time 1696.48ms\n",
            "iter 453: loss 0.0000, time 1614.20ms\n",
            "iter 454: loss 0.0000, time 1973.17ms\n",
            "iter 455: loss 0.0000, time 1652.24ms\n",
            "iter 456: loss 0.0000, time 1637.51ms\n",
            "iter 457: loss 0.0000, time 1734.45ms\n",
            "iter 458: loss 0.0000, time 1632.73ms\n",
            "iter 459: loss 0.0000, time 1730.63ms\n",
            "Step 460: train loss 0.1030, val loss 0.7942\n",
            "Validation accuracy: 0.9924\n",
            "iter 460: loss 0.0003, time 22016.68ms\n",
            "iter 461: loss 0.0000, time 1660.83ms\n",
            "iter 462: loss 0.0000, time 1682.22ms\n",
            "iter 463: loss 0.0000, time 1662.08ms\n",
            "iter 464: loss 0.0000, time 1678.71ms\n",
            "iter 465: loss 0.0000, time 1719.00ms\n",
            "iter 466: loss 0.0000, time 1672.92ms\n",
            "iter 467: loss 0.0000, time 1692.93ms\n",
            "iter 468: loss 0.0000, time 1640.46ms\n",
            "iter 469: loss 0.0000, time 1624.39ms\n",
            "iter 470: loss 0.0000, time 1727.63ms\n",
            "iter 471: loss 0.0000, time 1671.88ms\n",
            "iter 472: loss 0.0000, time 1965.04ms\n",
            "iter 473: loss 0.0000, time 1625.82ms\n",
            "iter 474: loss 0.0000, time 1713.67ms\n",
            "iter 475: loss 0.0000, time 1694.18ms\n",
            "iter 476: loss 0.0001, time 1619.00ms\n",
            "iter 477: loss 0.0000, time 1683.66ms\n",
            "iter 478: loss 0.0000, time 1633.80ms\n",
            "iter 479: loss 0.0000, time 1679.96ms\n",
            "Step 480: train loss 0.0987, val loss 0.7962\n",
            "Validation accuracy: 0.9903\n",
            "iter 480: loss 0.0000, time 21716.78ms\n",
            "iter 481: loss 0.0000, time 1661.80ms\n",
            "iter 482: loss 0.0000, time 1706.68ms\n",
            "iter 483: loss 0.0000, time 1679.86ms\n",
            "iter 484: loss 0.0000, time 1631.29ms\n",
            "iter 485: loss 0.0003, time 1639.25ms\n",
            "iter 486: loss 0.0000, time 1715.73ms\n",
            "iter 487: loss 0.0000, time 1630.57ms\n",
            "iter 488: loss 0.0000, time 1623.19ms\n",
            "iter 489: loss 0.0011, time 1677.92ms\n",
            "iter 490: loss 0.0000, time 1680.98ms\n",
            "iter 491: loss 0.0000, time 1986.16ms\n",
            "iter 492: loss 0.0000, time 1782.57ms\n",
            "iter 493: loss 0.0000, time 1701.89ms\n",
            "iter 494: loss 0.0000, time 1732.99ms\n",
            "iter 495: loss 0.0000, time 1699.94ms\n",
            "iter 496: loss 0.0000, time 1697.97ms\n",
            "iter 497: loss 0.0000, time 1715.73ms\n",
            "iter 498: loss 0.0000, time 1670.04ms\n",
            "iter 499: loss 0.0000, time 1755.90ms\n",
            "Step 500: train loss 0.0947, val loss 0.8171\n",
            "Validation accuracy: 0.9903\n",
            "Test accuracy 0.6667\n",
            "iter 500: loss 0.0000, time 22689.52ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅█▅▇▆</td></tr><tr><td>Test_F1_Score</td><td>▁▅█▆▇▆</td></tr><tr><td>Test_Precision</td><td>▁▅█▇██</td></tr><tr><td>Test_Recall</td><td>▁▅█▅▇▆</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▇▆▇█████████████████████</td></tr><tr><td>val/loss</td><td> ▁▂▄▄▄▅▆▇▆▇▆▆▆▆▆▆▇▇███▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.66667</td></tr><tr><td>Test_F1_Score</td><td>0.64883</td></tr><tr><td>Test_Precision</td><td>0.7963</td></tr><tr><td>Test_Recall</td><td>0.66667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.09475</td></tr><tr><td>val/acc</td><td>0.99028</td></tr><tr><td>val/loss</td><td>0.81708</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2022</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a236qqsp</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_202231-a236qqsp\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ntd0065d with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_204645-ntd0065d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d' target=\"_blank\">stellar-sweep-8</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2046\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-sweep-8</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_204645-ntd0065d\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_204650-ntd0065d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d' target=\"_blank\">gpt2_hyper_2046</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.4231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 8.1875, time 23186.24ms\n",
            "iter 1: loss 1.7422, time 1611.09ms\n",
            "iter 2: loss 5.6250, time 1963.65ms\n",
            "iter 3: loss 0.5469, time 1665.29ms\n",
            "iter 4: loss 4.1250, time 1676.34ms\n",
            "iter 5: loss 0.3750, time 1618.58ms\n",
            "iter 6: loss 1.0547, time 1631.79ms\n",
            "iter 7: loss 1.7734, time 1663.84ms\n",
            "iter 8: loss 1.9141, time 1654.59ms\n",
            "iter 9: loss 0.2949, time 1710.79ms\n",
            "iter 10: loss 0.4023, time 1611.05ms\n",
            "iter 11: loss 0.2305, time 1715.51ms\n",
            "iter 12: loss 0.0864, time 1658.19ms\n",
            "iter 13: loss 0.0894, time 1672.67ms\n",
            "iter 14: loss 0.6953, time 1635.48ms\n",
            "iter 15: loss 0.6484, time 1951.37ms\n",
            "iter 16: loss 0.7031, time 1636.07ms\n",
            "iter 17: loss 0.5937, time 1665.04ms\n",
            "iter 18: loss 0.3320, time 1735.43ms\n",
            "iter 19: loss 0.1660, time 1696.87ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 1.2299, val loss 1.0120\n",
            "Validation accuracy: 0.8380\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.4629, time 23964.02ms\n",
            "iter 21: loss 0.2695, time 1673.61ms\n",
            "iter 22: loss 0.1621, time 1738.93ms\n",
            "iter 23: loss 0.1777, time 1626.58ms\n",
            "iter 24: loss 0.4863, time 1665.15ms\n",
            "iter 25: loss 0.5859, time 1724.89ms\n",
            "iter 26: loss 0.1719, time 1673.36ms\n",
            "iter 27: loss 0.1484, time 1617.17ms\n",
            "iter 28: loss 0.0347, time 1660.95ms\n",
            "iter 29: loss 0.0021, time 1699.57ms\n",
            "iter 30: loss 0.0197, time 1665.45ms\n",
            "iter 31: loss 0.3828, time 1711.62ms\n",
            "iter 32: loss 0.0674, time 2069.47ms\n",
            "iter 33: loss 0.0466, time 1687.13ms\n",
            "iter 34: loss 0.8906, time 1721.62ms\n",
            "iter 35: loss 0.0471, time 1678.51ms\n",
            "iter 36: loss 4.1563, time 1688.51ms\n",
            "iter 37: loss 0.0060, time 1755.13ms\n",
            "iter 38: loss 0.7539, time 1665.07ms\n",
            "iter 39: loss 0.5234, time 1680.09ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.8288, val loss 0.7333\n",
            "Validation accuracy: 0.9147\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.4316, time 24030.44ms\n",
            "iter 41: loss 0.1235, time 1674.72ms\n",
            "iter 42: loss 2.7031, time 1677.61ms\n",
            "iter 43: loss 0.1475, time 1735.41ms\n",
            "iter 44: loss 0.0559, time 1755.62ms\n",
            "iter 45: loss 0.9297, time 1724.50ms\n",
            "iter 46: loss 0.1406, time 1594.61ms\n",
            "iter 47: loss 0.0574, time 1760.68ms\n",
            "iter 48: loss 0.2695, time 1716.59ms\n",
            "iter 49: loss 1.4375, time 1707.16ms\n",
            "iter 50: loss 0.0243, time 1687.60ms\n",
            "iter 51: loss 0.0991, time 1972.32ms\n",
            "iter 52: loss 0.0020, time 1620.57ms\n",
            "iter 53: loss 5.3750, time 1812.34ms\n",
            "iter 54: loss 0.0025, time 1754.14ms\n",
            "iter 55: loss 0.0160, time 1697.04ms\n",
            "iter 56: loss 0.0234, time 1680.48ms\n",
            "iter 57: loss 0.0017, time 1722.22ms\n",
            "iter 58: loss 0.0854, time 1641.81ms\n",
            "iter 59: loss 0.0156, time 1605.45ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.6447, val loss 0.6983\n",
            "Validation accuracy: 0.8639\n",
            "iter 60: loss 0.0010, time 22615.19ms\n",
            "iter 61: loss 0.1348, time 1688.65ms\n",
            "iter 62: loss 0.0051, time 1645.98ms\n",
            "iter 63: loss 0.0261, time 1687.40ms\n",
            "iter 64: loss 0.0020, time 1744.44ms\n",
            "iter 65: loss 0.0110, time 1591.02ms\n",
            "iter 66: loss 0.0330, time 1611.08ms\n",
            "iter 67: loss 0.0013, time 1687.36ms\n",
            "iter 68: loss 0.0020, time 1673.81ms\n",
            "iter 69: loss 0.0026, time 1689.48ms\n",
            "iter 70: loss 0.0028, time 1739.98ms\n",
            "iter 71: loss 0.0403, time 2171.82ms\n",
            "iter 72: loss 3.1875, time 1704.77ms\n",
            "iter 73: loss 0.0080, time 1601.45ms\n",
            "iter 74: loss 0.0122, time 1632.85ms\n",
            "iter 75: loss 0.0359, time 1626.44ms\n",
            "iter 76: loss 0.0014, time 1706.23ms\n",
            "iter 77: loss 3.3906, time 1749.33ms\n",
            "iter 78: loss 0.6250, time 1720.87ms\n",
            "iter 79: loss 0.0009, time 1692.43ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80: train loss 0.5327, val loss 0.6360\n",
            "Validation accuracy: 0.9600\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0255, time 24214.75ms\n",
            "iter 81: loss 3.5156, time 1820.43ms\n",
            "iter 82: loss 0.1064, time 1626.74ms\n",
            "iter 83: loss 0.0045, time 1670.61ms\n",
            "iter 84: loss 0.0068, time 1645.38ms\n",
            "iter 85: loss 0.0081, time 1593.22ms\n",
            "iter 86: loss 0.0010, time 1669.17ms\n",
            "iter 87: loss 0.8477, time 1707.50ms\n",
            "iter 88: loss 0.0605, time 1697.21ms\n",
            "iter 89: loss 0.1074, time 1969.11ms\n",
            "iter 90: loss 0.0069, time 1638.32ms\n",
            "iter 91: loss 0.0036, time 1678.55ms\n",
            "iter 92: loss 0.0069, time 1765.94ms\n",
            "iter 93: loss 0.0017, time 1672.01ms\n",
            "iter 94: loss 0.0173, time 1680.96ms\n",
            "iter 95: loss 0.0022, time 1716.28ms\n",
            "iter 96: loss 0.2871, time 1706.95ms\n",
            "iter 97: loss 0.0002, time 1672.45ms\n",
            "iter 98: loss 0.0002, time 1711.86ms\n",
            "iter 99: loss 0.0048, time 1716.07ms\n",
            "Step 100: train loss 0.4535, val loss 0.6244\n",
            "Validation accuracy: 0.9471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6000\n",
            "iter 100: loss 1.5156, time 22972.66ms\n",
            "iter 101: loss 0.0020, time 1663.65ms\n",
            "iter 102: loss 0.0003, time 1675.05ms\n",
            "iter 103: loss 0.0000, time 1802.35ms\n",
            "iter 104: loss 0.0003, time 1675.15ms\n",
            "iter 105: loss 0.0054, time 1611.64ms\n",
            "iter 106: loss 0.0004, time 1640.55ms\n",
            "iter 107: loss 0.0027, time 1991.03ms\n",
            "iter 108: loss 0.0021, time 1629.36ms\n",
            "iter 109: loss 0.0009, time 1664.02ms\n",
            "iter 110: loss 0.0001, time 1616.85ms\n",
            "iter 111: loss 0.0003, time 1641.29ms\n",
            "iter 112: loss 0.0001, time 1691.78ms\n",
            "iter 113: loss 0.0001, time 1673.24ms\n",
            "iter 114: loss 0.0001, time 1619.80ms\n",
            "iter 115: loss 0.0033, time 1717.06ms\n",
            "iter 116: loss 0.0004, time 1692.85ms\n",
            "iter 117: loss 0.0010, time 1633.61ms\n",
            "iter 118: loss 0.0520, time 1538.62ms\n",
            "iter 119: loss 0.0002, time 1956.90ms\n",
            "Step 120: train loss 0.3917, val loss 0.6305\n",
            "Validation accuracy: 0.9827\n",
            "iter 120: loss 0.0034, time 21746.51ms\n",
            "iter 121: loss 0.0011, time 1677.18ms\n",
            "iter 122: loss 0.4922, time 1699.21ms\n",
            "iter 123: loss 0.0021, time 1664.69ms\n",
            "iter 124: loss 0.0029, time 1634.77ms\n",
            "iter 125: loss 0.0011, time 2011.86ms\n",
            "iter 126: loss 0.0002, time 1690.44ms\n",
            "iter 127: loss 0.0000, time 1705.76ms\n",
            "iter 128: loss 0.0002, time 1653.38ms\n",
            "iter 129: loss 0.0000, time 1595.86ms\n",
            "iter 130: loss 0.0000, time 1671.91ms\n",
            "iter 131: loss 0.0000, time 1632.22ms\n",
            "iter 132: loss 0.0000, time 1672.52ms\n",
            "iter 133: loss 0.0005, time 1663.08ms\n",
            "iter 134: loss 0.0145, time 1635.40ms\n",
            "iter 135: loss 0.0001, time 1668.20ms\n",
            "iter 136: loss 0.0005, time 1683.49ms\n",
            "iter 137: loss 0.0000, time 1921.12ms\n",
            "iter 138: loss 0.0001, time 1679.14ms\n",
            "iter 139: loss 0.0003, time 1787.34ms\n",
            "Step 140: train loss 0.3396, val loss 0.6483\n",
            "Validation accuracy: 0.9741\n",
            "iter 140: loss 0.0005, time 21723.71ms\n",
            "iter 141: loss 0.0002, time 1602.14ms\n",
            "iter 142: loss 0.0089, time 1650.92ms\n",
            "iter 143: loss 0.0001, time 1977.55ms\n",
            "iter 144: loss 0.0004, time 1757.10ms\n",
            "iter 145: loss 0.0000, time 1777.53ms\n",
            "iter 146: loss 0.0009, time 1713.00ms\n",
            "iter 147: loss 0.0001, time 1674.97ms\n",
            "iter 148: loss 0.0011, time 1697.21ms\n",
            "iter 149: loss 0.0000, time 1638.27ms\n",
            "iter 150: loss 0.0001, time 1713.37ms\n",
            "iter 151: loss 0.0010, time 1753.76ms\n",
            "iter 152: loss 0.0001, time 1633.21ms\n",
            "iter 153: loss 0.0000, time 1614.76ms\n",
            "iter 154: loss 0.0007, time 1676.58ms\n",
            "iter 155: loss 0.0111, time 2093.22ms\n",
            "iter 156: loss 0.0000, time 1706.73ms\n",
            "iter 157: loss 0.0028, time 1695.42ms\n",
            "iter 158: loss 0.0000, time 1708.10ms\n",
            "iter 159: loss 0.0000, time 1638.85ms\n",
            "Step 160: train loss 0.2991, val loss 0.6742\n",
            "Validation accuracy: 0.9849\n",
            "iter 160: loss 0.0000, time 21890.10ms\n",
            "iter 161: loss 0.0000, time 1998.61ms\n",
            "iter 162: loss 0.0000, time 1678.63ms\n",
            "iter 163: loss 0.0000, time 1705.15ms\n",
            "iter 164: loss 0.0000, time 1663.59ms\n",
            "iter 165: loss 0.0000, time 1671.29ms\n",
            "iter 166: loss 0.0000, time 1669.27ms\n",
            "iter 167: loss 0.0000, time 1638.77ms\n",
            "iter 168: loss 0.0000, time 1765.93ms\n",
            "iter 169: loss 0.0006, time 1700.26ms\n",
            "iter 170: loss 0.0001, time 1631.94ms\n",
            "iter 171: loss 0.0000, time 1637.02ms\n",
            "iter 172: loss 0.0001, time 1665.67ms\n",
            "iter 173: loss 0.0008, time 1631.45ms\n",
            "iter 174: loss 0.0000, time 2082.90ms\n",
            "iter 175: loss 0.0001, time 1739.85ms\n",
            "iter 176: loss 0.0000, time 1674.73ms\n",
            "iter 177: loss 0.0000, time 1719.32ms\n",
            "iter 178: loss 0.0002, time 1671.75ms\n",
            "iter 179: loss 0.0000, time 1680.66ms\n",
            "Step 180: train loss 0.2670, val loss 0.7015\n",
            "Validation accuracy: 0.9838\n",
            "iter 180: loss 0.0000, time 22156.93ms\n",
            "iter 181: loss 0.0000, time 1711.27ms\n",
            "iter 182: loss 0.0000, time 1665.85ms\n",
            "iter 183: loss 0.0000, time 1647.83ms\n",
            "iter 184: loss 0.0000, time 1731.82ms\n",
            "iter 185: loss 0.0000, time 1692.40ms\n",
            "iter 186: loss 0.0000, time 1688.94ms\n",
            "iter 187: loss 0.0000, time 1738.88ms\n",
            "iter 188: loss 0.0000, time 1615.55ms\n",
            "iter 189: loss 0.0000, time 1684.25ms\n",
            "iter 190: loss 0.0000, time 1673.75ms\n",
            "iter 191: loss 0.0000, time 1717.29ms\n",
            "iter 192: loss 0.0000, time 1993.63ms\n",
            "iter 193: loss 0.0000, time 1650.03ms\n",
            "iter 194: loss 0.0549, time 1693.78ms\n",
            "iter 195: loss 0.0000, time 1668.17ms\n",
            "iter 196: loss 0.0000, time 1640.87ms\n",
            "iter 197: loss 0.0000, time 1795.22ms\n",
            "iter 198: loss 0.0000, time 1687.65ms\n",
            "iter 199: loss 0.0000, time 1735.56ms\n",
            "Step 200: train loss 0.2406, val loss 0.7418\n",
            "Validation accuracy: 0.9870\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.8333\n",
            "iter 200: loss 0.0000, time 24460.73ms\n",
            "iter 201: loss 0.0000, time 1662.63ms\n",
            "iter 202: loss 0.0000, time 1692.95ms\n",
            "iter 203: loss 0.0000, time 1639.85ms\n",
            "iter 204: loss 0.0000, time 1637.19ms\n",
            "iter 205: loss 0.0000, time 1695.07ms\n",
            "iter 206: loss 0.0000, time 1643.16ms\n",
            "iter 207: loss 0.0000, time 1589.04ms\n",
            "iter 208: loss 0.0000, time 1621.23ms\n",
            "iter 209: loss 0.0000, time 1686.57ms\n",
            "iter 210: loss 0.0000, time 1681.08ms\n",
            "iter 211: loss 0.0000, time 1985.29ms\n",
            "iter 212: loss 0.0000, time 1728.54ms\n",
            "iter 213: loss 0.0000, time 1690.49ms\n",
            "iter 214: loss 0.0000, time 1690.29ms\n",
            "iter 215: loss 0.0000, time 1650.19ms\n",
            "iter 216: loss 0.0000, time 1640.47ms\n",
            "iter 217: loss 0.0000, time 1635.54ms\n",
            "iter 218: loss 0.0005, time 1686.15ms\n",
            "iter 219: loss 0.0000, time 1710.40ms\n",
            "Step 220: train loss 0.2202, val loss 0.6967\n",
            "Validation accuracy: 0.9881\n",
            "iter 220: loss 0.0000, time 22216.68ms\n",
            "iter 221: loss 0.0000, time 1650.94ms\n",
            "iter 222: loss 0.0000, time 1651.47ms\n",
            "iter 223: loss 0.0000, time 1651.86ms\n",
            "iter 224: loss 0.0024, time 1650.47ms\n",
            "iter 225: loss 0.0000, time 1716.96ms\n",
            "iter 226: loss 0.0000, time 1703.02ms\n",
            "iter 227: loss 0.0000, time 1592.44ms\n",
            "iter 228: loss 0.0000, time 1610.63ms\n",
            "iter 229: loss 0.0000, time 1626.17ms\n",
            "iter 230: loss 0.0000, time 1964.03ms\n",
            "iter 231: loss 0.0000, time 1734.43ms\n",
            "iter 232: loss 0.0000, time 1660.68ms\n",
            "iter 233: loss 0.0039, time 1674.98ms\n",
            "iter 234: loss 0.0000, time 1666.58ms\n",
            "iter 235: loss 0.2217, time 1583.77ms\n",
            "iter 236: loss 0.0000, time 1671.26ms\n",
            "iter 237: loss 0.0001, time 1656.15ms\n",
            "iter 238: loss 0.0000, time 1580.23ms\n",
            "iter 239: loss 0.0767, time 1608.27ms\n",
            "Step 240: train loss 0.2023, val loss 0.6647\n",
            "Validation accuracy: 0.9806\n",
            "iter 240: loss 0.0000, time 22092.54ms\n",
            "iter 241: loss 0.0000, time 1702.05ms\n",
            "iter 242: loss 0.0000, time 1600.02ms\n",
            "iter 243: loss 0.0000, time 1683.01ms\n",
            "iter 244: loss 0.0000, time 1644.44ms\n",
            "iter 245: loss 0.0048, time 1588.26ms\n",
            "iter 246: loss 0.0000, time 1669.57ms\n",
            "iter 247: loss 0.0000, time 1662.24ms\n",
            "iter 248: loss 0.0001, time 1687.06ms\n",
            "iter 249: loss 0.0000, time 2003.11ms\n",
            "iter 250: loss 0.0000, time 1626.29ms\n",
            "iter 251: loss 0.0000, time 1636.97ms\n",
            "iter 252: loss 0.0000, time 1665.23ms\n",
            "iter 253: loss 0.0000, time 1677.92ms\n",
            "iter 254: loss 0.0000, time 1741.95ms\n",
            "iter 255: loss 0.0002, time 1635.54ms\n",
            "iter 256: loss 0.0000, time 1685.88ms\n",
            "iter 257: loss 0.0000, time 1657.76ms\n",
            "iter 258: loss 0.0000, time 1643.72ms\n",
            "iter 259: loss 0.0029, time 1661.12ms\n",
            "Step 260: train loss 0.1897, val loss 0.6695\n",
            "Validation accuracy: 0.9903\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 260: loss 0.0000, time 23876.53ms\n",
            "iter 261: loss 0.0000, time 1744.26ms\n",
            "iter 262: loss 0.0001, time 1689.91ms\n",
            "iter 263: loss 0.0000, time 1644.92ms\n",
            "iter 264: loss 0.0000, time 1732.29ms\n",
            "iter 265: loss 0.0003, time 1700.04ms\n",
            "iter 266: loss 0.0001, time 1704.46ms\n",
            "iter 267: loss 0.0000, time 1908.44ms\n",
            "iter 268: loss 0.0000, time 1639.90ms\n",
            "iter 269: loss 0.0101, time 1650.61ms\n",
            "iter 270: loss 0.0000, time 1668.24ms\n",
            "iter 271: loss 0.0000, time 1708.12ms\n",
            "iter 272: loss 0.0000, time 1702.58ms\n",
            "iter 273: loss 0.0000, time 1667.64ms\n",
            "iter 274: loss 0.0000, time 1609.11ms\n",
            "iter 275: loss 0.0000, time 1692.71ms\n",
            "iter 276: loss 0.0000, time 1730.97ms\n",
            "iter 277: loss 0.0000, time 1681.38ms\n",
            "iter 278: loss 0.0003, time 1702.48ms\n",
            "iter 279: loss 0.0000, time 1680.53ms\n",
            "Step 280: train loss 0.1780, val loss 0.6972\n",
            "Validation accuracy: 0.9849\n",
            "iter 280: loss 0.0000, time 22178.38ms\n",
            "iter 281: loss 0.0000, time 1652.68ms\n",
            "iter 282: loss 0.0000, time 1678.98ms\n",
            "iter 283: loss 0.0000, time 1727.39ms\n",
            "iter 284: loss 0.0000, time 1699.08ms\n",
            "iter 285: loss 0.0000, time 1968.02ms\n",
            "iter 286: loss 0.0000, time 1656.44ms\n",
            "iter 287: loss 0.0000, time 1696.68ms\n",
            "iter 288: loss 0.0013, time 1672.39ms\n",
            "iter 289: loss 0.0000, time 1717.96ms\n",
            "iter 290: loss 0.0000, time 1672.84ms\n",
            "iter 291: loss 0.0000, time 1693.06ms\n",
            "iter 292: loss 0.0000, time 1700.94ms\n",
            "iter 293: loss 0.0000, time 1703.91ms\n",
            "iter 294: loss 0.0000, time 1741.04ms\n",
            "iter 295: loss 0.0000, time 1662.24ms\n",
            "iter 296: loss 0.0000, time 1652.46ms\n",
            "iter 297: loss 0.0000, time 2026.14ms\n",
            "iter 298: loss 0.0000, time 1591.41ms\n",
            "iter 299: loss 0.0000, time 1677.50ms\n",
            "Step 300: train loss 0.1666, val loss 0.7042\n",
            "Validation accuracy: 0.9870\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.6667\n",
            "iter 300: loss 0.0000, time 24481.89ms\n",
            "iter 301: loss 0.0000, time 1705.11ms\n",
            "iter 302: loss 0.0000, time 1617.34ms\n",
            "iter 303: loss 0.0000, time 1945.29ms\n",
            "iter 304: loss 0.2676, time 1702.29ms\n",
            "iter 305: loss 0.0000, time 1683.13ms\n",
            "iter 306: loss 0.0000, time 1701.64ms\n",
            "iter 307: loss 0.0007, time 1643.98ms\n",
            "iter 308: loss 0.0000, time 1626.00ms\n",
            "iter 309: loss 0.0000, time 1628.79ms\n",
            "iter 310: loss 0.0000, time 1672.95ms\n",
            "iter 311: loss 0.0000, time 1682.02ms\n",
            "iter 312: loss 0.0071, time 1592.08ms\n",
            "iter 313: loss 0.0009, time 1623.33ms\n",
            "iter 314: loss 0.0000, time 1638.49ms\n",
            "iter 315: loss 0.0000, time 1896.06ms\n",
            "iter 316: loss 0.0000, time 1733.05ms\n",
            "iter 317: loss 0.0000, time 1688.38ms\n",
            "iter 318: loss 0.0000, time 1648.62ms\n",
            "iter 319: loss 0.0004, time 1672.34ms\n",
            "Step 320: train loss 0.1563, val loss 0.7246\n",
            "Validation accuracy: 0.9870\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 320: loss 0.0000, time 23700.70ms\n",
            "iter 321: loss 0.0000, time 1665.52ms\n",
            "iter 322: loss 0.0000, time 1691.67ms\n",
            "iter 323: loss 0.0000, time 1619.65ms\n",
            "iter 324: loss 0.0000, time 1694.19ms\n",
            "iter 325: loss 0.0000, time 1704.58ms\n",
            "iter 326: loss 0.0000, time 1720.63ms\n",
            "iter 327: loss 0.0001, time 1749.28ms\n",
            "iter 328: loss 0.0000, time 1721.46ms\n",
            "iter 329: loss 0.0000, time 1627.01ms\n",
            "iter 330: loss 0.0000, time 1629.84ms\n",
            "iter 331: loss 0.0000, time 1716.11ms\n",
            "iter 332: loss 0.0000, time 1985.86ms\n",
            "iter 333: loss 0.0059, time 1711.26ms\n",
            "iter 334: loss 0.0000, time 1691.73ms\n",
            "iter 335: loss 0.0000, time 1679.68ms\n",
            "iter 336: loss 0.0000, time 1700.60ms\n",
            "iter 337: loss 0.0000, time 1667.03ms\n",
            "iter 338: loss 0.0000, time 1747.19ms\n",
            "iter 339: loss 0.0000, time 1737.25ms\n",
            "Step 340: train loss 0.1476, val loss 0.7402\n",
            "Validation accuracy: 0.9870\n",
            "iter 340: loss 0.0000, time 22166.38ms\n",
            "iter 341: loss 0.0000, time 1744.58ms\n",
            "iter 342: loss 0.0000, time 1717.34ms\n",
            "iter 343: loss 0.0000, time 1733.91ms\n",
            "iter 344: loss 0.0000, time 1696.39ms\n",
            "iter 345: loss 0.0000, time 1692.40ms\n",
            "iter 346: loss 0.0000, time 1708.05ms\n",
            "iter 347: loss 0.0000, time 1650.43ms\n",
            "iter 348: loss 0.0000, time 1674.45ms\n",
            "iter 349: loss 0.0001, time 1691.52ms\n",
            "iter 350: loss 0.0000, time 1727.16ms\n",
            "iter 351: loss 0.0000, time 2018.40ms\n",
            "iter 352: loss 0.0000, time 1707.38ms\n",
            "iter 353: loss 0.0000, time 1700.06ms\n",
            "iter 354: loss 0.0000, time 1675.98ms\n",
            "iter 355: loss 0.0000, time 1695.15ms\n",
            "iter 356: loss 0.0000, time 1668.78ms\n",
            "iter 357: loss 0.0000, time 1632.64ms\n",
            "iter 358: loss 0.0000, time 1669.67ms\n",
            "iter 359: loss 0.0000, time 1613.59ms\n",
            "Step 360: train loss 0.1400, val loss 0.7337\n",
            "Validation accuracy: 0.9838\n",
            "iter 360: loss 0.0000, time 23013.48ms\n",
            "iter 361: loss 0.0000, time 1717.63ms\n",
            "iter 362: loss 0.0000, time 1703.60ms\n",
            "iter 363: loss 0.0135, time 1713.48ms\n",
            "iter 364: loss 0.0000, time 1687.92ms\n",
            "iter 365: loss 0.0000, time 1735.45ms\n",
            "iter 366: loss 0.0000, time 1698.99ms\n",
            "iter 367: loss 0.0000, time 1615.71ms\n",
            "iter 368: loss 0.0000, time 1697.13ms\n",
            "iter 369: loss 0.0000, time 1707.40ms\n",
            "iter 370: loss 0.0000, time 1710.44ms\n",
            "iter 371: loss 0.0000, time 1978.69ms\n",
            "iter 372: loss 0.0000, time 1690.41ms\n",
            "iter 373: loss 0.0000, time 1718.70ms\n",
            "iter 374: loss 0.0000, time 1719.11ms\n",
            "iter 375: loss 0.0000, time 1676.14ms\n",
            "iter 376: loss 0.0000, time 1765.57ms\n",
            "iter 377: loss 0.0000, time 1676.60ms\n",
            "iter 378: loss 0.0000, time 1690.93ms\n",
            "iter 379: loss 0.0000, time 1668.71ms\n",
            "Step 380: train loss 0.1327, val loss 0.7325\n",
            "Validation accuracy: 0.9860\n",
            "iter 380: loss 0.0000, time 23072.96ms\n",
            "iter 381: loss 0.0000, time 1659.73ms\n",
            "iter 382: loss 0.0000, time 1676.07ms\n",
            "iter 383: loss 0.0000, time 1744.12ms\n",
            "iter 384: loss 0.0000, time 1677.09ms\n",
            "iter 385: loss 0.0000, time 1701.18ms\n",
            "iter 386: loss 0.0000, time 1658.24ms\n",
            "iter 387: loss 0.0000, time 1687.25ms\n",
            "iter 388: loss 0.0002, time 1685.87ms\n",
            "iter 389: loss 0.0000, time 2020.91ms\n",
            "iter 390: loss 0.0000, time 1729.52ms\n",
            "iter 391: loss 0.0000, time 1664.32ms\n",
            "iter 392: loss 0.0000, time 1713.66ms\n",
            "iter 393: loss 0.0000, time 1625.20ms\n",
            "iter 394: loss 0.0000, time 1711.16ms\n",
            "iter 395: loss 0.0000, time 1718.35ms\n",
            "iter 396: loss 0.0000, time 1686.70ms\n",
            "iter 397: loss 0.0000, time 1726.85ms\n",
            "iter 398: loss 0.0000, time 1679.44ms\n",
            "iter 399: loss 0.0000, time 1713.45ms\n",
            "Step 400: train loss 0.1261, val loss 0.7487\n",
            "Validation accuracy: 0.9827\n",
            "Test accuracy 0.7333\n",
            "iter 400: loss 0.0000, time 24114.71ms\n",
            "iter 401: loss 0.0000, time 1783.19ms\n",
            "iter 402: loss 0.0000, time 1709.67ms\n",
            "iter 403: loss 0.0000, time 1680.91ms\n",
            "iter 404: loss 0.0000, time 1696.36ms\n",
            "iter 405: loss 0.0000, time 1658.89ms\n",
            "iter 406: loss 0.0000, time 1696.13ms\n",
            "iter 407: loss 0.0000, time 1991.62ms\n",
            "iter 408: loss 0.0000, time 1689.48ms\n",
            "iter 409: loss 0.0000, time 1664.29ms\n",
            "iter 410: loss 0.0000, time 1706.52ms\n",
            "iter 411: loss 0.0000, time 1691.74ms\n",
            "iter 412: loss 0.0000, time 1667.71ms\n",
            "iter 413: loss 0.0000, time 1677.70ms\n",
            "iter 414: loss 0.0000, time 1707.61ms\n",
            "iter 415: loss 0.0000, time 1685.33ms\n",
            "iter 416: loss 0.0000, time 1751.04ms\n",
            "iter 417: loss 0.0000, time 1689.01ms\n",
            "iter 418: loss 0.0000, time 1688.80ms\n",
            "iter 419: loss 0.0000, time 1678.86ms\n",
            "Step 420: train loss 0.1209, val loss 0.7590\n",
            "Validation accuracy: 0.9838\n",
            "iter 420: loss 0.0000, time 23233.88ms\n",
            "iter 421: loss 0.0000, time 1642.83ms\n",
            "iter 422: loss 0.0000, time 1658.31ms\n",
            "iter 423: loss 0.0000, time 1708.66ms\n",
            "iter 424: loss 0.0000, time 1726.85ms\n",
            "iter 425: loss 0.0000, time 1986.28ms\n",
            "iter 426: loss 0.0000, time 1667.03ms\n",
            "iter 427: loss 0.0000, time 1712.24ms\n",
            "iter 428: loss 0.0000, time 1724.14ms\n",
            "iter 429: loss 0.0000, time 1686.27ms\n",
            "iter 430: loss 0.0000, time 1697.60ms\n",
            "iter 431: loss 0.0000, time 1679.66ms\n",
            "iter 432: loss 0.0000, time 1658.04ms\n",
            "iter 433: loss 0.0002, time 1666.90ms\n",
            "iter 434: loss 0.0000, time 1678.41ms\n",
            "iter 435: loss 0.0000, time 1672.07ms\n",
            "iter 436: loss 0.0000, time 1697.41ms\n",
            "iter 437: loss 0.0000, time 1986.52ms\n",
            "iter 438: loss 0.0000, time 1754.53ms\n",
            "iter 439: loss 0.0000, time 1742.58ms\n",
            "Step 440: train loss 0.1159, val loss 0.7610\n",
            "Validation accuracy: 0.9849\n",
            "iter 440: loss 0.0000, time 22713.50ms\n",
            "iter 441: loss 0.0000, time 1668.49ms\n",
            "iter 442: loss 0.0005, time 2048.11ms\n",
            "iter 443: loss 0.0000, time 1707.52ms\n",
            "iter 444: loss 0.0000, time 1684.40ms\n",
            "iter 445: loss 0.0000, time 1695.38ms\n",
            "iter 446: loss 0.0001, time 1669.83ms\n",
            "iter 447: loss 0.0001, time 1666.89ms\n",
            "iter 448: loss 0.0000, time 1698.17ms\n",
            "iter 449: loss 0.0000, time 1684.72ms\n",
            "iter 450: loss 0.0000, time 1658.03ms\n",
            "iter 451: loss 0.0000, time 1733.93ms\n",
            "iter 452: loss 0.0000, time 1696.06ms\n",
            "iter 453: loss 0.0000, time 1667.35ms\n",
            "iter 454: loss 0.0001, time 1695.14ms\n",
            "iter 455: loss 0.0000, time 2000.05ms\n",
            "iter 456: loss 0.0000, time 1703.64ms\n",
            "iter 457: loss 0.0000, time 1782.63ms\n",
            "iter 458: loss 0.0000, time 1661.67ms\n",
            "iter 459: loss 0.0000, time 1801.01ms\n",
            "Step 460: train loss 0.1109, val loss 0.7834\n",
            "Validation accuracy: 0.9827\n",
            "iter 460: loss 0.0000, time 22944.45ms\n",
            "iter 461: loss 0.0000, time 1678.55ms\n",
            "iter 462: loss 0.0000, time 1665.43ms\n",
            "iter 463: loss 0.0000, time 1731.31ms\n",
            "iter 464: loss 0.0000, time 1675.15ms\n",
            "iter 465: loss 0.0000, time 1737.86ms\n",
            "iter 466: loss 0.0000, time 1663.89ms\n",
            "iter 467: loss 0.0000, time 1725.61ms\n",
            "iter 468: loss 0.0000, time 1682.91ms\n",
            "iter 469: loss 0.0003, time 1665.24ms\n",
            "iter 470: loss 0.0000, time 1751.11ms\n",
            "iter 471: loss 0.0313, time 1716.58ms\n",
            "iter 472: loss 0.0000, time 1680.91ms\n",
            "iter 473: loss 0.0000, time 1992.67ms\n",
            "iter 474: loss 0.0000, time 1694.27ms\n",
            "iter 475: loss 0.0000, time 1705.20ms\n",
            "iter 476: loss 0.0000, time 1649.07ms\n",
            "iter 477: loss 0.0000, time 1724.28ms\n",
            "iter 478: loss 0.0000, time 1728.91ms\n",
            "iter 479: loss 0.0000, time 1742.56ms\n",
            "Step 480: train loss 0.1063, val loss 0.7852\n",
            "Validation accuracy: 0.9795\n",
            "iter 480: loss 0.0000, time 23233.05ms\n",
            "iter 481: loss 0.0000, time 1658.61ms\n",
            "iter 482: loss 0.0000, time 1715.35ms\n",
            "iter 483: loss 0.0000, time 1634.59ms\n",
            "iter 484: loss 0.0000, time 1692.55ms\n",
            "iter 485: loss 0.0000, time 1652.93ms\n",
            "iter 486: loss 0.0000, time 1700.37ms\n",
            "iter 487: loss 0.0000, time 1655.67ms\n",
            "iter 488: loss 0.0000, time 1661.33ms\n",
            "iter 489: loss 0.0002, time 1723.20ms\n",
            "iter 490: loss 0.0000, time 1714.31ms\n",
            "iter 491: loss 0.0000, time 1993.71ms\n",
            "iter 492: loss 0.0000, time 1682.09ms\n",
            "iter 493: loss 0.0000, time 1718.01ms\n",
            "iter 494: loss 0.0000, time 1710.96ms\n",
            "iter 495: loss 0.0000, time 1688.08ms\n",
            "iter 496: loss 0.0000, time 1715.84ms\n",
            "iter 497: loss 0.0000, time 1715.36ms\n",
            "iter 498: loss 0.0000, time 1703.58ms\n",
            "iter 499: loss 0.0000, time 1748.20ms\n",
            "Step 500: train loss 0.1022, val loss 0.8087\n",
            "Validation accuracy: 0.9806\n",
            "Test accuracy 0.7333\n",
            "iter 500: loss 0.0000, time 23996.51ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅█▆▇▇</td></tr><tr><td>Test_F1_Score</td><td>▁▅█▆▇▇</td></tr><tr><td>Test_Precision</td><td>▁▅█▇██</td></tr><tr><td>Test_Recall</td><td>▁▅█▆▇▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▆█▇████████████████████</td></tr><tr><td>val/loss</td><td> █▃▂▁▁▁▁▂▂▃▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.73333</td></tr><tr><td>Test_F1_Score</td><td>0.72125</td></tr><tr><td>Test_Precision</td><td>0.82083</td></tr><tr><td>Test_Recall</td><td>0.73333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.10221</td></tr><tr><td>val/acc</td><td>0.98056</td></tr><tr><td>val/loss</td><td>0.80873</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2046</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ntd0065d</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_204650-ntd0065d\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7o601jbo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_211130-7o601jbo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo' target=\"_blank\">fragrant-sweep-9</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2111\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fragrant-sweep-9</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_211130-7o601jbo\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_211135-7o601jbo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo' target=\"_blank\">gpt2_hyper_2111</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.6055, time 27765.35ms\n",
            "iter 1: loss 4.9375, time 1222.52ms\n",
            "iter 2: loss 1.0000, time 1191.66ms\n",
            "iter 3: loss 1.6094, time 1128.79ms\n",
            "iter 4: loss 1.0781, time 1182.39ms\n",
            "iter 5: loss 0.5820, time 1150.22ms\n",
            "iter 6: loss 1.1016, time 1202.90ms\n",
            "iter 7: loss 0.9023, time 1196.05ms\n",
            "iter 8: loss 1.1328, time 1151.12ms\n",
            "iter 9: loss 0.7500, time 1175.95ms\n",
            "iter 10: loss 1.3516, time 1203.58ms\n",
            "iter 11: loss 1.4688, time 1135.29ms\n",
            "iter 12: loss 0.7031, time 1170.48ms\n",
            "iter 13: loss 0.3125, time 1153.85ms\n",
            "iter 14: loss 1.5937, time 1144.12ms\n",
            "iter 15: loss 0.3027, time 1151.28ms\n",
            "iter 16: loss 0.4141, time 1124.99ms\n",
            "iter 17: loss 1.8203, time 1185.36ms\n",
            "iter 18: loss 1.3516, time 1181.06ms\n",
            "iter 19: loss 1.0938, time 1117.69ms\n",
            "Step 20: train loss 1.1906, val loss 0.8102\n",
            "Validation accuracy: 0.7914\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.2910, time 10692.62ms\n",
            "iter 21: loss 0.5273, time 1154.58ms\n",
            "iter 22: loss 0.6719, time 1232.40ms\n",
            "iter 23: loss 0.4180, time 1188.82ms\n",
            "iter 24: loss 0.8398, time 1224.79ms\n",
            "iter 25: loss 0.1377, time 1206.17ms\n",
            "iter 26: loss 0.7617, time 1217.06ms\n",
            "iter 27: loss 0.3633, time 1217.45ms\n",
            "iter 28: loss 0.1816, time 1156.69ms\n",
            "iter 29: loss 0.1436, time 1182.30ms\n",
            "iter 30: loss 0.0050, time 1162.48ms\n",
            "iter 31: loss 0.7031, time 1176.33ms\n",
            "iter 32: loss 0.1299, time 1172.68ms\n",
            "iter 33: loss 0.0991, time 1186.74ms\n",
            "iter 34: loss 0.0309, time 1201.75ms\n",
            "iter 35: loss 0.1270, time 1163.71ms\n",
            "iter 36: loss 0.0903, time 1170.13ms\n",
            "iter 37: loss 0.8633, time 1168.28ms\n",
            "iter 38: loss 0.0493, time 1207.03ms\n",
            "iter 39: loss 1.3984, time 1172.59ms\n",
            "Step 40: train loss 0.8098, val loss 0.5412\n",
            "Validation accuracy: 0.7751\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.0273, time 10731.87ms\n",
            "iter 41: loss 1.8047, time 1144.54ms\n",
            "iter 42: loss 0.0057, time 1152.75ms\n",
            "iter 43: loss 0.0101, time 1148.11ms\n",
            "iter 44: loss 0.0160, time 1149.80ms\n",
            "iter 45: loss 1.2422, time 1232.98ms\n",
            "iter 46: loss 0.0718, time 1199.47ms\n",
            "iter 47: loss 0.0015, time 1136.50ms\n",
            "iter 48: loss 0.0043, time 1164.44ms\n",
            "iter 49: loss 0.0344, time 1174.78ms\n",
            "iter 50: loss 0.0005, time 1157.88ms\n",
            "iter 51: loss 0.0204, time 1166.17ms\n",
            "iter 52: loss 0.0190, time 1134.98ms\n",
            "iter 53: loss 0.0005, time 1181.01ms\n",
            "iter 54: loss 0.0952, time 1162.57ms\n",
            "iter 55: loss 0.0002, time 1118.15ms\n",
            "iter 56: loss 0.0003, time 1114.09ms\n",
            "iter 57: loss 0.0255, time 1161.14ms\n",
            "iter 58: loss 0.0052, time 1131.58ms\n",
            "iter 59: loss 0.0249, time 1126.55ms\n",
            "Step 60: train loss 0.6077, val loss 0.4653\n",
            "Validation accuracy: 0.8861\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.6836, time 10434.23ms\n",
            "iter 61: loss 0.0081, time 1184.10ms\n",
            "iter 62: loss 0.0085, time 1188.38ms\n",
            "iter 63: loss 0.0001, time 1191.21ms\n",
            "iter 64: loss 0.0454, time 1202.18ms\n",
            "iter 65: loss 0.2451, time 1172.55ms\n",
            "iter 66: loss 0.4004, time 1184.40ms\n",
            "iter 67: loss 0.0057, time 1136.28ms\n",
            "iter 68: loss 0.0042, time 1234.34ms\n",
            "iter 69: loss 0.0015, time 1201.62ms\n",
            "iter 70: loss 0.0126, time 1204.10ms\n",
            "iter 71: loss 0.0092, time 1136.89ms\n",
            "iter 72: loss 0.0209, time 1208.09ms\n",
            "iter 73: loss 0.0001, time 1149.90ms\n",
            "iter 74: loss 0.0043, time 1155.96ms\n",
            "iter 75: loss 0.0013, time 1203.62ms\n",
            "iter 76: loss 0.0107, time 1170.93ms\n",
            "iter 77: loss 0.0004, time 1185.18ms\n",
            "iter 78: loss 0.0083, time 1188.15ms\n",
            "iter 79: loss 0.0002, time 1197.32ms\n",
            "Step 80: train loss 0.4829, val loss 0.4091\n",
            "Validation accuracy: 0.9453\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0004, time 10537.20ms\n",
            "iter 81: loss 0.0002, time 1112.86ms\n",
            "iter 82: loss 0.0104, time 1118.74ms\n",
            "iter 83: loss 0.0020, time 1141.72ms\n",
            "iter 84: loss 0.0038, time 1109.91ms\n",
            "iter 85: loss 0.0003, time 1187.88ms\n",
            "iter 86: loss 0.5469, time 1158.00ms\n",
            "iter 87: loss 0.0040, time 1155.11ms\n",
            "iter 88: loss 0.0003, time 1148.24ms\n",
            "iter 89: loss 0.0001, time 1149.66ms\n",
            "iter 90: loss 0.0006, time 1139.33ms\n",
            "iter 91: loss 0.0013, time 1151.33ms\n",
            "iter 92: loss 0.0001, time 1124.47ms\n",
            "iter 93: loss 0.0008, time 1106.89ms\n",
            "iter 94: loss 0.0002, time 1160.50ms\n",
            "iter 95: loss 0.0002, time 1139.61ms\n",
            "iter 96: loss 0.0041, time 1159.51ms\n",
            "iter 97: loss 0.0145, time 1188.25ms\n",
            "iter 98: loss 0.0015, time 1174.69ms\n",
            "iter 99: loss 0.0008, time 1150.90ms\n",
            "Step 100: train loss 0.3954, val loss 0.4293\n",
            "Validation accuracy: 0.8876\n",
            "Test accuracy 0.7000\n",
            "iter 100: loss 0.0012, time 9629.49ms\n",
            "iter 101: loss 0.0094, time 1127.15ms\n",
            "iter 102: loss 0.0046, time 1123.33ms\n",
            "iter 103: loss 0.0002, time 1171.36ms\n",
            "iter 104: loss 0.0000, time 1130.34ms\n",
            "iter 105: loss 0.0513, time 1170.10ms\n",
            "iter 106: loss 0.0155, time 1161.04ms\n",
            "iter 107: loss 0.0967, time 1195.68ms\n",
            "iter 108: loss 0.0001, time 1177.00ms\n",
            "iter 109: loss 1.3281, time 1127.16ms\n",
            "iter 110: loss 0.0143, time 1170.62ms\n",
            "iter 111: loss 0.0007, time 1151.51ms\n",
            "iter 112: loss 0.0074, time 1189.68ms\n",
            "iter 113: loss 0.0048, time 1152.40ms\n",
            "iter 114: loss 0.0010, time 1164.67ms\n",
            "iter 115: loss 0.0129, time 1140.43ms\n",
            "iter 116: loss 0.0679, time 1152.85ms\n",
            "iter 117: loss 0.1895, time 1136.10ms\n",
            "iter 118: loss 0.0014, time 1167.43ms\n",
            "iter 119: loss 0.0017, time 1145.03ms\n",
            "Step 120: train loss 0.3500, val loss 0.3954\n",
            "Validation accuracy: 0.9571\n",
            "iter 120: loss 0.0063, time 8828.51ms\n",
            "iter 121: loss 0.0713, time 1155.82ms\n",
            "iter 122: loss 0.0075, time 1158.07ms\n",
            "iter 123: loss 0.0006, time 1135.90ms\n",
            "iter 124: loss 0.0023, time 1146.26ms\n",
            "iter 125: loss 0.0005, time 1150.50ms\n",
            "iter 126: loss 0.0003, time 1161.20ms\n",
            "iter 127: loss 0.0004, time 1151.03ms\n",
            "iter 128: loss 0.0003, time 1133.19ms\n",
            "iter 129: loss 0.0012, time 1133.21ms\n",
            "iter 130: loss 0.0032, time 1182.90ms\n",
            "iter 131: loss 0.0014, time 1107.82ms\n",
            "iter 132: loss 0.0029, time 1131.78ms\n",
            "iter 133: loss 0.0000, time 1143.63ms\n",
            "iter 134: loss 0.0001, time 1171.43ms\n",
            "iter 135: loss 0.0000, time 1134.43ms\n",
            "iter 136: loss 0.0000, time 1159.82ms\n",
            "iter 137: loss 0.0123, time 1096.32ms\n",
            "iter 138: loss 0.0009, time 1189.52ms\n",
            "iter 139: loss 0.0018, time 1139.02ms\n",
            "Step 140: train loss 0.3079, val loss 0.3953\n",
            "Validation accuracy: 0.9704\n",
            "iter 140: loss 0.0003, time 8790.01ms\n",
            "iter 141: loss 0.0003, time 1166.82ms\n",
            "iter 142: loss 6.4062, time 1174.09ms\n",
            "iter 143: loss 0.0063, time 1160.04ms\n",
            "iter 144: loss 0.0000, time 1200.60ms\n",
            "iter 145: loss 0.0000, time 1149.13ms\n",
            "iter 146: loss 0.0238, time 1192.82ms\n",
            "iter 147: loss 0.0001, time 1174.44ms\n",
            "iter 148: loss 0.0017, time 1139.48ms\n",
            "iter 149: loss 0.0002, time 1180.60ms\n",
            "iter 150: loss 0.0001, time 1132.56ms\n",
            "iter 151: loss 0.0000, time 1179.98ms\n",
            "iter 152: loss 0.0001, time 1148.97ms\n",
            "iter 153: loss 0.0001, time 1171.22ms\n",
            "iter 154: loss 0.0001, time 1129.83ms\n",
            "iter 155: loss 0.0027, time 1160.16ms\n",
            "iter 156: loss 0.0000, time 1131.16ms\n",
            "iter 157: loss 0.0000, time 1131.04ms\n",
            "iter 158: loss 0.0003, time 1139.99ms\n",
            "iter 159: loss 0.0007, time 1178.42ms\n",
            "Step 160: train loss 0.2832, val loss 0.3829\n",
            "Validation accuracy: 0.9630\n",
            "iter 160: loss 0.0009, time 8777.09ms\n",
            "iter 161: loss 0.0004, time 1159.89ms\n",
            "iter 162: loss 0.0002, time 1148.88ms\n",
            "iter 163: loss 0.0022, time 1144.50ms\n",
            "iter 164: loss 0.0001, time 1138.64ms\n",
            "iter 165: loss 0.0001, time 1130.97ms\n",
            "iter 166: loss 0.0004, time 1115.48ms\n",
            "iter 167: loss 0.0003, time 1136.54ms\n",
            "iter 168: loss 0.0000, time 1149.83ms\n",
            "iter 169: loss 0.0001, time 1170.02ms\n",
            "iter 170: loss 0.0012, time 1156.32ms\n",
            "iter 171: loss 0.0102, time 1116.60ms\n",
            "iter 172: loss 0.0000, time 1162.18ms\n",
            "iter 173: loss 0.0004, time 1113.58ms\n",
            "iter 174: loss 0.0016, time 1124.21ms\n",
            "iter 175: loss 0.0001, time 1161.80ms\n",
            "iter 176: loss 0.0002, time 1115.30ms\n",
            "iter 177: loss 0.0002, time 1118.82ms\n",
            "iter 178: loss 0.0005, time 1126.65ms\n",
            "iter 179: loss 0.0011, time 1122.36ms\n",
            "Step 180: train loss 0.2538, val loss 0.3545\n",
            "Validation accuracy: 0.9216\n",
            "iter 180: loss 0.0002, time 8774.57ms\n",
            "iter 181: loss 0.0000, time 1108.79ms\n",
            "iter 182: loss 0.0006, time 1169.07ms\n",
            "iter 183: loss 0.0000, time 1117.46ms\n",
            "iter 184: loss 0.0005, time 1090.84ms\n",
            "iter 185: loss 0.0000, time 1130.72ms\n",
            "iter 186: loss 0.0000, time 1166.29ms\n",
            "iter 187: loss 0.0001, time 1163.67ms\n",
            "iter 188: loss 0.0000, time 1130.96ms\n",
            "iter 189: loss 0.0000, time 1127.99ms\n",
            "iter 190: loss 0.0000, time 1151.03ms\n",
            "iter 191: loss 0.0000, time 1176.27ms\n",
            "iter 192: loss 0.0014, time 1137.90ms\n",
            "iter 193: loss 0.0000, time 1127.17ms\n",
            "iter 194: loss 0.0001, time 1140.50ms\n",
            "iter 195: loss 0.0002, time 1145.45ms\n",
            "iter 196: loss 0.0000, time 1133.50ms\n",
            "iter 197: loss 0.0000, time 1138.14ms\n",
            "iter 198: loss 0.0001, time 1179.48ms\n",
            "iter 199: loss 0.0001, time 1211.74ms\n",
            "Step 200: train loss 0.2297, val loss 0.3373\n",
            "Validation accuracy: 0.9689\n",
            "Test accuracy 0.8000\n",
            "iter 200: loss 0.0078, time 9611.28ms\n",
            "iter 201: loss 0.0000, time 1174.78ms\n",
            "iter 202: loss 0.0000, time 1147.62ms\n",
            "iter 203: loss 0.0001, time 1176.96ms\n",
            "iter 204: loss 0.0000, time 1159.65ms\n",
            "iter 205: loss 0.0001, time 1141.36ms\n",
            "iter 206: loss 0.0000, time 1194.80ms\n",
            "iter 207: loss 0.0000, time 1173.28ms\n",
            "iter 208: loss 0.0000, time 1143.47ms\n",
            "iter 209: loss 0.0000, time 1153.46ms\n",
            "iter 210: loss 0.0000, time 1141.11ms\n",
            "iter 211: loss 0.0002, time 1198.71ms\n",
            "iter 212: loss 0.0000, time 1153.69ms\n",
            "iter 213: loss 0.0000, time 1143.43ms\n",
            "iter 214: loss 0.0000, time 1140.85ms\n",
            "iter 215: loss 0.0000, time 1148.73ms\n",
            "iter 216: loss 0.0000, time 1164.30ms\n",
            "iter 217: loss 0.0000, time 1104.74ms\n",
            "iter 218: loss 0.0000, time 1138.28ms\n",
            "iter 219: loss 0.0000, time 1117.03ms\n",
            "Step 220: train loss 0.2096, val loss 0.3200\n",
            "Validation accuracy: 0.9882\n",
            "iter 220: loss 0.0000, time 8790.22ms\n",
            "iter 221: loss 0.0000, time 1134.75ms\n",
            "iter 222: loss 0.0000, time 1113.55ms\n",
            "iter 223: loss 0.0001, time 1128.38ms\n",
            "iter 224: loss 0.0001, time 1160.97ms\n",
            "iter 225: loss 0.0000, time 1169.19ms\n",
            "iter 226: loss 0.0000, time 1114.11ms\n",
            "iter 227: loss 0.0000, time 1178.98ms\n",
            "iter 228: loss 0.0000, time 1195.20ms\n",
            "iter 229: loss 0.0000, time 1196.04ms\n",
            "iter 230: loss 0.0000, time 1153.49ms\n",
            "iter 231: loss 0.0004, time 1141.02ms\n",
            "iter 232: loss 0.0000, time 1144.12ms\n",
            "iter 233: loss 0.0000, time 1133.97ms\n",
            "iter 234: loss 0.0000, time 1175.09ms\n",
            "iter 235: loss 0.0005, time 1156.79ms\n",
            "iter 236: loss 0.0000, time 1124.45ms\n",
            "iter 237: loss 0.0000, time 1249.02ms\n",
            "iter 238: loss 0.0005, time 1137.77ms\n",
            "iter 239: loss 0.0001, time 1222.66ms\n",
            "Step 240: train loss 0.1926, val loss 0.2958\n",
            "Validation accuracy: 0.9719\n",
            "iter 240: loss 0.0000, time 8838.23ms\n",
            "iter 241: loss 0.0022, time 1108.10ms\n",
            "iter 242: loss 0.0000, time 1152.19ms\n",
            "iter 243: loss 0.0000, time 1128.89ms\n",
            "iter 244: loss 0.0000, time 1171.31ms\n",
            "iter 245: loss 0.0013, time 1138.84ms\n",
            "iter 246: loss 0.0005, time 1132.83ms\n",
            "iter 247: loss 0.0001, time 1123.05ms\n",
            "iter 248: loss 0.0000, time 1137.35ms\n",
            "iter 249: loss 0.0000, time 1160.68ms\n",
            "iter 250: loss 0.0000, time 1187.71ms\n",
            "iter 251: loss 0.0000, time 1150.02ms\n",
            "iter 252: loss 0.0000, time 1188.04ms\n",
            "iter 253: loss 0.0000, time 1132.28ms\n",
            "iter 254: loss 0.0000, time 1164.06ms\n",
            "iter 255: loss 0.0000, time 1136.77ms\n",
            "iter 256: loss 0.0000, time 1203.72ms\n",
            "iter 257: loss 0.0001, time 1178.11ms\n",
            "iter 258: loss 0.0000, time 1157.21ms\n",
            "iter 259: loss 0.0000, time 1139.26ms\n",
            "Step 260: train loss 0.1784, val loss 0.3148\n",
            "Validation accuracy: 0.9689\n",
            "iter 260: loss 0.0004, time 8911.16ms\n",
            "iter 261: loss 0.0000, time 1158.00ms\n",
            "iter 262: loss 0.0003, time 1209.94ms\n",
            "iter 263: loss 0.0001, time 1163.20ms\n",
            "iter 264: loss 0.0000, time 1145.74ms\n",
            "iter 265: loss 0.0000, time 1193.90ms\n",
            "iter 266: loss 0.0000, time 1195.13ms\n",
            "iter 267: loss 0.0000, time 1163.74ms\n",
            "iter 268: loss 0.0000, time 1145.17ms\n",
            "iter 269: loss 0.0000, time 1125.48ms\n",
            "iter 270: loss 0.0000, time 1108.34ms\n",
            "iter 271: loss 0.0000, time 1176.39ms\n",
            "iter 272: loss 0.0000, time 1133.57ms\n",
            "iter 273: loss 0.0000, time 1197.33ms\n",
            "iter 274: loss 0.0001, time 1127.53ms\n",
            "iter 275: loss 0.0000, time 1189.41ms\n",
            "iter 276: loss 0.0000, time 1153.48ms\n",
            "iter 277: loss 0.0000, time 1143.06ms\n",
            "iter 278: loss 0.0000, time 1189.51ms\n",
            "iter 279: loss 0.0000, time 1165.54ms\n",
            "Step 280: train loss 0.1667, val loss 0.3283\n",
            "Validation accuracy: 0.9808\n",
            "iter 280: loss 0.0000, time 8717.97ms\n",
            "iter 281: loss 0.0000, time 1144.68ms\n",
            "iter 282: loss 0.0001, time 1140.63ms\n",
            "iter 283: loss 0.0000, time 1147.90ms\n",
            "iter 284: loss 0.0001, time 1159.06ms\n",
            "iter 285: loss 0.0000, time 1160.67ms\n",
            "iter 286: loss 0.0000, time 1196.63ms\n",
            "iter 287: loss 0.0000, time 1140.85ms\n",
            "iter 288: loss 0.0000, time 1153.96ms\n",
            "iter 289: loss 0.0000, time 1164.60ms\n",
            "iter 290: loss 0.0000, time 1163.70ms\n",
            "iter 291: loss 0.0001, time 1201.69ms\n",
            "iter 292: loss 0.0000, time 1161.06ms\n",
            "iter 293: loss 0.0000, time 1181.12ms\n",
            "iter 294: loss 0.0000, time 1163.06ms\n",
            "iter 295: loss 0.0000, time 1182.41ms\n",
            "iter 296: loss 0.0000, time 1169.79ms\n",
            "iter 297: loss 0.0000, time 1222.00ms\n",
            "iter 298: loss 0.0000, time 1158.60ms\n",
            "iter 299: loss 0.0002, time 1137.84ms\n",
            "Step 300: train loss 0.1563, val loss 0.3115\n",
            "Validation accuracy: 0.9778\n",
            "Test accuracy 0.8000\n",
            "iter 300: loss 0.0559, time 9610.29ms\n",
            "iter 301: loss 0.0000, time 1165.49ms\n",
            "iter 302: loss 0.0000, time 1128.15ms\n",
            "iter 303: loss 0.0000, time 1163.91ms\n",
            "iter 304: loss 0.0000, time 1205.99ms\n",
            "iter 305: loss 0.0000, time 1219.41ms\n",
            "iter 306: loss 0.0000, time 1174.32ms\n",
            "iter 307: loss 0.0000, time 1187.24ms\n",
            "iter 308: loss 0.0001, time 1155.75ms\n",
            "iter 309: loss 0.0000, time 1181.81ms\n",
            "iter 310: loss 0.0000, time 1181.02ms\n",
            "iter 311: loss 0.0012, time 1167.83ms\n",
            "iter 312: loss 0.0000, time 1224.50ms\n",
            "iter 313: loss 0.0000, time 1181.17ms\n",
            "iter 314: loss 0.0000, time 1152.91ms\n",
            "iter 315: loss 0.0005, time 1198.65ms\n",
            "iter 316: loss 0.0000, time 1163.05ms\n",
            "iter 317: loss 0.0009, time 1169.52ms\n",
            "iter 318: loss 0.0000, time 1202.68ms\n",
            "iter 319: loss 0.0000, time 1153.36ms\n",
            "Step 320: train loss 0.1471, val loss 0.3063\n",
            "Validation accuracy: 0.9882\n",
            "iter 320: loss 0.0000, time 8807.53ms\n",
            "iter 321: loss 0.0000, time 1202.32ms\n",
            "iter 322: loss 0.0000, time 1226.50ms\n",
            "iter 323: loss 0.0000, time 1165.22ms\n",
            "iter 324: loss 0.0097, time 1180.76ms\n",
            "iter 325: loss 0.0000, time 1144.33ms\n",
            "iter 326: loss 0.0000, time 1157.84ms\n",
            "iter 327: loss 0.0000, time 1165.07ms\n",
            "iter 328: loss 0.0000, time 1231.71ms\n",
            "iter 329: loss 0.0000, time 1146.34ms\n",
            "iter 330: loss 0.0000, time 1170.44ms\n",
            "iter 331: loss 0.0000, time 1148.66ms\n",
            "iter 332: loss 0.0000, time 1155.31ms\n",
            "iter 333: loss 0.0000, time 1185.66ms\n",
            "iter 334: loss 0.0000, time 1247.27ms\n",
            "iter 335: loss 0.0000, time 1122.42ms\n",
            "iter 336: loss 0.0000, time 1156.53ms\n",
            "iter 337: loss 0.0000, time 1160.31ms\n",
            "iter 338: loss 0.0000, time 1158.65ms\n",
            "iter 339: loss 0.0006, time 1193.10ms\n",
            "Step 340: train loss 0.1385, val loss 0.2944\n",
            "Validation accuracy: 0.9852\n",
            "iter 340: loss 0.0000, time 8751.17ms\n",
            "iter 341: loss 0.0000, time 1178.75ms\n",
            "iter 342: loss 0.0000, time 1156.70ms\n",
            "iter 343: loss 0.0000, time 1138.05ms\n",
            "iter 344: loss 0.0000, time 1186.88ms\n",
            "iter 345: loss 0.0000, time 1119.63ms\n",
            "iter 346: loss 0.0000, time 1202.24ms\n",
            "iter 347: loss 0.0000, time 1140.66ms\n",
            "iter 348: loss 0.0000, time 1210.74ms\n",
            "iter 349: loss 0.0000, time 1151.30ms\n",
            "iter 350: loss 0.0000, time 1152.58ms\n",
            "iter 351: loss 0.0000, time 1195.53ms\n",
            "iter 352: loss 0.0000, time 1180.33ms\n",
            "iter 353: loss 0.0009, time 1164.14ms\n",
            "iter 354: loss 0.0000, time 1158.10ms\n",
            "iter 355: loss 0.0000, time 1210.71ms\n",
            "iter 356: loss 0.0000, time 1194.06ms\n",
            "iter 357: loss 0.0000, time 1210.79ms\n",
            "iter 358: loss 0.0001, time 1182.54ms\n",
            "iter 359: loss 0.0000, time 1155.13ms\n",
            "Step 360: train loss 0.1315, val loss 0.2927\n",
            "Validation accuracy: 0.9778\n",
            "iter 360: loss 0.0001, time 8753.74ms\n",
            "iter 361: loss 0.0000, time 1205.68ms\n",
            "iter 362: loss 0.0000, time 1185.19ms\n",
            "iter 363: loss 0.0000, time 1202.23ms\n",
            "iter 364: loss 0.0000, time 1116.27ms\n",
            "iter 365: loss 0.0000, time 1143.32ms\n",
            "iter 366: loss 0.0000, time 1177.27ms\n",
            "iter 367: loss 0.0000, time 1157.44ms\n",
            "iter 368: loss 0.0000, time 1227.67ms\n",
            "iter 369: loss 0.0000, time 1164.14ms\n",
            "iter 370: loss 0.0000, time 1179.34ms\n",
            "iter 371: loss 0.0000, time 1149.99ms\n",
            "iter 372: loss 0.0000, time 1163.15ms\n",
            "iter 373: loss 0.0000, time 1173.79ms\n",
            "iter 374: loss 0.0000, time 1167.85ms\n",
            "iter 375: loss 0.0000, time 1149.70ms\n",
            "iter 376: loss 0.0000, time 1144.16ms\n",
            "iter 377: loss 0.0000, time 1150.24ms\n",
            "iter 378: loss 0.0000, time 1193.12ms\n",
            "iter 379: loss 0.0000, time 1185.39ms\n",
            "Step 380: train loss 0.1246, val loss 0.3001\n",
            "Validation accuracy: 0.9749\n",
            "iter 380: loss 0.0001, time 8911.46ms\n",
            "iter 381: loss 0.0000, time 1206.61ms\n",
            "iter 382: loss 0.0000, time 1177.28ms\n",
            "iter 383: loss 0.0000, time 1191.05ms\n",
            "iter 384: loss 0.0015, time 1170.77ms\n",
            "iter 385: loss 0.0000, time 1147.00ms\n",
            "iter 386: loss 0.0001, time 1162.57ms\n",
            "iter 387: loss 0.0000, time 1155.31ms\n",
            "iter 388: loss 0.0000, time 1182.30ms\n",
            "iter 389: loss 0.0000, time 1211.11ms\n",
            "iter 390: loss 0.0000, time 1152.40ms\n",
            "iter 391: loss 0.0000, time 1198.21ms\n",
            "iter 392: loss 0.0000, time 1148.26ms\n",
            "iter 393: loss 0.0000, time 1198.35ms\n",
            "iter 394: loss 0.0000, time 1265.44ms\n",
            "iter 395: loss 0.0000, time 1166.40ms\n",
            "iter 396: loss 0.0000, time 1174.68ms\n",
            "iter 397: loss 0.0000, time 1144.21ms\n",
            "iter 398: loss 0.0000, time 1146.79ms\n",
            "iter 399: loss 0.0004, time 1184.48ms\n",
            "Step 400: train loss 0.1185, val loss 0.2959\n",
            "Validation accuracy: 0.9808\n",
            "Test accuracy 0.8333\n",
            "iter 400: loss 0.0001, time 9767.03ms\n",
            "iter 401: loss 0.0000, time 1118.84ms\n",
            "iter 402: loss 0.0000, time 1174.67ms\n",
            "iter 403: loss 0.0000, time 1201.45ms\n",
            "iter 404: loss 0.0000, time 1245.37ms\n",
            "iter 405: loss 0.0000, time 1219.98ms\n",
            "iter 406: loss 0.0023, time 1220.12ms\n",
            "iter 407: loss 0.0000, time 1202.68ms\n",
            "iter 408: loss 0.0008, time 1235.92ms\n",
            "iter 409: loss 0.0000, time 1152.36ms\n",
            "iter 410: loss 0.0000, time 1266.69ms\n",
            "iter 411: loss 0.0000, time 1285.26ms\n",
            "iter 412: loss 0.0003, time 1139.68ms\n",
            "iter 413: loss 0.0000, time 1200.49ms\n",
            "iter 414: loss 0.0000, time 1211.92ms\n",
            "iter 415: loss 0.0000, time 1159.98ms\n",
            "iter 416: loss 0.0000, time 1181.37ms\n",
            "iter 417: loss 0.0000, time 1191.60ms\n",
            "iter 418: loss 0.0000, time 1122.83ms\n",
            "iter 419: loss 0.0000, time 1192.98ms\n",
            "Step 420: train loss 0.1132, val loss 0.2979\n",
            "Validation accuracy: 0.9793\n",
            "iter 420: loss 0.0000, time 8811.72ms\n",
            "iter 421: loss 0.0000, time 1221.05ms\n",
            "iter 422: loss 0.0000, time 1210.51ms\n",
            "iter 423: loss 0.0001, time 1169.93ms\n",
            "iter 424: loss 0.0000, time 1171.75ms\n",
            "iter 425: loss 0.0000, time 1160.12ms\n",
            "iter 426: loss 0.0000, time 1173.83ms\n",
            "iter 427: loss 0.0000, time 1167.72ms\n",
            "iter 428: loss 0.0000, time 1257.89ms\n",
            "iter 429: loss 0.0000, time 1242.61ms\n",
            "iter 430: loss 7.9063, time 1205.08ms\n",
            "iter 431: loss 0.0000, time 1197.51ms\n",
            "iter 432: loss 0.0000, time 1176.21ms\n",
            "iter 433: loss 0.0000, time 1152.24ms\n",
            "iter 434: loss 0.0000, time 1158.75ms\n",
            "iter 435: loss 0.0005, time 1143.08ms\n",
            "iter 436: loss 0.0000, time 1169.30ms\n",
            "iter 437: loss 0.0000, time 1178.27ms\n",
            "iter 438: loss 0.0000, time 1130.91ms\n",
            "iter 439: loss 0.0000, time 1161.54ms\n",
            "Step 440: train loss 0.1088, val loss 0.3174\n",
            "Validation accuracy: 0.9749\n",
            "iter 440: loss 0.0000, time 8739.28ms\n",
            "iter 441: loss 0.0003, time 1164.38ms\n",
            "iter 442: loss 0.0000, time 1191.31ms\n",
            "iter 443: loss 0.0000, time 1154.34ms\n",
            "iter 444: loss 0.0000, time 1122.07ms\n",
            "iter 445: loss 0.0000, time 1185.39ms\n",
            "iter 446: loss 0.0000, time 1206.18ms\n",
            "iter 447: loss 0.0000, time 1132.96ms\n",
            "iter 448: loss 0.0000, time 1162.81ms\n",
            "iter 449: loss 0.0000, time 1178.91ms\n",
            "iter 450: loss 0.0000, time 1186.89ms\n",
            "iter 451: loss 0.0000, time 1156.39ms\n",
            "iter 452: loss 0.0000, time 1145.85ms\n",
            "iter 453: loss 0.0000, time 1117.73ms\n",
            "iter 454: loss 0.0001, time 1149.21ms\n",
            "iter 455: loss 0.0000, time 1202.01ms\n",
            "iter 456: loss 0.0000, time 1118.15ms\n",
            "iter 457: loss 0.0000, time 1142.14ms\n",
            "iter 458: loss 0.0000, time 1112.10ms\n",
            "iter 459: loss 0.0000, time 1089.74ms\n",
            "Step 460: train loss 0.1052, val loss 0.3241\n",
            "Validation accuracy: 0.9822\n",
            "iter 460: loss 0.0000, time 8888.44ms\n",
            "iter 461: loss 0.0000, time 1155.04ms\n",
            "iter 462: loss 0.0000, time 1153.08ms\n",
            "iter 463: loss 0.0000, time 1126.17ms\n",
            "iter 464: loss 0.0000, time 1148.12ms\n",
            "iter 465: loss 0.0000, time 1123.04ms\n",
            "iter 466: loss 0.0000, time 1152.55ms\n",
            "iter 467: loss 0.0000, time 1108.99ms\n",
            "iter 468: loss 0.0000, time 1141.09ms\n",
            "iter 469: loss 0.0000, time 1202.01ms\n",
            "iter 470: loss 0.0005, time 1213.91ms\n",
            "iter 471: loss 0.0000, time 1172.83ms\n",
            "iter 472: loss 0.0000, time 1175.83ms\n",
            "iter 473: loss 0.0001, time 1137.56ms\n",
            "iter 474: loss 0.0000, time 1130.06ms\n",
            "iter 475: loss 0.0000, time 1201.50ms\n",
            "iter 476: loss 0.0000, time 1147.02ms\n",
            "iter 477: loss 0.0000, time 1111.52ms\n",
            "iter 478: loss 0.0000, time 1154.64ms\n",
            "iter 479: loss 0.0000, time 1134.69ms\n",
            "Step 480: train loss 0.1012, val loss 0.3286\n",
            "Validation accuracy: 0.9867\n",
            "iter 480: loss 0.0000, time 8775.58ms\n",
            "iter 481: loss 0.0000, time 1147.48ms\n",
            "iter 482: loss 0.0000, time 1147.95ms\n",
            "iter 483: loss 0.0000, time 1100.01ms\n",
            "iter 484: loss 0.0001, time 1127.28ms\n",
            "iter 485: loss 0.0000, time 1172.34ms\n",
            "iter 486: loss 0.0013, time 1135.77ms\n",
            "iter 487: loss 0.0000, time 1160.65ms\n",
            "iter 488: loss 0.0000, time 1136.26ms\n",
            "iter 489: loss 0.0000, time 1110.50ms\n",
            "iter 490: loss 0.0000, time 1152.23ms\n",
            "iter 491: loss 0.0000, time 1114.31ms\n",
            "iter 492: loss 0.0000, time 1148.00ms\n",
            "iter 493: loss 0.0000, time 1093.21ms\n",
            "iter 494: loss 0.0000, time 1155.71ms\n",
            "iter 495: loss 0.0000, time 1126.66ms\n",
            "iter 496: loss 0.0000, time 1149.09ms\n",
            "iter 497: loss 0.0000, time 1134.49ms\n",
            "iter 498: loss 0.0052, time 1139.26ms\n",
            "iter 499: loss 0.0000, time 1155.58ms\n",
            "Step 500: train loss 0.0974, val loss 0.3484\n",
            "Validation accuracy: 0.9822\n",
            "Test accuracy 0.8000\n",
            "iter 500: loss 0.0000, time 9558.69ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▇▇█▇</td></tr><tr><td>Test_F1_Score</td><td>▁▆███▇</td></tr><tr><td>Test_Precision</td><td>▁▆████</td></tr><tr><td>Test_Recall</td><td>▁▅▇▇█▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▆▇█▇███▇████████████████</td></tr><tr><td>val/loss</td><td> █▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.8</td></tr><tr><td>Test_F1_Score</td><td>0.78968</td></tr><tr><td>Test_Precision</td><td>0.85065</td></tr><tr><td>Test_Recall</td><td>0.8</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.09737</td></tr><tr><td>val/acc</td><td>0.98225</td></tr><tr><td>val/loss</td><td>0.34838</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2111</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/7o601jbo</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_211135-7o601jbo\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1qdevbzk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_212803-1qdevbzk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk' target=\"_blank\">electric-sweep-10</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2128\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">electric-sweep-10</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_212803-1qdevbzk\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_212809-1qdevbzk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk' target=\"_blank\">gpt2_hyper_2128</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n",
            "iter 0: loss 0.6055, time 18810.37ms\n",
            "iter 1: loss 2.9531, time 1234.62ms\n",
            "iter 2: loss 1.4688, time 1716.59ms\n",
            "iter 3: loss 3.0156, time 3241.36ms\n",
            "iter 4: loss 1.2500, time 1700.96ms\n",
            "iter 5: loss 0.7461, time 1457.14ms\n",
            "iter 6: loss 0.9648, time 1462.57ms\n",
            "iter 7: loss 0.9023, time 1438.54ms\n",
            "iter 8: loss 1.9766, time 1476.26ms\n",
            "iter 9: loss 1.4375, time 1474.74ms\n",
            "iter 10: loss 0.9844, time 1475.71ms\n",
            "iter 11: loss 1.4453, time 1513.31ms\n",
            "iter 12: loss 0.5781, time 1534.03ms\n",
            "iter 13: loss 0.8047, time 1512.40ms\n",
            "iter 14: loss 1.3984, time 1526.08ms\n",
            "iter 15: loss 0.5742, time 1454.27ms\n",
            "iter 16: loss 0.8984, time 1494.01ms\n",
            "iter 17: loss 0.9688, time 1554.63ms\n",
            "iter 18: loss 1.3125, time 1443.93ms\n",
            "iter 19: loss 0.9766, time 1478.76ms\n",
            "Step 20: train loss 1.2481, val loss 1.0270\n",
            "Validation accuracy: 0.8062\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.4609, time 11343.13ms\n",
            "iter 21: loss 0.4180, time 1207.51ms\n",
            "iter 22: loss 0.9258, time 1184.95ms\n",
            "iter 23: loss 0.2793, time 1163.47ms\n",
            "iter 24: loss 0.3867, time 1160.89ms\n",
            "iter 25: loss 0.0684, time 1176.51ms\n",
            "iter 26: loss 0.4434, time 1182.89ms\n",
            "iter 27: loss 0.5117, time 1151.71ms\n",
            "iter 28: loss 0.1523, time 1177.72ms\n",
            "iter 29: loss 0.0520, time 1183.17ms\n",
            "iter 30: loss 0.0232, time 1214.35ms\n",
            "iter 31: loss 2.1094, time 1156.55ms\n",
            "iter 32: loss 0.0067, time 1202.13ms\n",
            "iter 33: loss 0.0327, time 1152.60ms\n",
            "iter 34: loss 3.1563, time 1125.63ms\n",
            "iter 35: loss 0.0009, time 1166.81ms\n",
            "iter 36: loss 0.0069, time 1154.29ms\n",
            "iter 37: loss 0.0894, time 1136.78ms\n",
            "iter 38: loss 0.0159, time 1198.27ms\n",
            "iter 39: loss 3.4844, time 1195.87ms\n",
            "Step 40: train loss 0.8912, val loss 0.7296\n",
            "Validation accuracy: 0.9009\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.4043, time 10502.11ms\n",
            "iter 41: loss 0.0029, time 1139.65ms\n",
            "iter 42: loss 0.0569, time 1189.68ms\n",
            "iter 43: loss 0.0013, time 1134.02ms\n",
            "iter 44: loss 0.0157, time 1177.39ms\n",
            "iter 45: loss 0.1113, time 1180.71ms\n",
            "iter 46: loss 0.1465, time 1163.71ms\n",
            "iter 47: loss 0.0037, time 1192.14ms\n",
            "iter 48: loss 0.0427, time 1158.54ms\n",
            "iter 49: loss 0.0197, time 1165.36ms\n",
            "iter 50: loss 0.0232, time 1164.10ms\n",
            "iter 51: loss 0.0038, time 1155.45ms\n",
            "iter 52: loss 0.0374, time 1169.12ms\n",
            "iter 53: loss 0.0062, time 1153.12ms\n",
            "iter 54: loss 0.0098, time 1194.01ms\n",
            "iter 55: loss 0.0038, time 1233.54ms\n",
            "iter 56: loss 0.1699, time 1165.04ms\n",
            "iter 57: loss 0.0001, time 1170.52ms\n",
            "iter 58: loss 0.3633, time 1270.82ms\n",
            "iter 59: loss 0.0004, time 1193.78ms\n",
            "Step 60: train loss 0.6750, val loss 0.6681\n",
            "Validation accuracy: 0.9541\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 5.5000, time 10859.46ms\n",
            "iter 61: loss 0.2988, time 1176.35ms\n",
            "iter 62: loss 0.0010, time 1128.80ms\n",
            "iter 63: loss 0.0195, time 1141.43ms\n",
            "iter 64: loss 0.0015, time 1177.18ms\n",
            "iter 65: loss 0.1865, time 1215.50ms\n",
            "iter 66: loss 0.5937, time 1186.59ms\n",
            "iter 67: loss 0.0142, time 1205.74ms\n",
            "iter 68: loss 0.0008, time 1177.29ms\n",
            "iter 69: loss 0.0081, time 1179.99ms\n",
            "iter 70: loss 0.0035, time 1161.16ms\n",
            "iter 71: loss 0.0036, time 1142.47ms\n",
            "iter 72: loss 0.0058, time 1184.50ms\n",
            "iter 73: loss 0.0002, time 1149.78ms\n",
            "iter 74: loss 0.0108, time 1210.96ms\n",
            "iter 75: loss 0.0001, time 1219.15ms\n",
            "iter 76: loss 0.0184, time 1181.30ms\n",
            "iter 77: loss 0.0030, time 1181.51ms\n",
            "iter 78: loss 0.0008, time 1273.06ms\n",
            "iter 79: loss 0.0486, time 1119.85ms\n",
            "Step 80: train loss 0.5464, val loss 0.5862\n",
            "Validation accuracy: 0.9571\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0005, time 10534.21ms\n",
            "iter 81: loss 0.0003, time 1137.55ms\n",
            "iter 82: loss 0.0007, time 1148.01ms\n",
            "iter 83: loss 0.0009, time 1181.77ms\n",
            "iter 84: loss 0.0004, time 1161.13ms\n",
            "iter 85: loss 0.0000, time 1164.36ms\n",
            "iter 86: loss 0.0522, time 1176.82ms\n",
            "iter 87: loss 0.0014, time 1204.29ms\n",
            "iter 88: loss 0.0002, time 1214.34ms\n",
            "iter 89: loss 0.0000, time 1240.77ms\n",
            "iter 90: loss 0.0001, time 1216.92ms\n",
            "iter 91: loss 0.0002, time 1233.19ms\n",
            "iter 92: loss 0.0001, time 1196.90ms\n",
            "iter 93: loss 0.0081, time 1165.34ms\n",
            "iter 94: loss 0.0032, time 1192.67ms\n",
            "iter 95: loss 0.0009, time 1212.43ms\n",
            "iter 96: loss 0.0063, time 1196.36ms\n",
            "iter 97: loss 0.0016, time 1204.11ms\n",
            "iter 98: loss 0.0000, time 1256.42ms\n",
            "iter 99: loss 0.0003, time 1247.08ms\n",
            "Step 100: train loss 0.4514, val loss 0.5639\n",
            "Validation accuracy: 0.9127\n",
            "Test accuracy 0.6667\n",
            "iter 100: loss 0.0011, time 10288.78ms\n",
            "iter 101: loss 0.0225, time 1395.95ms\n",
            "iter 102: loss 0.0003, time 1266.25ms\n",
            "iter 103: loss 0.0019, time 1166.49ms\n",
            "iter 104: loss 0.0001, time 1225.54ms\n",
            "iter 105: loss 0.0010, time 1258.82ms\n",
            "iter 106: loss 0.0032, time 1286.06ms\n",
            "iter 107: loss 0.0017, time 1199.65ms\n",
            "iter 108: loss 0.0001, time 1269.33ms\n",
            "iter 109: loss 8.8750, time 1273.37ms\n",
            "iter 110: loss 0.0000, time 1377.24ms\n",
            "iter 111: loss 0.0004, time 1395.23ms\n",
            "iter 112: loss 0.0021, time 1240.43ms\n",
            "iter 113: loss 0.0004, time 1150.20ms\n",
            "iter 114: loss 0.0004, time 1126.49ms\n",
            "iter 115: loss 0.0840, time 1154.79ms\n",
            "iter 116: loss 0.0006, time 1170.18ms\n",
            "iter 117: loss 0.5195, time 1177.94ms\n",
            "iter 118: loss 0.0005, time 1173.30ms\n",
            "iter 119: loss 0.0017, time 1159.99ms\n",
            "Step 120: train loss 0.3984, val loss 0.5379\n",
            "Validation accuracy: 0.9571\n",
            "iter 120: loss 0.0004, time 9148.43ms\n",
            "iter 121: loss 0.0013, time 1202.19ms\n",
            "iter 122: loss 0.0099, time 1190.22ms\n",
            "iter 123: loss 0.0080, time 1152.79ms\n",
            "iter 124: loss 0.0075, time 1152.47ms\n",
            "iter 125: loss 0.0166, time 1165.92ms\n",
            "iter 126: loss 0.0012, time 1223.18ms\n",
            "iter 127: loss 0.0140, time 1197.11ms\n",
            "iter 128: loss 0.0007, time 1205.41ms\n",
            "iter 129: loss 0.0000, time 1115.24ms\n",
            "iter 130: loss 0.0002, time 1164.09ms\n",
            "iter 131: loss 0.0001, time 1182.13ms\n",
            "iter 132: loss 0.0003, time 1127.53ms\n",
            "iter 133: loss 0.0000, time 1146.18ms\n",
            "iter 134: loss 0.0002, time 1133.75ms\n",
            "iter 135: loss 0.0001, time 1170.64ms\n",
            "iter 136: loss 0.0008, time 1135.86ms\n",
            "iter 137: loss 0.0006, time 1137.13ms\n",
            "iter 138: loss 0.0006, time 1115.80ms\n",
            "iter 139: loss 0.0001, time 1153.47ms\n",
            "Step 140: train loss 0.3479, val loss 0.5191\n",
            "Validation accuracy: 0.9615\n",
            "iter 140: loss 0.0002, time 8964.08ms\n",
            "iter 141: loss 0.0000, time 1135.80ms\n",
            "iter 142: loss 0.0072, time 1177.82ms\n",
            "iter 143: loss 0.0105, time 1175.72ms\n",
            "iter 144: loss 0.0000, time 1193.70ms\n",
            "iter 145: loss 0.0000, time 1174.18ms\n",
            "iter 146: loss 0.0003, time 1151.17ms\n",
            "iter 147: loss 0.0000, time 1105.80ms\n",
            "iter 148: loss 0.0000, time 1179.10ms\n",
            "iter 149: loss 0.0000, time 1165.84ms\n",
            "iter 150: loss 0.0001, time 1146.37ms\n",
            "iter 151: loss 0.0216, time 1174.51ms\n",
            "iter 152: loss 0.0000, time 1122.89ms\n",
            "iter 153: loss 0.0003, time 1176.30ms\n",
            "iter 154: loss 0.0003, time 1145.78ms\n",
            "iter 155: loss 0.0008, time 1147.26ms\n",
            "iter 156: loss 0.0002, time 1148.29ms\n",
            "iter 157: loss 0.0019, time 1179.17ms\n",
            "iter 158: loss 0.0002, time 1217.50ms\n",
            "iter 159: loss 0.0005, time 1207.42ms\n",
            "Step 160: train loss 0.3139, val loss 0.5146\n",
            "Validation accuracy: 0.9660\n",
            "iter 160: loss 0.0013, time 9308.81ms\n",
            "iter 161: loss 0.0000, time 1215.78ms\n",
            "iter 162: loss 0.0008, time 1196.96ms\n",
            "iter 163: loss 0.0050, time 1194.91ms\n",
            "iter 164: loss 0.0001, time 1166.44ms\n",
            "iter 165: loss 0.0000, time 1223.90ms\n",
            "iter 166: loss 0.0002, time 1223.05ms\n",
            "iter 167: loss 0.0001, time 1196.76ms\n",
            "iter 168: loss 0.0001, time 1183.96ms\n",
            "iter 169: loss 0.0002, time 1152.58ms\n",
            "iter 170: loss 0.0009, time 1207.26ms\n",
            "iter 171: loss 0.0002, time 1232.32ms\n",
            "iter 172: loss 0.0000, time 1258.87ms\n",
            "iter 173: loss 0.0001, time 1213.62ms\n",
            "iter 174: loss 0.0016, time 1228.78ms\n",
            "iter 175: loss 0.0003, time 1240.43ms\n",
            "iter 176: loss 0.0001, time 1204.03ms\n",
            "iter 177: loss 0.0004, time 1211.72ms\n",
            "iter 178: loss 0.0001, time 1216.81ms\n",
            "iter 179: loss 0.0001, time 1174.64ms\n",
            "Step 180: train loss 0.2814, val loss 0.5544\n",
            "Validation accuracy: 0.9334\n",
            "iter 180: loss 0.0000, time 9249.87ms\n",
            "iter 181: loss 0.0006, time 1149.21ms\n",
            "iter 182: loss 0.1143, time 1136.71ms\n",
            "iter 183: loss 0.0002, time 1259.34ms\n",
            "iter 184: loss 0.0003, time 1184.12ms\n",
            "iter 185: loss 0.0000, time 1207.29ms\n",
            "iter 186: loss 0.0000, time 1218.73ms\n",
            "iter 187: loss 0.0001, time 1158.38ms\n",
            "iter 188: loss 0.0001, time 1183.77ms\n",
            "iter 189: loss 0.0002, time 1193.56ms\n",
            "iter 190: loss 0.0000, time 1149.48ms\n",
            "iter 191: loss 0.0000, time 1212.07ms\n",
            "iter 192: loss 0.0167, time 1211.51ms\n",
            "iter 193: loss 0.0000, time 1152.40ms\n",
            "iter 194: loss 0.0001, time 1178.32ms\n",
            "iter 195: loss 0.2129, time 1224.29ms\n",
            "iter 196: loss 0.0003, time 1170.05ms\n",
            "iter 197: loss 0.0000, time 1181.74ms\n",
            "iter 198: loss 0.0002, time 1167.20ms\n",
            "iter 199: loss 0.0002, time 1185.80ms\n",
            "Step 200: train loss 0.2543, val loss 0.5513\n",
            "Validation accuracy: 0.9645\n",
            "Test accuracy 0.6333\n",
            "iter 200: loss 0.0061, time 10001.57ms\n",
            "iter 201: loss 0.0001, time 1204.06ms\n",
            "iter 202: loss 0.0000, time 1172.92ms\n",
            "iter 203: loss 0.0000, time 1210.47ms\n",
            "iter 204: loss 0.0000, time 1198.09ms\n",
            "iter 205: loss 0.0000, time 1185.10ms\n",
            "iter 206: loss 0.0000, time 1176.92ms\n",
            "iter 207: loss 0.0001, time 1204.84ms\n",
            "iter 208: loss 0.0000, time 1233.90ms\n",
            "iter 209: loss 0.0000, time 1182.18ms\n",
            "iter 210: loss 0.0000, time 1171.44ms\n",
            "iter 211: loss 0.0001, time 1187.26ms\n",
            "iter 212: loss 0.0000, time 1176.47ms\n",
            "iter 213: loss 0.0000, time 1182.66ms\n",
            "iter 214: loss 0.0000, time 1191.56ms\n",
            "iter 215: loss 0.0003, time 1193.03ms\n",
            "iter 216: loss 0.0000, time 1177.84ms\n",
            "iter 217: loss 0.0000, time 1134.97ms\n",
            "iter 218: loss 0.0000, time 1188.73ms\n",
            "iter 219: loss 0.0000, time 1178.78ms\n",
            "Step 220: train loss 0.2322, val loss 0.5953\n",
            "Validation accuracy: 0.9645\n",
            "iter 220: loss 0.0000, time 9189.84ms\n",
            "iter 221: loss 0.0000, time 1132.27ms\n",
            "iter 222: loss 0.0222, time 1168.48ms\n",
            "iter 223: loss 0.0007, time 1195.84ms\n",
            "iter 224: loss 0.0000, time 1189.16ms\n",
            "iter 225: loss 0.0000, time 1158.06ms\n",
            "iter 226: loss 0.0000, time 1166.28ms\n",
            "iter 227: loss 0.0000, time 1209.73ms\n",
            "iter 228: loss 0.0000, time 1189.87ms\n",
            "iter 229: loss 0.0000, time 1162.82ms\n",
            "iter 230: loss 0.0000, time 1189.72ms\n",
            "iter 231: loss 0.0001, time 1176.58ms\n",
            "iter 232: loss 0.0000, time 1178.46ms\n",
            "iter 233: loss 0.0000, time 1179.94ms\n",
            "iter 234: loss 0.0000, time 1199.63ms\n",
            "iter 235: loss 0.0000, time 1176.42ms\n",
            "iter 236: loss 0.0001, time 1142.13ms\n",
            "iter 237: loss 0.0000, time 1159.45ms\n",
            "iter 238: loss 0.0005, time 1160.04ms\n",
            "iter 239: loss 0.0000, time 1157.01ms\n",
            "Step 240: train loss 0.2136, val loss 0.5830\n",
            "Validation accuracy: 0.9704\n",
            "iter 240: loss 0.0000, time 9030.89ms\n",
            "iter 241: loss 0.0020, time 1197.88ms\n",
            "iter 242: loss 0.0000, time 1227.11ms\n",
            "iter 243: loss 0.0000, time 1159.69ms\n",
            "iter 244: loss 0.0001, time 1164.87ms\n",
            "iter 245: loss 0.0000, time 1239.37ms\n",
            "iter 246: loss 0.0000, time 1154.56ms\n",
            "iter 247: loss 0.0000, time 1151.43ms\n",
            "iter 248: loss 0.0000, time 1135.79ms\n",
            "iter 249: loss 0.0001, time 1188.15ms\n",
            "iter 250: loss 0.0000, time 1175.18ms\n",
            "iter 251: loss 1.1172, time 1234.51ms\n",
            "iter 252: loss 0.0001, time 1192.24ms\n",
            "iter 253: loss 0.0001, time 1177.99ms\n",
            "iter 254: loss 0.0000, time 1170.23ms\n",
            "iter 255: loss 0.0000, time 1181.06ms\n",
            "iter 256: loss 0.0000, time 1173.93ms\n",
            "iter 257: loss 0.0044, time 1266.94ms\n",
            "iter 258: loss 0.0000, time 1239.65ms\n",
            "iter 259: loss 0.0001, time 1200.41ms\n",
            "Step 260: train loss 0.1977, val loss 0.5917\n",
            "Validation accuracy: 0.9660\n",
            "iter 260: loss 0.0000, time 9084.08ms\n",
            "iter 261: loss 0.0000, time 1190.99ms\n",
            "iter 262: loss 0.0000, time 1155.63ms\n",
            "iter 263: loss 0.0000, time 1157.73ms\n",
            "iter 264: loss 0.0001, time 1170.63ms\n",
            "iter 265: loss 0.0001, time 1197.73ms\n",
            "iter 266: loss 0.0000, time 1177.34ms\n",
            "iter 267: loss 0.0000, time 1182.03ms\n",
            "iter 268: loss 0.0000, time 1176.74ms\n",
            "iter 269: loss 0.0000, time 1197.90ms\n",
            "iter 270: loss 0.0002, time 1206.12ms\n",
            "iter 271: loss 0.0000, time 1171.80ms\n",
            "iter 272: loss 0.0000, time 1241.66ms\n",
            "iter 273: loss 0.0000, time 1180.21ms\n",
            "iter 274: loss 0.0018, time 1544.08ms\n",
            "iter 275: loss 0.0000, time 1474.44ms\n",
            "iter 276: loss 0.0000, time 1386.90ms\n",
            "iter 277: loss 0.0000, time 1328.59ms\n",
            "iter 278: loss 0.0000, time 1258.30ms\n",
            "iter 279: loss 0.0005, time 1369.93ms\n",
            "Step 280: train loss 0.1843, val loss 0.6074\n",
            "Validation accuracy: 0.9438\n",
            "iter 280: loss 0.0000, time 9586.10ms\n",
            "iter 281: loss 0.0000, time 1222.14ms\n",
            "iter 282: loss 0.0000, time 1186.40ms\n",
            "iter 283: loss 0.0000, time 1183.57ms\n",
            "iter 284: loss 0.0001, time 1312.37ms\n",
            "iter 285: loss 0.0000, time 1224.51ms\n",
            "iter 286: loss 0.0000, time 1250.55ms\n",
            "iter 287: loss 0.0000, time 1240.95ms\n",
            "iter 288: loss 0.0000, time 1254.64ms\n",
            "iter 289: loss 0.0010, time 1248.53ms\n",
            "iter 290: loss 0.0000, time 1306.10ms\n",
            "iter 291: loss 0.0027, time 1214.38ms\n",
            "iter 292: loss 0.0004, time 1309.86ms\n",
            "iter 293: loss 0.0000, time 1194.70ms\n",
            "iter 294: loss 0.0000, time 1295.70ms\n",
            "iter 295: loss 0.0000, time 1267.83ms\n",
            "iter 296: loss 0.0000, time 1383.70ms\n",
            "iter 297: loss 0.0000, time 1198.12ms\n",
            "iter 298: loss 0.0000, time 1218.39ms\n",
            "iter 299: loss 0.0059, time 1154.05ms\n",
            "Step 300: train loss 0.1723, val loss 0.5847\n",
            "Validation accuracy: 0.9719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6333\n",
            "iter 300: loss 0.0001, time 10177.64ms\n",
            "iter 301: loss 0.0000, time 1158.27ms\n",
            "iter 302: loss 0.0000, time 1155.25ms\n",
            "iter 303: loss 0.0000, time 1197.18ms\n",
            "iter 304: loss 0.0000, time 1199.01ms\n",
            "iter 305: loss 0.0000, time 1181.87ms\n",
            "iter 306: loss 0.0000, time 1144.12ms\n",
            "iter 307: loss 0.0000, time 1143.54ms\n",
            "iter 308: loss 0.0000, time 1166.26ms\n",
            "iter 309: loss 0.0000, time 1228.23ms\n",
            "iter 310: loss 0.0007, time 1168.80ms\n",
            "iter 311: loss 0.0000, time 1214.97ms\n",
            "iter 312: loss 0.0000, time 1207.42ms\n",
            "iter 313: loss 0.0000, time 1279.11ms\n",
            "iter 314: loss 0.0000, time 1246.81ms\n",
            "iter 315: loss 0.0000, time 1187.52ms\n",
            "iter 316: loss 0.0000, time 1189.11ms\n",
            "iter 317: loss 0.0000, time 1188.47ms\n",
            "iter 318: loss 0.0000, time 1234.45ms\n",
            "iter 319: loss 0.0000, time 1182.09ms\n",
            "Step 320: train loss 0.1620, val loss 0.6027\n",
            "Validation accuracy: 0.9719\n",
            "iter 320: loss 0.0000, time 9121.24ms\n",
            "iter 321: loss 0.0001, time 1181.40ms\n",
            "iter 322: loss 0.0001, time 1165.72ms\n",
            "iter 323: loss 0.0000, time 1170.52ms\n",
            "iter 324: loss 0.0000, time 1147.40ms\n",
            "iter 325: loss 0.0000, time 1207.00ms\n",
            "iter 326: loss 0.0000, time 1185.36ms\n",
            "iter 327: loss 0.0000, time 1174.79ms\n",
            "iter 328: loss 0.0000, time 1161.67ms\n",
            "iter 329: loss 0.0000, time 1202.99ms\n",
            "iter 330: loss 0.0000, time 1198.83ms\n",
            "iter 331: loss 0.0000, time 1251.19ms\n",
            "iter 332: loss 0.0000, time 1165.92ms\n",
            "iter 333: loss 0.0000, time 1190.37ms\n",
            "iter 334: loss 0.0001, time 1174.65ms\n",
            "iter 335: loss 0.0000, time 1241.76ms\n",
            "iter 336: loss 0.0002, time 1232.94ms\n",
            "iter 337: loss 0.0000, time 1214.51ms\n",
            "iter 338: loss 0.0000, time 1242.26ms\n",
            "iter 339: loss 0.0000, time 1193.54ms\n",
            "Step 340: train loss 0.1529, val loss 0.5903\n",
            "Validation accuracy: 0.9719\n",
            "iter 340: loss 0.0001, time 9281.01ms\n",
            "iter 341: loss 0.0000, time 1198.17ms\n",
            "iter 342: loss 0.0000, time 1245.01ms\n",
            "iter 343: loss 0.0000, time 1244.33ms\n",
            "iter 344: loss 0.0000, time 1166.07ms\n",
            "iter 345: loss 0.0000, time 1160.72ms\n",
            "iter 346: loss 0.0001, time 1161.80ms\n",
            "iter 347: loss 0.0000, time 1179.40ms\n",
            "iter 348: loss 0.0000, time 1249.27ms\n",
            "iter 349: loss 0.0000, time 1153.31ms\n",
            "iter 350: loss 0.0000, time 1180.95ms\n",
            "iter 351: loss 0.0000, time 1158.24ms\n",
            "iter 352: loss 0.0000, time 1171.51ms\n",
            "iter 353: loss 0.0001, time 1269.37ms\n",
            "iter 354: loss 0.0000, time 1179.91ms\n",
            "iter 355: loss 0.0000, time 1256.23ms\n",
            "iter 356: loss 0.0014, time 1236.22ms\n",
            "iter 357: loss 0.0000, time 1202.61ms\n",
            "iter 358: loss 0.0000, time 1198.17ms\n",
            "iter 359: loss 0.0000, time 1237.17ms\n",
            "Step 360: train loss 0.1447, val loss 0.5841\n",
            "Validation accuracy: 0.9793\n",
            "iter 360: loss 0.0000, time 9406.71ms\n",
            "iter 361: loss 0.0000, time 1142.24ms\n",
            "iter 362: loss 0.0000, time 1168.67ms\n",
            "iter 363: loss 0.0098, time 1167.25ms\n",
            "iter 364: loss 0.0000, time 1146.75ms\n",
            "iter 365: loss 0.0000, time 1149.73ms\n",
            "iter 366: loss 0.0000, time 1133.06ms\n",
            "iter 367: loss 0.0000, time 1290.05ms\n",
            "iter 368: loss 0.0000, time 1215.01ms\n",
            "iter 369: loss 0.0000, time 1166.19ms\n",
            "iter 370: loss 0.0000, time 1344.21ms\n",
            "iter 371: loss 0.0000, time 1244.10ms\n",
            "iter 372: loss 0.0000, time 1247.84ms\n",
            "iter 373: loss 0.0002, time 1197.27ms\n",
            "iter 374: loss 0.0000, time 1224.24ms\n",
            "iter 375: loss 0.0000, time 1162.57ms\n",
            "iter 376: loss 0.0000, time 1163.83ms\n",
            "iter 377: loss 0.0000, time 1157.11ms\n",
            "iter 378: loss 0.0000, time 1200.72ms\n",
            "iter 379: loss 0.0000, time 1445.62ms\n",
            "Step 380: train loss 0.1377, val loss 0.5728\n",
            "Validation accuracy: 0.9734\n",
            "iter 380: loss 0.0000, time 10131.12ms\n",
            "iter 381: loss 0.0028, time 1355.69ms\n",
            "iter 382: loss 0.0000, time 1201.90ms\n",
            "iter 383: loss 0.0000, time 1147.30ms\n",
            "iter 384: loss 0.0015, time 1148.53ms\n",
            "iter 385: loss 0.0000, time 1154.50ms\n",
            "iter 386: loss 0.0000, time 1192.63ms\n",
            "iter 387: loss 0.0000, time 1152.48ms\n",
            "iter 388: loss 0.0000, time 1170.02ms\n",
            "iter 389: loss 0.0000, time 1154.06ms\n",
            "iter 390: loss 0.0008, time 1205.68ms\n",
            "iter 391: loss 0.0000, time 1172.02ms\n",
            "iter 392: loss 0.0000, time 1177.18ms\n",
            "iter 393: loss 0.0000, time 1146.27ms\n",
            "iter 394: loss 0.0000, time 1258.73ms\n",
            "iter 395: loss 0.0000, time 1191.51ms\n",
            "iter 396: loss 0.0000, time 1198.90ms\n",
            "iter 397: loss 0.0000, time 1204.88ms\n",
            "iter 398: loss 0.0000, time 1190.68ms\n",
            "iter 399: loss 0.0014, time 1259.56ms\n",
            "Step 400: train loss 0.1318, val loss 0.5768\n",
            "Validation accuracy: 0.9719\n",
            "Test accuracy 0.7000\n",
            "iter 400: loss 0.0006, time 10264.60ms\n",
            "iter 401: loss 0.0000, time 1260.35ms\n",
            "iter 402: loss 0.0000, time 1248.00ms\n",
            "iter 403: loss 0.0000, time 1223.34ms\n",
            "iter 404: loss 0.0000, time 1275.13ms\n",
            "iter 405: loss 0.0000, time 1187.02ms\n",
            "iter 406: loss 0.0000, time 1209.75ms\n",
            "iter 407: loss 0.0000, time 1204.17ms\n",
            "iter 408: loss 0.0000, time 1154.40ms\n",
            "iter 409: loss 0.0001, time 1211.72ms\n",
            "iter 410: loss 0.0000, time 1234.36ms\n",
            "iter 411: loss 0.0000, time 1194.60ms\n",
            "iter 412: loss 0.0000, time 1171.07ms\n",
            "iter 413: loss 0.0000, time 1196.08ms\n",
            "iter 414: loss 0.0001, time 1199.98ms\n",
            "iter 415: loss 0.0000, time 1238.64ms\n",
            "iter 416: loss 0.0000, time 1230.16ms\n",
            "iter 417: loss 0.0000, time 1196.31ms\n",
            "iter 418: loss 0.0000, time 1246.35ms\n",
            "iter 419: loss 0.0000, time 1230.00ms\n",
            "Step 420: train loss 0.1272, val loss 0.5882\n",
            "Validation accuracy: 0.9822\n",
            "iter 420: loss 0.0000, time 9445.98ms\n",
            "iter 421: loss 0.0000, time 1196.01ms\n",
            "iter 422: loss 0.0000, time 1206.40ms\n",
            "iter 423: loss 0.0003, time 1192.82ms\n",
            "iter 424: loss 0.0001, time 1210.42ms\n",
            "iter 425: loss 0.0000, time 1246.01ms\n",
            "iter 426: loss 0.0000, time 1211.66ms\n",
            "iter 427: loss 0.0000, time 1296.38ms\n",
            "iter 428: loss 0.0000, time 1287.30ms\n",
            "iter 429: loss 0.0000, time 1384.72ms\n",
            "iter 430: loss 0.0000, time 1250.89ms\n",
            "iter 431: loss 0.0000, time 1244.76ms\n",
            "iter 432: loss 0.0000, time 1227.47ms\n",
            "iter 433: loss 0.0000, time 1172.22ms\n",
            "iter 434: loss 0.0000, time 1216.07ms\n",
            "iter 435: loss 0.0000, time 1193.04ms\n",
            "iter 436: loss 0.0000, time 1202.71ms\n",
            "iter 437: loss 0.0000, time 1212.39ms\n",
            "iter 438: loss 0.0006, time 1204.29ms\n",
            "iter 439: loss 0.0000, time 1170.08ms\n",
            "Step 440: train loss 0.1218, val loss 0.5871\n",
            "Validation accuracy: 0.9808\n",
            "iter 440: loss 0.0000, time 9690.96ms\n",
            "iter 441: loss 0.0000, time 1217.48ms\n",
            "iter 442: loss 0.0000, time 1209.32ms\n",
            "iter 443: loss 0.0001, time 1204.89ms\n",
            "iter 444: loss 0.0000, time 1226.42ms\n",
            "iter 445: loss 0.0000, time 1280.46ms\n",
            "iter 446: loss 0.0000, time 1409.66ms\n",
            "iter 447: loss 0.0000, time 1397.04ms\n",
            "iter 448: loss 0.0000, time 1255.17ms\n",
            "iter 449: loss 0.0000, time 1260.17ms\n",
            "iter 450: loss 0.0000, time 1216.36ms\n",
            "iter 451: loss 0.0000, time 1235.29ms\n",
            "iter 452: loss 0.0000, time 1206.22ms\n",
            "iter 453: loss 0.0000, time 1413.56ms\n",
            "iter 454: loss 0.0000, time 1535.10ms\n",
            "iter 455: loss 0.0000, time 1259.91ms\n",
            "iter 456: loss 0.0002, time 1176.25ms\n",
            "iter 457: loss 0.0001, time 1196.84ms\n",
            "iter 458: loss 0.0001, time 1329.68ms\n",
            "iter 459: loss 0.0000, time 1478.14ms\n",
            "Step 460: train loss 0.1166, val loss 0.5816\n",
            "Validation accuracy: 0.9734\n",
            "iter 460: loss 0.0000, time 9867.54ms\n",
            "iter 461: loss 0.0000, time 1301.50ms\n",
            "iter 462: loss 0.0000, time 1349.72ms\n",
            "iter 463: loss 0.0000, time 1235.04ms\n",
            "iter 464: loss 0.0000, time 1261.41ms\n",
            "iter 465: loss 0.0000, time 1271.74ms\n",
            "iter 466: loss 0.0000, time 1204.30ms\n",
            "iter 467: loss 0.0000, time 1410.38ms\n",
            "iter 468: loss 0.0000, time 1201.76ms\n",
            "iter 469: loss 0.0000, time 1263.84ms\n",
            "iter 470: loss 0.0000, time 1222.55ms\n",
            "iter 471: loss 0.0000, time 1277.27ms\n",
            "iter 472: loss 0.0000, time 1194.85ms\n",
            "iter 473: loss 0.0000, time 1235.89ms\n",
            "iter 474: loss 0.0000, time 1203.15ms\n",
            "iter 475: loss 0.0000, time 1206.47ms\n",
            "iter 476: loss 0.0000, time 1237.34ms\n",
            "iter 477: loss 0.0000, time 1186.69ms\n",
            "iter 478: loss 0.0000, time 1407.43ms\n",
            "iter 479: loss 0.0000, time 1214.72ms\n",
            "Step 480: train loss 0.1117, val loss 0.5887\n",
            "Validation accuracy: 0.9808\n",
            "iter 480: loss 0.0000, time 9471.44ms\n",
            "iter 481: loss 0.0000, time 1217.08ms\n",
            "iter 482: loss 0.0000, time 1205.73ms\n",
            "iter 483: loss 0.0000, time 1229.46ms\n",
            "iter 484: loss 0.0001, time 1339.88ms\n",
            "iter 485: loss 0.0000, time 1260.40ms\n",
            "iter 486: loss 0.0000, time 1206.63ms\n",
            "iter 487: loss 0.0000, time 1171.99ms\n",
            "iter 488: loss 0.0000, time 1314.45ms\n",
            "iter 489: loss 0.0000, time 1200.21ms\n",
            "iter 490: loss 0.0000, time 1257.69ms\n",
            "iter 491: loss 0.0000, time 1175.04ms\n",
            "iter 492: loss 0.0000, time 1170.33ms\n",
            "iter 493: loss 0.0000, time 1182.71ms\n",
            "iter 494: loss 0.0000, time 1162.42ms\n",
            "iter 495: loss 0.0000, time 1158.59ms\n",
            "iter 496: loss 0.0000, time 1177.43ms\n",
            "iter 497: loss 0.0000, time 1148.35ms\n",
            "iter 498: loss 0.0000, time 1193.64ms\n",
            "iter 499: loss 0.0000, time 1221.13ms\n",
            "Step 500: train loss 0.1073, val loss 0.5879\n",
            "Validation accuracy: 0.9852\n",
            "Test accuracy 0.7333\n",
            "iter 500: loss 0.0000, time 10444.88ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▆▅▅▇█</td></tr><tr><td>Test_F1_Score</td><td>▁▇▅▄▇█</td></tr><tr><td>Test_Precision</td><td>▁▆█▃██</td></tr><tr><td>Test_Recall</td><td>▁▆▅▅▇█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇██▇███▇████████████████</td></tr><tr><td>val/loss</td><td> █▄▃▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.73333</td></tr><tr><td>Test_F1_Score</td><td>0.70773</td></tr><tr><td>Test_Precision</td><td>0.85185</td></tr><tr><td>Test_Recall</td><td>0.73333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.10727</td></tr><tr><td>val/acc</td><td>0.98521</td></tr><tr><td>val/loss</td><td>0.58793</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2128</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/1qdevbzk</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_212809-1qdevbzk\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: td7qiqpk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_214506-td7qiqpk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk' target=\"_blank\">driven-sweep-11</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2145\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">driven-sweep-11</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_214506-td7qiqpk\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_214511-td7qiqpk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk' target=\"_blank\">gpt2_hyper_2145</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.2322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n",
            "iter 0: loss 8.3125, time 20680.92ms\n",
            "iter 1: loss 5.1875, time 1865.18ms\n",
            "iter 2: loss 0.4961, time 1835.67ms\n",
            "iter 3: loss 2.8750, time 1843.55ms\n",
            "iter 4: loss 2.4375, time 1782.47ms\n",
            "iter 5: loss 1.2422, time 1790.49ms\n",
            "iter 6: loss 1.1016, time 1817.50ms\n",
            "iter 7: loss 1.1641, time 1754.81ms\n",
            "iter 8: loss 1.0156, time 1789.53ms\n",
            "iter 9: loss 0.8789, time 1788.66ms\n",
            "iter 10: loss 0.5508, time 1810.00ms\n",
            "iter 11: loss 0.3477, time 1825.98ms\n",
            "iter 12: loss 0.3301, time 1779.28ms\n",
            "iter 13: loss 0.1777, time 1784.26ms\n",
            "iter 14: loss 1.1250, time 1812.03ms\n",
            "iter 15: loss 0.7734, time 1808.62ms\n",
            "iter 16: loss 0.5391, time 1804.19ms\n",
            "iter 17: loss 0.2656, time 1783.65ms\n",
            "iter 18: loss 0.1699, time 1817.65ms\n",
            "iter 19: loss 0.1089, time 1807.04ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 1.2756, val loss 0.7148\n",
            "Validation accuracy: 0.6469\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.1621, time 22535.66ms\n",
            "iter 21: loss 0.1592, time 1840.56ms\n",
            "iter 22: loss 0.5859, time 1891.56ms\n",
            "iter 23: loss 0.0996, time 1848.17ms\n",
            "iter 24: loss 0.6758, time 1815.38ms\n",
            "iter 25: loss 0.3672, time 1821.29ms\n",
            "iter 26: loss 0.2637, time 1800.92ms\n",
            "iter 27: loss 0.1416, time 1811.20ms\n",
            "iter 28: loss 0.0500, time 1849.67ms\n",
            "iter 29: loss 0.0098, time 1866.36ms\n",
            "iter 30: loss 0.0256, time 1815.33ms\n",
            "iter 31: loss 0.2754, time 1782.59ms\n",
            "iter 32: loss 0.2559, time 1802.70ms\n",
            "iter 33: loss 0.0503, time 1809.81ms\n",
            "iter 34: loss 4.0000, time 1805.29ms\n",
            "iter 35: loss 0.0352, time 1796.02ms\n",
            "iter 36: loss 6.0313, time 1808.21ms\n",
            "iter 37: loss 0.0009, time 1831.20ms\n",
            "iter 38: loss 0.8984, time 1950.61ms\n",
            "iter 39: loss 0.8438, time 1811.63ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.8739, val loss 0.6202\n",
            "Validation accuracy: 0.8672\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.1846, time 21649.24ms\n",
            "iter 41: loss 0.0325, time 2155.14ms\n",
            "iter 42: loss 0.8984, time 1919.76ms\n",
            "iter 43: loss 0.1270, time 1967.86ms\n",
            "iter 44: loss 0.0417, time 1914.67ms\n",
            "iter 45: loss 2.8125, time 1858.53ms\n",
            "iter 46: loss 0.6211, time 1836.12ms\n",
            "iter 47: loss 0.6758, time 1881.97ms\n",
            "iter 48: loss 0.0825, time 1823.57ms\n",
            "iter 49: loss 0.0549, time 1773.04ms\n",
            "iter 50: loss 0.0378, time 1863.12ms\n",
            "iter 51: loss 0.6016, time 1800.69ms\n",
            "iter 52: loss 0.0014, time 1844.24ms\n",
            "iter 53: loss 6.7813, time 1811.15ms\n",
            "iter 54: loss 0.0004, time 1882.91ms\n",
            "iter 55: loss 0.0173, time 1774.32ms\n",
            "iter 56: loss 0.0016, time 1830.46ms\n",
            "iter 57: loss 0.0089, time 1819.13ms\n",
            "iter 58: loss 0.0320, time 1842.19ms\n",
            "iter 59: loss 0.0138, time 1823.21ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.6734, val loss 0.5456\n",
            "Validation accuracy: 0.8812\n",
            "iter 60: loss 0.0005, time 19481.66ms\n",
            "iter 61: loss 0.0801, time 1854.22ms\n",
            "iter 62: loss 0.0024, time 1835.77ms\n",
            "iter 63: loss 0.0114, time 1830.56ms\n",
            "iter 64: loss 0.0033, time 1884.77ms\n",
            "iter 65: loss 0.0036, time 1868.11ms\n",
            "iter 66: loss 0.0062, time 1881.05ms\n",
            "iter 67: loss 0.0005, time 2168.92ms\n",
            "iter 68: loss 0.0005, time 1895.47ms\n",
            "iter 69: loss 0.0126, time 1840.69ms\n",
            "iter 70: loss 0.0097, time 1812.32ms\n",
            "iter 71: loss 0.0442, time 1823.12ms\n",
            "iter 72: loss 0.8438, time 1984.97ms\n",
            "iter 73: loss 0.0447, time 1859.67ms\n",
            "iter 74: loss 0.0144, time 1806.01ms\n",
            "iter 75: loss 0.0164, time 1830.20ms\n",
            "iter 76: loss 0.0044, time 1846.88ms\n",
            "iter 77: loss 2.0938, time 1911.85ms\n",
            "iter 78: loss 1.3906, time 2024.44ms\n",
            "iter 79: loss 0.0011, time 1850.10ms\n",
            "Step 80: train loss 0.5513, val loss 0.5400\n",
            "Validation accuracy: 0.9417\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0098, time 21998.98ms\n",
            "iter 81: loss 3.5469, time 1801.00ms\n",
            "iter 82: loss 0.0142, time 1860.46ms\n",
            "iter 83: loss 0.0046, time 1982.80ms\n",
            "iter 84: loss 0.0011, time 1946.54ms\n",
            "iter 85: loss 0.1299, time 1950.99ms\n",
            "iter 86: loss 0.0003, time 2013.48ms\n",
            "iter 87: loss 1.1875, time 2052.45ms\n",
            "iter 88: loss 0.0957, time 2016.20ms\n",
            "iter 89: loss 0.0908, time 1747.17ms\n",
            "iter 90: loss 0.0060, time 1811.76ms\n",
            "iter 91: loss 0.0005, time 1822.13ms\n",
            "iter 92: loss 0.0239, time 1781.32ms\n",
            "iter 93: loss 0.0052, time 2013.06ms\n",
            "iter 94: loss 0.0035, time 1842.48ms\n",
            "iter 95: loss 0.0134, time 1816.13ms\n",
            "iter 96: loss 0.0962, time 1734.42ms\n",
            "iter 97: loss 0.0033, time 1767.46ms\n",
            "iter 98: loss 0.0039, time 1886.97ms\n",
            "iter 99: loss 0.0027, time 1934.64ms\n",
            "Step 100: train loss 0.4672, val loss 0.5783\n",
            "Validation accuracy: 0.9687\n",
            "Saving checkpoint to out/ckpt.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5333\n",
            "iter 100: loss 2.0938, time 23737.69ms\n",
            "iter 101: loss 0.0023, time 1785.69ms\n",
            "iter 102: loss 0.0005, time 1831.78ms\n",
            "iter 103: loss 0.0000, time 1875.22ms\n",
            "iter 104: loss 0.0000, time 1874.87ms\n",
            "iter 105: loss 0.0021, time 1820.14ms\n",
            "iter 106: loss 0.0003, time 1800.15ms\n",
            "iter 107: loss 0.0004, time 1912.56ms\n",
            "iter 108: loss 0.0039, time 2274.32ms\n",
            "iter 109: loss 0.0004, time 2034.01ms\n",
            "iter 110: loss 0.0022, time 1873.77ms\n",
            "iter 111: loss 0.0010, time 1959.39ms\n",
            "iter 112: loss 0.0002, time 1836.98ms\n",
            "iter 113: loss 0.0004, time 1907.34ms\n",
            "iter 114: loss 0.0006, time 1865.99ms\n",
            "iter 115: loss 0.0014, time 1817.13ms\n",
            "iter 116: loss 0.0000, time 1860.72ms\n",
            "iter 117: loss 0.0215, time 1865.27ms\n",
            "iter 118: loss 0.0006, time 1841.56ms\n",
            "iter 119: loss 0.0002, time 1814.09ms\n",
            "Step 120: train loss 0.3957, val loss 0.5452\n",
            "Validation accuracy: 0.9827\n",
            "iter 120: loss 0.0006, time 19215.16ms\n",
            "iter 121: loss 0.0000, time 1797.24ms\n",
            "iter 122: loss 0.0021, time 1783.57ms\n",
            "iter 123: loss 0.0004, time 1809.02ms\n",
            "iter 124: loss 0.0000, time 1798.54ms\n",
            "iter 125: loss 0.0000, time 1816.03ms\n",
            "iter 126: loss 0.0025, time 1829.56ms\n",
            "iter 127: loss 0.0008, time 1827.02ms\n",
            "iter 128: loss 0.0000, time 1717.80ms\n",
            "iter 129: loss 0.0000, time 1802.93ms\n",
            "iter 130: loss 0.0000, time 1769.11ms\n",
            "iter 131: loss 0.0006, time 1775.36ms\n",
            "iter 132: loss 0.0003, time 1768.26ms\n",
            "iter 133: loss 0.0001, time 1765.78ms\n",
            "iter 134: loss 0.0001, time 1736.44ms\n",
            "iter 135: loss 0.0015, time 1754.11ms\n",
            "iter 136: loss 0.0001, time 1797.80ms\n",
            "iter 137: loss 0.0001, time 1728.94ms\n",
            "iter 138: loss 0.1943, time 1793.63ms\n",
            "iter 139: loss 0.0000, time 1786.74ms\n",
            "Step 140: train loss 0.3423, val loss 0.5164\n",
            "Validation accuracy: 0.9816\n",
            "iter 140: loss 0.0011, time 19213.98ms\n",
            "iter 141: loss 0.0012, time 1785.08ms\n",
            "iter 142: loss 0.0001, time 1845.50ms\n",
            "iter 143: loss 0.0039, time 1728.69ms\n",
            "iter 144: loss 0.0001, time 1734.39ms\n",
            "iter 145: loss 0.0000, time 1820.64ms\n",
            "iter 146: loss 0.0000, time 1763.58ms\n",
            "iter 147: loss 0.0001, time 1758.43ms\n",
            "iter 148: loss 0.0000, time 1731.00ms\n",
            "iter 149: loss 0.0002, time 1716.66ms\n",
            "iter 150: loss 0.0135, time 1748.53ms\n",
            "iter 151: loss 0.0000, time 1775.95ms\n",
            "iter 152: loss 0.0001, time 1785.46ms\n",
            "iter 153: loss 0.0000, time 1802.01ms\n",
            "iter 154: loss 0.0000, time 1756.26ms\n",
            "iter 155: loss 0.0001, time 1837.44ms\n",
            "iter 156: loss 0.0000, time 1855.24ms\n",
            "iter 157: loss 0.0011, time 1748.12ms\n",
            "iter 158: loss 0.0002, time 1719.24ms\n",
            "iter 159: loss 0.0000, time 1866.36ms\n",
            "Step 160: train loss 0.3005, val loss 0.5035\n",
            "Validation accuracy: 0.9773\n",
            "iter 160: loss 0.0000, time 19153.78ms\n",
            "iter 161: loss 0.0001, time 1770.04ms\n",
            "iter 162: loss 0.0000, time 1808.56ms\n",
            "iter 163: loss 0.0000, time 1758.31ms\n",
            "iter 164: loss 0.0017, time 1832.61ms\n",
            "iter 165: loss 0.0001, time 1852.32ms\n",
            "iter 166: loss 0.0001, time 1852.30ms\n",
            "iter 167: loss 0.0000, time 1785.63ms\n",
            "iter 168: loss 0.0000, time 1787.23ms\n",
            "iter 169: loss 0.0001, time 1760.10ms\n",
            "iter 170: loss 0.0004, time 1790.10ms\n",
            "iter 171: loss 0.0000, time 1729.33ms\n",
            "iter 172: loss 0.0002, time 1808.11ms\n",
            "iter 173: loss 0.0002, time 1799.15ms\n",
            "iter 174: loss 0.0001, time 1782.49ms\n",
            "iter 175: loss 0.0000, time 1788.62ms\n",
            "iter 176: loss 0.0000, time 1791.56ms\n",
            "iter 177: loss 0.0000, time 1765.06ms\n",
            "iter 178: loss 0.0052, time 1702.56ms\n",
            "iter 179: loss 0.0000, time 1787.13ms\n",
            "Step 180: train loss 0.2674, val loss 0.4850\n",
            "Validation accuracy: 0.9870\n",
            "iter 180: loss 0.0000, time 18667.22ms\n",
            "iter 181: loss 0.0000, time 1738.65ms\n",
            "iter 182: loss 0.0000, time 1744.72ms\n",
            "iter 183: loss 0.0752, time 1757.11ms\n",
            "iter 184: loss 0.0151, time 1766.07ms\n",
            "iter 185: loss 0.0000, time 1764.25ms\n",
            "iter 186: loss 0.0009, time 1786.18ms\n",
            "iter 187: loss 0.0000, time 1787.24ms\n",
            "iter 188: loss 0.0000, time 1795.97ms\n",
            "iter 189: loss 0.0000, time 1763.39ms\n",
            "iter 190: loss 0.0002, time 1822.85ms\n",
            "iter 191: loss 0.0000, time 1755.50ms\n",
            "iter 192: loss 0.3711, time 1787.80ms\n",
            "iter 193: loss 0.0000, time 1744.31ms\n",
            "iter 194: loss 0.0000, time 1869.28ms\n",
            "iter 195: loss 0.0000, time 1759.65ms\n",
            "iter 196: loss 0.0000, time 1816.33ms\n",
            "iter 197: loss 0.0000, time 1741.19ms\n",
            "iter 198: loss 0.0000, time 1765.68ms\n",
            "iter 199: loss 0.0005, time 1799.88ms\n",
            "Step 200: train loss 0.2416, val loss 0.4660\n",
            "Validation accuracy: 0.9838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6333\n",
            "iter 200: loss 0.0000, time 22366.15ms\n",
            "iter 201: loss 0.0001, time 1866.57ms\n",
            "iter 202: loss 0.0000, time 1833.91ms\n",
            "iter 203: loss 0.0000, time 1837.31ms\n",
            "iter 204: loss 0.0247, time 1867.09ms\n",
            "iter 205: loss 0.0000, time 1784.49ms\n",
            "iter 206: loss 0.0000, time 1823.22ms\n",
            "iter 207: loss 0.0000, time 1872.85ms\n",
            "iter 208: loss 0.0000, time 1869.28ms\n",
            "iter 209: loss 0.0001, time 1837.95ms\n",
            "iter 210: loss 0.0001, time 1852.41ms\n",
            "iter 211: loss 0.0000, time 1899.26ms\n",
            "iter 212: loss 0.0000, time 1874.46ms\n",
            "iter 213: loss 0.0000, time 1844.89ms\n",
            "iter 214: loss 0.0000, time 1843.55ms\n",
            "iter 215: loss 0.0000, time 1908.70ms\n",
            "iter 216: loss 0.0000, time 1809.35ms\n",
            "iter 217: loss 0.0000, time 1819.72ms\n",
            "iter 218: loss 0.0001, time 1796.11ms\n",
            "iter 219: loss 0.0000, time 2062.82ms\n",
            "Step 220: train loss 0.2197, val loss 0.4591\n",
            "Validation accuracy: 0.9795\n",
            "iter 220: loss 0.0000, time 19288.06ms\n",
            "iter 221: loss 0.0000, time 1845.89ms\n",
            "iter 222: loss 0.0000, time 1819.34ms\n",
            "iter 223: loss 0.0000, time 1810.31ms\n",
            "iter 224: loss 0.0002, time 1801.01ms\n",
            "iter 225: loss 0.0000, time 1772.29ms\n",
            "iter 226: loss 0.0000, time 1811.53ms\n",
            "iter 227: loss 0.0000, time 1879.60ms\n",
            "iter 228: loss 0.0000, time 1748.71ms\n",
            "iter 229: loss 0.0000, time 1740.63ms\n",
            "iter 230: loss 0.0000, time 1793.59ms\n",
            "iter 231: loss 0.0000, time 1764.43ms\n",
            "iter 232: loss 0.0000, time 1805.27ms\n",
            "iter 233: loss 0.0000, time 1738.01ms\n",
            "iter 234: loss 0.0000, time 1747.60ms\n",
            "iter 235: loss 0.0000, time 1773.62ms\n",
            "iter 236: loss 0.0002, time 1740.50ms\n",
            "iter 237: loss 0.0000, time 1734.40ms\n",
            "iter 238: loss 0.0000, time 1807.83ms\n",
            "iter 239: loss 0.0031, time 1829.54ms\n",
            "Step 240: train loss 0.2014, val loss 0.4781\n",
            "Validation accuracy: 0.9838\n",
            "iter 240: loss 0.0000, time 18880.69ms\n",
            "iter 241: loss 0.0000, time 1761.96ms\n",
            "iter 242: loss 0.0001, time 1758.68ms\n",
            "iter 243: loss 0.0000, time 1767.31ms\n",
            "iter 244: loss 0.0001, time 1790.15ms\n",
            "iter 245: loss 0.0000, time 1804.89ms\n",
            "iter 246: loss 0.0000, time 1780.62ms\n",
            "iter 247: loss 0.0000, time 1760.20ms\n",
            "iter 248: loss 0.0000, time 1741.62ms\n",
            "iter 249: loss 0.0000, time 1772.35ms\n",
            "iter 250: loss 0.0000, time 1770.12ms\n",
            "iter 251: loss 0.0000, time 1791.99ms\n",
            "iter 252: loss 0.0000, time 1743.50ms\n",
            "iter 253: loss 0.0000, time 1796.40ms\n",
            "iter 254: loss 0.0000, time 1787.73ms\n",
            "iter 255: loss 0.0010, time 1719.75ms\n",
            "iter 256: loss 0.0000, time 1792.77ms\n",
            "iter 257: loss 0.0000, time 1766.02ms\n",
            "iter 258: loss 0.0000, time 1733.50ms\n",
            "iter 259: loss 0.0001, time 1841.93ms\n",
            "Step 260: train loss 0.1860, val loss 0.4945\n",
            "Validation accuracy: 0.9827\n",
            "iter 260: loss 0.0000, time 19112.38ms\n",
            "iter 261: loss 0.0000, time 1785.99ms\n",
            "iter 262: loss 0.0000, time 1811.32ms\n",
            "iter 263: loss 0.0000, time 1860.30ms\n",
            "iter 264: loss 0.0000, time 1841.94ms\n",
            "iter 265: loss 0.0000, time 1790.25ms\n",
            "iter 266: loss 0.0000, time 1780.55ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 267: loss 0.0000, time 1887.43ms\n",
            "iter 268: loss 0.0000, time 1860.86ms\n",
            "iter 269: loss 0.0000, time 1844.12ms\n",
            "iter 270: loss 0.0000, time 1813.59ms\n",
            "iter 271: loss 0.0000, time 1820.98ms\n",
            "iter 272: loss 0.0000, time 1832.61ms\n",
            "iter 273: loss 0.0000, time 1821.02ms\n",
            "iter 274: loss 0.0000, time 1847.17ms\n",
            "iter 275: loss 0.0000, time 1832.88ms\n",
            "iter 276: loss 0.0000, time 1844.97ms\n",
            "iter 277: loss 0.0000, time 1837.93ms\n",
            "iter 278: loss 0.0003, time 1845.84ms\n",
            "iter 279: loss 0.0000, time 1849.26ms\n",
            "Step 280: train loss 0.1727, val loss 0.5026\n",
            "Validation accuracy: 0.9860\n",
            "iter 280: loss 0.0000, time 19110.06ms\n",
            "iter 281: loss 0.0000, time 1766.11ms\n",
            "iter 282: loss 0.0000, time 1769.36ms\n",
            "iter 283: loss 0.0007, time 1856.68ms\n",
            "iter 284: loss 0.0000, time 1827.74ms\n",
            "iter 285: loss 0.0001, time 1835.83ms\n",
            "iter 286: loss 0.0000, time 1830.68ms\n",
            "iter 287: loss 0.0000, time 1989.46ms\n",
            "iter 288: loss 0.0000, time 2092.71ms\n",
            "iter 289: loss 0.0000, time 1964.63ms\n",
            "iter 290: loss 0.0000, time 1894.77ms\n",
            "iter 291: loss 0.0000, time 1868.45ms\n",
            "iter 292: loss 0.0000, time 1923.28ms\n",
            "iter 293: loss 0.0000, time 1838.60ms\n",
            "iter 294: loss 0.0000, time 1852.73ms\n",
            "iter 295: loss 0.0000, time 1811.93ms\n",
            "iter 296: loss 0.0000, time 1746.35ms\n",
            "iter 297: loss 0.0000, time 1762.74ms\n",
            "iter 298: loss 0.0000, time 2011.78ms\n",
            "iter 299: loss 0.0000, time 2226.89ms\n",
            "Step 300: train loss 0.1612, val loss 0.5124\n",
            "Validation accuracy: 0.9849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5667\n",
            "iter 300: loss 0.0000, time 21453.81ms\n",
            "iter 301: loss 0.0000, time 1913.50ms\n",
            "iter 302: loss 0.0000, time 1914.10ms\n",
            "iter 303: loss 0.0000, time 1923.45ms\n",
            "iter 304: loss 0.0000, time 1894.60ms\n",
            "iter 305: loss 0.0000, time 1910.96ms\n",
            "iter 306: loss 0.0000, time 1872.52ms\n",
            "iter 307: loss 0.0000, time 1861.97ms\n",
            "iter 308: loss 0.0000, time 1862.83ms\n",
            "iter 309: loss 0.0000, time 1817.64ms\n",
            "iter 310: loss 0.0000, time 1967.14ms\n",
            "iter 311: loss 0.0000, time 1922.54ms\n",
            "iter 312: loss 0.0000, time 1890.39ms\n",
            "iter 313: loss 0.0001, time 2168.61ms\n",
            "iter 314: loss 0.0000, time 1848.21ms\n",
            "iter 315: loss 0.0000, time 1811.38ms\n",
            "iter 316: loss 0.0000, time 1830.31ms\n",
            "iter 317: loss 0.0000, time 1841.03ms\n",
            "iter 318: loss 0.0000, time 1895.05ms\n",
            "iter 319: loss 0.0000, time 1851.76ms\n",
            "Step 320: train loss 0.1511, val loss 0.5241\n",
            "Validation accuracy: 0.9849\n",
            "iter 320: loss 0.0000, time 19155.56ms\n",
            "iter 321: loss 0.0000, time 1802.50ms\n",
            "iter 322: loss 0.0000, time 1867.49ms\n",
            "iter 323: loss 0.0000, time 1850.09ms\n",
            "iter 324: loss 0.0000, time 1834.25ms\n",
            "iter 325: loss 0.0000, time 1809.09ms\n",
            "iter 326: loss 0.0000, time 1803.49ms\n",
            "iter 327: loss 0.0000, time 1789.30ms\n",
            "iter 328: loss 0.0000, time 1804.73ms\n",
            "iter 329: loss 0.0000, time 1897.96ms\n",
            "iter 330: loss 0.0000, time 1799.32ms\n",
            "iter 331: loss 0.0000, time 1836.63ms\n",
            "iter 332: loss 0.0000, time 1766.27ms\n",
            "iter 333: loss 0.0000, time 1741.35ms\n",
            "iter 334: loss 0.0000, time 1841.32ms\n",
            "iter 335: loss 0.0000, time 1770.62ms\n",
            "iter 336: loss 0.0000, time 1723.28ms\n",
            "iter 337: loss 0.0000, time 1780.58ms\n",
            "iter 338: loss 0.0000, time 1790.81ms\n",
            "iter 339: loss 0.0000, time 1767.98ms\n",
            "Step 340: train loss 0.1422, val loss 0.5327\n",
            "Validation accuracy: 0.9849\n",
            "iter 340: loss 0.0000, time 18934.03ms\n",
            "iter 341: loss 0.0000, time 1869.33ms\n",
            "iter 342: loss 0.0000, time 1765.77ms\n",
            "iter 343: loss 0.0000, time 1762.60ms\n",
            "iter 344: loss 0.0000, time 1765.27ms\n",
            "iter 345: loss 0.0000, time 1822.54ms\n",
            "iter 346: loss 0.0000, time 1749.10ms\n",
            "iter 347: loss 0.0000, time 1767.42ms\n",
            "iter 348: loss 0.0000, time 1782.54ms\n",
            "iter 349: loss 0.0000, time 1780.84ms\n",
            "iter 350: loss 0.0000, time 1755.29ms\n",
            "iter 351: loss 0.0000, time 1746.70ms\n",
            "iter 352: loss 0.0000, time 1768.86ms\n",
            "iter 353: loss 0.0000, time 1754.48ms\n",
            "iter 354: loss 0.0000, time 1746.43ms\n",
            "iter 355: loss 0.0000, time 1770.28ms\n",
            "iter 356: loss 0.0000, time 1736.55ms\n",
            "iter 357: loss 0.0000, time 1775.60ms\n",
            "iter 358: loss 0.0000, time 1843.58ms\n",
            "iter 359: loss 0.0000, time 1758.78ms\n",
            "Step 360: train loss 0.1343, val loss 0.5452\n",
            "Validation accuracy: 0.9849\n",
            "iter 360: loss 0.0000, time 19421.45ms\n",
            "iter 361: loss 0.0000, time 1741.73ms\n",
            "iter 362: loss 0.0000, time 1741.46ms\n",
            "iter 363: loss 0.0000, time 1716.99ms\n",
            "iter 364: loss 0.0000, time 1712.35ms\n",
            "iter 365: loss 0.0000, time 1811.03ms\n",
            "iter 366: loss 0.0000, time 1784.99ms\n",
            "iter 367: loss 0.0000, time 1711.40ms\n",
            "iter 368: loss 0.0000, time 1705.33ms\n",
            "iter 369: loss 0.0000, time 1730.24ms\n",
            "iter 370: loss 0.0000, time 1708.65ms\n",
            "iter 371: loss 0.0000, time 1732.79ms\n",
            "iter 372: loss 0.0000, time 1732.11ms\n",
            "iter 373: loss 0.0000, time 1766.63ms\n",
            "iter 374: loss 0.0000, time 1758.87ms\n",
            "iter 375: loss 0.0000, time 1751.54ms\n",
            "iter 376: loss 0.0000, time 1791.08ms\n",
            "iter 377: loss 0.0000, time 1838.87ms\n",
            "iter 378: loss 0.0000, time 1785.36ms\n",
            "iter 379: loss 0.0000, time 1839.78ms\n",
            "Step 380: train loss 0.1272, val loss 0.5563\n",
            "Validation accuracy: 0.9849\n",
            "iter 380: loss 0.0000, time 19088.64ms\n",
            "iter 381: loss 0.0000, time 1861.91ms\n",
            "iter 382: loss 0.0000, time 1783.71ms\n",
            "iter 383: loss 0.0000, time 1765.69ms\n",
            "iter 384: loss 0.0000, time 1781.52ms\n",
            "iter 385: loss 0.0000, time 1794.32ms\n",
            "iter 386: loss 0.0000, time 1772.59ms\n",
            "iter 387: loss 0.0000, time 1773.40ms\n",
            "iter 388: loss 0.0000, time 1800.02ms\n",
            "iter 389: loss 0.0000, time 1818.22ms\n",
            "iter 390: loss 0.0000, time 1725.32ms\n",
            "iter 391: loss 0.0000, time 1796.60ms\n",
            "iter 392: loss 0.0000, time 1871.05ms\n",
            "iter 393: loss 0.0000, time 1750.78ms\n",
            "iter 394: loss 0.0000, time 1743.31ms\n",
            "iter 395: loss 0.0000, time 1728.21ms\n",
            "iter 396: loss 0.0000, time 1718.59ms\n",
            "iter 397: loss 0.0000, time 1719.65ms\n",
            "iter 398: loss 0.0000, time 1725.49ms\n",
            "iter 399: loss 0.0000, time 1711.06ms\n",
            "Step 400: train loss 0.1209, val loss 0.5698\n",
            "Validation accuracy: 0.9849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5667\n",
            "iter 400: loss 0.0000, time 20017.65ms\n",
            "iter 401: loss 0.0000, time 1823.51ms\n",
            "iter 402: loss 0.0000, time 1830.23ms\n",
            "iter 403: loss 0.0000, time 1805.05ms\n",
            "iter 404: loss 0.0000, time 1803.35ms\n",
            "iter 405: loss 0.0000, time 1866.86ms\n",
            "iter 406: loss 0.0000, time 1851.87ms\n",
            "iter 407: loss 0.0000, time 1858.11ms\n",
            "iter 408: loss 0.0000, time 1803.43ms\n",
            "iter 409: loss 0.0000, time 1851.68ms\n",
            "iter 410: loss 0.0000, time 1814.51ms\n",
            "iter 411: loss 0.0000, time 1824.67ms\n",
            "iter 412: loss 0.0000, time 1842.32ms\n",
            "iter 413: loss 0.0000, time 1926.03ms\n",
            "iter 414: loss 0.0000, time 1852.52ms\n",
            "iter 415: loss 0.0000, time 1871.95ms\n",
            "iter 416: loss 0.0000, time 2070.05ms\n",
            "iter 417: loss 0.0000, time 1877.08ms\n",
            "iter 418: loss 0.0000, time 1836.96ms\n",
            "iter 419: loss 0.0000, time 1843.87ms\n",
            "Step 420: train loss 0.1151, val loss 0.5831\n",
            "Validation accuracy: 0.9849\n",
            "iter 420: loss 0.0000, time 19238.27ms\n",
            "iter 421: loss 0.0000, time 1842.08ms\n",
            "iter 422: loss 0.0000, time 1777.22ms\n",
            "iter 423: loss 0.0000, time 1912.88ms\n",
            "iter 424: loss 0.0000, time 1899.32ms\n",
            "iter 425: loss 0.0000, time 1812.13ms\n",
            "iter 426: loss 0.0000, time 1901.76ms\n",
            "iter 427: loss 0.0000, time 1791.66ms\n",
            "iter 428: loss 0.0000, time 1799.72ms\n",
            "iter 429: loss 0.0000, time 1741.54ms\n",
            "iter 430: loss 0.0000, time 1732.92ms\n",
            "iter 431: loss 0.0000, time 1727.07ms\n",
            "iter 432: loss 0.0000, time 1757.61ms\n",
            "iter 433: loss 0.0000, time 1760.42ms\n",
            "iter 434: loss 0.0000, time 1733.81ms\n",
            "iter 435: loss 0.0000, time 1738.70ms\n",
            "iter 436: loss 0.0000, time 1800.27ms\n",
            "iter 437: loss 0.0000, time 1733.49ms\n",
            "iter 438: loss 0.0000, time 1753.30ms\n",
            "iter 439: loss 0.0000, time 1796.20ms\n",
            "Step 440: train loss 0.1099, val loss 0.5985\n",
            "Validation accuracy: 0.9849\n",
            "iter 440: loss 0.0000, time 19211.37ms\n",
            "iter 441: loss 0.0000, time 1794.72ms\n",
            "iter 442: loss 0.0000, time 1788.24ms\n",
            "iter 443: loss 0.0000, time 1772.46ms\n",
            "iter 444: loss 0.0000, time 1793.63ms\n",
            "iter 445: loss 0.0000, time 1763.34ms\n",
            "iter 446: loss 0.0000, time 1816.69ms\n",
            "iter 447: loss 0.0000, time 1789.00ms\n",
            "iter 448: loss 0.0000, time 1797.65ms\n",
            "iter 449: loss 0.0000, time 1793.39ms\n",
            "iter 450: loss 0.0000, time 1775.13ms\n",
            "iter 451: loss 0.0000, time 1833.77ms\n",
            "iter 452: loss 0.0000, time 1778.79ms\n",
            "iter 453: loss 0.0000, time 1773.02ms\n",
            "iter 454: loss 0.0000, time 1815.52ms\n",
            "iter 455: loss 0.0000, time 1748.83ms\n",
            "iter 456: loss 0.0000, time 1750.25ms\n",
            "iter 457: loss 0.0000, time 1803.72ms\n",
            "iter 458: loss 0.0000, time 1773.08ms\n",
            "iter 459: loss 0.0000, time 1798.85ms\n",
            "Step 460: train loss 0.1051, val loss 0.6141\n",
            "Validation accuracy: 0.9860\n",
            "iter 460: loss 0.0000, time 19334.94ms\n",
            "iter 461: loss 0.0000, time 1781.06ms\n",
            "iter 462: loss 0.0000, time 1846.14ms\n",
            "iter 463: loss 0.0000, time 1796.89ms\n",
            "iter 464: loss 0.0000, time 1835.64ms\n",
            "iter 465: loss 0.0000, time 1765.22ms\n",
            "iter 466: loss 0.0000, time 1815.95ms\n",
            "iter 467: loss 0.0000, time 1871.95ms\n",
            "iter 468: loss 0.0000, time 1804.40ms\n",
            "iter 469: loss 0.0000, time 1748.12ms\n",
            "iter 470: loss 0.0000, time 1759.22ms\n",
            "iter 471: loss 0.0000, time 1859.56ms\n",
            "iter 472: loss 0.0000, time 1796.20ms\n",
            "iter 473: loss 0.0000, time 1807.51ms\n",
            "iter 474: loss 0.0000, time 1801.21ms\n",
            "iter 475: loss 0.0000, time 1862.38ms\n",
            "iter 476: loss 0.0000, time 1802.98ms\n",
            "iter 477: loss 0.0000, time 1798.33ms\n",
            "iter 478: loss 0.0000, time 1811.14ms\n",
            "iter 479: loss 0.0000, time 1802.33ms\n",
            "Step 480: train loss 0.1007, val loss 0.6297\n",
            "Validation accuracy: 0.9860\n",
            "iter 480: loss 0.0000, time 19128.89ms\n",
            "iter 481: loss 0.0000, time 1805.96ms\n",
            "iter 482: loss 0.0000, time 1754.35ms\n",
            "iter 483: loss 0.0000, time 1791.22ms\n",
            "iter 484: loss 0.0000, time 1736.55ms\n",
            "iter 485: loss 0.0000, time 1774.33ms\n",
            "iter 486: loss 0.0000, time 1862.95ms\n",
            "iter 487: loss 0.0000, time 1874.58ms\n",
            "iter 488: loss 0.0000, time 1778.09ms\n",
            "iter 489: loss 0.0000, time 1818.70ms\n",
            "iter 490: loss 0.0000, time 1825.22ms\n",
            "iter 491: loss 0.0000, time 1725.89ms\n",
            "iter 492: loss 0.0000, time 1743.57ms\n",
            "iter 493: loss 0.0000, time 1759.24ms\n",
            "iter 494: loss 0.0000, time 1761.11ms\n",
            "iter 495: loss 0.0000, time 1773.48ms\n",
            "iter 496: loss 0.0000, time 1805.18ms\n",
            "iter 497: loss 0.0000, time 1826.62ms\n",
            "iter 498: loss 0.0000, time 1786.91ms\n",
            "iter 499: loss 0.0000, time 1828.64ms\n",
            "Step 500: train loss 0.0967, val loss 0.6463\n",
            "Validation accuracy: 0.9860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5333\n",
            "iter 500: loss 0.0000, time 21217.62ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▃█▅▅▃</td></tr><tr><td>Test_F1_Score</td><td>▁▄█▅▅▃</td></tr><tr><td>Test_Precision</td><td>▁▇█▄▄▃</td></tr><tr><td>Test_Recall</td><td>▁▃█▅▅▃</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▇▇██████████████████████</td></tr><tr><td>val/loss</td><td> █▅▃▃▄▃▃▂▂▁▁▂▂▂▂▃▃▃▄▄▄▅▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.53333</td></tr><tr><td>Test_F1_Score</td><td>0.43939</td></tr><tr><td>Test_Precision</td><td>0.37963</td></tr><tr><td>Test_Recall</td><td>0.53333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.0967</td></tr><tr><td>val/acc</td><td>0.98596</td></tr><tr><td>val/loss</td><td>0.64634</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2145</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/td7qiqpk</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_214511-td7qiqpk\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ctbgv5hk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_221049-ctbgv5hk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk' target=\"_blank\">twilight-sweep-12</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2210\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">twilight-sweep-12</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_221049-ctbgv5hk\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_221057-ctbgv5hk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk' target=\"_blank\">gpt2_hyper_2210</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.2322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.5000\n",
            "iter 0: loss 8.3125, time 21071.07ms\n",
            "iter 1: loss 3.2656, time 1815.50ms\n",
            "iter 2: loss 0.3730, time 1861.76ms\n",
            "iter 3: loss 0.0610, time 1803.84ms\n",
            "iter 4: loss 1.2344, time 1837.93ms\n",
            "iter 5: loss 1.5703, time 1814.20ms\n",
            "iter 6: loss 1.7266, time 1821.44ms\n",
            "iter 7: loss 1.5078, time 1854.65ms\n",
            "iter 8: loss 1.1016, time 1808.14ms\n",
            "iter 9: loss 0.8164, time 1796.95ms\n",
            "iter 10: loss 0.4160, time 1794.23ms\n",
            "iter 11: loss 0.4844, time 1868.76ms\n",
            "iter 12: loss 0.6094, time 1839.40ms\n",
            "iter 13: loss 0.6445, time 1816.82ms\n",
            "iter 14: loss 0.6445, time 1822.74ms\n",
            "iter 15: loss 0.6133, time 1831.56ms\n",
            "iter 16: loss 0.6172, time 1838.68ms\n",
            "iter 17: loss 0.4473, time 1825.50ms\n",
            "iter 18: loss 0.3379, time 1871.09ms\n",
            "iter 19: loss 0.1191, time 1860.70ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 1.1378, val loss 0.7216\n",
            "Validation accuracy: 0.7138\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.0835, time 21973.76ms\n",
            "iter 21: loss 0.0479, time 1820.31ms\n",
            "iter 22: loss 0.0243, time 1789.37ms\n",
            "iter 23: loss 0.0012, time 1880.70ms\n",
            "iter 24: loss 0.2910, time 1781.94ms\n",
            "iter 25: loss 0.0025, time 1840.86ms\n",
            "iter 26: loss 1.1719, time 1779.78ms\n",
            "iter 27: loss 0.4629, time 1814.84ms\n",
            "iter 28: loss 0.0035, time 1835.72ms\n",
            "iter 29: loss 0.0004, time 1885.83ms\n",
            "iter 30: loss 0.0005, time 1811.47ms\n",
            "iter 31: loss 0.1992, time 1837.21ms\n",
            "iter 32: loss 0.0569, time 1806.90ms\n",
            "iter 33: loss 0.0101, time 1819.80ms\n",
            "iter 34: loss 3.0625, time 1814.97ms\n",
            "iter 35: loss 0.0309, time 1789.53ms\n",
            "iter 36: loss 4.5312, time 1819.25ms\n",
            "iter 37: loss 0.0064, time 1860.67ms\n",
            "iter 38: loss 2.7500, time 1833.12ms\n",
            "iter 39: loss 0.5078, time 1816.34ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.8513, val loss 0.6203\n",
            "Validation accuracy: 0.9028\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.1572, time 21374.45ms\n",
            "iter 41: loss 0.0540, time 1826.90ms\n",
            "iter 42: loss 0.7148, time 1857.00ms\n",
            "iter 43: loss 0.1216, time 1818.78ms\n",
            "iter 44: loss 0.0884, time 1775.65ms\n",
            "iter 45: loss 3.7656, time 1789.91ms\n",
            "iter 46: loss 0.4434, time 1807.81ms\n",
            "iter 47: loss 0.3965, time 1787.75ms\n",
            "iter 48: loss 0.0635, time 1777.13ms\n",
            "iter 49: loss 0.0177, time 1775.35ms\n",
            "iter 50: loss 0.1089, time 1825.39ms\n",
            "iter 51: loss 0.7539, time 1821.30ms\n",
            "iter 52: loss 0.0004, time 1808.95ms\n",
            "iter 53: loss 7.4063, time 1853.94ms\n",
            "iter 54: loss 0.0006, time 1919.76ms\n",
            "iter 55: loss 0.0182, time 1904.92ms\n",
            "iter 56: loss 0.0053, time 1873.47ms\n",
            "iter 57: loss 0.0154, time 1833.03ms\n",
            "iter 58: loss 0.0104, time 1874.93ms\n",
            "iter 59: loss 0.0085, time 1853.54ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.6584, val loss 0.5209\n",
            "Validation accuracy: 0.8456\n",
            "iter 60: loss 0.0005, time 19782.02ms\n",
            "iter 61: loss 0.4941, time 1826.82ms\n",
            "iter 62: loss 0.0053, time 1848.06ms\n",
            "iter 63: loss 0.0161, time 1833.70ms\n",
            "iter 64: loss 0.0037, time 1853.16ms\n",
            "iter 65: loss 0.0044, time 1831.04ms\n",
            "iter 66: loss 0.0138, time 1795.02ms\n",
            "iter 67: loss 0.0007, time 1863.82ms\n",
            "iter 68: loss 0.0003, time 1884.82ms\n",
            "iter 69: loss 0.0164, time 1855.59ms\n",
            "iter 70: loss 0.0153, time 1821.52ms\n",
            "iter 71: loss 0.0371, time 1849.71ms\n",
            "iter 72: loss 0.3848, time 1900.50ms\n",
            "iter 73: loss 0.0033, time 1826.77ms\n",
            "iter 74: loss 0.2100, time 1810.53ms\n",
            "iter 75: loss 0.0160, time 1834.41ms\n",
            "iter 76: loss 0.0124, time 1828.61ms\n",
            "iter 77: loss 0.0190, time 1837.20ms\n",
            "iter 78: loss 0.0034, time 1812.25ms\n",
            "iter 79: loss 0.0002, time 1814.81ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80: train loss 0.5415, val loss 0.4898\n",
            "Validation accuracy: 0.9471\n",
            "iter 80: loss 0.0118, time 19400.10ms\n",
            "iter 81: loss 1.1641, time 1823.21ms\n",
            "iter 82: loss 0.0000, time 1860.78ms\n",
            "iter 83: loss 0.0120, time 1818.03ms\n",
            "iter 84: loss 0.0003, time 1862.40ms\n",
            "iter 85: loss 0.0046, time 1978.08ms\n",
            "iter 86: loss 0.0000, time 1862.37ms\n",
            "iter 87: loss 2.9375, time 1810.25ms\n",
            "iter 88: loss 0.0310, time 1819.68ms\n",
            "iter 89: loss 0.0035, time 1805.36ms\n",
            "iter 90: loss 0.0028, time 1833.77ms\n",
            "iter 91: loss 0.0037, time 1833.19ms\n",
            "iter 92: loss 0.0036, time 1904.05ms\n",
            "iter 93: loss 0.0280, time 1840.91ms\n",
            "iter 94: loss 0.0811, time 1829.04ms\n",
            "iter 95: loss 0.0124, time 1829.03ms\n",
            "iter 96: loss 0.0096, time 1847.30ms\n",
            "iter 97: loss 0.0089, time 1867.00ms\n",
            "iter 98: loss 0.0000, time 1800.91ms\n",
            "iter 99: loss 0.0000, time 1861.64ms\n",
            "Step 100: train loss 0.4548, val loss 0.4571\n",
            "Validation accuracy: 0.9665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6333\n",
            "iter 100: loss 0.3438, time 22575.16ms\n",
            "iter 101: loss 0.0006, time 1885.26ms\n",
            "iter 102: loss 0.0002, time 1871.52ms\n",
            "iter 103: loss 0.0000, time 1882.80ms\n",
            "iter 104: loss 0.0000, time 1774.41ms\n",
            "iter 105: loss 0.0006, time 1870.58ms\n",
            "iter 106: loss 0.0002, time 1859.24ms\n",
            "iter 107: loss 0.0002, time 1817.53ms\n",
            "iter 108: loss 0.0908, time 1865.47ms\n",
            "iter 109: loss 0.0942, time 1894.25ms\n",
            "iter 110: loss 0.0165, time 1883.52ms\n",
            "iter 111: loss 0.0004, time 1811.96ms\n",
            "iter 112: loss 0.0001, time 1810.08ms\n",
            "iter 113: loss 0.0048, time 1911.25ms\n",
            "iter 114: loss 0.0067, time 1834.95ms\n",
            "iter 115: loss 0.0034, time 1886.06ms\n",
            "iter 116: loss 0.0004, time 1783.23ms\n",
            "iter 117: loss 0.0012, time 1857.40ms\n",
            "iter 118: loss 0.0334, time 1842.56ms\n",
            "iter 119: loss 0.0054, time 1844.27ms\n",
            "Step 120: train loss 0.3894, val loss 0.4608\n",
            "Validation accuracy: 0.9536\n",
            "iter 120: loss 0.1133, time 19325.29ms\n",
            "iter 121: loss 0.0027, time 1818.61ms\n",
            "iter 122: loss 0.1514, time 1822.07ms\n",
            "iter 123: loss 0.0036, time 1836.07ms\n",
            "iter 124: loss 0.0001, time 1841.81ms\n",
            "iter 125: loss 0.0000, time 2097.04ms\n",
            "iter 126: loss 0.0008, time 1972.05ms\n",
            "iter 127: loss 0.0136, time 2124.73ms\n",
            "iter 128: loss 0.0000, time 2229.37ms\n",
            "iter 129: loss 0.0000, time 1896.31ms\n",
            "iter 130: loss 0.0000, time 1736.43ms\n",
            "iter 131: loss 0.0000, time 1737.58ms\n",
            "iter 132: loss 0.0001, time 1782.84ms\n",
            "iter 133: loss 0.0000, time 1738.72ms\n",
            "iter 134: loss 0.0000, time 1783.96ms\n",
            "iter 135: loss 0.0000, time 1740.28ms\n",
            "iter 136: loss 0.0004, time 1786.17ms\n",
            "iter 137: loss 0.0001, time 1785.55ms\n",
            "iter 138: loss 0.0000, time 1737.65ms\n",
            "iter 139: loss 0.0000, time 1760.75ms\n",
            "Step 140: train loss 0.3364, val loss 0.4082\n",
            "Validation accuracy: 0.9752\n",
            "iter 140: loss 0.0008, time 19141.07ms\n",
            "iter 141: loss 0.0000, time 1828.77ms\n",
            "iter 142: loss 0.0000, time 1766.74ms\n",
            "iter 143: loss 0.0000, time 1763.48ms\n",
            "iter 144: loss 0.0001, time 1783.83ms\n",
            "iter 145: loss 0.0000, time 1780.25ms\n",
            "iter 146: loss 0.0000, time 1782.47ms\n",
            "iter 147: loss 0.0000, time 1755.74ms\n",
            "iter 148: loss 0.0000, time 1775.29ms\n",
            "iter 149: loss 0.0000, time 1767.33ms\n",
            "iter 150: loss 0.0003, time 1764.30ms\n",
            "iter 151: loss 0.0000, time 1788.74ms\n",
            "iter 152: loss 0.0000, time 1774.89ms\n",
            "iter 153: loss 0.0000, time 1744.33ms\n",
            "iter 154: loss 0.0000, time 1786.35ms\n",
            "iter 155: loss 0.0008, time 1795.01ms\n",
            "iter 156: loss 0.0000, time 1864.43ms\n",
            "iter 157: loss 0.0001, time 1751.47ms\n",
            "iter 158: loss 0.0000, time 1770.99ms\n",
            "iter 159: loss 0.0000, time 1846.99ms\n",
            "Step 160: train loss 0.2973, val loss 0.3982\n",
            "Validation accuracy: 0.9838\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 160: loss 0.0000, time 21823.08ms\n",
            "iter 161: loss 0.0001, time 1753.11ms\n",
            "iter 162: loss 0.0002, time 1764.09ms\n",
            "iter 163: loss 0.0000, time 1748.68ms\n",
            "iter 164: loss 0.0000, time 1737.92ms\n",
            "iter 165: loss 0.0004, time 1816.74ms\n",
            "iter 166: loss 0.0000, time 1762.67ms\n",
            "iter 167: loss 0.0000, time 1760.53ms\n",
            "iter 168: loss 0.0023, time 1756.82ms\n",
            "iter 169: loss 0.0000, time 1767.71ms\n",
            "iter 170: loss 0.0000, time 1754.54ms\n",
            "iter 171: loss 0.0000, time 1776.60ms\n",
            "iter 172: loss 0.0000, time 1834.69ms\n",
            "iter 173: loss 0.0000, time 1800.13ms\n",
            "iter 174: loss 0.0000, time 1778.86ms\n",
            "iter 175: loss 0.0001, time 1824.31ms\n",
            "iter 176: loss 0.0000, time 1774.78ms\n",
            "iter 177: loss 0.0000, time 1824.92ms\n",
            "iter 178: loss 0.0021, time 1904.20ms\n",
            "iter 179: loss 0.0000, time 1823.01ms\n",
            "Step 180: train loss 0.2657, val loss 0.3818\n",
            "Validation accuracy: 0.9827\n",
            "iter 180: loss 0.0000, time 19134.15ms\n",
            "iter 181: loss 0.0000, time 1746.33ms\n",
            "iter 182: loss 0.0000, time 1743.18ms\n",
            "iter 183: loss 0.0000, time 1718.51ms\n",
            "iter 184: loss 0.0000, time 1764.73ms\n",
            "iter 185: loss 0.0000, time 1767.21ms\n",
            "iter 186: loss 0.0000, time 1790.67ms\n",
            "iter 187: loss 0.0016, time 1819.84ms\n",
            "iter 188: loss 0.0003, time 1770.86ms\n",
            "iter 189: loss 0.0000, time 1787.04ms\n",
            "iter 190: loss 0.0001, time 1761.32ms\n",
            "iter 191: loss 0.0000, time 1745.23ms\n",
            "iter 192: loss 0.0000, time 1779.58ms\n",
            "iter 193: loss 0.0000, time 1775.29ms\n",
            "iter 194: loss 0.0000, time 1784.90ms\n",
            "iter 195: loss 0.0000, time 1758.05ms\n",
            "iter 196: loss 0.0000, time 1800.74ms\n",
            "iter 197: loss 0.0000, time 1789.07ms\n",
            "iter 198: loss 0.0079, time 1773.57ms\n",
            "iter 199: loss 0.0000, time 1752.11ms\n",
            "Step 200: train loss 0.2405, val loss 0.3724\n",
            "Validation accuracy: 0.9752\n",
            "Test accuracy 0.7000\n",
            "iter 200: loss 0.0000, time 20600.68ms\n",
            "iter 201: loss 0.0001, time 1760.49ms\n",
            "iter 202: loss 0.0000, time 1738.40ms\n",
            "iter 203: loss 0.0000, time 1784.73ms\n",
            "iter 204: loss 0.0000, time 1799.63ms\n",
            "iter 205: loss 0.0000, time 1754.69ms\n",
            "iter 206: loss 0.0000, time 1724.16ms\n",
            "iter 207: loss 0.0000, time 1737.13ms\n",
            "iter 208: loss 0.0000, time 1729.75ms\n",
            "iter 209: loss 0.0000, time 1762.42ms\n",
            "iter 210: loss 0.0000, time 1731.80ms\n",
            "iter 211: loss 0.0000, time 1715.59ms\n",
            "iter 212: loss 0.0000, time 1752.43ms\n",
            "iter 213: loss 0.0000, time 1763.30ms\n",
            "iter 214: loss 0.0000, time 1719.85ms\n",
            "iter 215: loss 0.0000, time 1722.85ms\n",
            "iter 216: loss 0.0001, time 1761.52ms\n",
            "iter 217: loss 0.0000, time 1767.12ms\n",
            "iter 218: loss 0.0001, time 1740.35ms\n",
            "iter 219: loss 0.0000, time 1723.32ms\n",
            "Step 220: train loss 0.2200, val loss 0.3546\n",
            "Validation accuracy: 0.9784\n",
            "iter 220: loss 0.0003, time 18526.16ms\n",
            "iter 221: loss 0.0000, time 1750.16ms\n",
            "iter 222: loss 0.0004, time 1748.56ms\n",
            "iter 223: loss 0.0000, time 1725.23ms\n",
            "iter 224: loss 0.0000, time 1724.65ms\n",
            "iter 225: loss 0.0000, time 1749.30ms\n",
            "iter 226: loss 0.0000, time 1750.94ms\n",
            "iter 227: loss 0.0000, time 1731.74ms\n",
            "iter 228: loss 0.0000, time 1722.29ms\n",
            "iter 229: loss 0.0000, time 1845.18ms\n",
            "iter 230: loss 0.0000, time 1750.01ms\n",
            "iter 231: loss 0.0004, time 1790.50ms\n",
            "iter 232: loss 0.0000, time 1775.07ms\n",
            "iter 233: loss 0.0000, time 1722.58ms\n",
            "iter 234: loss 0.0001, time 1736.95ms\n",
            "iter 235: loss 0.0001, time 1734.19ms\n",
            "iter 236: loss 0.0000, time 1834.65ms\n",
            "iter 237: loss 0.0001, time 1884.63ms\n",
            "iter 238: loss 0.0000, time 1743.74ms\n",
            "iter 239: loss 0.2656, time 1620.37ms\n",
            "Step 240: train loss 0.2026, val loss 0.3638\n",
            "Validation accuracy: 0.9611\n",
            "iter 240: loss 0.0000, time 15991.10ms\n",
            "iter 241: loss 0.0000, time 1662.31ms\n",
            "iter 242: loss 0.0000, time 1615.35ms\n",
            "iter 243: loss 0.0000, time 1703.31ms\n",
            "iter 244: loss 0.0000, time 1608.58ms\n",
            "iter 245: loss 0.0286, time 1663.05ms\n",
            "iter 246: loss 0.0000, time 1609.07ms\n",
            "iter 247: loss 0.0000, time 1666.94ms\n",
            "iter 248: loss 0.0000, time 1616.19ms\n",
            "iter 249: loss 0.0000, time 1726.69ms\n",
            "iter 250: loss 0.0000, time 1631.06ms\n",
            "iter 251: loss 0.0000, time 1601.79ms\n",
            "iter 252: loss 0.0000, time 1581.13ms\n",
            "iter 253: loss 0.0000, time 1685.46ms\n",
            "iter 254: loss 0.0000, time 1700.70ms\n",
            "iter 255: loss 0.0762, time 1604.09ms\n",
            "iter 256: loss 0.0000, time 1599.24ms\n",
            "iter 257: loss 0.0000, time 1640.96ms\n",
            "iter 258: loss 0.6055, time 1581.26ms\n",
            "iter 259: loss 0.0002, time 1612.49ms\n",
            "Step 260: train loss 0.1904, val loss 0.3816\n",
            "Validation accuracy: 0.9762\n",
            "iter 260: loss 0.0000, time 17256.94ms\n",
            "iter 261: loss 0.0000, time 1674.47ms\n",
            "iter 262: loss 0.0002, time 1617.92ms\n",
            "iter 263: loss 0.0000, time 1582.67ms\n",
            "iter 264: loss 0.0003, time 1639.46ms\n",
            "iter 265: loss 0.0000, time 1612.08ms\n",
            "iter 266: loss 0.0004, time 1596.10ms\n",
            "iter 267: loss 0.0000, time 1609.26ms\n",
            "iter 268: loss 0.0002, time 1636.72ms\n",
            "iter 269: loss 0.0001, time 1628.98ms\n",
            "iter 270: loss 0.0000, time 1625.08ms\n",
            "iter 271: loss 0.0000, time 1633.74ms\n",
            "iter 272: loss 0.0000, time 1641.18ms\n",
            "iter 273: loss 0.0000, time 1567.94ms\n",
            "iter 274: loss 0.0007, time 1610.41ms\n",
            "iter 275: loss 0.0000, time 1602.51ms\n",
            "iter 276: loss 0.0000, time 1604.84ms\n",
            "iter 277: loss 0.0000, time 1596.44ms\n",
            "iter 278: loss 0.0576, time 1586.02ms\n",
            "iter 279: loss 0.0000, time 1624.22ms\n",
            "Step 280: train loss 0.1769, val loss 0.3686\n",
            "Validation accuracy: 0.9881\n",
            "iter 280: loss 0.0000, time 17556.22ms\n",
            "iter 281: loss 0.0000, time 1636.35ms\n",
            "iter 282: loss 0.0000, time 1612.57ms\n",
            "iter 283: loss 0.0000, time 1596.57ms\n",
            "iter 284: loss 0.0000, time 1622.08ms\n",
            "iter 285: loss 0.0000, time 1597.43ms\n",
            "iter 286: loss 0.0000, time 1636.64ms\n",
            "iter 287: loss 0.0000, time 1633.75ms\n",
            "iter 288: loss 0.0000, time 1650.22ms\n",
            "iter 289: loss 0.0000, time 1610.47ms\n",
            "iter 290: loss 0.0000, time 1602.13ms\n",
            "iter 291: loss 0.0000, time 1563.60ms\n",
            "iter 292: loss 0.0000, time 1626.97ms\n",
            "iter 293: loss 0.0000, time 1588.60ms\n",
            "iter 294: loss 0.0000, time 1631.30ms\n",
            "iter 295: loss 0.0000, time 1620.72ms\n",
            "iter 296: loss 0.0000, time 1649.85ms\n",
            "iter 297: loss 0.0000, time 1649.44ms\n",
            "iter 298: loss 0.0000, time 1584.06ms\n",
            "iter 299: loss 0.0000, time 1586.35ms\n",
            "Step 300: train loss 0.1659, val loss 0.3981\n",
            "Validation accuracy: 0.9795\n",
            "Test accuracy 0.7000\n",
            "iter 300: loss 0.0000, time 18266.23ms\n",
            "iter 301: loss 0.0000, time 1554.54ms\n",
            "iter 302: loss 0.0000, time 1604.09ms\n",
            "iter 303: loss 0.0001, time 1596.26ms\n",
            "iter 304: loss 0.0000, time 1649.31ms\n",
            "iter 305: loss 0.0000, time 1600.90ms\n",
            "iter 306: loss 0.0000, time 1555.46ms\n",
            "iter 307: loss 0.0000, time 1558.89ms\n",
            "iter 308: loss 0.0000, time 1535.78ms\n",
            "iter 309: loss 0.0000, time 1535.36ms\n",
            "iter 310: loss 0.0000, time 1595.78ms\n",
            "iter 311: loss 0.0000, time 1595.86ms\n",
            "iter 312: loss 0.0000, time 1593.03ms\n",
            "iter 313: loss 0.7656, time 1613.47ms\n",
            "iter 314: loss 0.0000, time 1571.97ms\n",
            "iter 315: loss 0.0000, time 1580.89ms\n",
            "iter 316: loss 0.0000, time 1651.30ms\n",
            "iter 317: loss 0.0000, time 1575.96ms\n",
            "iter 318: loss 0.0000, time 1697.24ms\n",
            "iter 319: loss 0.0023, time 1567.01ms\n",
            "Step 320: train loss 0.1559, val loss 0.4308\n",
            "Validation accuracy: 0.9579\n",
            "iter 320: loss 0.0000, time 17109.47ms\n",
            "iter 321: loss 0.0000, time 1590.79ms\n",
            "iter 322: loss 0.0000, time 1586.63ms\n",
            "iter 323: loss 0.0000, time 1600.63ms\n",
            "iter 324: loss 0.0000, time 1612.01ms\n",
            "iter 325: loss 0.0000, time 1627.78ms\n",
            "iter 326: loss 0.0000, time 1524.16ms\n",
            "iter 327: loss 0.0003, time 1516.66ms\n",
            "iter 328: loss 0.0000, time 1499.15ms\n",
            "iter 329: loss 0.0000, time 1525.92ms\n",
            "iter 330: loss 0.0000, time 1529.54ms\n",
            "iter 331: loss 0.0013, time 1595.85ms\n",
            "iter 332: loss 0.0000, time 1578.61ms\n",
            "iter 333: loss 0.0015, time 1616.95ms\n",
            "iter 334: loss 0.0000, time 1581.89ms\n",
            "iter 335: loss 0.0000, time 1580.20ms\n",
            "iter 336: loss 0.0000, time 1568.24ms\n",
            "iter 337: loss 0.0000, time 1553.86ms\n",
            "iter 338: loss 0.0005, time 1608.19ms\n",
            "iter 339: loss 0.0000, time 1616.63ms\n",
            "Step 340: train loss 0.1494, val loss 0.4355\n",
            "Validation accuracy: 0.9773\n",
            "iter 340: loss 0.0000, time 17181.82ms\n",
            "iter 341: loss 0.0000, time 1618.67ms\n",
            "iter 342: loss 0.0000, time 1551.12ms\n",
            "iter 343: loss 0.0013, time 1640.04ms\n",
            "iter 344: loss 0.0001, time 1768.94ms\n",
            "iter 345: loss 0.0003, time 2065.48ms\n",
            "iter 346: loss 0.0001, time 1779.11ms\n",
            "iter 347: loss 0.0003, time 1836.22ms\n",
            "iter 348: loss 0.0006, time 1571.79ms\n",
            "iter 349: loss 0.0001, time 1612.65ms\n",
            "iter 350: loss 0.0000, time 1621.53ms\n",
            "iter 351: loss 0.0000, time 1524.05ms\n",
            "iter 352: loss 0.0000, time 1647.25ms\n",
            "iter 353: loss 0.0000, time 1643.38ms\n",
            "iter 354: loss 0.0000, time 1518.01ms\n",
            "iter 355: loss 0.0002, time 1547.14ms\n",
            "iter 356: loss 0.0000, time 1568.90ms\n",
            "iter 357: loss 0.0000, time 1536.43ms\n",
            "iter 358: loss 0.0003, time 1531.40ms\n",
            "iter 359: loss 0.0000, time 1749.83ms\n",
            "Step 360: train loss 0.1417, val loss 0.4433\n",
            "Validation accuracy: 0.9773\n",
            "iter 360: loss 0.0000, time 16809.64ms\n",
            "iter 361: loss 0.0000, time 1536.00ms\n",
            "iter 362: loss 0.0000, time 1529.07ms\n",
            "iter 363: loss 0.0000, time 1543.30ms\n",
            "iter 364: loss 0.0000, time 1485.01ms\n",
            "iter 365: loss 0.0000, time 1534.76ms\n",
            "iter 366: loss 0.0000, time 1540.24ms\n",
            "iter 367: loss 0.0042, time 1545.24ms\n",
            "iter 368: loss 0.0000, time 1535.88ms\n",
            "iter 369: loss 0.0000, time 1567.93ms\n",
            "iter 370: loss 0.0000, time 1522.47ms\n",
            "iter 371: loss 0.0031, time 1577.52ms\n",
            "iter 372: loss 0.0000, time 1504.98ms\n",
            "iter 373: loss 0.0000, time 1569.24ms\n",
            "iter 374: loss 0.0000, time 1643.06ms\n",
            "iter 375: loss 0.0000, time 1584.70ms\n",
            "iter 376: loss 0.0000, time 1606.24ms\n",
            "iter 377: loss 0.0000, time 1499.70ms\n",
            "iter 378: loss 0.0001, time 1520.89ms\n",
            "iter 379: loss 0.0000, time 1569.03ms\n",
            "Step 380: train loss 0.1359, val loss 0.4469\n",
            "Validation accuracy: 0.9838\n",
            "iter 380: loss 0.0000, time 16588.84ms\n",
            "iter 381: loss 0.0000, time 1520.78ms\n",
            "iter 382: loss 0.0000, time 1529.34ms\n",
            "iter 383: loss 0.0000, time 1527.86ms\n",
            "iter 384: loss 0.0000, time 1573.19ms\n",
            "iter 385: loss 0.0000, time 1564.33ms\n",
            "iter 386: loss 0.0000, time 1504.71ms\n",
            "iter 387: loss 0.0000, time 1539.22ms\n",
            "iter 388: loss 0.0172, time 1517.73ms\n",
            "iter 389: loss 0.0000, time 1585.24ms\n",
            "iter 390: loss 0.0001, time 1512.64ms\n",
            "iter 391: loss 0.0000, time 1536.07ms\n",
            "iter 392: loss 0.0000, time 1544.92ms\n",
            "iter 393: loss 0.0000, time 1536.00ms\n",
            "iter 394: loss 0.0000, time 1532.57ms\n",
            "iter 395: loss 0.0000, time 1528.15ms\n",
            "iter 396: loss 0.0796, time 1504.86ms\n",
            "iter 397: loss 0.0000, time 1524.76ms\n",
            "iter 398: loss 0.0000, time 1574.28ms\n",
            "iter 399: loss 0.0000, time 1578.69ms\n",
            "Step 400: train loss 0.1292, val loss 0.4660\n",
            "Validation accuracy: 0.9827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6333\n",
            "iter 400: loss 0.0000, time 17517.06ms\n",
            "iter 401: loss 0.0001, time 1518.37ms\n",
            "iter 402: loss 0.0000, time 1565.72ms\n",
            "iter 403: loss 0.0000, time 1511.52ms\n",
            "iter 404: loss 0.0000, time 1522.11ms\n",
            "iter 405: loss 0.0000, time 1519.45ms\n",
            "iter 406: loss 0.0000, time 1542.05ms\n",
            "iter 407: loss 0.0000, time 1487.90ms\n",
            "iter 408: loss 0.0000, time 1533.58ms\n",
            "iter 409: loss 0.0000, time 1570.72ms\n",
            "iter 410: loss 0.0000, time 1526.06ms\n",
            "iter 411: loss 0.0000, time 1586.23ms\n",
            "iter 412: loss 0.0000, time 1524.64ms\n",
            "iter 413: loss 0.0000, time 1599.87ms\n",
            "iter 414: loss 0.0000, time 1497.16ms\n",
            "iter 415: loss 0.0000, time 1577.71ms\n",
            "iter 416: loss 0.0000, time 1527.27ms\n",
            "iter 417: loss 0.0000, time 1526.17ms\n",
            "iter 418: loss 0.0000, time 1533.82ms\n",
            "iter 419: loss 0.0000, time 1551.75ms\n",
            "Step 420: train loss 0.1231, val loss 0.5129\n",
            "Validation accuracy: 0.9773\n",
            "iter 420: loss 0.0000, time 16941.58ms\n",
            "iter 421: loss 0.0000, time 1532.84ms\n",
            "iter 422: loss 0.0000, time 1593.82ms\n",
            "iter 423: loss 0.0000, time 1570.05ms\n",
            "iter 424: loss 0.0000, time 1550.37ms\n",
            "iter 425: loss 0.0000, time 1535.89ms\n",
            "iter 426: loss 0.0000, time 1561.13ms\n",
            "iter 427: loss 0.0000, time 1495.87ms\n",
            "iter 428: loss 0.0000, time 1510.42ms\n",
            "iter 429: loss 0.0000, time 1554.72ms\n",
            "iter 430: loss 0.0000, time 1505.73ms\n",
            "iter 431: loss 0.0000, time 1541.31ms\n",
            "iter 432: loss 0.0000, time 1536.88ms\n",
            "iter 433: loss 0.0001, time 1540.12ms\n",
            "iter 434: loss 0.0000, time 1526.69ms\n",
            "iter 435: loss 0.0000, time 1543.79ms\n",
            "iter 436: loss 0.0000, time 1542.91ms\n",
            "iter 437: loss 0.0000, time 1580.43ms\n",
            "iter 438: loss 0.0000, time 1568.92ms\n",
            "iter 439: loss 0.0000, time 1529.44ms\n",
            "Step 440: train loss 0.1175, val loss 0.5062\n",
            "Validation accuracy: 0.9849\n",
            "iter 440: loss 0.0000, time 16482.30ms\n",
            "iter 441: loss 0.0000, time 1518.25ms\n",
            "iter 442: loss 0.0122, time 1547.12ms\n",
            "iter 443: loss 0.0000, time 1518.16ms\n",
            "iter 444: loss 0.0000, time 1542.14ms\n",
            "iter 445: loss 0.0000, time 1519.06ms\n",
            "iter 446: loss 0.0000, time 1605.59ms\n",
            "iter 447: loss 0.0001, time 1545.07ms\n",
            "iter 448: loss 0.0000, time 1687.56ms\n",
            "iter 449: loss 0.0000, time 1524.96ms\n",
            "iter 450: loss 0.0000, time 1529.03ms\n",
            "iter 451: loss 0.0000, time 1604.01ms\n",
            "iter 452: loss 0.0000, time 1559.09ms\n",
            "iter 453: loss 0.0000, time 1533.33ms\n",
            "iter 454: loss 0.0001, time 1551.37ms\n",
            "iter 455: loss 0.0000, time 1570.86ms\n",
            "iter 456: loss 0.0000, time 1564.00ms\n",
            "iter 457: loss 0.0000, time 1558.25ms\n",
            "iter 458: loss 0.0000, time 1601.27ms\n",
            "iter 459: loss 0.0000, time 1511.02ms\n",
            "Step 460: train loss 0.1129, val loss 0.5508\n",
            "Validation accuracy: 0.9795\n",
            "iter 460: loss 0.0001, time 16484.91ms\n",
            "iter 461: loss 0.0000, time 1557.93ms\n",
            "iter 462: loss 0.0000, time 1625.52ms\n",
            "iter 463: loss 0.0000, time 1534.54ms\n",
            "iter 464: loss 0.0000, time 1569.03ms\n",
            "iter 465: loss 0.0000, time 1554.32ms\n",
            "iter 466: loss 0.0000, time 1516.53ms\n",
            "iter 467: loss 0.0000, time 1548.46ms\n",
            "iter 468: loss 0.0000, time 1528.92ms\n",
            "iter 469: loss 0.0000, time 1533.50ms\n",
            "iter 470: loss 0.0000, time 1514.73ms\n",
            "iter 471: loss 0.0000, time 1537.82ms\n",
            "iter 472: loss 0.0000, time 1500.55ms\n",
            "iter 473: loss 0.0000, time 1535.85ms\n",
            "iter 474: loss 0.0000, time 1535.39ms\n",
            "iter 475: loss 0.0000, time 1560.94ms\n",
            "iter 476: loss 0.0000, time 1524.70ms\n",
            "iter 477: loss 0.0000, time 1591.97ms\n",
            "iter 478: loss 0.0000, time 1527.70ms\n",
            "iter 479: loss 0.0000, time 1543.41ms\n",
            "Step 480: train loss 0.1083, val loss 0.5535\n",
            "Validation accuracy: 0.9838\n",
            "iter 480: loss 0.0000, time 16524.20ms\n",
            "iter 481: loss 0.0000, time 1517.65ms\n",
            "iter 482: loss 0.0000, time 1551.22ms\n",
            "iter 483: loss 0.0000, time 1556.56ms\n",
            "iter 484: loss 0.0000, time 1529.12ms\n",
            "iter 485: loss 0.0000, time 1762.80ms\n",
            "iter 486: loss 0.0000, time 1563.92ms\n",
            "iter 487: loss 0.0000, time 1605.35ms\n",
            "iter 488: loss 0.0000, time 1517.08ms\n",
            "iter 489: loss 0.0002, time 1611.90ms\n",
            "iter 490: loss 0.0000, time 1692.94ms\n",
            "iter 491: loss 0.0000, time 1520.03ms\n",
            "iter 492: loss 0.0000, time 1540.76ms\n",
            "iter 493: loss 0.0000, time 1571.79ms\n",
            "iter 494: loss 0.0000, time 1542.58ms\n",
            "iter 495: loss 0.0000, time 1520.28ms\n",
            "iter 496: loss 0.0000, time 1534.06ms\n",
            "iter 497: loss 0.0000, time 1504.06ms\n",
            "iter 498: loss 0.0000, time 1509.32ms\n",
            "iter 499: loss 0.0000, time 1547.25ms\n",
            "Step 500: train loss 0.1050, val loss 0.5819\n",
            "Validation accuracy: 0.9762\n",
            "Test accuracy 0.6333\n",
            "iter 500: loss 0.0000, time 17452.00ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▆██▆▆</td></tr><tr><td>Test_F1_Score</td><td>▁▄██▄▅</td></tr><tr><td>Test_Precision</td><td>▁▃██▃▇</td></tr><tr><td>Test_Recall</td><td>▁▆██▆▆</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▇▇██████████████████████</td></tr><tr><td>val/loss</td><td> █▆▄▄▃▃▂▂▂▁▁▁▂▁▂▂▃▃▃▃▄▄▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.63333</td></tr><tr><td>Test_F1_Score</td><td>0.5675</td></tr><tr><td>Test_Precision</td><td>0.79123</td></tr><tr><td>Test_Recall</td><td>0.63333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.10495</td></tr><tr><td>val/acc</td><td>0.97624</td></tr><tr><td>val/loss</td><td>0.58186</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2210</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/ctbgv5hk</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_221057-ctbgv5hk\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hj6ge8gh with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_223613-hj6ge8gh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh' target=\"_blank\">dashing-sweep-13</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2236\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dashing-sweep-13</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_223613-hj6ge8gh\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_223620-hj6ge8gh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh' target=\"_blank\">gpt2_hyper_2236</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 0.5156, time 29867.48ms\n",
            "iter 1: loss 2.4219, time 1651.76ms\n",
            "iter 2: loss 2.3125, time 1571.83ms\n",
            "iter 3: loss 1.0938, time 1627.12ms\n",
            "iter 4: loss 0.6914, time 1567.90ms\n",
            "iter 5: loss 0.7187, time 1554.84ms\n",
            "iter 6: loss 1.0156, time 1618.40ms\n",
            "iter 7: loss 1.7969, time 1524.90ms\n",
            "iter 8: loss 0.3613, time 1624.53ms\n",
            "iter 9: loss 0.6523, time 1617.85ms\n",
            "iter 10: loss 1.0625, time 1574.63ms\n",
            "iter 11: loss 0.7812, time 1586.66ms\n",
            "iter 12: loss 0.8906, time 1622.26ms\n",
            "iter 13: loss 0.8477, time 1632.49ms\n",
            "iter 14: loss 1.5000, time 1575.29ms\n",
            "iter 15: loss 0.6758, time 1609.68ms\n",
            "iter 16: loss 1.4531, time 1605.57ms\n",
            "iter 17: loss 0.9219, time 1569.94ms\n",
            "iter 18: loss 1.0078, time 1658.64ms\n",
            "iter 19: loss 1.4375, time 1588.23ms\n",
            "Step 20: train loss 1.1182, val loss 1.0461\n",
            "Validation accuracy: 0.7308\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.4316, time 16051.16ms\n",
            "iter 21: loss 0.5742, time 1579.39ms\n",
            "iter 22: loss 0.5117, time 1610.05ms\n",
            "iter 23: loss 1.1406, time 1582.86ms\n",
            "iter 24: loss 1.6797, time 1577.67ms\n",
            "iter 25: loss 0.5117, time 1561.72ms\n",
            "iter 26: loss 0.8828, time 1680.32ms\n",
            "iter 27: loss 0.8750, time 1770.00ms\n",
            "iter 28: loss 0.1455, time 1642.67ms\n",
            "iter 29: loss 0.6484, time 1591.08ms\n",
            "iter 30: loss 0.2969, time 1590.69ms\n",
            "iter 31: loss 0.8125, time 1594.30ms\n",
            "iter 32: loss 0.1914, time 1606.40ms\n",
            "iter 33: loss 0.2695, time 1603.47ms\n",
            "iter 34: loss 0.0496, time 1560.56ms\n",
            "iter 35: loss 0.2334, time 1579.34ms\n",
            "iter 36: loss 0.0014, time 1578.08ms\n",
            "iter 37: loss 0.2852, time 1593.62ms\n",
            "iter 38: loss 0.1001, time 1558.91ms\n",
            "iter 39: loss 3.2188, time 1572.52ms\n",
            "Step 40: train loss 0.8433, val loss 0.8119\n",
            "Validation accuracy: 0.8565\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.0972, time 15800.13ms\n",
            "iter 41: loss 0.1250, time 1671.86ms\n",
            "iter 42: loss 0.0903, time 1586.66ms\n",
            "iter 43: loss 0.0047, time 1581.40ms\n",
            "iter 44: loss 0.0166, time 1516.40ms\n",
            "iter 45: loss 1.7969, time 1618.59ms\n",
            "iter 46: loss 0.0309, time 1605.01ms\n",
            "iter 47: loss 0.0015, time 1552.31ms\n",
            "iter 48: loss 0.0106, time 1593.87ms\n",
            "iter 49: loss 0.1807, time 1572.48ms\n",
            "iter 50: loss 0.0028, time 1562.25ms\n",
            "iter 51: loss 0.0186, time 1554.18ms\n",
            "iter 52: loss 0.1152, time 1602.25ms\n",
            "iter 53: loss 0.0022, time 1578.92ms\n",
            "iter 54: loss 0.1084, time 1583.91ms\n",
            "iter 55: loss 0.0002, time 1613.69ms\n",
            "iter 56: loss 0.0006, time 1624.59ms\n",
            "iter 57: loss 0.0013, time 1618.13ms\n",
            "iter 58: loss 0.0640, time 1623.47ms\n",
            "iter 59: loss 0.0182, time 1583.13ms\n",
            "Step 60: train loss 0.6398, val loss 0.7544\n",
            "Validation accuracy: 0.9009\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 1.0000, time 15875.54ms\n",
            "iter 61: loss 1.0078, time 1631.26ms\n",
            "iter 62: loss 0.0371, time 1590.11ms\n",
            "iter 63: loss 0.0171, time 1587.23ms\n",
            "iter 64: loss 0.0226, time 1578.98ms\n",
            "iter 65: loss 2.1563, time 1623.18ms\n",
            "iter 66: loss 1.0078, time 1641.73ms\n",
            "iter 67: loss 0.0017, time 1565.09ms\n",
            "iter 68: loss 0.0055, time 1629.08ms\n",
            "iter 69: loss 0.1484, time 1612.45ms\n",
            "iter 70: loss 0.0013, time 1625.97ms\n",
            "iter 71: loss 0.0012, time 1632.93ms\n",
            "iter 72: loss 0.0012, time 1601.54ms\n",
            "iter 73: loss 0.0007, time 1647.29ms\n",
            "iter 74: loss 0.0044, time 1604.53ms\n",
            "iter 75: loss 0.0020, time 1591.76ms\n",
            "iter 76: loss 0.8594, time 1605.43ms\n",
            "iter 77: loss 0.0000, time 1567.52ms\n",
            "iter 78: loss 0.0046, time 1600.28ms\n",
            "iter 79: loss 0.0002, time 1602.35ms\n",
            "Step 80: train loss 0.5142, val loss 0.6195\n",
            "Validation accuracy: 0.9645\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 80: loss 0.0000, time 15882.55ms\n",
            "iter 81: loss 0.0004, time 1616.73ms\n",
            "iter 82: loss 0.4023, time 1587.99ms\n",
            "iter 83: loss 0.0099, time 1564.02ms\n",
            "iter 84: loss 0.0903, time 1603.38ms\n",
            "iter 85: loss 0.0002, time 1652.74ms\n",
            "iter 86: loss 0.6758, time 1686.42ms\n",
            "iter 87: loss 0.0022, time 1571.71ms\n",
            "iter 88: loss 0.0008, time 1591.42ms\n",
            "iter 89: loss 0.0005, time 1655.52ms\n",
            "iter 90: loss 0.0018, time 1617.02ms\n",
            "iter 91: loss 0.0013, time 1561.09ms\n",
            "iter 92: loss 0.0000, time 1598.61ms\n",
            "iter 93: loss 0.0002, time 1600.73ms\n",
            "iter 94: loss 0.0000, time 1660.39ms\n",
            "iter 95: loss 0.0000, time 1601.87ms\n",
            "iter 96: loss 0.0001, time 1604.02ms\n",
            "iter 97: loss 0.0205, time 1562.57ms\n",
            "iter 98: loss 0.0166, time 1658.87ms\n",
            "iter 99: loss 0.0119, time 1622.09ms\n",
            "Step 100: train loss 0.4262, val loss 0.5435\n",
            "Validation accuracy: 0.9763\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.8000\n",
            "iter 100: loss 0.0806, time 16790.44ms\n",
            "iter 101: loss 0.0069, time 1569.34ms\n",
            "iter 102: loss 0.0025, time 1622.65ms\n",
            "iter 103: loss 0.0001, time 1585.97ms\n",
            "iter 104: loss 0.0002, time 1573.49ms\n",
            "iter 105: loss 0.0003, time 1577.10ms\n",
            "iter 106: loss 0.0025, time 1581.04ms\n",
            "iter 107: loss 0.0070, time 1684.97ms\n",
            "iter 108: loss 0.0001, time 1591.17ms\n",
            "iter 109: loss 0.0132, time 1595.61ms\n",
            "iter 110: loss 0.0004, time 1603.27ms\n",
            "iter 111: loss 0.0000, time 1574.04ms\n",
            "iter 112: loss 0.0009, time 1621.05ms\n",
            "iter 113: loss 0.0000, time 1599.28ms\n",
            "iter 114: loss 0.0005, time 1579.65ms\n",
            "iter 115: loss 0.0447, time 1562.97ms\n",
            "iter 116: loss 0.0005, time 1577.01ms\n",
            "iter 117: loss 0.3574, time 1628.95ms\n",
            "iter 118: loss 0.0000, time 1621.07ms\n",
            "iter 119: loss 0.0003, time 1552.42ms\n",
            "Step 120: train loss 0.3642, val loss 0.4793\n",
            "Validation accuracy: 0.9837\n",
            "iter 120: loss 0.0003, time 13723.52ms\n",
            "iter 121: loss 0.0012, time 1575.28ms\n",
            "iter 122: loss 0.5625, time 1608.10ms\n",
            "iter 123: loss 0.0000, time 1590.28ms\n",
            "iter 124: loss 0.0001, time 1553.97ms\n",
            "iter 125: loss 0.0000, time 1599.46ms\n",
            "iter 126: loss 0.0063, time 1570.80ms\n",
            "iter 127: loss 0.0003, time 1611.13ms\n",
            "iter 128: loss 0.0000, time 1611.88ms\n",
            "iter 129: loss 0.0007, time 1580.10ms\n",
            "iter 130: loss 0.0013, time 1575.76ms\n",
            "iter 131: loss 0.0004, time 1561.63ms\n",
            "iter 132: loss 0.0006, time 1606.70ms\n",
            "iter 133: loss 0.0000, time 1589.38ms\n",
            "iter 134: loss 0.0000, time 1557.70ms\n",
            "iter 135: loss 0.0000, time 1632.85ms\n",
            "iter 136: loss 0.0005, time 1576.82ms\n",
            "iter 137: loss 0.0008, time 1615.04ms\n",
            "iter 138: loss 0.0049, time 1611.93ms\n",
            "iter 139: loss 0.0020, time 1588.40ms\n",
            "Step 140: train loss 0.3188, val loss 0.4337\n",
            "Validation accuracy: 0.9896\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 140: loss 0.0001, time 15694.28ms\n",
            "iter 141: loss 0.0005, time 1576.08ms\n",
            "iter 142: loss 0.0393, time 1622.07ms\n",
            "iter 143: loss 0.0007, time 1626.36ms\n",
            "iter 144: loss 0.0000, time 1609.66ms\n",
            "iter 145: loss 0.0000, time 1601.48ms\n",
            "iter 146: loss 0.0002, time 1606.58ms\n",
            "iter 147: loss 0.0001, time 1588.96ms\n",
            "iter 148: loss 0.0081, time 1663.87ms\n",
            "iter 149: loss 0.0879, time 1612.92ms\n",
            "iter 150: loss 0.0001, time 1607.41ms\n",
            "iter 151: loss 0.0000, time 1544.27ms\n",
            "iter 152: loss 0.0000, time 1579.40ms\n",
            "iter 153: loss 0.0000, time 1633.67ms\n",
            "iter 154: loss 0.0000, time 1605.04ms\n",
            "iter 155: loss 0.0002, time 1609.94ms\n",
            "iter 156: loss 0.0000, time 1607.14ms\n",
            "iter 157: loss 0.0015, time 1597.25ms\n",
            "iter 158: loss 0.0004, time 1600.02ms\n",
            "iter 159: loss 0.0002, time 1582.90ms\n",
            "Step 160: train loss 0.2840, val loss 0.4269\n",
            "Validation accuracy: 0.9882\n",
            "iter 160: loss 0.0014, time 14185.65ms\n",
            "iter 161: loss 0.0003, time 1583.47ms\n",
            "iter 162: loss 0.0001, time 1618.84ms\n",
            "iter 163: loss 0.0024, time 1600.36ms\n",
            "iter 164: loss 0.0001, time 1579.31ms\n",
            "iter 165: loss 0.0002, time 1576.06ms\n",
            "iter 166: loss 0.0151, time 1613.11ms\n",
            "iter 167: loss 0.0000, time 1566.48ms\n",
            "iter 168: loss 0.0000, time 1573.33ms\n",
            "iter 169: loss 0.0000, time 1608.98ms\n",
            "iter 170: loss 0.0001, time 1577.52ms\n",
            "iter 171: loss 0.0143, time 1606.94ms\n",
            "iter 172: loss 0.0001, time 1613.25ms\n",
            "iter 173: loss 0.0003, time 1692.17ms\n",
            "iter 174: loss 0.0018, time 1634.41ms\n",
            "iter 175: loss 0.0032, time 1571.91ms\n",
            "iter 176: loss 0.0003, time 1583.89ms\n",
            "iter 177: loss 0.0000, time 1763.64ms\n",
            "iter 178: loss 0.0001, time 1872.19ms\n",
            "iter 179: loss 0.0000, time 1563.23ms\n",
            "Step 180: train loss 0.2534, val loss 0.4156\n",
            "Validation accuracy: 0.9882\n",
            "iter 180: loss 0.0003, time 13806.34ms\n",
            "iter 181: loss 0.0000, time 1588.19ms\n",
            "iter 182: loss 0.0003, time 1603.11ms\n",
            "iter 183: loss 0.0000, time 1602.29ms\n",
            "iter 184: loss 0.0000, time 1607.71ms\n",
            "iter 185: loss 0.0001, time 1594.24ms\n",
            "iter 186: loss 0.0000, time 1613.81ms\n",
            "iter 187: loss 0.0002, time 1592.74ms\n",
            "iter 188: loss 0.0000, time 1665.95ms\n",
            "iter 189: loss 0.0000, time 1606.03ms\n",
            "iter 190: loss 0.0001, time 1612.17ms\n",
            "iter 191: loss 0.0000, time 1585.83ms\n",
            "iter 192: loss 0.0002, time 1553.61ms\n",
            "iter 193: loss 0.0000, time 1633.34ms\n",
            "iter 194: loss 0.0000, time 1639.99ms\n",
            "iter 195: loss 0.0001, time 1562.35ms\n",
            "iter 196: loss 0.0000, time 1611.31ms\n",
            "iter 197: loss 0.0000, time 1598.79ms\n",
            "iter 198: loss 0.0000, time 1632.37ms\n",
            "iter 199: loss 0.0000, time 1575.23ms\n",
            "Step 200: train loss 0.2295, val loss 0.4405\n",
            "Validation accuracy: 0.9852\n",
            "Test accuracy 0.8333\n",
            "iter 200: loss 0.0009, time 14856.44ms\n",
            "iter 201: loss 0.0000, time 1626.17ms\n",
            "iter 202: loss 0.0000, time 1598.17ms\n",
            "iter 203: loss 0.0000, time 1590.82ms\n",
            "iter 204: loss 0.0000, time 1615.10ms\n",
            "iter 205: loss 0.0000, time 1564.10ms\n",
            "iter 206: loss 0.0000, time 1599.60ms\n",
            "iter 207: loss 0.0000, time 1594.23ms\n",
            "iter 208: loss 0.0000, time 1651.18ms\n",
            "iter 209: loss 0.0001, time 1610.24ms\n",
            "iter 210: loss 0.0000, time 1579.24ms\n",
            "iter 211: loss 0.0000, time 1596.82ms\n",
            "iter 212: loss 0.0000, time 1629.88ms\n",
            "iter 213: loss 0.0000, time 1607.97ms\n",
            "iter 214: loss 0.0000, time 1603.27ms\n",
            "iter 215: loss 0.0002, time 1589.93ms\n",
            "iter 216: loss 0.0000, time 1622.73ms\n",
            "iter 217: loss 0.0000, time 1658.77ms\n",
            "iter 218: loss 0.0000, time 1612.03ms\n",
            "iter 219: loss 0.0000, time 1628.74ms\n",
            "Step 220: train loss 0.2094, val loss 0.4366\n",
            "Validation accuracy: 0.9867\n",
            "iter 220: loss 0.0001, time 13807.32ms\n",
            "iter 221: loss 0.0000, time 1583.90ms\n",
            "iter 222: loss 0.0000, time 1626.88ms\n",
            "iter 223: loss 0.0001, time 1606.58ms\n",
            "iter 224: loss 0.0001, time 1672.64ms\n",
            "iter 225: loss 0.0002, time 1638.62ms\n",
            "iter 226: loss 0.0000, time 1586.61ms\n",
            "iter 227: loss 0.0000, time 1594.00ms\n",
            "iter 228: loss 0.0016, time 1677.10ms\n",
            "iter 229: loss 0.0000, time 1576.59ms\n",
            "iter 230: loss 0.0000, time 1576.97ms\n",
            "iter 231: loss 0.0000, time 1621.43ms\n",
            "iter 232: loss 0.0000, time 1561.05ms\n",
            "iter 233: loss 0.0001, time 1605.52ms\n",
            "iter 234: loss 0.0000, time 1609.60ms\n",
            "iter 235: loss 0.0000, time 1631.20ms\n",
            "iter 236: loss 0.0045, time 1593.95ms\n",
            "iter 237: loss 0.0000, time 1605.26ms\n",
            "iter 238: loss 0.0005, time 1586.78ms\n",
            "iter 239: loss 0.0004, time 1588.78ms\n",
            "Step 240: train loss 0.1923, val loss 0.4640\n",
            "Validation accuracy: 0.9689\n",
            "iter 240: loss 0.0003, time 13785.91ms\n",
            "iter 241: loss 0.0004, time 1613.53ms\n",
            "iter 242: loss 0.0000, time 1588.47ms\n",
            "iter 243: loss 0.0000, time 1604.95ms\n",
            "iter 244: loss 0.0000, time 1591.61ms\n",
            "iter 245: loss 0.0000, time 1595.65ms\n",
            "iter 246: loss 0.0000, time 1633.14ms\n",
            "iter 247: loss 0.0000, time 1571.30ms\n",
            "iter 248: loss 0.0003, time 1609.87ms\n",
            "iter 249: loss 0.0000, time 1618.06ms\n",
            "iter 250: loss 0.0000, time 1577.24ms\n",
            "iter 251: loss 0.0000, time 1589.88ms\n",
            "iter 252: loss 0.0001, time 1588.93ms\n",
            "iter 253: loss 0.0001, time 1627.43ms\n",
            "iter 254: loss 0.0000, time 1706.86ms\n",
            "iter 255: loss 0.0018, time 1584.85ms\n",
            "iter 256: loss 0.0000, time 1583.78ms\n",
            "iter 257: loss 0.0020, time 1590.25ms\n",
            "iter 258: loss 0.0000, time 1685.07ms\n",
            "iter 259: loss 0.0000, time 1612.99ms\n",
            "Step 260: train loss 0.1782, val loss 0.4378\n",
            "Validation accuracy: 0.9882\n",
            "iter 260: loss 0.0010, time 13651.50ms\n",
            "iter 261: loss 0.0000, time 1643.78ms\n",
            "iter 262: loss 0.0002, time 1628.27ms\n",
            "iter 263: loss 0.0000, time 1639.63ms\n",
            "iter 264: loss 0.0000, time 1657.53ms\n",
            "iter 265: loss 0.0002, time 1607.23ms\n",
            "iter 266: loss 0.0000, time 1583.42ms\n",
            "iter 267: loss 0.0000, time 1648.43ms\n",
            "iter 268: loss 0.0000, time 1557.89ms\n",
            "iter 269: loss 0.0000, time 1635.07ms\n",
            "iter 270: loss 0.0000, time 1622.10ms\n",
            "iter 271: loss 0.0029, time 1581.20ms\n",
            "iter 272: loss 0.0005, time 1964.50ms\n",
            "iter 273: loss 0.0000, time 1735.49ms\n",
            "iter 274: loss 0.0166, time 1600.11ms\n",
            "iter 275: loss 0.0000, time 1592.57ms\n",
            "iter 276: loss 0.0000, time 1586.46ms\n",
            "iter 277: loss 0.0000, time 1572.12ms\n",
            "iter 278: loss 0.0000, time 1619.51ms\n",
            "iter 279: loss 0.0002, time 1558.15ms\n",
            "Step 280: train loss 0.1658, val loss 0.4725\n",
            "Validation accuracy: 0.9615\n",
            "iter 280: loss 0.0000, time 13903.80ms\n",
            "iter 281: loss 0.0000, time 1614.87ms\n",
            "iter 282: loss 0.0000, time 1597.60ms\n",
            "iter 283: loss 0.0000, time 1593.86ms\n",
            "iter 284: loss 0.0000, time 1630.15ms\n",
            "iter 285: loss 0.0002, time 1584.68ms\n",
            "iter 286: loss 0.0009, time 1635.80ms\n",
            "iter 287: loss 0.0000, time 1598.01ms\n",
            "iter 288: loss 0.0000, time 1584.76ms\n",
            "iter 289: loss 0.0000, time 1583.17ms\n",
            "iter 290: loss 0.0000, time 1568.10ms\n",
            "iter 291: loss 0.0000, time 1599.94ms\n",
            "iter 292: loss 0.0000, time 1566.00ms\n",
            "iter 293: loss 0.0000, time 1610.09ms\n",
            "iter 294: loss 0.0000, time 1594.55ms\n",
            "iter 295: loss 0.0000, time 1709.10ms\n",
            "iter 296: loss 0.0000, time 1692.44ms\n",
            "iter 297: loss 0.0000, time 1634.44ms\n",
            "iter 298: loss 0.0000, time 1589.97ms\n",
            "iter 299: loss 0.0000, time 1618.35ms\n",
            "Step 300: train loss 0.1548, val loss 0.4547\n",
            "Validation accuracy: 0.9882\n",
            "Test accuracy 0.8000\n",
            "iter 300: loss 0.0000, time 15032.77ms\n",
            "iter 301: loss 0.0000, time 1616.88ms\n",
            "iter 302: loss 0.0000, time 1613.73ms\n",
            "iter 303: loss 0.0000, time 1606.94ms\n",
            "iter 304: loss 0.0000, time 1599.70ms\n",
            "iter 305: loss 0.0000, time 1988.44ms\n",
            "iter 306: loss 0.0000, time 1759.58ms\n",
            "iter 307: loss 0.0000, time 1604.58ms\n",
            "iter 308: loss 0.0000, time 1623.42ms\n",
            "iter 309: loss 0.0000, time 1616.00ms\n",
            "iter 310: loss 0.0000, time 1652.95ms\n",
            "iter 311: loss 0.0000, time 1560.81ms\n",
            "iter 312: loss 0.0000, time 1586.48ms\n",
            "iter 313: loss 0.0000, time 1592.65ms\n",
            "iter 314: loss 0.0000, time 1627.69ms\n",
            "iter 315: loss 0.0000, time 1582.51ms\n",
            "iter 316: loss 0.0000, time 1551.45ms\n",
            "iter 317: loss 0.0000, time 1613.38ms\n",
            "iter 318: loss 0.0000, time 1609.73ms\n",
            "iter 319: loss 0.0000, time 1623.07ms\n",
            "Step 320: train loss 0.1457, val loss 0.4442\n",
            "Validation accuracy: 0.9852\n",
            "iter 320: loss 0.0000, time 13766.69ms\n",
            "iter 321: loss 0.0000, time 1612.22ms\n",
            "iter 322: loss 0.0000, time 1618.54ms\n",
            "iter 323: loss 0.0000, time 1609.52ms\n",
            "iter 324: loss 0.0000, time 1542.95ms\n",
            "iter 325: loss 0.0000, time 1696.25ms\n",
            "iter 326: loss 0.0000, time 1598.73ms\n",
            "iter 327: loss 0.0000, time 1797.77ms\n",
            "iter 328: loss 0.0000, time 1643.85ms\n",
            "iter 329: loss 0.0001, time 1608.56ms\n",
            "iter 330: loss 0.0000, time 1624.39ms\n",
            "iter 331: loss 0.0000, time 1599.74ms\n",
            "iter 332: loss 0.0061, time 1606.65ms\n",
            "iter 333: loss 0.0000, time 1616.44ms\n",
            "iter 334: loss 0.0000, time 1742.64ms\n",
            "iter 335: loss 0.0000, time 1637.65ms\n",
            "iter 336: loss 0.0000, time 1582.83ms\n",
            "iter 337: loss 0.0000, time 1693.78ms\n",
            "iter 338: loss 0.0000, time 1623.67ms\n",
            "iter 339: loss 0.0000, time 1589.54ms\n",
            "Step 340: train loss 0.1376, val loss 0.4220\n",
            "Validation accuracy: 0.9822\n",
            "iter 340: loss 0.0000, time 14441.09ms\n",
            "iter 341: loss 0.0000, time 1599.35ms\n",
            "iter 342: loss 0.0000, time 1559.56ms\n",
            "iter 343: loss 0.0000, time 1612.17ms\n",
            "iter 344: loss 0.0000, time 1603.40ms\n",
            "iter 345: loss 0.0000, time 1628.89ms\n",
            "iter 346: loss 0.0000, time 2013.26ms\n",
            "iter 347: loss 0.0000, time 1776.05ms\n",
            "iter 348: loss 0.0000, time 1748.99ms\n",
            "iter 349: loss 0.0000, time 1610.65ms\n",
            "iter 350: loss 0.0000, time 1631.52ms\n",
            "iter 351: loss 0.0000, time 1697.76ms\n",
            "iter 352: loss 0.0000, time 1594.48ms\n",
            "iter 353: loss 0.0049, time 1633.64ms\n",
            "iter 354: loss 0.0000, time 1658.93ms\n",
            "iter 355: loss 0.0000, time 1658.56ms\n",
            "iter 356: loss 0.0003, time 1629.27ms\n",
            "iter 357: loss 0.0000, time 1601.90ms\n",
            "iter 358: loss 0.0000, time 1617.26ms\n",
            "iter 359: loss 0.0000, time 1567.61ms\n",
            "Step 360: train loss 0.1301, val loss 0.4041\n",
            "Validation accuracy: 0.9719\n",
            "iter 360: loss 0.0000, time 13842.95ms\n",
            "iter 361: loss 0.0000, time 1664.63ms\n",
            "iter 362: loss 0.0000, time 1626.72ms\n",
            "iter 363: loss 0.0000, time 1575.00ms\n",
            "iter 364: loss 0.0000, time 1604.05ms\n",
            "iter 365: loss 0.0026, time 1621.68ms\n",
            "iter 366: loss 0.0000, time 1610.46ms\n",
            "iter 367: loss 0.0000, time 1591.40ms\n",
            "iter 368: loss 0.0000, time 1596.30ms\n",
            "iter 369: loss 0.0000, time 1691.83ms\n",
            "iter 370: loss 0.0000, time 1618.80ms\n",
            "iter 371: loss 0.0000, time 1578.91ms\n",
            "iter 372: loss 0.0000, time 1607.19ms\n",
            "iter 373: loss 0.0000, time 1568.73ms\n",
            "iter 374: loss 0.0000, time 1580.36ms\n",
            "iter 375: loss 0.0000, time 1604.55ms\n",
            "iter 376: loss 0.0000, time 1585.88ms\n",
            "iter 377: loss 0.0000, time 1631.53ms\n",
            "iter 378: loss 0.0000, time 1725.71ms\n",
            "iter 379: loss 0.0000, time 1598.96ms\n",
            "Step 380: train loss 0.1236, val loss 0.3829\n",
            "Validation accuracy: 0.9867\n",
            "iter 380: loss 0.0000, time 13712.08ms\n",
            "iter 381: loss 0.0000, time 1581.77ms\n",
            "iter 382: loss 0.0000, time 1612.48ms\n",
            "iter 383: loss 0.0000, time 1631.02ms\n",
            "iter 384: loss 0.0413, time 1557.65ms\n",
            "iter 385: loss 0.0000, time 1659.49ms\n",
            "iter 386: loss 0.0000, time 1714.71ms\n",
            "iter 387: loss 0.0000, time 1597.81ms\n",
            "iter 388: loss 0.0000, time 1600.02ms\n",
            "iter 389: loss 0.0000, time 1625.00ms\n",
            "iter 390: loss 0.0000, time 1636.29ms\n",
            "iter 391: loss 0.0000, time 1623.34ms\n",
            "iter 392: loss 0.0000, time 1546.23ms\n",
            "iter 393: loss 0.0000, time 1610.06ms\n",
            "iter 394: loss 0.0000, time 1632.07ms\n",
            "iter 395: loss 0.0000, time 1658.06ms\n",
            "iter 396: loss 0.0000, time 1608.49ms\n",
            "iter 397: loss 0.0000, time 1592.60ms\n",
            "iter 398: loss 0.0000, time 1601.79ms\n",
            "iter 399: loss 0.0000, time 1620.76ms\n",
            "Step 400: train loss 0.1176, val loss 0.3899\n",
            "Validation accuracy: 0.9926\n",
            "Test accuracy 0.8333\n",
            "iter 400: loss 0.0040, time 14870.15ms\n",
            "iter 401: loss 0.0000, time 1615.71ms\n",
            "iter 402: loss 0.0000, time 1598.43ms\n",
            "iter 403: loss 0.0000, time 1574.00ms\n",
            "iter 404: loss 0.0000, time 1630.80ms\n",
            "iter 405: loss 0.0000, time 1631.75ms\n",
            "iter 406: loss 0.0000, time 1590.00ms\n",
            "iter 407: loss 0.0000, time 1632.38ms\n",
            "iter 408: loss 0.0000, time 1579.54ms\n",
            "iter 409: loss 0.0000, time 1609.49ms\n",
            "iter 410: loss 0.0000, time 1575.74ms\n",
            "iter 411: loss 0.0000, time 1592.69ms\n",
            "iter 412: loss 0.0000, time 1647.38ms\n",
            "iter 413: loss 0.0000, time 1602.00ms\n",
            "iter 414: loss 0.0000, time 2037.21ms\n",
            "iter 415: loss 0.0000, time 1778.42ms\n",
            "iter 416: loss 0.0002, time 1629.36ms\n",
            "iter 417: loss 0.3535, time 1642.42ms\n",
            "iter 418: loss 0.0000, time 1616.94ms\n",
            "iter 419: loss 0.0000, time 1568.83ms\n",
            "Step 420: train loss 0.1120, val loss 0.3951\n",
            "Validation accuracy: 0.9882\n",
            "iter 420: loss 0.0000, time 13651.30ms\n",
            "iter 421: loss 0.0001, time 1597.21ms\n",
            "iter 422: loss 0.0000, time 1631.65ms\n",
            "iter 423: loss 0.0000, time 1604.14ms\n",
            "iter 424: loss 0.0000, time 1580.72ms\n",
            "iter 425: loss 0.0000, time 1642.81ms\n",
            "iter 426: loss 0.0000, time 1825.66ms\n",
            "iter 427: loss 0.0000, time 1648.93ms\n",
            "iter 428: loss 0.0000, time 1696.23ms\n",
            "iter 429: loss 0.0000, time 1630.74ms\n",
            "iter 430: loss 0.0001, time 1612.80ms\n",
            "iter 431: loss 0.0000, time 1598.04ms\n",
            "iter 432: loss 0.0000, time 1648.90ms\n",
            "iter 433: loss 0.0000, time 1633.25ms\n",
            "iter 434: loss 0.0000, time 1592.64ms\n",
            "iter 435: loss 0.0000, time 1584.25ms\n",
            "iter 436: loss 0.0000, time 1578.44ms\n",
            "iter 437: loss 0.0000, time 1613.86ms\n",
            "iter 438: loss 0.0000, time 1599.66ms\n",
            "iter 439: loss 0.0000, time 1576.80ms\n",
            "Step 440: train loss 0.1070, val loss 0.3793\n",
            "Validation accuracy: 0.9852\n",
            "iter 440: loss 0.0001, time 13987.79ms\n",
            "iter 441: loss 0.0000, time 1589.52ms\n",
            "iter 442: loss 0.0000, time 1586.84ms\n",
            "iter 443: loss 0.0000, time 1610.63ms\n",
            "iter 444: loss 0.0000, time 1577.01ms\n",
            "iter 445: loss 0.0000, time 1549.32ms\n",
            "iter 446: loss 0.0000, time 1640.19ms\n",
            "iter 447: loss 0.0000, time 1590.82ms\n",
            "iter 448: loss 0.0001, time 1618.97ms\n",
            "iter 449: loss 0.0000, time 1596.98ms\n",
            "iter 450: loss 0.0000, time 1597.17ms\n",
            "iter 451: loss 0.0000, time 1632.82ms\n",
            "iter 452: loss 0.0000, time 1556.01ms\n",
            "iter 453: loss 0.0000, time 1602.07ms\n",
            "iter 454: loss 0.0000, time 1628.86ms\n",
            "iter 455: loss 0.0000, time 1585.89ms\n",
            "iter 456: loss 0.0000, time 1618.69ms\n",
            "iter 457: loss 0.0000, time 1611.57ms\n",
            "iter 458: loss 0.0001, time 1643.98ms\n",
            "iter 459: loss 0.0002, time 1613.70ms\n",
            "Step 460: train loss 0.1032, val loss 0.3817\n",
            "Validation accuracy: 0.9822\n",
            "iter 460: loss 0.0000, time 13876.64ms\n",
            "iter 461: loss 0.0000, time 1603.14ms\n",
            "iter 462: loss 0.0000, time 1605.13ms\n",
            "iter 463: loss 0.0000, time 1625.26ms\n",
            "iter 464: loss 0.0000, time 1637.79ms\n",
            "iter 465: loss 0.0000, time 1646.52ms\n",
            "iter 466: loss 0.0000, time 1581.53ms\n",
            "iter 467: loss 0.0000, time 1614.36ms\n",
            "iter 468: loss 0.0000, time 1579.81ms\n",
            "iter 469: loss 0.0000, time 1650.95ms\n",
            "iter 470: loss 0.0022, time 1605.98ms\n",
            "iter 471: loss 0.0000, time 1594.48ms\n",
            "iter 472: loss 0.0000, time 1671.60ms\n",
            "iter 473: loss 0.0001, time 1540.82ms\n",
            "iter 474: loss 0.0000, time 1601.55ms\n",
            "iter 475: loss 0.0000, time 1642.45ms\n",
            "iter 476: loss 0.0000, time 1563.09ms\n",
            "iter 477: loss 0.0000, time 1616.76ms\n",
            "iter 478: loss 0.0000, time 1631.49ms\n",
            "iter 479: loss 0.0000, time 1633.88ms\n",
            "Step 480: train loss 0.0989, val loss 0.3830\n",
            "Validation accuracy: 0.9852\n",
            "iter 480: loss 0.0000, time 14441.61ms\n",
            "iter 481: loss 0.0000, time 1661.10ms\n",
            "iter 482: loss 0.0000, time 1595.48ms\n",
            "iter 483: loss 0.0000, time 1559.77ms\n",
            "iter 484: loss 0.0000, time 1631.85ms\n",
            "iter 485: loss 0.0000, time 1585.37ms\n",
            "iter 486: loss 0.0000, time 1576.80ms\n",
            "iter 487: loss 0.0000, time 1598.42ms\n",
            "iter 488: loss 0.0000, time 1625.90ms\n",
            "iter 489: loss 0.0000, time 1647.93ms\n",
            "iter 490: loss 0.0000, time 1650.92ms\n",
            "iter 491: loss 0.0000, time 1606.21ms\n",
            "iter 492: loss 0.0000, time 1621.30ms\n",
            "iter 493: loss 0.0000, time 1560.74ms\n",
            "iter 494: loss 0.0000, time 1681.24ms\n",
            "iter 495: loss 0.0000, time 1619.08ms\n",
            "iter 496: loss 0.0000, time 1618.80ms\n",
            "iter 497: loss 0.0000, time 1645.06ms\n",
            "iter 498: loss 0.0000, time 1656.29ms\n",
            "iter 499: loss 0.0000, time 1599.46ms\n",
            "Step 500: train loss 0.0950, val loss 0.3679\n",
            "Validation accuracy: 0.9852\n",
            "Test accuracy 0.8667\n",
            "iter 500: loss 0.0000, time 19662.21ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▇█▇██</td></tr><tr><td>Test_F1_Score</td><td>▁▇█▇██</td></tr><tr><td>Test_Precision</td><td>▁▇█▇██</td></tr><tr><td>Test_Recall</td><td>▁▇█▇██</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▇▇██████████████████████</td></tr><tr><td>val/loss</td><td> █▆▅▄▃▂▂▂▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.86667</td></tr><tr><td>Test_F1_Score</td><td>0.86111</td></tr><tr><td>Test_Precision</td><td>0.90476</td></tr><tr><td>Test_Recall</td><td>0.86667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.09504</td></tr><tr><td>val/acc</td><td>0.98521</td></tr><tr><td>val/loss</td><td>0.36792</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2236</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/hj6ge8gh</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_223620-hj6ge8gh\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6rr4kx1v with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_225836-6rr4kx1v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v' target=\"_blank\">bright-sweep-14</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2258\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bright-sweep-14</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_225836-6rr4kx1v\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_225846-6rr4kx1v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v' target=\"_blank\">gpt2_hyper_2258</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.3445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 0.5156, time 41137.97ms\n",
            "iter 1: loss 0.1924, time 2195.77ms\n",
            "iter 2: loss 0.8516, time 2232.41ms\n",
            "iter 3: loss 0.8086, time 2190.70ms\n",
            "iter 4: loss 0.5156, time 2190.53ms\n",
            "iter 5: loss 0.7383, time 2139.88ms\n",
            "iter 6: loss 0.7891, time 2331.99ms\n",
            "iter 7: loss 1.2109, time 2162.45ms\n",
            "iter 8: loss 0.6875, time 2188.37ms\n",
            "iter 9: loss 1.1172, time 2205.06ms\n",
            "iter 10: loss 0.8984, time 2160.28ms\n",
            "iter 11: loss 0.7969, time 2191.90ms\n",
            "iter 12: loss 0.3809, time 2168.02ms\n",
            "iter 13: loss 0.6328, time 2323.22ms\n",
            "iter 14: loss 0.5742, time 2244.29ms\n",
            "iter 15: loss 0.4648, time 2208.68ms\n",
            "iter 16: loss 0.8086, time 2153.74ms\n",
            "iter 17: loss 0.6445, time 2254.12ms\n",
            "iter 18: loss 2.7656, time 2203.72ms\n",
            "iter 19: loss 0.5469, time 2049.33ms\n",
            "Step 20: train loss 1.1660, val loss 1.3717\n",
            "Validation accuracy: 0.7973\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.2734, time 21999.85ms\n",
            "iter 21: loss 0.2871, time 2125.12ms\n",
            "iter 22: loss 0.6836, time 2152.16ms\n",
            "iter 23: loss 0.2578, time 2350.96ms\n",
            "iter 24: loss 0.4688, time 2304.28ms\n",
            "iter 25: loss 0.0547, time 2195.45ms\n",
            "iter 26: loss 0.4785, time 2323.56ms\n",
            "iter 27: loss 0.9219, time 2226.85ms\n",
            "iter 28: loss 0.0019, time 2275.50ms\n",
            "iter 29: loss 0.1089, time 2219.11ms\n",
            "iter 30: loss 0.0018, time 2312.23ms\n",
            "iter 31: loss 0.1074, time 2143.10ms\n",
            "iter 32: loss 0.0206, time 2174.08ms\n",
            "iter 33: loss 0.0481, time 2155.47ms\n",
            "iter 34: loss 0.0082, time 2197.53ms\n",
            "iter 35: loss 0.0027, time 2264.69ms\n",
            "iter 36: loss 0.0053, time 2203.51ms\n",
            "iter 37: loss 0.0928, time 2127.34ms\n",
            "iter 38: loss 0.0173, time 2152.53ms\n",
            "iter 39: loss 1.5547, time 2148.22ms\n",
            "Step 40: train loss 0.7556, val loss 0.8877\n",
            "Validation accuracy: 0.7988\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.2266, time 21618.10ms\n",
            "iter 41: loss 0.0073, time 2172.18ms\n",
            "iter 42: loss 0.0005, time 2101.13ms\n",
            "iter 43: loss 0.0008, time 2150.41ms\n",
            "iter 44: loss 0.0035, time 2130.41ms\n",
            "iter 45: loss 0.0106, time 2155.04ms\n",
            "iter 46: loss 0.0011, time 2078.43ms\n",
            "iter 47: loss 0.0000, time 2229.63ms\n",
            "iter 48: loss 0.0042, time 2112.17ms\n",
            "iter 49: loss 0.0225, time 2286.25ms\n",
            "iter 50: loss 0.0062, time 2203.05ms\n",
            "iter 51: loss 0.1016, time 2216.13ms\n",
            "iter 52: loss 0.0042, time 2250.98ms\n",
            "iter 53: loss 0.0009, time 2359.24ms\n",
            "iter 54: loss 0.0315, time 2210.67ms\n",
            "iter 55: loss 0.0010, time 2241.48ms\n",
            "iter 56: loss 0.0000, time 2264.13ms\n",
            "iter 57: loss 0.0015, time 2160.54ms\n",
            "iter 58: loss 0.0454, time 2185.12ms\n",
            "iter 59: loss 0.0918, time 2283.71ms\n",
            "Step 60: train loss 0.5698, val loss 0.6652\n",
            "Validation accuracy: 0.9305\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.1523, time 21617.42ms\n",
            "iter 61: loss 0.1118, time 2128.45ms\n",
            "iter 62: loss 0.0432, time 2226.02ms\n",
            "iter 63: loss 0.0015, time 2169.87ms\n",
            "iter 64: loss 0.0157, time 2178.20ms\n",
            "iter 65: loss 0.7539, time 2078.56ms\n",
            "iter 66: loss 0.6953, time 2211.49ms\n",
            "iter 67: loss 0.0250, time 2275.88ms\n",
            "iter 68: loss 0.0017, time 2310.82ms\n",
            "iter 69: loss 0.1621, time 2150.05ms\n",
            "iter 70: loss 0.0000, time 2386.82ms\n",
            "iter 71: loss 0.0000, time 2152.26ms\n",
            "iter 72: loss 0.0016, time 2178.98ms\n",
            "iter 73: loss 0.0001, time 2185.48ms\n",
            "iter 74: loss 0.0001, time 2190.20ms\n",
            "iter 75: loss 0.0006, time 2138.20ms\n",
            "iter 76: loss 0.0352, time 2269.32ms\n",
            "iter 77: loss 0.0000, time 2261.51ms\n",
            "iter 78: loss 0.0009, time 2285.07ms\n",
            "iter 79: loss 0.0000, time 2180.07ms\n",
            "Step 80: train loss 0.4520, val loss 0.5248\n",
            "Validation accuracy: 0.9068\n",
            "iter 80: loss 0.0003, time 19217.47ms\n",
            "iter 81: loss 0.0000, time 2250.19ms\n",
            "iter 82: loss 0.0417, time 2158.21ms\n",
            "iter 83: loss 0.0015, time 2195.77ms\n",
            "iter 84: loss 0.0005, time 2269.21ms\n",
            "iter 85: loss 0.0001, time 2163.57ms\n",
            "iter 86: loss 0.6680, time 2163.93ms\n",
            "iter 87: loss 0.0001, time 2187.05ms\n",
            "iter 88: loss 0.0018, time 2254.11ms\n",
            "iter 89: loss 0.0032, time 2066.65ms\n",
            "iter 90: loss 0.0002, time 2114.67ms\n",
            "iter 91: loss 0.0002, time 2136.45ms\n",
            "iter 92: loss 0.0022, time 2143.65ms\n",
            "iter 93: loss 0.0001, time 2178.76ms\n",
            "iter 94: loss 0.0000, time 2099.18ms\n",
            "iter 95: loss 0.0000, time 2267.71ms\n",
            "iter 96: loss 0.0003, time 2089.45ms\n",
            "iter 97: loss 0.0035, time 2063.29ms\n",
            "iter 98: loss 0.0005, time 2241.86ms\n",
            "iter 99: loss 0.0003, time 2114.24ms\n",
            "Step 100: train loss 0.3741, val loss 0.4628\n",
            "Validation accuracy: 0.9438\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.7667\n",
            "iter 100: loss 0.0022, time 24517.44ms\n",
            "iter 101: loss 0.0010, time 2133.66ms\n",
            "iter 102: loss 0.0005, time 2247.60ms\n",
            "iter 103: loss 0.0000, time 2249.40ms\n",
            "iter 104: loss 0.0000, time 2211.69ms\n",
            "iter 105: loss 0.0000, time 2222.23ms\n",
            "iter 106: loss 0.0153, time 2395.15ms\n",
            "iter 107: loss 0.3965, time 2284.43ms\n",
            "iter 108: loss 0.0000, time 2141.86ms\n",
            "iter 109: loss 0.0006, time 2155.98ms\n",
            "iter 110: loss 0.0003, time 2100.68ms\n",
            "iter 111: loss 0.0000, time 2080.24ms\n",
            "iter 112: loss 0.0001, time 2231.79ms\n",
            "iter 113: loss 0.0001, time 2272.46ms\n",
            "iter 114: loss 0.0001, time 2181.33ms\n",
            "iter 115: loss 0.0017, time 2208.44ms\n",
            "iter 116: loss 0.0002, time 2239.15ms\n",
            "iter 117: loss 0.0352, time 2163.95ms\n",
            "iter 118: loss 0.0000, time 2166.02ms\n",
            "iter 119: loss 0.0001, time 2231.77ms\n",
            "Step 120: train loss 0.3306, val loss 0.4141\n",
            "Validation accuracy: 0.9689\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 120: loss 0.0000, time 21752.73ms\n",
            "iter 121: loss 0.0118, time 2117.08ms\n",
            "iter 122: loss 0.0156, time 2188.18ms\n",
            "iter 123: loss 0.0000, time 2245.77ms\n",
            "iter 124: loss 0.0001, time 2231.54ms\n",
            "iter 125: loss 0.0000, time 2199.70ms\n",
            "iter 126: loss 0.0031, time 2144.14ms\n",
            "iter 127: loss 0.0000, time 2229.22ms\n",
            "iter 128: loss 0.0004, time 2104.32ms\n",
            "iter 129: loss 0.0002, time 2104.68ms\n",
            "iter 130: loss 0.0001, time 2166.00ms\n",
            "iter 131: loss 0.0016, time 2105.57ms\n",
            "iter 132: loss 0.0005, time 2127.00ms\n",
            "iter 133: loss 0.1553, time 2246.83ms\n",
            "iter 134: loss 0.0000, time 2196.65ms\n",
            "iter 135: loss 0.0000, time 2083.32ms\n",
            "iter 136: loss 0.0000, time 2193.49ms\n",
            "iter 137: loss 0.0000, time 2166.99ms\n",
            "iter 138: loss 0.0005, time 2171.56ms\n",
            "iter 139: loss 0.0004, time 2062.86ms\n",
            "Step 140: train loss 0.2996, val loss 0.4847\n",
            "Validation accuracy: 0.9882\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 140: loss 0.0000, time 21346.08ms\n",
            "iter 141: loss 0.0002, time 2013.04ms\n",
            "iter 142: loss 0.0004, time 2209.98ms\n",
            "iter 143: loss 0.0016, time 2142.43ms\n",
            "iter 144: loss 0.0000, time 2258.18ms\n",
            "iter 145: loss 0.0000, time 2123.18ms\n",
            "iter 146: loss 0.0009, time 2193.34ms\n",
            "iter 147: loss 0.0000, time 2050.21ms\n",
            "iter 148: loss 0.0013, time 2116.43ms\n",
            "iter 149: loss 0.0004, time 2257.93ms\n",
            "iter 150: loss 0.0000, time 2176.99ms\n",
            "iter 151: loss 0.0000, time 2169.21ms\n",
            "iter 152: loss 0.0000, time 2248.74ms\n",
            "iter 153: loss 0.0000, time 2174.92ms\n",
            "iter 154: loss 0.0000, time 2290.75ms\n",
            "iter 155: loss 0.0001, time 2137.50ms\n",
            "iter 156: loss 0.0000, time 2226.11ms\n",
            "iter 157: loss 0.0161, time 2219.21ms\n",
            "iter 158: loss 0.0000, time 2146.22ms\n",
            "iter 159: loss 0.0001, time 2140.21ms\n",
            "Step 160: train loss 0.2659, val loss 0.4571\n",
            "Validation accuracy: 0.9512\n",
            "iter 160: loss 0.0001, time 18720.08ms\n",
            "iter 161: loss 0.0000, time 2061.36ms\n",
            "iter 162: loss 0.0080, time 2197.60ms\n",
            "iter 163: loss 0.0005, time 2217.32ms\n",
            "iter 164: loss 0.0000, time 2232.52ms\n",
            "iter 165: loss 0.0001, time 2193.58ms\n",
            "iter 166: loss 0.0013, time 2181.95ms\n",
            "iter 167: loss 0.0000, time 2224.71ms\n",
            "iter 168: loss 0.0000, time 2172.99ms\n",
            "iter 169: loss 0.9844, time 2207.74ms\n",
            "iter 170: loss 0.0005, time 2205.05ms\n",
            "iter 171: loss 0.2373, time 2093.86ms\n",
            "iter 172: loss 0.0000, time 2157.34ms\n",
            "iter 173: loss 0.0000, time 2174.87ms\n",
            "iter 174: loss 0.0002, time 2091.92ms\n",
            "iter 175: loss 0.0015, time 2137.97ms\n",
            "iter 176: loss 0.0000, time 2279.41ms\n",
            "iter 177: loss 0.0001, time 2106.25ms\n",
            "iter 178: loss 0.0007, time 2099.61ms\n",
            "iter 179: loss 0.0000, time 2186.20ms\n",
            "Step 180: train loss 0.2386, val loss 0.4126\n",
            "Validation accuracy: 0.9808\n",
            "iter 180: loss 0.0000, time 18756.75ms\n",
            "iter 181: loss 0.0029, time 2183.98ms\n",
            "iter 182: loss 0.0066, time 2227.21ms\n",
            "iter 183: loss 0.0014, time 2160.43ms\n",
            "iter 184: loss 0.0000, time 2232.89ms\n",
            "iter 185: loss 0.0018, time 2180.11ms\n",
            "iter 186: loss 0.0000, time 2098.90ms\n",
            "iter 187: loss 0.0000, time 2204.02ms\n",
            "iter 188: loss 0.0000, time 2163.15ms\n",
            "iter 189: loss 0.0000, time 2142.59ms\n",
            "iter 190: loss 0.0004, time 2169.79ms\n",
            "iter 191: loss 0.0000, time 2227.85ms\n",
            "iter 192: loss 0.0008, time 2175.30ms\n",
            "iter 193: loss 0.0000, time 2170.05ms\n",
            "iter 194: loss 0.0000, time 2358.41ms\n",
            "iter 195: loss 0.0011, time 2450.49ms\n",
            "iter 196: loss 0.0000, time 2356.82ms\n",
            "iter 197: loss 0.0000, time 2067.03ms\n",
            "iter 198: loss 0.0000, time 2218.23ms\n",
            "iter 199: loss 0.0000, time 2245.15ms\n",
            "Step 200: train loss 0.2150, val loss 0.3858\n",
            "Validation accuracy: 0.9837\n",
            "Test accuracy 0.7667\n",
            "iter 200: loss 0.0001, time 20688.54ms\n",
            "iter 201: loss 0.0000, time 2274.41ms\n",
            "iter 202: loss 0.0000, time 2168.82ms\n",
            "iter 203: loss 0.0000, time 2211.85ms\n",
            "iter 204: loss 0.0000, time 2133.64ms\n",
            "iter 205: loss 0.0000, time 2165.05ms\n",
            "iter 206: loss 0.0003, time 2209.65ms\n",
            "iter 207: loss 0.0000, time 2136.83ms\n",
            "iter 208: loss 0.0001, time 2070.14ms\n",
            "iter 209: loss 0.0000, time 2162.16ms\n",
            "iter 210: loss 0.0330, time 2270.56ms\n",
            "iter 211: loss 3.7188, time 2230.93ms\n",
            "iter 212: loss 0.0000, time 2163.90ms\n",
            "iter 213: loss 0.0000, time 2121.77ms\n",
            "iter 214: loss 0.0000, time 2170.37ms\n",
            "iter 215: loss 0.0000, time 2246.77ms\n",
            "iter 216: loss 0.0000, time 2128.76ms\n",
            "iter 217: loss 0.0000, time 2163.75ms\n",
            "iter 218: loss 0.0001, time 2230.85ms\n",
            "iter 219: loss 0.0000, time 2061.18ms\n",
            "Step 220: train loss 0.1966, val loss 0.3842\n",
            "Validation accuracy: 0.9453\n",
            "iter 220: loss 0.0000, time 18964.95ms\n",
            "iter 221: loss 0.0005, time 2193.15ms\n",
            "iter 222: loss 0.0060, time 2211.66ms\n",
            "iter 223: loss 0.0000, time 2252.28ms\n",
            "iter 224: loss 0.0068, time 2116.75ms\n",
            "iter 225: loss 0.0000, time 2197.52ms\n",
            "iter 226: loss 0.0000, time 2240.23ms\n",
            "iter 227: loss 0.0000, time 2159.17ms\n",
            "iter 228: loss 0.0000, time 2124.01ms\n",
            "iter 229: loss 0.0000, time 2177.50ms\n",
            "iter 230: loss 0.0000, time 2177.76ms\n",
            "iter 231: loss 0.0000, time 2187.67ms\n",
            "iter 232: loss 0.0000, time 2073.14ms\n",
            "iter 233: loss 0.0000, time 2134.75ms\n",
            "iter 234: loss 0.0000, time 2262.55ms\n",
            "iter 235: loss 0.0001, time 2238.57ms\n",
            "iter 236: loss 0.0000, time 2114.68ms\n",
            "iter 237: loss 0.0000, time 2072.32ms\n",
            "iter 238: loss 0.0000, time 2177.48ms\n",
            "iter 239: loss 0.0000, time 2239.37ms\n",
            "Step 240: train loss 0.1836, val loss 0.3538\n",
            "Validation accuracy: 0.9867\n",
            "iter 240: loss 0.0000, time 18734.43ms\n",
            "iter 241: loss 0.0003, time 2273.78ms\n",
            "iter 242: loss 0.0000, time 2205.38ms\n",
            "iter 243: loss 0.0000, time 2328.64ms\n",
            "iter 244: loss 0.0003, time 2230.91ms\n",
            "iter 245: loss 0.0000, time 2560.73ms\n",
            "iter 246: loss 0.0001, time 2280.95ms\n",
            "iter 247: loss 0.0000, time 2526.15ms\n",
            "iter 248: loss 0.0001, time 2235.27ms\n",
            "iter 249: loss 0.0000, time 2236.41ms\n",
            "iter 250: loss 0.0000, time 2268.84ms\n",
            "iter 251: loss 0.0000, time 2115.78ms\n",
            "iter 252: loss 0.0001, time 2168.76ms\n",
            "iter 253: loss 0.0001, time 2203.24ms\n",
            "iter 254: loss 0.0000, time 2247.37ms\n",
            "iter 255: loss 0.0000, time 2167.09ms\n",
            "iter 256: loss 0.0000, time 2283.29ms\n",
            "iter 257: loss 0.0002, time 2457.32ms\n",
            "iter 258: loss 0.0000, time 2100.52ms\n",
            "iter 259: loss 0.0000, time 2077.44ms\n",
            "Step 260: train loss 0.1708, val loss 0.3670\n",
            "Validation accuracy: 0.9867\n",
            "iter 260: loss 0.0000, time 18148.76ms\n",
            "iter 261: loss 0.0000, time 2204.19ms\n",
            "iter 262: loss 0.0000, time 2079.07ms\n",
            "iter 263: loss 0.0000, time 2075.15ms\n",
            "iter 264: loss 0.0000, time 2071.13ms\n",
            "iter 265: loss 0.0000, time 2194.77ms\n",
            "iter 266: loss 0.0000, time 2076.52ms\n",
            "iter 267: loss 0.0000, time 2054.80ms\n",
            "iter 268: loss 0.0000, time 2144.99ms\n",
            "iter 269: loss 0.0000, time 2169.34ms\n",
            "iter 270: loss 0.0000, time 2226.83ms\n",
            "iter 271: loss 0.0000, time 2161.04ms\n",
            "iter 272: loss 0.0000, time 2105.73ms\n",
            "iter 273: loss 0.0000, time 2305.82ms\n",
            "iter 274: loss 0.0000, time 2020.98ms\n",
            "iter 275: loss 0.0000, time 2407.83ms\n",
            "iter 276: loss 0.0000, time 2202.42ms\n",
            "iter 277: loss 0.0000, time 2139.71ms\n",
            "iter 278: loss 0.0000, time 2151.04ms\n",
            "iter 279: loss 0.0000, time 2149.14ms\n",
            "Step 280: train loss 0.1592, val loss 0.3813\n",
            "Validation accuracy: 0.9837\n",
            "iter 280: loss 0.0008, time 19106.67ms\n",
            "iter 281: loss 0.0000, time 2338.26ms\n",
            "iter 282: loss 0.0000, time 2296.69ms\n",
            "iter 283: loss 0.0000, time 2241.65ms\n",
            "iter 284: loss 0.0000, time 2361.77ms\n",
            "iter 285: loss 0.0000, time 2286.50ms\n",
            "iter 286: loss 0.0002, time 2312.32ms\n",
            "iter 287: loss 0.0000, time 2365.10ms\n",
            "iter 288: loss 0.0000, time 2254.42ms\n",
            "iter 289: loss 0.0000, time 2350.60ms\n",
            "iter 290: loss 0.0000, time 2290.16ms\n",
            "iter 291: loss 0.0000, time 2303.33ms\n",
            "iter 292: loss 0.0000, time 2235.89ms\n",
            "iter 293: loss 0.0000, time 2192.09ms\n",
            "iter 294: loss 0.0000, time 2297.00ms\n",
            "iter 295: loss 0.0001, time 2297.31ms\n",
            "iter 296: loss 0.0000, time 2214.64ms\n",
            "iter 297: loss 0.0000, time 2335.45ms\n",
            "iter 298: loss 0.0000, time 2184.20ms\n",
            "iter 299: loss 0.0005, time 2215.35ms\n",
            "Step 300: train loss 0.1487, val loss 0.4148\n",
            "Validation accuracy: 0.9852\n",
            "Test accuracy 0.8000\n",
            "iter 300: loss 0.0000, time 21076.47ms\n",
            "iter 301: loss 0.0000, time 2442.89ms\n",
            "iter 302: loss 0.0000, time 2197.34ms\n",
            "iter 303: loss 0.0000, time 2302.21ms\n",
            "iter 304: loss 0.0000, time 2298.15ms\n",
            "iter 305: loss 0.0000, time 2099.01ms\n",
            "iter 306: loss 0.0000, time 2277.50ms\n",
            "iter 307: loss 0.0000, time 2192.40ms\n",
            "iter 308: loss 0.0000, time 2190.67ms\n",
            "iter 309: loss 0.0000, time 2114.53ms\n",
            "iter 310: loss 0.0000, time 2166.50ms\n",
            "iter 311: loss 0.0000, time 2181.67ms\n",
            "iter 312: loss 0.0000, time 2270.93ms\n",
            "iter 313: loss 0.0000, time 2285.15ms\n",
            "iter 314: loss 0.0000, time 2230.05ms\n",
            "iter 315: loss 0.0000, time 2113.24ms\n",
            "iter 316: loss 0.0000, time 2086.39ms\n",
            "iter 317: loss 0.0000, time 2221.47ms\n",
            "iter 318: loss 0.0000, time 2244.02ms\n",
            "iter 319: loss 0.0000, time 2171.22ms\n",
            "Step 320: train loss 0.1395, val loss 0.4028\n",
            "Validation accuracy: 0.9882\n",
            "iter 320: loss 0.0000, time 19096.45ms\n",
            "iter 321: loss 0.0000, time 2212.09ms\n",
            "iter 322: loss 0.0000, time 2186.00ms\n",
            "iter 323: loss 0.0000, time 2190.91ms\n",
            "iter 324: loss 0.0000, time 2310.28ms\n",
            "iter 325: loss 0.0000, time 2136.04ms\n",
            "iter 326: loss 0.0000, time 2123.54ms\n",
            "iter 327: loss 0.0000, time 2147.81ms\n",
            "iter 328: loss 0.0000, time 2199.88ms\n",
            "iter 329: loss 0.0000, time 2148.19ms\n",
            "iter 330: loss 0.0005, time 2366.24ms\n",
            "iter 331: loss 0.0000, time 2174.99ms\n",
            "iter 332: loss 0.0000, time 2221.97ms\n",
            "iter 333: loss 0.0000, time 2169.73ms\n",
            "iter 334: loss 0.0000, time 2298.36ms\n",
            "iter 335: loss 0.0000, time 2154.19ms\n",
            "iter 336: loss 0.0000, time 2142.60ms\n",
            "iter 337: loss 0.0000, time 2211.16ms\n",
            "iter 338: loss 0.0005, time 2117.59ms\n",
            "iter 339: loss 0.0000, time 2171.50ms\n",
            "Step 340: train loss 0.1316, val loss 0.4283\n",
            "Validation accuracy: 0.9896\n",
            "iter 340: loss 0.0000, time 19296.82ms\n",
            "iter 341: loss 0.0000, time 2307.74ms\n",
            "iter 342: loss 0.0000, time 2312.94ms\n",
            "iter 343: loss 0.0000, time 2273.18ms\n",
            "iter 344: loss 0.0000, time 2200.25ms\n",
            "iter 345: loss 0.0000, time 2097.18ms\n",
            "iter 346: loss 0.0000, time 2095.93ms\n",
            "iter 347: loss 0.0000, time 2180.26ms\n",
            "iter 348: loss 0.0000, time 2201.30ms\n",
            "iter 349: loss 0.0000, time 2299.73ms\n",
            "iter 350: loss 0.0000, time 2155.10ms\n",
            "iter 351: loss 0.0000, time 2294.90ms\n",
            "iter 352: loss 0.0000, time 2398.07ms\n",
            "iter 353: loss 0.0000, time 2131.76ms\n",
            "iter 354: loss 0.0000, time 2226.77ms\n",
            "iter 355: loss 0.0000, time 2147.07ms\n",
            "iter 356: loss 0.0000, time 2285.52ms\n",
            "iter 357: loss 0.0000, time 2146.21ms\n",
            "iter 358: loss 0.0000, time 2218.50ms\n",
            "iter 359: loss 0.0000, time 2070.24ms\n",
            "Step 360: train loss 0.1246, val loss 0.4328\n",
            "Validation accuracy: 0.9852\n",
            "iter 360: loss 0.0000, time 19287.39ms\n",
            "iter 361: loss 0.0000, time 2191.36ms\n",
            "iter 362: loss 0.0000, time 2192.37ms\n",
            "iter 363: loss 0.0000, time 2193.45ms\n",
            "iter 364: loss 0.0000, time 2247.08ms\n",
            "iter 365: loss 0.0000, time 2036.39ms\n",
            "iter 366: loss 0.0000, time 2170.97ms\n",
            "iter 367: loss 0.0000, time 2219.27ms\n",
            "iter 368: loss 0.0000, time 2147.61ms\n",
            "iter 369: loss 0.0000, time 2193.85ms\n",
            "iter 370: loss 0.0000, time 2196.24ms\n",
            "iter 371: loss 0.0000, time 2197.50ms\n",
            "iter 372: loss 0.0000, time 2192.62ms\n",
            "iter 373: loss 0.0000, time 2147.39ms\n",
            "iter 374: loss 0.0000, time 2198.40ms\n",
            "iter 375: loss 0.0000, time 2109.71ms\n",
            "iter 376: loss 0.6797, time 2121.03ms\n",
            "iter 377: loss 0.0000, time 2151.62ms\n",
            "iter 378: loss 0.0000, time 2188.52ms\n",
            "iter 379: loss 0.0000, time 2264.56ms\n",
            "Step 380: train loss 0.1189, val loss 0.4114\n",
            "Validation accuracy: 0.9896\n",
            "iter 380: loss 0.0000, time 19507.42ms\n",
            "iter 381: loss 0.0000, time 2130.27ms\n",
            "iter 382: loss 0.0000, time 2142.46ms\n",
            "iter 383: loss 0.0000, time 2209.13ms\n",
            "iter 384: loss 0.0000, time 2160.68ms\n",
            "iter 385: loss 0.0000, time 2092.45ms\n",
            "iter 386: loss 0.0000, time 2146.04ms\n",
            "iter 387: loss 0.0000, time 2069.47ms\n",
            "iter 388: loss 0.0000, time 2261.82ms\n",
            "iter 389: loss 0.0000, time 2094.38ms\n",
            "iter 390: loss 0.0000, time 2048.54ms\n",
            "iter 391: loss 0.0000, time 2082.46ms\n",
            "iter 392: loss 0.0000, time 2053.72ms\n",
            "iter 393: loss 0.0000, time 2068.94ms\n",
            "iter 394: loss 0.0000, time 2083.61ms\n",
            "iter 395: loss 0.0000, time 2082.73ms\n",
            "iter 396: loss 0.0000, time 2124.70ms\n",
            "iter 397: loss 0.0000, time 2409.33ms\n",
            "iter 398: loss 0.0000, time 2219.19ms\n",
            "iter 399: loss 0.0000, time 2135.97ms\n",
            "Step 400: train loss 0.1132, val loss 0.3917\n",
            "Validation accuracy: 0.9852\n",
            "Test accuracy 0.8000\n",
            "iter 400: loss 0.0000, time 20130.78ms\n",
            "iter 401: loss 0.0000, time 2294.44ms\n",
            "iter 402: loss 0.0000, time 2305.45ms\n",
            "iter 403: loss 0.0000, time 2219.44ms\n",
            "iter 404: loss 0.0000, time 2206.18ms\n",
            "iter 405: loss 0.0000, time 2175.77ms\n",
            "iter 406: loss 0.0000, time 2094.73ms\n",
            "iter 407: loss 0.0000, time 2123.77ms\n",
            "iter 408: loss 0.0000, time 2185.16ms\n",
            "iter 409: loss 0.0000, time 2163.31ms\n",
            "iter 410: loss 0.0000, time 2184.04ms\n",
            "iter 411: loss 0.0000, time 2166.45ms\n",
            "iter 412: loss 0.0000, time 2123.78ms\n",
            "iter 413: loss 0.0000, time 2184.05ms\n",
            "iter 414: loss 0.0000, time 2181.24ms\n",
            "iter 415: loss 0.0000, time 2175.15ms\n",
            "iter 416: loss 0.0000, time 2416.28ms\n",
            "iter 417: loss 0.0000, time 2071.01ms\n",
            "iter 418: loss 0.0000, time 2204.12ms\n",
            "iter 419: loss 0.0000, time 2340.18ms\n",
            "Step 420: train loss 0.1080, val loss 0.3868\n",
            "Validation accuracy: 0.9867\n",
            "iter 420: loss 0.0000, time 18994.97ms\n",
            "iter 421: loss 0.0000, time 2278.31ms\n",
            "iter 422: loss 0.0000, time 2167.03ms\n",
            "iter 423: loss 0.0000, time 2088.91ms\n",
            "iter 424: loss 0.0000, time 2229.86ms\n",
            "iter 425: loss 0.0000, time 2088.49ms\n",
            "iter 426: loss 0.0000, time 2151.37ms\n",
            "iter 427: loss 0.0000, time 2187.61ms\n",
            "iter 428: loss 0.0000, time 2246.97ms\n",
            "iter 429: loss 0.0000, time 2112.54ms\n",
            "iter 430: loss 0.0000, time 2377.26ms\n",
            "iter 431: loss 0.0000, time 2178.12ms\n",
            "iter 432: loss 0.0000, time 2130.20ms\n",
            "iter 433: loss 0.0000, time 2095.12ms\n",
            "iter 434: loss 0.0000, time 2165.79ms\n",
            "iter 435: loss 0.0000, time 2210.84ms\n",
            "iter 436: loss 0.0000, time 2171.33ms\n",
            "iter 437: loss 0.0000, time 2282.19ms\n",
            "iter 438: loss 0.0000, time 2331.05ms\n",
            "iter 439: loss 0.0000, time 2178.39ms\n",
            "Step 440: train loss 0.1032, val loss 0.3979\n",
            "Validation accuracy: 0.9837\n",
            "iter 440: loss 0.0000, time 19482.23ms\n",
            "iter 441: loss 0.0000, time 2389.52ms\n",
            "iter 442: loss 0.0000, time 2241.34ms\n",
            "iter 443: loss 0.0000, time 2146.99ms\n",
            "iter 444: loss 0.0000, time 2274.40ms\n",
            "iter 445: loss 0.0000, time 2158.60ms\n",
            "iter 446: loss 0.0000, time 2148.19ms\n",
            "iter 447: loss 0.0000, time 2180.45ms\n",
            "iter 448: loss 0.0000, time 2228.54ms\n",
            "iter 449: loss 0.0000, time 2231.95ms\n",
            "iter 450: loss 0.0000, time 2397.79ms\n",
            "iter 451: loss 0.0000, time 2123.30ms\n",
            "iter 452: loss 0.0000, time 2146.50ms\n",
            "iter 453: loss 0.0000, time 2143.55ms\n",
            "iter 454: loss 0.0000, time 2089.00ms\n",
            "iter 455: loss 0.0000, time 2185.42ms\n",
            "iter 456: loss 0.0000, time 2199.16ms\n",
            "iter 457: loss 0.0000, time 2108.14ms\n",
            "iter 458: loss 0.0000, time 2155.79ms\n",
            "iter 459: loss 0.0000, time 2204.48ms\n",
            "Step 460: train loss 0.0989, val loss 0.3988\n",
            "Validation accuracy: 0.9808\n",
            "iter 460: loss 0.0001, time 19055.33ms\n",
            "iter 461: loss 0.0000, time 2061.50ms\n",
            "iter 462: loss 0.0000, time 2408.87ms\n",
            "iter 463: loss 0.0000, time 2261.91ms\n",
            "iter 464: loss 0.0000, time 2232.99ms\n",
            "iter 465: loss 0.0000, time 2175.10ms\n",
            "iter 466: loss 0.0000, time 2303.68ms\n",
            "iter 467: loss 0.0000, time 2386.20ms\n",
            "iter 468: loss 0.0000, time 2281.20ms\n",
            "iter 469: loss 0.0000, time 2355.81ms\n",
            "iter 470: loss 0.0000, time 2162.95ms\n",
            "iter 471: loss 0.0000, time 2416.90ms\n",
            "iter 472: loss 0.0000, time 2227.61ms\n",
            "iter 473: loss 0.0000, time 2264.69ms\n",
            "iter 474: loss 0.0000, time 2393.73ms\n",
            "iter 475: loss 0.0000, time 2181.32ms\n",
            "iter 476: loss 0.0000, time 2096.81ms\n",
            "iter 477: loss 0.0001, time 2085.83ms\n",
            "iter 478: loss 0.0000, time 2351.30ms\n",
            "iter 479: loss 0.0000, time 2178.97ms\n",
            "Step 480: train loss 0.0959, val loss 0.4037\n",
            "Validation accuracy: 0.9793\n",
            "iter 480: loss 0.0000, time 19410.74ms\n",
            "iter 481: loss 0.0000, time 2420.39ms\n",
            "iter 482: loss 0.0000, time 2116.24ms\n",
            "iter 483: loss 0.0000, time 2276.03ms\n",
            "iter 484: loss 0.0000, time 2429.35ms\n",
            "iter 485: loss 0.0000, time 2171.65ms\n",
            "iter 486: loss 0.0000, time 2464.77ms\n",
            "iter 487: loss 0.0000, time 2621.75ms\n",
            "iter 488: loss 0.0000, time 2407.65ms\n",
            "iter 489: loss 0.0000, time 2120.32ms\n",
            "iter 490: loss 0.0000, time 2179.48ms\n",
            "iter 491: loss 0.0000, time 2117.16ms\n",
            "iter 492: loss 0.0000, time 2188.07ms\n",
            "iter 493: loss 0.0000, time 2133.74ms\n",
            "iter 494: loss 0.0000, time 2149.52ms\n",
            "iter 495: loss 0.0000, time 2076.15ms\n",
            "iter 496: loss 0.0000, time 2105.54ms\n",
            "iter 497: loss 0.0000, time 2090.77ms\n",
            "iter 498: loss 0.0000, time 2246.86ms\n",
            "iter 499: loss 0.0000, time 2116.91ms\n",
            "Step 500: train loss 0.0923, val loss 0.3887\n",
            "Validation accuracy: 0.9867\n",
            "Test accuracy 0.9000\n",
            "iter 500: loss 0.0000, time 21146.45ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▆▆▇▇█</td></tr><tr><td>Test_F1_Score</td><td>▁▇▇▇▇█</td></tr><tr><td>Test_Precision</td><td>▁▇▇▇██</td></tr><tr><td>Test_Recall</td><td>▁▆▆▇▇█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▆▇▇█████████████████████</td></tr><tr><td>val/loss</td><td> █▅▃▂▂▁▂▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.9</td></tr><tr><td>Test_F1_Score</td><td>0.89975</td></tr><tr><td>Test_Precision</td><td>0.90236</td></tr><tr><td>Test_Recall</td><td>0.9</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.0923</td></tr><tr><td>val/acc</td><td>0.98669</td></tr><tr><td>val/loss</td><td>0.38869</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2258</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/6rr4kx1v</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_225846-6rr4kx1v\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a9ukmt8c with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (0.2s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_232838-a9ukmt8c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c' target=\"_blank\">lunar-sweep-15</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2328\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lunar-sweep-15</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_232838-a9ukmt8c\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_232848-a9ukmt8c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c' target=\"_blank\">gpt2_hyper_2328</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.4241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 7.6562, time 29362.30ms\n",
            "iter 1: loss 2.7500, time 2193.73ms\n",
            "iter 2: loss 1.7656, time 2178.42ms\n",
            "iter 3: loss 1.2969, time 2222.46ms\n",
            "iter 4: loss 1.2812, time 2126.86ms\n",
            "iter 5: loss 0.5937, time 2211.59ms\n",
            "iter 6: loss 0.8164, time 2205.62ms\n",
            "iter 7: loss 0.8945, time 2155.95ms\n",
            "iter 8: loss 0.2256, time 2107.94ms\n",
            "iter 9: loss 0.5039, time 2169.40ms\n",
            "iter 10: loss 0.0172, time 2096.09ms\n",
            "iter 11: loss 0.1338, time 2210.54ms\n",
            "iter 12: loss 0.2129, time 2212.33ms\n",
            "iter 13: loss 0.2520, time 2146.32ms\n",
            "iter 14: loss 0.2871, time 2150.75ms\n",
            "iter 15: loss 0.6875, time 2101.07ms\n",
            "iter 16: loss 0.2988, time 2164.73ms\n",
            "iter 17: loss 0.2187, time 2171.57ms\n",
            "iter 18: loss 0.3047, time 2256.98ms\n",
            "iter 19: loss 0.0815, time 2125.11ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 0.9365, val loss 0.5251\n",
            "Validation accuracy: 0.8866\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.1621, time 28729.98ms\n",
            "iter 21: loss 0.1709, time 2176.01ms\n",
            "iter 22: loss 0.0265, time 2132.66ms\n",
            "iter 23: loss 0.1875, time 2321.48ms\n",
            "iter 24: loss 0.2422, time 2108.33ms\n",
            "iter 25: loss 0.9453, time 2155.60ms\n",
            "iter 26: loss 0.0459, time 2241.15ms\n",
            "iter 27: loss 0.0552, time 2147.87ms\n",
            "iter 28: loss 0.0188, time 2277.97ms\n",
            "iter 29: loss 0.0082, time 2213.31ms\n",
            "iter 30: loss 0.0238, time 2301.97ms\n",
            "iter 31: loss 0.5039, time 2153.61ms\n",
            "iter 32: loss 0.2676, time 2195.67ms\n",
            "iter 33: loss 0.0432, time 2314.47ms\n",
            "iter 34: loss 0.4746, time 2133.04ms\n",
            "iter 35: loss 0.0300, time 2188.89ms\n",
            "iter 36: loss 4.2188, time 1592.70ms\n",
            "iter 37: loss 0.0038, time 1603.46ms\n",
            "iter 38: loss 0.3027, time 1616.41ms\n",
            "iter 39: loss 0.1309, time 1561.43ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.6437, val loss 0.5289\n",
            "Validation accuracy: 0.9212\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.0879, time 20938.87ms\n",
            "iter 41: loss 0.0869, time 1662.73ms\n",
            "iter 42: loss 1.8906, time 1619.85ms\n",
            "iter 43: loss 1.1250, time 1665.92ms\n",
            "iter 44: loss 0.2217, time 1608.15ms\n",
            "iter 45: loss 0.5000, time 1667.82ms\n",
            "iter 46: loss 0.1436, time 1655.60ms\n",
            "iter 47: loss 0.1123, time 1598.36ms\n",
            "iter 48: loss 0.6758, time 1651.29ms\n",
            "iter 49: loss 0.7500, time 1707.60ms\n",
            "iter 50: loss 0.1270, time 1679.37ms\n",
            "iter 51: loss 0.1157, time 1670.17ms\n",
            "iter 52: loss 0.0152, time 1690.23ms\n",
            "iter 53: loss 5.3438, time 1628.84ms\n",
            "iter 54: loss 0.0178, time 1659.99ms\n",
            "iter 55: loss 0.0437, time 1686.38ms\n",
            "iter 56: loss 0.0107, time 1736.89ms\n",
            "iter 57: loss 0.0469, time 1699.69ms\n",
            "iter 58: loss 0.3203, time 1682.76ms\n",
            "iter 59: loss 0.0145, time 1592.44ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.5129, val loss 0.5981\n",
            "Validation accuracy: 0.9557\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 60: loss 0.0156, time 21371.63ms\n",
            "iter 61: loss 0.0493, time 1663.93ms\n",
            "iter 62: loss 0.0014, time 1601.82ms\n",
            "iter 63: loss 0.0166, time 1686.31ms\n",
            "iter 64: loss 0.0011, time 1744.09ms\n",
            "iter 65: loss 0.0420, time 1639.68ms\n",
            "iter 66: loss 0.0126, time 1691.07ms\n",
            "iter 67: loss 0.0067, time 1655.23ms\n",
            "iter 68: loss 0.0064, time 1605.97ms\n",
            "iter 69: loss 0.0004, time 1634.89ms\n",
            "iter 70: loss 0.0004, time 1640.55ms\n",
            "iter 71: loss 0.1416, time 1613.39ms\n",
            "iter 72: loss 0.8086, time 1680.64ms\n",
            "iter 73: loss 0.0092, time 1607.27ms\n",
            "iter 74: loss 0.0008, time 1703.35ms\n",
            "iter 75: loss 0.0131, time 1680.40ms\n",
            "iter 76: loss 0.0080, time 1640.48ms\n",
            "iter 77: loss 1.5234, time 1667.41ms\n",
            "iter 78: loss 2.9844, time 1658.13ms\n",
            "iter 79: loss 0.0081, time 1583.89ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80: train loss 0.4264, val loss 0.6188\n",
            "Validation accuracy: 0.9438\n",
            "iter 80: loss 0.0042, time 18926.29ms\n",
            "iter 81: loss 3.6875, time 1625.57ms\n",
            "iter 82: loss 0.0552, time 1551.99ms\n",
            "iter 83: loss 0.0011, time 1566.76ms\n",
            "iter 84: loss 0.0108, time 1653.50ms\n",
            "iter 85: loss 0.0023, time 1519.65ms\n",
            "iter 86: loss 0.0052, time 1593.52ms\n",
            "iter 87: loss 0.3008, time 1591.70ms\n",
            "iter 88: loss 0.0313, time 1618.19ms\n",
            "iter 89: loss 0.0117, time 1659.04ms\n",
            "iter 90: loss 0.0074, time 1614.17ms\n",
            "iter 91: loss 0.0002, time 1786.65ms\n",
            "iter 92: loss 0.0149, time 1671.39ms\n",
            "iter 93: loss 0.0009, time 1595.34ms\n",
            "iter 94: loss 0.5820, time 1649.10ms\n",
            "iter 95: loss 0.1270, time 1669.78ms\n",
            "iter 96: loss 0.9219, time 1648.90ms\n",
            "iter 97: loss 0.0154, time 1678.28ms\n",
            "iter 98: loss 0.0118, time 1691.56ms\n",
            "iter 99: loss 0.0383, time 1659.65ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100: train loss 0.3619, val loss 0.6456\n",
            "Validation accuracy: 0.9633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.6000\n",
            "iter 100: loss 1.2734, time 20681.83ms\n",
            "iter 101: loss 0.0042, time 1733.68ms\n",
            "iter 102: loss 0.0025, time 1634.77ms\n",
            "iter 103: loss 0.0012, time 1637.38ms\n",
            "iter 104: loss 0.0015, time 1704.48ms\n",
            "iter 105: loss 0.0008, time 1650.21ms\n",
            "iter 106: loss 0.0019, time 1665.31ms\n",
            "iter 107: loss 0.0077, time 1681.61ms\n",
            "iter 108: loss 0.0001, time 1632.65ms\n",
            "iter 109: loss 0.0010, time 1637.98ms\n",
            "iter 110: loss 0.0000, time 1628.06ms\n",
            "iter 111: loss 0.0015, time 1646.42ms\n",
            "iter 112: loss 0.0015, time 1678.61ms\n",
            "iter 113: loss 0.0052, time 1688.44ms\n",
            "iter 114: loss 0.0023, time 1722.79ms\n",
            "iter 115: loss 0.0003, time 1749.20ms\n",
            "iter 116: loss 0.0000, time 1681.17ms\n",
            "iter 117: loss 0.0374, time 1690.51ms\n",
            "iter 118: loss 0.0011, time 1668.00ms\n",
            "iter 119: loss 0.0003, time 1674.99ms\n",
            "Step 120: train loss 0.3103, val loss 0.6279\n",
            "Validation accuracy: 0.9795\n",
            "iter 120: loss 0.0007, time 19515.87ms\n",
            "iter 121: loss 0.0007, time 1727.22ms\n",
            "iter 122: loss 0.0080, time 1705.78ms\n",
            "iter 123: loss 0.0093, time 1640.94ms\n",
            "iter 124: loss 0.0002, time 1630.82ms\n",
            "iter 125: loss 0.0015, time 1712.85ms\n",
            "iter 126: loss 0.0806, time 1660.45ms\n",
            "iter 127: loss 0.0065, time 1646.42ms\n",
            "iter 128: loss 0.0006, time 1649.82ms\n",
            "iter 129: loss 0.0005, time 1724.35ms\n",
            "iter 130: loss 0.0006, time 1639.34ms\n",
            "iter 131: loss 0.0006, time 1667.59ms\n",
            "iter 132: loss 0.0001, time 1619.23ms\n",
            "iter 133: loss 0.0015, time 1652.10ms\n",
            "iter 134: loss 0.0150, time 1631.70ms\n",
            "iter 135: loss 0.0062, time 1646.31ms\n",
            "iter 136: loss 0.0004, time 1617.22ms\n",
            "iter 137: loss 0.0002, time 1666.75ms\n",
            "iter 138: loss 0.0002, time 1609.34ms\n",
            "iter 139: loss 0.0000, time 1807.74ms\n",
            "Step 140: train loss 0.2716, val loss 0.6099\n",
            "Validation accuracy: 0.9741\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 140: loss 0.0011, time 21689.76ms\n",
            "iter 141: loss 0.0008, time 1676.02ms\n",
            "iter 142: loss 0.0025, time 1762.71ms\n",
            "iter 143: loss 0.0000, time 1669.57ms\n",
            "iter 144: loss 0.0001, time 1648.18ms\n",
            "iter 145: loss 0.0000, time 1782.47ms\n",
            "iter 146: loss 0.0001, time 1711.25ms\n",
            "iter 147: loss 0.0000, time 1769.41ms\n",
            "iter 148: loss 0.0000, time 1730.12ms\n",
            "iter 149: loss 0.0002, time 1658.63ms\n",
            "iter 150: loss 0.0007, time 1734.80ms\n",
            "iter 151: loss 0.0000, time 1677.06ms\n",
            "iter 152: loss 0.0000, time 1609.01ms\n",
            "iter 153: loss 0.0000, time 1675.98ms\n",
            "iter 154: loss 0.0403, time 1665.04ms\n",
            "iter 155: loss 0.0000, time 1672.73ms\n",
            "iter 156: loss 0.0001, time 1650.57ms\n",
            "iter 157: loss 0.0007, time 1635.77ms\n",
            "iter 158: loss 0.0002, time 1727.74ms\n",
            "iter 159: loss 0.0000, time 1646.45ms\n",
            "Step 160: train loss 0.2437, val loss 0.6206\n",
            "Validation accuracy: 0.9860\n",
            "iter 160: loss 0.0001, time 19682.47ms\n",
            "iter 161: loss 0.0442, time 1648.19ms\n",
            "iter 162: loss 0.0001, time 1649.44ms\n",
            "iter 163: loss 0.0007, time 1690.42ms\n",
            "iter 164: loss 0.0002, time 1767.45ms\n",
            "iter 165: loss 0.0093, time 1739.42ms\n",
            "iter 166: loss 0.0007, time 1685.08ms\n",
            "iter 167: loss 0.0001, time 1639.69ms\n",
            "iter 168: loss 0.0004, time 1700.38ms\n",
            "iter 169: loss 0.0002, time 1694.39ms\n",
            "iter 170: loss 0.0002, time 1684.82ms\n",
            "iter 171: loss 0.0000, time 1667.71ms\n",
            "iter 172: loss 0.0002, time 1689.62ms\n",
            "iter 173: loss 0.0000, time 1686.27ms\n",
            "iter 174: loss 0.0009, time 1627.05ms\n",
            "iter 175: loss 0.0000, time 1678.48ms\n",
            "iter 176: loss 0.0004, time 1660.66ms\n",
            "iter 177: loss 0.0001, time 1664.88ms\n",
            "iter 178: loss 0.0054, time 1690.54ms\n",
            "iter 179: loss 0.0000, time 1675.75ms\n",
            "Step 180: train loss 0.2200, val loss 0.7199\n",
            "Validation accuracy: 0.9633\n",
            "iter 180: loss 0.0000, time 19879.67ms\n",
            "iter 181: loss 0.0001, time 1691.12ms\n",
            "iter 182: loss 0.0000, time 1690.67ms\n",
            "iter 183: loss 0.0001, time 1683.02ms\n",
            "iter 184: loss 0.0000, time 1702.49ms\n",
            "iter 185: loss 0.0000, time 1649.34ms\n",
            "iter 186: loss 0.0000, time 1618.49ms\n",
            "iter 187: loss 0.0000, time 1689.66ms\n",
            "iter 188: loss 0.0000, time 1669.04ms\n",
            "iter 189: loss 0.0000, time 1643.53ms\n",
            "iter 190: loss 0.0001, time 1696.83ms\n",
            "iter 191: loss 0.0003, time 1683.35ms\n",
            "iter 192: loss 0.0031, time 1685.62ms\n",
            "iter 193: loss 0.0002, time 1652.64ms\n",
            "iter 194: loss 0.0001, time 1751.54ms\n",
            "iter 195: loss 0.0000, time 1723.28ms\n",
            "iter 196: loss 0.0000, time 1605.35ms\n",
            "iter 197: loss 0.0000, time 1694.35ms\n",
            "iter 198: loss 0.0000, time 1640.17ms\n",
            "iter 199: loss 0.0000, time 1643.07ms\n",
            "Step 200: train loss 0.2013, val loss 0.7305\n",
            "Validation accuracy: 0.9881\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "Test accuracy 0.8000\n",
            "iter 200: loss 0.0010, time 22841.52ms\n",
            "iter 201: loss 0.0004, time 1665.89ms\n",
            "iter 202: loss 0.0003, time 1650.35ms\n",
            "iter 203: loss 0.0000, time 1912.92ms\n",
            "iter 204: loss 0.0008, time 1678.40ms\n",
            "iter 205: loss 0.0001, time 1641.90ms\n",
            "iter 206: loss 0.0000, time 1613.67ms\n",
            "iter 207: loss 0.0000, time 1633.75ms\n",
            "iter 208: loss 0.0002, time 1652.25ms\n",
            "iter 209: loss 0.0003, time 1771.41ms\n",
            "iter 210: loss 0.0002, time 1713.38ms\n",
            "iter 211: loss 0.0001, time 1781.11ms\n",
            "iter 212: loss 0.0000, time 1649.06ms\n",
            "iter 213: loss 0.0000, time 1733.28ms\n",
            "iter 214: loss 0.0002, time 1692.11ms\n",
            "iter 215: loss 0.0000, time 1651.90ms\n",
            "iter 216: loss 0.0003, time 1733.38ms\n",
            "iter 217: loss 0.0000, time 1695.37ms\n",
            "iter 218: loss 0.0000, time 1777.29ms\n",
            "iter 219: loss 0.0001, time 1657.56ms\n",
            "Step 220: train loss 0.1840, val loss 0.6967\n",
            "Validation accuracy: 0.9870\n",
            "iter 220: loss 0.0000, time 19567.98ms\n",
            "iter 221: loss 0.0000, time 1580.81ms\n",
            "iter 222: loss 0.0000, time 1614.62ms\n",
            "iter 223: loss 0.0000, time 1595.31ms\n",
            "iter 224: loss 0.0000, time 1566.59ms\n",
            "iter 225: loss 0.0000, time 1688.84ms\n",
            "iter 226: loss 0.0913, time 1667.65ms\n",
            "iter 227: loss 0.0000, time 1675.16ms\n",
            "iter 228: loss 0.0001, time 1658.75ms\n",
            "iter 229: loss 0.0000, time 1627.00ms\n",
            "iter 230: loss 0.0000, time 1668.43ms\n",
            "iter 231: loss 0.0001, time 1682.46ms\n",
            "iter 232: loss 0.0000, time 1674.42ms\n",
            "iter 233: loss 0.0000, time 1670.54ms\n",
            "iter 234: loss 0.0001, time 1715.17ms\n",
            "iter 235: loss 0.0000, time 1643.35ms\n",
            "iter 236: loss 0.0003, time 1618.05ms\n",
            "iter 237: loss 0.0004, time 1651.48ms\n",
            "iter 238: loss 0.0000, time 1740.33ms\n",
            "iter 239: loss 0.0000, time 1653.05ms\n",
            "Step 240: train loss 0.1690, val loss 0.6700\n",
            "Validation accuracy: 0.9827\n",
            "iter 240: loss 0.0000, time 19420.55ms\n",
            "iter 241: loss 0.0002, time 1632.03ms\n",
            "iter 242: loss 0.0000, time 1693.64ms\n",
            "iter 243: loss 0.0000, time 1661.35ms\n",
            "iter 244: loss 0.0002, time 1580.63ms\n",
            "iter 245: loss 0.0000, time 1630.87ms\n",
            "iter 246: loss 0.0000, time 1625.80ms\n",
            "iter 247: loss 0.0000, time 1668.45ms\n",
            "iter 248: loss 0.0035, time 1666.36ms\n",
            "iter 249: loss 0.0000, time 1677.88ms\n",
            "iter 250: loss 0.0000, time 1711.55ms\n",
            "iter 251: loss 0.0000, time 1695.35ms\n",
            "iter 252: loss 0.0000, time 1723.35ms\n",
            "iter 253: loss 0.0000, time 1701.50ms\n",
            "iter 254: loss 0.0000, time 1639.82ms\n",
            "iter 255: loss 0.0010, time 1632.59ms\n",
            "iter 256: loss 0.0000, time 1624.46ms\n",
            "iter 257: loss 0.0000, time 1691.31ms\n",
            "iter 258: loss 0.0000, time 1642.42ms\n",
            "iter 259: loss 0.0013, time 1676.05ms\n",
            "Step 260: train loss 0.1561, val loss 0.6402\n",
            "Validation accuracy: 0.9892\n",
            "iter 260: loss 0.0000, time 19560.58ms\n",
            "iter 261: loss 0.0000, time 1661.18ms\n",
            "iter 262: loss 0.0000, time 1736.11ms\n",
            "iter 263: loss 0.0000, time 1676.61ms\n",
            "iter 264: loss 0.0001, time 1707.75ms\n",
            "iter 265: loss 0.0000, time 1730.48ms\n",
            "iter 266: loss 0.0000, time 1773.54ms\n",
            "iter 267: loss 0.0000, time 1727.02ms\n",
            "iter 268: loss 0.0000, time 1631.29ms\n",
            "iter 269: loss 0.0000, time 1804.76ms\n",
            "iter 270: loss 0.0000, time 1674.89ms\n",
            "iter 271: loss 0.0146, time 1650.29ms\n",
            "iter 272: loss 0.0000, time 1658.34ms\n",
            "iter 273: loss 0.0000, time 1640.60ms\n",
            "iter 274: loss 0.0000, time 1813.73ms\n",
            "iter 275: loss 0.0000, time 1809.21ms\n",
            "iter 276: loss 0.0000, time 1846.48ms\n",
            "iter 277: loss 0.0000, time 2015.49ms\n",
            "iter 278: loss 0.0017, time 1716.89ms\n",
            "iter 279: loss 0.0000, time 1821.73ms\n",
            "Step 280: train loss 0.1450, val loss 0.6063\n",
            "Validation accuracy: 0.9881\n",
            "iter 280: loss 0.0000, time 19146.33ms\n",
            "iter 281: loss 0.0000, time 1671.43ms\n",
            "iter 282: loss 0.0000, time 1751.92ms\n",
            "iter 283: loss 0.0000, time 1650.84ms\n",
            "iter 284: loss 0.0000, time 1624.45ms\n",
            "iter 285: loss 0.0000, time 1667.54ms\n",
            "iter 286: loss 0.0000, time 1685.16ms\n",
            "iter 287: loss 0.0000, time 1621.33ms\n",
            "iter 288: loss 0.0002, time 1625.90ms\n",
            "iter 289: loss 0.0000, time 1712.70ms\n",
            "iter 290: loss 0.0000, time 1679.21ms\n",
            "iter 291: loss 0.0000, time 1647.47ms\n",
            "iter 292: loss 0.0000, time 1695.42ms\n",
            "iter 293: loss 0.0000, time 1659.13ms\n",
            "iter 294: loss 0.0000, time 1573.42ms\n",
            "iter 295: loss 0.0000, time 1636.22ms\n",
            "iter 296: loss 0.0000, time 1630.64ms\n",
            "iter 297: loss 0.0000, time 1645.19ms\n",
            "iter 298: loss 0.0000, time 1615.09ms\n",
            "iter 299: loss 0.0000, time 1580.22ms\n",
            "Step 300: train loss 0.1362, val loss 0.6142\n",
            "Validation accuracy: 0.9903\n",
            "Test accuracy 0.8000\n",
            "iter 300: loss 0.0000, time 20321.21ms\n",
            "iter 301: loss 0.0000, time 1636.25ms\n",
            "iter 302: loss 0.0000, time 1686.86ms\n",
            "iter 303: loss 0.0000, time 1723.29ms\n",
            "iter 304: loss 0.0000, time 1593.29ms\n",
            "iter 305: loss 0.0000, time 1641.55ms\n",
            "iter 306: loss 0.0000, time 1585.42ms\n",
            "iter 307: loss 0.0000, time 1688.41ms\n",
            "iter 308: loss 0.0000, time 1668.97ms\n",
            "iter 309: loss 0.0002, time 1689.17ms\n",
            "iter 310: loss 0.0000, time 1671.92ms\n",
            "iter 311: loss 0.0000, time 1662.80ms\n",
            "iter 312: loss 0.0000, time 1622.81ms\n",
            "iter 313: loss 0.0004, time 1684.69ms\n",
            "iter 314: loss 0.4336, time 1670.92ms\n",
            "iter 315: loss 0.0000, time 1662.83ms\n",
            "iter 316: loss 0.0005, time 1649.04ms\n",
            "iter 317: loss 0.0000, time 1684.39ms\n",
            "iter 318: loss 0.0000, time 1657.69ms\n",
            "iter 319: loss 0.0000, time 1631.93ms\n",
            "Step 320: train loss 0.1278, val loss 0.5810\n",
            "Validation accuracy: 0.9914\n",
            "iter 320: loss 0.0000, time 19809.85ms\n",
            "iter 321: loss 0.0000, time 1726.15ms\n",
            "iter 322: loss 0.0000, time 1642.27ms\n",
            "iter 323: loss 0.0000, time 1681.95ms\n",
            "iter 324: loss 0.0000, time 1691.97ms\n",
            "iter 325: loss 0.0000, time 1640.60ms\n",
            "iter 326: loss 0.0000, time 1712.83ms\n",
            "iter 327: loss 0.0000, time 1775.49ms\n",
            "iter 328: loss 0.0000, time 1755.09ms\n",
            "iter 329: loss 0.0000, time 1686.97ms\n",
            "iter 330: loss 0.0000, time 1744.98ms\n",
            "iter 331: loss 0.0002, time 1684.72ms\n",
            "iter 332: loss 0.0000, time 1755.12ms\n",
            "iter 333: loss 0.0000, time 1690.98ms\n",
            "iter 334: loss 0.0000, time 1716.15ms\n",
            "iter 335: loss 0.0000, time 1687.40ms\n",
            "iter 336: loss 0.0000, time 1630.10ms\n",
            "iter 337: loss 0.0000, time 1633.10ms\n",
            "iter 338: loss 0.0000, time 1650.23ms\n",
            "iter 339: loss 0.0000, time 1746.94ms\n",
            "Step 340: train loss 0.1205, val loss 0.5539\n",
            "Validation accuracy: 0.9935\n",
            "iter 340: loss 0.0000, time 19627.78ms\n",
            "iter 341: loss 0.0000, time 1725.58ms\n",
            "iter 342: loss 0.0000, time 1647.70ms\n",
            "iter 343: loss 0.0021, time 1715.72ms\n",
            "iter 344: loss 0.0000, time 1638.70ms\n",
            "iter 345: loss 0.0000, time 1568.29ms\n",
            "iter 346: loss 0.0000, time 1625.05ms\n",
            "iter 347: loss 0.0000, time 1701.81ms\n",
            "iter 348: loss 0.0000, time 1768.45ms\n",
            "iter 349: loss 0.0000, time 1810.06ms\n",
            "iter 350: loss 0.0000, time 1749.57ms\n",
            "iter 351: loss 0.0000, time 1673.28ms\n",
            "iter 352: loss 0.0000, time 1803.10ms\n",
            "iter 353: loss 0.0000, time 1819.95ms\n",
            "iter 354: loss 0.0000, time 1629.94ms\n",
            "iter 355: loss 0.0001, time 1635.91ms\n",
            "iter 356: loss 0.0000, time 1667.97ms\n",
            "iter 357: loss 0.0000, time 1731.06ms\n",
            "iter 358: loss 0.0001, time 1791.93ms\n",
            "iter 359: loss 0.0000, time 1674.71ms\n",
            "Step 360: train loss 0.1139, val loss 0.5324\n",
            "Validation accuracy: 0.9946\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 360: loss 0.0000, time 21785.77ms\n",
            "iter 361: loss 0.0000, time 1680.54ms\n",
            "iter 362: loss 0.0000, time 1722.39ms\n",
            "iter 363: loss 0.0000, time 1882.65ms\n",
            "iter 364: loss 0.0001, time 1843.53ms\n",
            "iter 365: loss 0.0000, time 1756.20ms\n",
            "iter 366: loss 0.0000, time 1703.43ms\n",
            "iter 367: loss 0.0003, time 1678.31ms\n",
            "iter 368: loss 0.0000, time 1755.71ms\n",
            "iter 369: loss 0.0000, time 1669.71ms\n",
            "iter 370: loss 0.0000, time 1719.20ms\n",
            "iter 371: loss 0.0000, time 1694.66ms\n",
            "iter 372: loss 0.0000, time 1699.66ms\n",
            "iter 373: loss 0.0000, time 1735.22ms\n",
            "iter 374: loss 0.0000, time 1736.64ms\n",
            "iter 375: loss 0.0000, time 1707.30ms\n",
            "iter 376: loss 0.0000, time 1749.29ms\n",
            "iter 377: loss 0.0000, time 1668.92ms\n",
            "iter 378: loss 0.0000, time 1769.63ms\n",
            "iter 379: loss 0.0000, time 1663.63ms\n",
            "Step 380: train loss 0.1094, val loss 0.5096\n",
            "Validation accuracy: 0.9924\n",
            "iter 380: loss 0.0000, time 20154.13ms\n",
            "iter 381: loss 0.0000, time 1668.53ms\n",
            "iter 382: loss 0.0000, time 1641.63ms\n",
            "iter 383: loss 0.0000, time 1670.21ms\n",
            "iter 384: loss 0.0000, time 1645.52ms\n",
            "iter 385: loss 0.0000, time 1774.64ms\n",
            "iter 386: loss 0.0000, time 1739.50ms\n",
            "iter 387: loss 0.0000, time 1721.76ms\n",
            "iter 388: loss 0.0000, time 1651.05ms\n",
            "iter 389: loss 0.0000, time 1721.43ms\n",
            "iter 390: loss 0.0000, time 1804.27ms\n",
            "iter 391: loss 0.0000, time 1696.60ms\n",
            "iter 392: loss 0.0000, time 1662.59ms\n",
            "iter 393: loss 0.0000, time 1656.81ms\n",
            "iter 394: loss 0.0000, time 1714.06ms\n",
            "iter 395: loss 0.0000, time 1628.55ms\n",
            "iter 396: loss 0.0000, time 1713.59ms\n",
            "iter 397: loss 0.0000, time 1649.07ms\n",
            "iter 398: loss 0.0001, time 1652.03ms\n",
            "iter 399: loss 0.0000, time 1708.18ms\n",
            "Step 400: train loss 0.1042, val loss 0.4969\n",
            "Validation accuracy: 0.9903\n",
            "Test accuracy 0.6667\n",
            "iter 400: loss 0.0000, time 20199.33ms\n",
            "iter 401: loss 0.0000, time 1671.64ms\n",
            "iter 402: loss 0.0000, time 1627.41ms\n",
            "iter 403: loss 0.0000, time 1604.55ms\n",
            "iter 404: loss 0.4453, time 1641.99ms\n",
            "iter 405: loss 0.0000, time 1675.91ms\n",
            "iter 406: loss 0.0000, time 1605.37ms\n",
            "iter 407: loss 0.0000, time 1668.01ms\n",
            "iter 408: loss 0.0000, time 1603.08ms\n",
            "iter 409: loss 0.0000, time 1660.44ms\n",
            "iter 410: loss 0.0000, time 1633.00ms\n",
            "iter 411: loss 0.0000, time 1658.56ms\n",
            "iter 412: loss 0.0000, time 1861.45ms\n",
            "iter 413: loss 0.0000, time 1745.10ms\n",
            "iter 414: loss 0.0000, time 1660.40ms\n",
            "iter 415: loss 0.0000, time 1711.86ms\n",
            "iter 416: loss 0.0000, time 1658.29ms\n",
            "iter 417: loss 0.0000, time 1660.60ms\n",
            "iter 418: loss 0.0000, time 1638.73ms\n",
            "iter 419: loss 0.0000, time 1714.41ms\n",
            "Step 420: train loss 0.0997, val loss 0.5188\n",
            "Validation accuracy: 0.9870\n",
            "iter 420: loss 0.0000, time 19436.31ms\n",
            "iter 421: loss 0.0000, time 1665.62ms\n",
            "iter 422: loss 0.0020, time 1695.30ms\n",
            "iter 423: loss 0.0001, time 1663.27ms\n",
            "iter 424: loss 0.0000, time 1658.30ms\n",
            "iter 425: loss 0.0000, time 1760.19ms\n",
            "iter 426: loss 0.0000, time 1729.83ms\n",
            "iter 427: loss 0.0000, time 1619.40ms\n",
            "iter 428: loss 0.0000, time 1703.75ms\n",
            "iter 429: loss 0.0000, time 1719.67ms\n",
            "iter 430: loss 0.0000, time 1728.75ms\n",
            "iter 431: loss 0.0000, time 1671.16ms\n",
            "iter 432: loss 0.0000, time 1662.78ms\n",
            "iter 433: loss 0.0000, time 1691.13ms\n",
            "iter 434: loss 0.0000, time 1882.74ms\n",
            "iter 435: loss 0.0000, time 1810.39ms\n",
            "iter 436: loss 0.0000, time 1689.96ms\n",
            "iter 437: loss 0.0000, time 1725.49ms\n",
            "iter 438: loss 0.0000, time 1600.93ms\n",
            "iter 439: loss 0.0000, time 1639.35ms\n",
            "Step 440: train loss 0.0954, val loss 0.4953\n",
            "Validation accuracy: 0.9827\n",
            "iter 440: loss 0.0000, time 19216.02ms\n",
            "iter 441: loss 0.0000, time 1785.48ms\n",
            "iter 442: loss 0.0000, time 1610.30ms\n",
            "iter 443: loss 0.0000, time 1640.95ms\n",
            "iter 444: loss 0.0000, time 1699.85ms\n",
            "iter 445: loss 0.0000, time 1625.69ms\n",
            "iter 446: loss 0.0000, time 1719.13ms\n",
            "iter 447: loss 0.0000, time 1711.30ms\n",
            "iter 448: loss 0.0000, time 1598.31ms\n",
            "iter 449: loss 0.0000, time 1675.30ms\n",
            "iter 450: loss 0.0000, time 1842.56ms\n",
            "iter 451: loss 0.0000, time 1662.29ms\n",
            "iter 452: loss 0.0000, time 1717.89ms\n",
            "iter 453: loss 0.0000, time 1700.19ms\n",
            "iter 454: loss 0.0000, time 1651.42ms\n",
            "iter 455: loss 0.0000, time 1653.63ms\n",
            "iter 456: loss 0.0000, time 1635.75ms\n",
            "iter 457: loss 0.0000, time 1619.14ms\n",
            "iter 458: loss 0.0000, time 1593.27ms\n",
            "iter 459: loss 0.3770, time 1635.27ms\n",
            "Step 460: train loss 0.0913, val loss 0.4868\n",
            "Validation accuracy: 0.9849\n",
            "iter 460: loss 0.0000, time 18845.05ms\n",
            "iter 461: loss 0.0000, time 1641.42ms\n",
            "iter 462: loss 0.0000, time 1608.14ms\n",
            "iter 463: loss 0.0000, time 1621.56ms\n",
            "iter 464: loss 0.0000, time 1612.95ms\n",
            "iter 465: loss 0.0000, time 1720.30ms\n",
            "iter 466: loss 0.0000, time 1853.76ms\n",
            "iter 467: loss 0.0000, time 1725.04ms\n",
            "iter 468: loss 0.0000, time 1695.57ms\n",
            "iter 469: loss 0.0000, time 1847.41ms\n",
            "iter 470: loss 0.0062, time 1785.58ms\n",
            "iter 471: loss 0.0002, time 2029.21ms\n",
            "iter 472: loss 0.0043, time 1689.75ms\n",
            "iter 473: loss 0.0000, time 1676.71ms\n",
            "iter 474: loss 0.0000, time 1686.61ms\n",
            "iter 475: loss 0.0000, time 1597.81ms\n",
            "iter 476: loss 0.0000, time 1631.35ms\n",
            "iter 477: loss 0.0000, time 1621.98ms\n",
            "iter 478: loss 0.0000, time 1614.93ms\n",
            "iter 479: loss 0.0000, time 1710.27ms\n",
            "Step 480: train loss 0.0875, val loss 0.4941\n",
            "Validation accuracy: 0.9860\n",
            "iter 480: loss 0.0000, time 18866.27ms\n",
            "iter 481: loss 0.0000, time 1905.65ms\n",
            "iter 482: loss 0.0000, time 1600.15ms\n",
            "iter 483: loss 0.0004, time 1693.47ms\n",
            "iter 484: loss 0.0000, time 1668.57ms\n",
            "iter 485: loss 0.0000, time 1614.76ms\n",
            "iter 486: loss 0.0000, time 1740.48ms\n",
            "iter 487: loss 0.0000, time 1683.26ms\n",
            "iter 488: loss 0.0000, time 1588.41ms\n",
            "iter 489: loss 0.0009, time 1618.80ms\n",
            "iter 490: loss 0.0000, time 1700.62ms\n",
            "iter 491: loss 0.0000, time 1608.82ms\n",
            "iter 492: loss 0.0000, time 1608.61ms\n",
            "iter 493: loss 0.0000, time 1634.31ms\n",
            "iter 494: loss 0.0000, time 1632.70ms\n",
            "iter 495: loss 0.0000, time 1707.50ms\n",
            "iter 496: loss 0.0000, time 1643.26ms\n",
            "iter 497: loss 0.0000, time 1698.47ms\n",
            "iter 498: loss 0.0001, time 1630.38ms\n",
            "iter 499: loss 0.0000, time 1647.82ms\n",
            "Step 500: train loss 0.0840, val loss 0.5001\n",
            "Validation accuracy: 0.9870\n",
            "Test accuracy 0.8667\n",
            "iter 500: loss 0.0000, time 19604.76ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▇▇▅█</td></tr><tr><td>Test_F1_Score</td><td>▁▄▇▇▆█</td></tr><tr><td>Test_Precision</td><td>▁▄██▇█</td></tr><tr><td>Test_Recall</td><td>▁▅▇▇▅█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▇▇█▇█████████████████████</td></tr><tr><td>val/loss</td><td> ▂▂▄▅▆▅▅▅██▇▆▅▄▅▄▃▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.86667</td></tr><tr><td>Test_F1_Score</td><td>0.8647</td></tr><tr><td>Test_Precision</td><td>0.88636</td></tr><tr><td>Test_Recall</td><td>0.86667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.08403</td></tr><tr><td>val/acc</td><td>0.98704</td></tr><tr><td>val/loss</td><td>0.50009</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2328</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/a9ukmt8c</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_232848-a9ukmt8c\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bcov7r21 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_235341-bcov7r21</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21' target=\"_blank\">quiet-sweep-16</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2353\n",
            "Initializing from GPT2: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_24856\\3085892497.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Ignoring project 'DI725-Asg1-HyperParamTune' when running a sweep."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">quiet-sweep-16</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_235341-bcov7r21\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_Asg1_Repo2\\DI725_Assignment1\\DI725\\assignment_1\\wandb\\run-20250405_235349-bcov7r21</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21' target=\"_blank\">gpt2_hyper_2353</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/sweeps/zm57at65</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss nan, val loss nan\n",
            "Validation accuracy: 0.4241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "iter 0: loss 7.6562, time 20660.08ms\n",
            "iter 1: loss 0.1904, time 1662.22ms\n",
            "iter 2: loss 0.3359, time 1673.45ms\n",
            "iter 3: loss 3.0625, time 1649.08ms\n",
            "iter 4: loss 0.2891, time 1595.44ms\n",
            "iter 5: loss 0.4746, time 1569.09ms\n",
            "iter 6: loss 0.3223, time 1687.09ms\n",
            "iter 7: loss 0.2813, time 1599.89ms\n",
            "iter 8: loss 0.4180, time 1652.76ms\n",
            "iter 9: loss 0.3340, time 1618.24ms\n",
            "iter 10: loss 0.0266, time 1591.14ms\n",
            "iter 11: loss 0.2305, time 1643.35ms\n",
            "iter 12: loss 0.4180, time 1599.31ms\n",
            "iter 13: loss 0.5430, time 1604.42ms\n",
            "iter 14: loss 0.3203, time 1630.03ms\n",
            "iter 15: loss 2.5313, time 1591.00ms\n",
            "iter 16: loss 0.0933, time 1613.14ms\n",
            "iter 17: loss 0.0703, time 1655.10ms\n",
            "iter 18: loss 0.0559, time 1603.57ms\n",
            "iter 19: loss 0.0471, time 1653.75ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 20: train loss 1.0123, val loss 0.7044\n",
            "Validation accuracy: 0.8607\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 20: loss 0.2432, time 20940.58ms\n",
            "iter 21: loss 0.0471, time 1604.55ms\n",
            "iter 22: loss 0.0815, time 1623.84ms\n",
            "iter 23: loss 0.2578, time 1583.24ms\n",
            "iter 24: loss 0.2070, time 1650.92ms\n",
            "iter 25: loss 0.4941, time 1646.90ms\n",
            "iter 26: loss 0.0669, time 1771.81ms\n",
            "iter 27: loss 0.0762, time 1664.37ms\n",
            "iter 28: loss 0.0332, time 1644.09ms\n",
            "iter 29: loss 0.0031, time 1604.40ms\n",
            "iter 30: loss 0.0447, time 1600.51ms\n",
            "iter 31: loss 0.3594, time 1652.74ms\n",
            "iter 32: loss 0.3711, time 1619.31ms\n",
            "iter 33: loss 0.0479, time 1656.40ms\n",
            "iter 34: loss 0.4512, time 1661.02ms\n",
            "iter 35: loss 0.0146, time 1558.30ms\n",
            "iter 36: loss 4.2812, time 1738.95ms\n",
            "iter 37: loss 0.0025, time 1628.65ms\n",
            "iter 38: loss 0.7891, time 1563.74ms\n",
            "iter 39: loss 0.0874, time 1675.29ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 40: train loss 0.6748, val loss 0.6521\n",
            "Validation accuracy: 0.9212\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 40: loss 0.3809, time 21180.15ms\n",
            "iter 41: loss 0.0305, time 1691.65ms\n",
            "iter 42: loss 1.2734, time 1582.98ms\n",
            "iter 43: loss 0.1660, time 1648.28ms\n",
            "iter 44: loss 0.0122, time 1628.78ms\n",
            "iter 45: loss 2.1406, time 1590.31ms\n",
            "iter 46: loss 0.1523, time 1652.82ms\n",
            "iter 47: loss 0.0342, time 1640.47ms\n",
            "iter 48: loss 0.0791, time 1607.40ms\n",
            "iter 49: loss 0.1816, time 1586.09ms\n",
            "iter 50: loss 0.0400, time 1625.26ms\n",
            "iter 51: loss 0.0542, time 1671.08ms\n",
            "iter 52: loss 0.0020, time 1633.29ms\n",
            "iter 53: loss 4.0938, time 1580.93ms\n",
            "iter 54: loss 0.0011, time 1615.75ms\n",
            "iter 55: loss 0.8164, time 1650.09ms\n",
            "iter 56: loss 0.0038, time 1661.31ms\n",
            "iter 57: loss 0.0347, time 1607.54ms\n",
            "iter 58: loss 0.0164, time 1594.29ms\n",
            "iter 59: loss 0.0074, time 1624.96ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 60: train loss 0.5247, val loss 0.6044\n",
            "Validation accuracy: 0.8823\n",
            "iter 60: loss 0.0020, time 18566.45ms\n",
            "iter 61: loss 0.1875, time 1592.70ms\n",
            "iter 62: loss 0.0039, time 1645.41ms\n",
            "iter 63: loss 0.0248, time 1583.07ms\n",
            "iter 64: loss 0.0008, time 1660.67ms\n",
            "iter 65: loss 0.0171, time 1660.39ms\n",
            "iter 66: loss 0.0415, time 1572.77ms\n",
            "iter 67: loss 0.0016, time 1625.58ms\n",
            "iter 68: loss 0.0017, time 1626.10ms\n",
            "iter 69: loss 0.0038, time 1628.72ms\n",
            "iter 70: loss 0.0008, time 1572.07ms\n",
            "iter 71: loss 0.0613, time 1596.71ms\n",
            "iter 72: loss 2.5781, time 1670.93ms\n",
            "iter 73: loss 0.0012, time 1593.59ms\n",
            "iter 74: loss 0.0167, time 1578.76ms\n",
            "iter 75: loss 0.0405, time 1612.30ms\n",
            "iter 76: loss 0.0075, time 1646.41ms\n",
            "iter 77: loss 0.4434, time 1637.97ms\n",
            "iter 78: loss 0.0128, time 1811.39ms\n",
            "iter 79: loss 0.0220, time 1665.39ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 80: train loss 0.4291, val loss 0.5531\n",
            "Validation accuracy: 0.9546\n",
            "iter 80: loss 0.0020, time 18616.73ms\n",
            "iter 81: loss 1.8828, time 1643.11ms\n",
            "iter 82: loss 0.0016, time 1588.81ms\n",
            "iter 83: loss 0.0036, time 1624.30ms\n",
            "iter 84: loss 0.0015, time 1655.09ms\n",
            "iter 85: loss 0.0006, time 1596.83ms\n",
            "iter 86: loss 0.0004, time 1607.97ms\n",
            "iter 87: loss 0.4883, time 1638.65ms\n",
            "iter 88: loss 0.0408, time 1566.74ms\n",
            "iter 89: loss 0.0011, time 1656.29ms\n",
            "iter 90: loss 0.0172, time 1675.66ms\n",
            "iter 91: loss 0.0005, time 1612.11ms\n",
            "iter 92: loss 0.0010, time 1613.51ms\n",
            "iter 93: loss 0.0006, time 1604.10ms\n",
            "iter 94: loss 0.0038, time 1573.12ms\n",
            "iter 95: loss 0.0001, time 1624.71ms\n",
            "iter 96: loss 0.0010, time 1570.24ms\n",
            "iter 97: loss 0.0004, time 1677.07ms\n",
            "iter 98: loss 0.0003, time 1579.10ms\n",
            "iter 99: loss 0.0001, time 1633.78ms\n",
            "Step 100: train loss 0.3555, val loss 0.5557\n",
            "Validation accuracy: 0.9438\n",
            "Test accuracy 0.6667\n",
            "iter 100: loss 0.1416, time 19684.34ms\n",
            "iter 101: loss 0.0001, time 1608.48ms\n",
            "iter 102: loss 0.0001, time 1614.51ms\n",
            "iter 103: loss 0.0000, time 1632.46ms\n",
            "iter 104: loss 0.0000, time 1594.58ms\n",
            "iter 105: loss 0.0003, time 1635.42ms\n",
            "iter 106: loss 0.0000, time 1619.20ms\n",
            "iter 107: loss 0.0002, time 1649.40ms\n",
            "iter 108: loss 0.0002, time 1596.16ms\n",
            "iter 109: loss 0.0000, time 1661.75ms\n",
            "iter 110: loss 0.0001, time 1664.97ms\n",
            "iter 111: loss 0.0002, time 1567.11ms\n",
            "iter 112: loss 0.0001, time 1597.67ms\n",
            "iter 113: loss 0.0001, time 1636.32ms\n",
            "iter 114: loss 0.0001, time 1586.08ms\n",
            "iter 115: loss 0.0020, time 1622.81ms\n",
            "iter 116: loss 0.0000, time 1662.51ms\n",
            "iter 117: loss 0.0145, time 1648.50ms\n",
            "iter 118: loss 0.0004, time 1616.76ms\n",
            "iter 119: loss 0.0001, time 1719.80ms\n",
            "Step 120: train loss 0.3027, val loss 0.5631\n",
            "Validation accuracy: 0.9860\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 120: loss 0.2236, time 21618.94ms\n",
            "iter 121: loss 0.0004, time 1659.63ms\n",
            "iter 122: loss 0.0001, time 1682.27ms\n",
            "iter 123: loss 0.0075, time 1589.87ms\n",
            "iter 124: loss 0.0000, time 1623.14ms\n",
            "iter 125: loss 0.0003, time 1621.48ms\n",
            "iter 126: loss 0.0002, time 1667.34ms\n",
            "iter 127: loss 0.0000, time 1594.48ms\n",
            "iter 128: loss 0.0001, time 1610.53ms\n",
            "iter 129: loss 0.0000, time 1660.86ms\n",
            "iter 130: loss 0.0000, time 1600.10ms\n",
            "iter 131: loss 0.0004, time 1608.25ms\n",
            "iter 132: loss 0.0000, time 1637.83ms\n",
            "iter 133: loss 0.0002, time 1592.75ms\n",
            "iter 134: loss 0.0020, time 1619.31ms\n",
            "iter 135: loss 0.0001, time 1621.42ms\n",
            "iter 136: loss 0.0042, time 1609.38ms\n",
            "iter 137: loss 0.0003, time 1639.34ms\n",
            "iter 138: loss 0.0000, time 1585.06ms\n",
            "iter 139: loss 0.0000, time 1631.71ms\n",
            "Step 140: train loss 0.2629, val loss 0.5467\n",
            "Validation accuracy: 0.9784\n",
            "iter 140: loss 0.0002, time 18635.98ms\n",
            "iter 141: loss 0.0002, time 1604.08ms\n",
            "iter 142: loss 0.0001, time 1703.93ms\n",
            "iter 143: loss 0.0078, time 1641.04ms\n",
            "iter 144: loss 0.0002, time 1637.28ms\n",
            "iter 145: loss 0.0000, time 1635.59ms\n",
            "iter 146: loss 0.0002, time 1616.64ms\n",
            "iter 147: loss 0.0000, time 1608.54ms\n",
            "iter 148: loss 0.0000, time 1651.22ms\n",
            "iter 149: loss 0.0000, time 1646.42ms\n",
            "iter 150: loss 0.0000, time 1635.35ms\n",
            "iter 151: loss 0.0000, time 1619.51ms\n",
            "iter 152: loss 0.0000, time 1561.85ms\n",
            "iter 153: loss 0.0000, time 1627.94ms\n",
            "iter 154: loss 0.0050, time 1598.64ms\n",
            "iter 155: loss 0.1162, time 1587.48ms\n",
            "iter 156: loss 0.0000, time 1613.12ms\n",
            "iter 157: loss 0.0001, time 1612.09ms\n",
            "iter 158: loss 0.0000, time 1648.74ms\n",
            "iter 159: loss 0.0000, time 1585.45ms\n",
            "Step 160: train loss 0.2316, val loss 0.5589\n",
            "Validation accuracy: 0.9806\n",
            "iter 160: loss 0.0000, time 18617.67ms\n",
            "iter 161: loss 0.0002, time 1680.70ms\n",
            "iter 162: loss 0.0000, time 1589.19ms\n",
            "iter 163: loss 0.0000, time 1597.11ms\n",
            "iter 164: loss 0.0098, time 1621.65ms\n",
            "iter 165: loss 0.0002, time 1611.27ms\n",
            "iter 166: loss 0.0000, time 1657.93ms\n",
            "iter 167: loss 0.0000, time 1641.06ms\n",
            "iter 168: loss 0.0000, time 1571.01ms\n",
            "iter 169: loss 0.0035, time 1597.87ms\n",
            "iter 170: loss 0.0001, time 1616.94ms\n",
            "iter 171: loss 0.0000, time 1710.96ms\n",
            "iter 172: loss 0.0000, time 1641.96ms\n",
            "iter 173: loss 0.0001, time 1557.31ms\n",
            "iter 174: loss 0.0000, time 1633.23ms\n",
            "iter 175: loss 0.0002, time 1685.15ms\n",
            "iter 176: loss 0.0000, time 1585.64ms\n",
            "iter 177: loss 0.0000, time 1588.81ms\n",
            "iter 178: loss 0.0006, time 1633.31ms\n",
            "iter 179: loss 0.0000, time 1647.44ms\n",
            "Step 180: train loss 0.2090, val loss 0.5871\n",
            "Validation accuracy: 0.9881\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 180: loss 0.0000, time 20665.59ms\n",
            "iter 181: loss 0.0004, time 1599.13ms\n",
            "iter 182: loss 0.0000, time 1654.67ms\n",
            "iter 183: loss 0.0000, time 1627.47ms\n",
            "iter 184: loss 0.0001, time 1629.85ms\n",
            "iter 185: loss 0.0000, time 1672.63ms\n",
            "iter 186: loss 0.0000, time 1608.07ms\n",
            "iter 187: loss 0.0000, time 1587.49ms\n",
            "iter 188: loss 0.0000, time 1638.05ms\n",
            "iter 189: loss 0.0000, time 1589.26ms\n",
            "iter 190: loss 0.0000, time 1595.92ms\n",
            "iter 191: loss 0.0000, time 1633.35ms\n",
            "iter 192: loss 0.0000, time 1599.52ms\n",
            "iter 193: loss 0.0000, time 1643.73ms\n",
            "iter 194: loss 0.0000, time 1620.55ms\n",
            "iter 195: loss 0.0000, time 1654.45ms\n",
            "iter 196: loss 0.0000, time 1667.86ms\n",
            "iter 197: loss 0.0000, time 1615.24ms\n",
            "iter 198: loss 0.0000, time 1652.57ms\n",
            "iter 199: loss 0.0000, time 1628.70ms\n",
            "Step 200: train loss 0.1883, val loss 0.6136\n",
            "Validation accuracy: 0.9903\n",
            "Test accuracy 0.8000\n",
            "iter 200: loss 0.0000, time 19598.67ms\n",
            "iter 201: loss 0.0001, time 1650.39ms\n",
            "iter 202: loss 0.0000, time 1571.91ms\n",
            "iter 203: loss 0.0000, time 1606.10ms\n",
            "iter 204: loss 0.0000, time 1629.11ms\n",
            "iter 205: loss 0.0000, time 1657.57ms\n",
            "iter 206: loss 0.0000, time 1643.13ms\n",
            "iter 207: loss 0.0000, time 1621.55ms\n",
            "iter 208: loss 0.0000, time 1638.28ms\n",
            "iter 209: loss 0.0000, time 1630.89ms\n",
            "iter 210: loss 0.0002, time 1583.27ms\n",
            "iter 211: loss 0.0000, time 1679.24ms\n",
            "iter 212: loss 0.0000, time 1633.79ms\n",
            "iter 213: loss 0.0001, time 1571.89ms\n",
            "iter 214: loss 0.0000, time 1609.22ms\n",
            "iter 215: loss 0.0000, time 1661.58ms\n",
            "iter 216: loss 0.0002, time 1584.06ms\n",
            "iter 217: loss 0.0000, time 1614.08ms\n",
            "iter 218: loss 0.0001, time 1592.79ms\n",
            "iter 219: loss 0.0000, time 1634.82ms\n",
            "Step 220: train loss 0.1718, val loss 0.6010\n",
            "Validation accuracy: 0.9460\n",
            "iter 220: loss 0.0002, time 18560.19ms\n",
            "iter 221: loss 0.0000, time 1596.57ms\n",
            "iter 222: loss 0.0000, time 1600.98ms\n",
            "iter 223: loss 0.0000, time 1560.40ms\n",
            "iter 224: loss 0.0000, time 1638.08ms\n",
            "iter 225: loss 0.0000, time 1633.66ms\n",
            "iter 226: loss 0.0000, time 1615.20ms\n",
            "iter 227: loss 0.0000, time 1583.95ms\n",
            "iter 228: loss 0.0000, time 1590.03ms\n",
            "iter 229: loss 0.0000, time 1636.42ms\n",
            "iter 230: loss 0.0000, time 1596.57ms\n",
            "iter 231: loss 0.0503, time 1553.66ms\n",
            "iter 232: loss 0.0000, time 1626.62ms\n",
            "iter 233: loss 0.0001, time 1584.71ms\n",
            "iter 234: loss 0.0000, time 1601.33ms\n",
            "iter 235: loss 0.0000, time 1682.81ms\n",
            "iter 236: loss 0.0000, time 1599.79ms\n",
            "iter 237: loss 0.0003, time 1593.00ms\n",
            "iter 238: loss 0.0000, time 1585.72ms\n",
            "iter 239: loss 0.0006, time 1595.28ms\n",
            "Step 240: train loss 0.1601, val loss 0.6424\n",
            "Validation accuracy: 0.9914\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 240: loss 0.0000, time 20948.16ms\n",
            "iter 241: loss 0.0002, time 1729.90ms\n",
            "iter 242: loss 0.0007, time 1590.81ms\n",
            "iter 243: loss 0.0000, time 1656.01ms\n",
            "iter 244: loss 0.0000, time 1582.27ms\n",
            "iter 245: loss 0.0003, time 1586.30ms\n",
            "iter 246: loss 0.0000, time 1605.00ms\n",
            "iter 247: loss 0.0000, time 1626.10ms\n",
            "iter 248: loss 0.0000, time 1579.75ms\n",
            "iter 249: loss 0.0000, time 1634.87ms\n",
            "iter 250: loss 0.0000, time 1627.60ms\n",
            "iter 251: loss 0.0000, time 1630.54ms\n",
            "iter 252: loss 0.0000, time 1588.37ms\n",
            "iter 253: loss 0.0000, time 1669.40ms\n",
            "iter 254: loss 0.0000, time 1695.65ms\n",
            "iter 255: loss 0.0001, time 1575.46ms\n",
            "iter 256: loss 0.0000, time 1636.50ms\n",
            "iter 257: loss 0.0000, time 1635.28ms\n",
            "iter 258: loss 0.0000, time 1593.69ms\n",
            "iter 259: loss 0.0008, time 1711.03ms\n",
            "Step 260: train loss 0.1482, val loss 0.6486\n",
            "Validation accuracy: 0.9935\n",
            "iter 260: loss 0.0000, time 19582.93ms\n",
            "iter 261: loss 0.0000, time 1607.70ms\n",
            "iter 262: loss 0.0000, time 1661.09ms\n",
            "iter 263: loss 0.0000, time 1586.33ms\n",
            "iter 264: loss 0.0006, time 1640.66ms\n",
            "iter 265: loss 0.0000, time 1598.01ms\n",
            "iter 266: loss 0.0000, time 1604.29ms\n",
            "iter 267: loss 0.0000, time 1658.73ms\n",
            "iter 268: loss 0.0000, time 1621.29ms\n",
            "iter 269: loss 0.0000, time 1584.20ms\n",
            "iter 270: loss 0.0000, time 1673.93ms\n",
            "iter 271: loss 0.0000, time 1594.08ms\n",
            "iter 272: loss 0.0000, time 1652.55ms\n",
            "iter 273: loss 0.0000, time 1641.54ms\n",
            "iter 274: loss 0.0000, time 1607.24ms\n",
            "iter 275: loss 0.0000, time 1690.85ms\n",
            "iter 276: loss 0.0000, time 1631.05ms\n",
            "iter 277: loss 0.0000, time 1597.18ms\n",
            "iter 278: loss 0.0001, time 1626.17ms\n",
            "iter 279: loss 0.0000, time 1566.24ms\n",
            "Step 280: train loss 0.1379, val loss 0.6430\n",
            "Validation accuracy: 0.9892\n",
            "iter 280: loss 0.0000, time 18573.72ms\n",
            "iter 281: loss 0.0000, time 1600.97ms\n",
            "iter 282: loss 0.0000, time 1596.33ms\n",
            "iter 283: loss 0.0000, time 1608.93ms\n",
            "iter 284: loss 0.0000, time 1597.73ms\n",
            "iter 285: loss 0.0000, time 1650.93ms\n",
            "iter 286: loss 0.0000, time 1600.63ms\n",
            "iter 287: loss 0.0000, time 1667.29ms\n",
            "iter 288: loss 0.0000, time 1652.70ms\n",
            "iter 289: loss 0.0000, time 1735.93ms\n",
            "iter 290: loss 0.0000, time 1596.24ms\n",
            "iter 291: loss 0.0000, time 1679.16ms\n",
            "iter 292: loss 0.0000, time 1625.45ms\n",
            "iter 293: loss 0.0000, time 1647.05ms\n",
            "iter 294: loss 0.0000, time 1680.90ms\n",
            "iter 295: loss 0.0000, time 1629.25ms\n",
            "iter 296: loss 0.0000, time 1597.25ms\n",
            "iter 297: loss 0.0000, time 1616.25ms\n",
            "iter 298: loss 0.0000, time 1587.47ms\n",
            "iter 299: loss 0.0000, time 1628.15ms\n",
            "Step 300: train loss 0.1290, val loss 0.6481\n",
            "Validation accuracy: 0.9881\n",
            "Test accuracy 0.7667\n",
            "iter 300: loss 0.0000, time 19735.84ms\n",
            "iter 301: loss 0.0007, time 1741.93ms\n",
            "iter 302: loss 0.0000, time 1834.02ms\n",
            "iter 303: loss 0.0000, time 1693.25ms\n",
            "iter 304: loss 0.0000, time 1740.26ms\n",
            "iter 305: loss 0.0000, time 1916.14ms\n",
            "iter 306: loss 0.0000, time 1850.94ms\n",
            "iter 307: loss 0.0002, time 1864.63ms\n",
            "iter 308: loss 0.0000, time 1560.25ms\n",
            "iter 309: loss 0.0000, time 1244.38ms\n",
            "iter 310: loss 0.0000, time 1361.55ms\n",
            "iter 311: loss 0.0000, time 1245.33ms\n",
            "iter 312: loss 0.0000, time 1298.51ms\n",
            "iter 313: loss 0.0000, time 1265.10ms\n",
            "iter 314: loss 0.0003, time 1223.26ms\n",
            "iter 315: loss 0.0000, time 1231.97ms\n",
            "iter 316: loss 0.0000, time 1281.58ms\n",
            "iter 317: loss 0.0000, time 1228.13ms\n",
            "iter 318: loss 0.0000, time 1265.67ms\n",
            "iter 319: loss 0.0000, time 1262.04ms\n",
            "Step 320: train loss 0.1211, val loss 0.6460\n",
            "Validation accuracy: 0.9914\n",
            "iter 320: loss 0.0000, time 14129.63ms\n",
            "iter 321: loss 0.0000, time 1392.09ms\n",
            "iter 322: loss 0.0000, time 1261.71ms\n",
            "iter 323: loss 0.0000, time 1675.23ms\n",
            "iter 324: loss 0.0000, time 1894.84ms\n",
            "iter 325: loss 0.0000, time 1824.46ms\n",
            "iter 326: loss 0.0000, time 1844.07ms\n",
            "iter 327: loss 0.0000, time 1866.69ms\n",
            "iter 328: loss 0.0000, time 1879.16ms\n",
            "iter 329: loss 0.0000, time 1849.77ms\n",
            "iter 330: loss 0.0000, time 1761.16ms\n",
            "iter 331: loss 0.0000, time 1858.49ms\n",
            "iter 332: loss 0.0000, time 1939.10ms\n",
            "iter 333: loss 0.0000, time 1837.67ms\n",
            "iter 334: loss 0.0000, time 1807.69ms\n",
            "iter 335: loss 0.0000, time 1825.22ms\n",
            "iter 336: loss 0.0000, time 1777.88ms\n",
            "iter 337: loss 0.0000, time 1840.90ms\n",
            "iter 338: loss 0.0000, time 1887.74ms\n",
            "iter 339: loss 0.0002, time 1876.35ms\n",
            "Step 340: train loss 0.1140, val loss 0.6616\n",
            "Validation accuracy: 0.9892\n",
            "iter 340: loss 0.0001, time 22123.75ms\n",
            "iter 341: loss 0.0000, time 1788.29ms\n",
            "iter 342: loss 0.0000, time 1799.04ms\n",
            "iter 343: loss 0.0002, time 1841.60ms\n",
            "iter 344: loss 0.0000, time 1838.75ms\n",
            "iter 345: loss 0.0000, time 1807.03ms\n",
            "iter 346: loss 0.0000, time 1793.19ms\n",
            "iter 347: loss 0.0000, time 1775.27ms\n",
            "iter 348: loss 0.0000, time 2009.98ms\n",
            "iter 349: loss 0.0000, time 2031.30ms\n",
            "iter 350: loss 0.0001, time 1743.32ms\n",
            "iter 351: loss 0.0000, time 1821.01ms\n",
            "iter 352: loss 0.0000, time 1223.09ms\n",
            "iter 353: loss 0.0000, time 1316.01ms\n",
            "iter 354: loss 0.0000, time 1350.64ms\n",
            "iter 355: loss 0.0042, time 1245.65ms\n",
            "iter 356: loss 0.0000, time 1218.48ms\n",
            "iter 357: loss 0.0000, time 1216.51ms\n",
            "iter 358: loss 0.0000, time 1177.93ms\n",
            "iter 359: loss 0.0000, time 1205.65ms\n",
            "Step 360: train loss 0.1078, val loss 0.6892\n",
            "Validation accuracy: 0.9860\n",
            "iter 360: loss 0.0000, time 14273.08ms\n",
            "iter 361: loss 0.0000, time 1297.08ms\n",
            "iter 362: loss 0.0000, time 1264.03ms\n",
            "iter 363: loss 0.0000, time 1348.42ms\n",
            "iter 364: loss 0.0000, time 1371.43ms\n",
            "iter 365: loss 0.0000, time 1433.09ms\n",
            "iter 366: loss 0.0000, time 1515.68ms\n",
            "iter 367: loss 0.0009, time 1885.76ms\n",
            "iter 368: loss 0.0000, time 1813.34ms\n",
            "iter 369: loss 0.0000, time 1890.74ms\n",
            "iter 370: loss 0.0000, time 2243.45ms\n",
            "iter 371: loss 0.0000, time 1927.00ms\n",
            "iter 372: loss 0.0000, time 1857.52ms\n",
            "iter 373: loss 0.0000, time 1857.43ms\n",
            "iter 374: loss 0.0000, time 1865.32ms\n",
            "iter 375: loss 0.0000, time 1817.14ms\n",
            "iter 376: loss 0.0000, time 1815.66ms\n",
            "iter 377: loss 0.0000, time 1830.88ms\n",
            "iter 378: loss 0.0000, time 1805.98ms\n",
            "iter 379: loss 0.0000, time 1806.05ms\n",
            "Step 380: train loss 0.1030, val loss 0.7093\n",
            "Validation accuracy: 0.9838\n",
            "iter 380: loss 0.0000, time 21028.85ms\n",
            "iter 381: loss 0.0000, time 1825.45ms\n",
            "iter 382: loss 0.0000, time 1812.40ms\n",
            "iter 383: loss 0.0000, time 1816.64ms\n",
            "iter 384: loss 0.0000, time 1868.33ms\n",
            "iter 385: loss 0.0001, time 1816.27ms\n",
            "iter 386: loss 0.0000, time 1808.83ms\n",
            "iter 387: loss 0.0000, time 1841.03ms\n",
            "iter 388: loss 0.0002, time 1925.51ms\n",
            "iter 389: loss 0.0000, time 1826.95ms\n",
            "iter 390: loss 0.0001, time 1825.50ms\n",
            "iter 391: loss 0.0000, time 1832.12ms\n",
            "iter 392: loss 0.0320, time 1838.94ms\n",
            "iter 393: loss 0.0000, time 1809.19ms\n",
            "iter 394: loss 0.0000, time 1823.40ms\n",
            "iter 395: loss 0.0000, time 1951.56ms\n",
            "iter 396: loss 0.0000, time 1835.84ms\n",
            "iter 397: loss 0.0000, time 1840.03ms\n",
            "iter 398: loss 0.0000, time 1836.22ms\n",
            "iter 399: loss 0.0000, time 1834.19ms\n",
            "Step 400: train loss 0.1000, val loss 0.7396\n",
            "Validation accuracy: 0.9870\n",
            "Test accuracy 0.7000\n",
            "iter 400: loss 0.0001, time 22112.66ms\n",
            "iter 401: loss 0.0000, time 1821.15ms\n",
            "iter 402: loss 0.0000, time 1828.18ms\n",
            "iter 403: loss 0.0000, time 1811.11ms\n",
            "iter 404: loss 0.0000, time 1812.02ms\n",
            "iter 405: loss 0.0000, time 1849.16ms\n",
            "iter 406: loss 0.0035, time 1986.59ms\n",
            "iter 407: loss 0.0000, time 1866.54ms\n",
            "iter 408: loss 0.0000, time 1808.19ms\n",
            "iter 409: loss 0.0000, time 1828.70ms\n",
            "iter 410: loss 0.0000, time 1870.14ms\n",
            "iter 411: loss 0.0000, time 1808.70ms\n",
            "iter 412: loss 0.0000, time 1832.12ms\n",
            "iter 413: loss 0.0000, time 1815.11ms\n",
            "iter 414: loss 0.0000, time 1865.28ms\n",
            "iter 415: loss 0.0000, time 1852.41ms\n",
            "iter 416: loss 0.0000, time 1812.61ms\n",
            "iter 417: loss 0.0000, time 1813.03ms\n",
            "iter 418: loss 0.0188, time 1832.67ms\n",
            "iter 419: loss 0.0000, time 1809.78ms\n",
            "Step 420: train loss 0.0961, val loss 0.7282\n",
            "Validation accuracy: 0.9849\n",
            "iter 420: loss 0.0000, time 20773.72ms\n",
            "iter 421: loss 0.0000, time 1836.44ms\n",
            "iter 422: loss 0.0006, time 1813.33ms\n",
            "iter 423: loss 0.0000, time 1944.69ms\n",
            "iter 424: loss 0.0000, time 1814.75ms\n",
            "iter 425: loss 0.0001, time 1815.67ms\n",
            "iter 426: loss 0.0000, time 1818.49ms\n",
            "iter 427: loss 0.0000, time 1821.61ms\n",
            "iter 428: loss 0.0000, time 1858.11ms\n",
            "iter 429: loss 0.0000, time 1827.77ms\n",
            "iter 430: loss 0.0000, time 1809.25ms\n",
            "iter 431: loss 0.0000, time 1782.99ms\n",
            "iter 432: loss 0.0000, time 1892.00ms\n",
            "iter 433: loss 0.0000, time 1793.11ms\n",
            "iter 434: loss 0.0000, time 1823.12ms\n",
            "iter 435: loss 0.0000, time 1813.93ms\n",
            "iter 436: loss 0.0000, time 1800.54ms\n",
            "iter 437: loss 0.0000, time 1802.12ms\n",
            "iter 438: loss 0.0000, time 1826.42ms\n",
            "iter 439: loss 0.0000, time 1822.09ms\n",
            "Step 440: train loss 0.0924, val loss 0.7292\n",
            "Validation accuracy: 0.9870\n",
            "iter 440: loss 0.0039, time 20763.04ms\n",
            "iter 441: loss 0.0000, time 1837.02ms\n",
            "iter 442: loss 0.0006, time 1920.43ms\n",
            "iter 443: loss 0.0000, time 1795.34ms\n",
            "iter 444: loss 0.0000, time 1804.34ms\n",
            "iter 445: loss 0.0000, time 1784.38ms\n",
            "iter 446: loss 0.0000, time 1826.58ms\n",
            "iter 447: loss 0.0033, time 1801.14ms\n",
            "iter 448: loss 0.0006, time 1827.63ms\n",
            "iter 449: loss 0.0000, time 1817.21ms\n",
            "iter 450: loss 0.0000, time 1839.78ms\n",
            "iter 451: loss 0.0000, time 1824.74ms\n",
            "iter 452: loss 0.0000, time 1828.91ms\n",
            "iter 453: loss 0.0000, time 1820.11ms\n",
            "iter 454: loss 0.0000, time 1815.70ms\n",
            "iter 455: loss 0.0000, time 1877.92ms\n",
            "iter 456: loss 0.0000, time 1802.36ms\n",
            "iter 457: loss 0.0000, time 1819.24ms\n",
            "iter 458: loss 0.0000, time 1917.68ms\n",
            "iter 459: loss 0.0014, time 1859.39ms\n",
            "Step 460: train loss 0.0889, val loss 0.7382\n",
            "Validation accuracy: 0.9849\n",
            "iter 460: loss 0.0000, time 20835.28ms\n",
            "iter 461: loss 0.0000, time 1855.97ms\n",
            "iter 462: loss 0.0000, time 1860.27ms\n",
            "iter 463: loss 0.0000, time 1807.79ms\n",
            "iter 464: loss 0.0000, time 1864.04ms\n",
            "iter 465: loss 0.0000, time 1809.34ms\n",
            "iter 466: loss 0.0000, time 1828.73ms\n",
            "iter 467: loss 0.0000, time 1870.27ms\n",
            "iter 468: loss 0.0000, time 1802.81ms\n",
            "iter 469: loss 0.0000, time 1808.29ms\n",
            "iter 470: loss 0.0001, time 1813.28ms\n",
            "iter 471: loss 0.0083, time 1852.42ms\n",
            "iter 472: loss 0.0002, time 1854.42ms\n",
            "iter 473: loss 0.0000, time 1880.42ms\n",
            "iter 474: loss 0.0001, time 1828.82ms\n",
            "iter 475: loss 0.0000, time 1808.02ms\n",
            "iter 476: loss 0.0000, time 1847.61ms\n",
            "iter 477: loss 0.0000, time 1896.60ms\n",
            "iter 478: loss 0.0000, time 1793.39ms\n",
            "iter 479: loss 0.0000, time 1830.18ms\n",
            "Step 480: train loss 0.0854, val loss 0.7302\n",
            "Validation accuracy: 0.9914\n",
            "Saving checkpoint to out/ckpt.pt\n",
            "iter 480: loss 0.0000, time 22897.87ms\n",
            "iter 481: loss 0.0000, time 1859.13ms\n",
            "iter 482: loss 0.0000, time 1876.89ms\n",
            "iter 483: loss 0.0000, time 1805.93ms\n",
            "iter 484: loss 0.0000, time 1794.61ms\n",
            "iter 485: loss 0.0000, time 1814.80ms\n",
            "iter 486: loss 0.0000, time 1797.54ms\n",
            "iter 487: loss 0.0000, time 1788.37ms\n",
            "iter 488: loss 0.0000, time 1806.40ms\n",
            "iter 489: loss 0.0000, time 1822.62ms\n",
            "iter 490: loss 0.0000, time 1902.74ms\n",
            "iter 491: loss 0.0000, time 1831.48ms\n",
            "iter 492: loss 0.0000, time 1802.79ms\n",
            "iter 493: loss 0.0000, time 1805.28ms\n",
            "iter 494: loss 0.0000, time 1845.17ms\n",
            "iter 495: loss 0.0000, time 1817.44ms\n",
            "iter 496: loss 0.0000, time 1814.27ms\n",
            "iter 497: loss 0.0000, time 1813.03ms\n",
            "iter 498: loss 0.0001, time 1812.77ms\n",
            "iter 499: loss 0.0000, time 1831.08ms\n",
            "Step 500: train loss 0.0824, val loss 0.7188\n",
            "Validation accuracy: 0.9881\n",
            "Test accuracy 0.9000\n",
            "iter 500: loss 0.0000, time 21681.09ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▇▆▆█</td></tr><tr><td>Test_F1_Score</td><td>▁▆▇▇▆█</td></tr><tr><td>Test_Precision</td><td>▁▆█▇▇█</td></tr><tr><td>Test_Recall</td><td>▁▅▇▆▆█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▇█▇█████▇██████████████</td></tr><tr><td>val/loss</td><td> ▇▅▃▁▁▂▁▁▂▃▃▄▅▄▅▅▅▆▇█████▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.9</td></tr><tr><td>Test_F1_Score</td><td>0.89975</td></tr><tr><td>Test_Precision</td><td>0.90236</td></tr><tr><td>Test_Recall</td><td>0.9</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.08242</td></tr><tr><td>val/acc</td><td>0.98812</td></tr><tr><td>val/loss</td><td>0.71879</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt2_hyper_2353</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune/runs/bcov7r21</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-Asg1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250405_235349-bcov7r21\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ],
      "source": [
        "wandb_project = 'DI725-Asg1-HyperParamTune'\n",
        "\n",
        "# Create a sweep and run the agent.\n",
        "sweep_id = wandb.sweep(sweep_config, project=wandb_project)\n",
        "wandb.agent(sweep_id, hyper_train_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
