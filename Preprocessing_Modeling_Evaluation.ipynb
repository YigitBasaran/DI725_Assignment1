{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.2.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_jayPLY5tIK",
        "outputId": "b31910ea-8ce3-4a21-83ff-4c34bf53abad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: transformers in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.0)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: wandb in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.8)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.25.0)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.11.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (65.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (2.2.4)\n",
            "Requirement already satisfied: packaging>17.1 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (0.14.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.5.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2024.12.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (2.2.4)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn) (1.15.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nlpaug in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.11)\n",
            "Requirement already satisfied: numpy in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: nltk in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (2.2.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: click in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.18.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
            "Requirement already satisfied: colorama in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.13.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (65.5.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (4.13.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.25.0)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (2.11.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\ali\\appdata\\roaming\\python\\python310\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ali\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Installing dependencies for training and augmentation, preprocessing\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
        "!pip install torchmetrics\n",
        "!pip install imbalanced-learn\n",
        "!pip install nlpaug numpy nltk\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "!pip install -U wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERlsOGYx1zvl",
        "outputId": "f7dbeea1-5f9a-4a20-969c-cf6dd59d01e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DI725' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/caglarmert/DI725.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K_BTLxD12Q5",
        "outputId": "40d45891-8f3e-4e04-cc97-c8dd3c079d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd DI725/assignment_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c3_KsHth1MuX"
      },
      "outputs": [],
      "source": [
        "# Creating GPT sentiment class to convert decoder only GPT2 model to sentiment analysis model\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from model import GPT\n",
        "class GPTSentiment(GPT):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        # We will add 3 classes for positive,neural and negative\n",
        "        self.sentiment_head = nn.Linear(config.n_embd, 3)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        # Assertion for block size (exists in original repo)\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        x = x.mean(dim=1)  # Take the mean embedding of the tokens\n",
        "        logits = self.sentiment_head(x)  # Geting logits for 3-class classification\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, targets)\n",
        "\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6wWP5Ke1PG7",
        "outputId": "14bbda0a-15a6-40a1-fc0d-7e1b4c0321d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "import nlpaug.augmenter.word as naw\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import RegexpTokenizer, stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "# Creating dataloader for our dataset\n",
        "# Dataloader contains preprocess, augment, padding all these\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length, padding=None, augment=False,take_only_customer=False,if_preprocess=True):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label_to_int = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "        if padding is None:\n",
        "            self.padding = \"max_length\"\n",
        "        else:\n",
        "            self.padding = False\n",
        "        self.augment = augment\n",
        "        self.if_take_customer = take_only_customer\n",
        "        self.if_preprocess = if_preprocess\n",
        "        if augment:\n",
        "            self.aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "        if self.if_preprocess:\n",
        "          self.df = self.preprocess(self.df)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    # Remove Punctuation\n",
        "    def remove_punctuation(self,text):\n",
        "        no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
        "        return no_punct\n",
        "\n",
        "    # Remove Stopwords\n",
        "    def remove_stopwords(self,text):\n",
        "        words = [w for w in text if w not in stopwords.words('english')]\n",
        "        return words\n",
        "\n",
        "    def word_lemmatizer(self,text):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        return [lemmatizer.lemmatize(x) for x in text]\n",
        "\n",
        "    def remove_numbers(self,text):\n",
        "        return [x for x in text if not x.isdigit()]\n",
        "\n",
        "\n",
        "    def take_only_customer(self,text):\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        customer_lines = [line.replace(\"Customer: \", \"\") for line in lines if line.startswith(\"Customer:\")]\n",
        "        customer_conversation = \"\\n\".join(customer_lines)\n",
        "        return customer_conversation\n",
        "\n",
        "    def preprocess(self,df):\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.remove_punctuation(x))\n",
        "        # Tokenize the text\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.remove_stopwords(x))\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.word_lemmatizer(x))\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: self.remove_numbers(x))\n",
        "\n",
        "        df[\"conversation\"] = df[\"conversation\"].apply(lambda x: \" \".join(x))\n",
        "        return df\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        row = self.df.iloc[idx]\n",
        "        #if self.if_preprocess:\n",
        "        #   text = self.preprocess(row)\n",
        "        text = row[\"conversation\"]\n",
        "        if self.if_take_customer:\n",
        "           text = self.take_only_customer(text)\n",
        "        if self.augment:\n",
        "           text = self.aug.augment(text)[0]\n",
        "        sentiment_label = self.label_to_int[row[\"customer_sentiment\"]]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_length,\n",
        "          return_token_type_ids=False,\n",
        "          padding=self.padding,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt',\n",
        "          truncation=True\n",
        "        )\n",
        "        if len(encoding['input_ids'].flatten()) == 0:\n",
        "          return {\n",
        "          'input_ids': torch.tensor([0]),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'labels': torch.tensor(sentiment_label, dtype=torch.long)\n",
        "          }\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(sentiment_label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XQ-m1Bvq1Q8e",
        "outputId": "f7955db8-8037-413a-9c36-2badcef64309"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 123.59M\n",
            "number of parameters: 123.59M\n",
            "num decayed parameter tensors: 51, with 124,356,864 parameters\n",
            "num non-decayed parameter tensors: 26, with 19,203 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-763ebe6df08c>:243: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maliyigitbasaran\u001b[0m (\u001b[33maliyigitbasaran-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/DI725/assignment_1/wandb/run-20250331_212539-r4b3zxht</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1/runs/r4b3zxht' target=\"_blank\">gpt2_scratch</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1/runs/r4b3zxht' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1/runs/r4b3zxht</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.2943\n",
            "Number of Samples Test =  30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy 0.3333\n",
            "Test precision 0.1149\n",
            "Test recall 0.3333\n",
            "Test F1 0.1709\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 0  0 10]\n",
            " [ 1  0  9]\n",
            " [ 0  0 10]]\n",
            "iter 0: loss 1.3125, time 69357.87ms\n",
            "iter 1: loss 11.3125, time 2794.59ms\n",
            "iter 2: loss 0.2080, time 2814.23ms\n",
            "iter 3: loss 2.8594, time 3199.42ms\n",
            "iter 4: loss 0.7461, time 3235.71ms\n",
            "iter 5: loss 0.7891, time 3001.80ms\n",
            "iter 6: loss 1.7734, time 2991.91ms\n",
            "iter 7: loss 1.3359, time 2973.93ms\n",
            "iter 8: loss 1.5078, time 3199.47ms\n",
            "iter 9: loss 1.1563, time 3029.41ms\n",
            "iter 10: loss 1.7031, time 2839.33ms\n",
            "iter 11: loss 1.7656, time 2972.85ms\n",
            "iter 12: loss 1.3281, time 3122.76ms\n",
            "iter 13: loss 1.3984, time 2880.09ms\n",
            "iter 14: loss 0.4785, time 2829.61ms\n",
            "iter 15: loss 1.8594, time 2891.76ms\n",
            "iter 16: loss 1.4766, time 3030.59ms\n",
            "iter 17: loss 0.7773, time 3346.64ms\n",
            "iter 18: loss 1.9766, time 3533.66ms\n",
            "iter 19: loss 1.5156, time 3111.10ms\n",
            "step 20: train loss 1.5760, val loss 1.2840\n",
            "step val accuracy 0.5089\n",
            "saving checkpoint to out\n",
            "iter 20: loss 1.3281, time 30995.47ms\n",
            "iter 21: loss 0.9375, time 3285.81ms\n",
            "iter 22: loss 0.0299, time 3958.34ms\n",
            "iter 23: loss 0.0101, time 3511.08ms\n",
            "iter 24: loss 1.8125, time 4116.53ms\n",
            "iter 25: loss 0.2051, time 5127.61ms\n",
            "iter 26: loss 1.6563, time 3126.20ms\n",
            "iter 27: loss 1.2734, time 2947.62ms\n",
            "iter 28: loss 0.9609, time 3238.46ms\n",
            "iter 29: loss 1.3672, time 3192.38ms\n",
            "iter 30: loss 0.2656, time 2887.57ms\n",
            "iter 31: loss 0.4297, time 3031.72ms\n",
            "iter 32: loss 0.1553, time 2948.71ms\n",
            "iter 33: loss 0.7773, time 3239.18ms\n",
            "iter 34: loss 0.3340, time 2957.33ms\n",
            "iter 35: loss 0.0002, time 2850.65ms\n",
            "iter 36: loss 0.7422, time 2795.91ms\n",
            "iter 37: loss 1.0859, time 3091.62ms\n",
            "iter 38: loss 0.0649, time 3070.63ms\n",
            "iter 39: loss 0.2354, time 2933.42ms\n",
            "step 40: train loss 1.0946, val loss 1.0226\n",
            "step val accuracy 0.7027\n",
            "saving checkpoint to out\n",
            "iter 40: loss 2.3281, time 32238.17ms\n",
            "iter 41: loss 0.0820, time 3068.50ms\n",
            "iter 42: loss 0.0014, time 3278.58ms\n",
            "iter 43: loss 0.0393, time 2953.21ms\n",
            "iter 44: loss 0.4746, time 2931.00ms\n",
            "iter 45: loss 0.1875, time 2987.80ms\n",
            "iter 46: loss 0.0762, time 3188.60ms\n",
            "iter 47: loss 0.0106, time 3038.14ms\n",
            "iter 48: loss 0.0574, time 2811.55ms\n",
            "iter 49: loss 0.0674, time 2873.51ms\n",
            "iter 50: loss 0.0214, time 3122.50ms\n",
            "iter 51: loss 0.0061, time 3188.28ms\n",
            "iter 52: loss 0.0154, time 3018.38ms\n",
            "iter 53: loss 0.1162, time 3008.30ms\n",
            "iter 54: loss 0.6719, time 2948.01ms\n",
            "iter 55: loss 0.0101, time 3226.59ms\n",
            "iter 56: loss 0.0439, time 3251.59ms\n",
            "iter 57: loss 0.0889, time 2999.92ms\n",
            "iter 58: loss 0.0012, time 2990.55ms\n",
            "iter 59: loss 1.2422, time 2992.92ms\n",
            "step 60: train loss 0.8633, val loss 0.9506\n",
            "step val accuracy 0.8107\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0013, time 29941.38ms\n",
            "iter 61: loss 0.0007, time 2984.17ms\n",
            "iter 62: loss 0.0005, time 2908.54ms\n",
            "iter 63: loss 0.8086, time 2902.72ms\n",
            "iter 64: loss 0.0032, time 3866.69ms\n",
            "iter 65: loss 0.0125, time 3078.71ms\n",
            "iter 66: loss 0.0767, time 2911.62ms\n",
            "iter 67: loss 0.0334, time 2961.92ms\n",
            "iter 68: loss 0.0908, time 3100.54ms\n",
            "iter 69: loss 0.0007, time 3409.12ms\n",
            "iter 70: loss 0.0141, time 2992.83ms\n",
            "iter 71: loss 0.0099, time 2967.51ms\n",
            "iter 72: loss 0.8789, time 2865.30ms\n",
            "iter 73: loss 0.0052, time 3127.57ms\n",
            "iter 74: loss 0.0004, time 3031.87ms\n",
            "iter 75: loss 0.0125, time 2983.06ms\n",
            "iter 76: loss 0.2617, time 2930.92ms\n",
            "iter 77: loss 0.0077, time 3215.19ms\n",
            "iter 78: loss 0.0908, time 3183.65ms\n",
            "iter 79: loss 4.4375, time 2948.45ms\n",
            "step 80: train loss 0.7110, val loss 0.8357\n",
            "step val accuracy 0.8624\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0156, time 30964.57ms\n",
            "iter 81: loss 0.1719, time 3078.95ms\n",
            "iter 82: loss 0.0957, time 3158.14ms\n",
            "iter 83: loss 0.1631, time 2929.76ms\n",
            "iter 84: loss 0.0815, time 2935.97ms\n",
            "iter 85: loss 0.0593, time 3065.33ms\n",
            "iter 86: loss 0.0017, time 3191.08ms\n",
            "iter 87: loss 0.0208, time 3067.53ms\n",
            "iter 88: loss 0.1226, time 2959.30ms\n",
            "iter 89: loss 0.0004, time 2862.84ms\n",
            "iter 90: loss 1.0859, time 3042.04ms\n",
            "iter 91: loss 0.0001, time 3305.18ms\n",
            "iter 92: loss 3.5469, time 2880.20ms\n",
            "iter 93: loss 0.0004, time 2996.66ms\n",
            "iter 94: loss 0.0010, time 2989.83ms\n",
            "iter 95: loss 0.0610, time 3322.51ms\n",
            "iter 96: loss 0.0009, time 3114.39ms\n",
            "iter 97: loss 0.0094, time 2876.55ms\n",
            "iter 98: loss 0.2129, time 2991.35ms\n",
            "iter 99: loss 0.6250, time 2975.22ms\n",
            "step 100: train loss 0.6051, val loss 0.7672\n",
            "step val accuracy 0.9127\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5667\n",
            "Test precision 0.5782\n",
            "Test recall 0.5667\n",
            "Test F1 0.5510\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [3 5 2]\n",
            " [0 7 3]]\n",
            "iter 100: loss 0.0002, time 33231.79ms\n",
            "iter 101: loss 0.0008, time 2931.19ms\n",
            "iter 102: loss 0.0038, time 2844.62ms\n",
            "iter 103: loss 0.0007, time 3309.73ms\n",
            "iter 104: loss 0.0014, time 3146.53ms\n",
            "iter 105: loss 0.0037, time 2974.50ms\n",
            "iter 106: loss 0.0006, time 3031.04ms\n",
            "iter 107: loss 0.0005, time 2924.26ms\n",
            "iter 108: loss 0.0874, time 3316.54ms\n",
            "iter 109: loss 0.0001, time 3002.42ms\n",
            "iter 110: loss 0.0009, time 3026.70ms\n",
            "iter 111: loss 0.5234, time 2956.80ms\n",
            "iter 112: loss 5.9688, time 3144.97ms\n",
            "iter 113: loss 0.0035, time 3076.60ms\n",
            "iter 114: loss 0.0085, time 2955.05ms\n",
            "iter 115: loss 1.7266, time 2949.46ms\n",
            "iter 116: loss 0.0197, time 3013.03ms\n",
            "iter 117: loss 0.0010, time 3272.71ms\n",
            "iter 118: loss 0.1138, time 2909.23ms\n",
            "iter 119: loss 0.0074, time 3084.06ms\n",
            "step 120: train loss 0.5487, val loss 0.7965\n",
            "step val accuracy 0.9201\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0001, time 35844.17ms\n",
            "iter 121: loss 0.0104, time 2903.25ms\n",
            "iter 122: loss 0.0010, time 2921.24ms\n",
            "iter 123: loss 0.0010, time 3021.23ms\n",
            "iter 124: loss 0.0322, time 3208.28ms\n",
            "iter 125: loss 0.0011, time 2924.35ms\n",
            "iter 126: loss 0.0454, time 2871.67ms\n",
            "iter 127: loss 0.0038, time 2947.63ms\n",
            "iter 128: loss 0.0031, time 3293.71ms\n",
            "iter 129: loss 0.0008, time 3073.39ms\n",
            "iter 130: loss 0.0019, time 3036.54ms\n",
            "iter 131: loss 0.0009, time 2928.47ms\n",
            "iter 132: loss 0.0006, time 3157.63ms\n",
            "iter 133: loss 0.0005, time 3177.69ms\n",
            "iter 134: loss 0.0006, time 3037.66ms\n",
            "iter 135: loss 0.0544, time 3037.05ms\n",
            "iter 136: loss 0.0005, time 2994.86ms\n",
            "iter 137: loss 0.6758, time 3312.64ms\n",
            "iter 138: loss 0.0004, time 3049.18ms\n",
            "iter 139: loss 0.0069, time 2912.78ms\n",
            "step 140: train loss 0.4895, val loss 0.7766\n",
            "step val accuracy 0.9334\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0259, time 29353.66ms\n",
            "iter 141: loss 0.0016, time 2829.36ms\n",
            "iter 142: loss 0.0461, time 3148.44ms\n",
            "iter 143: loss 0.0022, time 3025.36ms\n",
            "iter 144: loss 0.0020, time 2890.18ms\n",
            "iter 145: loss 0.0021, time 2914.82ms\n",
            "iter 146: loss 0.0054, time 3018.09ms\n",
            "iter 147: loss 0.0023, time 3216.08ms\n",
            "iter 148: loss 0.0115, time 3047.70ms\n",
            "iter 149: loss 0.0047, time 2904.33ms\n",
            "iter 150: loss 0.0092, time 2974.41ms\n",
            "iter 151: loss 0.0044, time 3166.92ms\n",
            "iter 152: loss 0.0435, time 3064.42ms\n",
            "iter 153: loss 0.0000, time 3032.07ms\n",
            "iter 154: loss 0.0040, time 3097.12ms\n",
            "iter 155: loss 0.0009, time 2994.69ms\n",
            "iter 156: loss 0.0001, time 3205.28ms\n",
            "iter 157: loss 0.0000, time 3030.53ms\n",
            "iter 158: loss 0.0001, time 2950.95ms\n",
            "iter 159: loss 0.0022, time 3033.38ms\n",
            "step 160: train loss 0.4420, val loss 0.7634\n",
            "step val accuracy 0.9201\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 31795.07ms\n",
            "iter 161: loss 0.0000, time 2903.67ms\n",
            "iter 162: loss 0.0007, time 2961.27ms\n",
            "iter 163: loss 0.9844, time 2847.59ms\n",
            "iter 164: loss 0.1416, time 3280.88ms\n",
            "iter 165: loss 0.0001, time 3144.06ms\n",
            "iter 166: loss 0.0001, time 3075.67ms\n",
            "iter 167: loss 0.0001, time 2954.26ms\n",
            "iter 168: loss 0.0005, time 3135.31ms\n",
            "iter 169: loss 0.0000, time 3120.63ms\n",
            "iter 170: loss 0.0000, time 2893.75ms\n",
            "iter 171: loss 0.0129, time 2988.93ms\n",
            "iter 172: loss 0.0001, time 3028.71ms\n",
            "iter 173: loss 0.0000, time 3238.73ms\n",
            "iter 174: loss 0.2256, time 2942.75ms\n",
            "iter 175: loss 0.0004, time 2883.25ms\n",
            "iter 176: loss 0.0000, time 3019.04ms\n",
            "iter 177: loss 0.1147, time 2992.71ms\n",
            "iter 178: loss 0.0001, time 3161.16ms\n",
            "iter 179: loss 0.5625, time 3095.36ms\n",
            "step 180: train loss 0.4069, val loss 0.7577\n",
            "step val accuracy 0.9527\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0007, time 32667.89ms\n",
            "iter 181: loss 0.0001, time 3138.10ms\n",
            "iter 182: loss 0.0065, time 2994.60ms\n",
            "iter 183: loss 0.0009, time 2867.05ms\n",
            "iter 184: loss 0.0002, time 2961.20ms\n",
            "iter 185: loss 0.0513, time 3049.03ms\n",
            "iter 186: loss 0.0001, time 3303.35ms\n",
            "iter 187: loss 0.0017, time 2956.04ms\n",
            "iter 188: loss 0.0000, time 3095.92ms\n",
            "iter 189: loss 0.0001, time 2989.96ms\n",
            "iter 190: loss 0.0000, time 3059.15ms\n",
            "iter 191: loss 0.0000, time 3113.96ms\n",
            "iter 192: loss 0.0001, time 2916.60ms\n",
            "iter 193: loss 0.0101, time 3033.15ms\n",
            "iter 194: loss 0.0001, time 3018.74ms\n",
            "iter 195: loss 0.0001, time 3257.64ms\n",
            "iter 196: loss 0.0000, time 2975.29ms\n",
            "iter 197: loss 0.0001, time 2929.27ms\n",
            "iter 198: loss 0.0035, time 3063.27ms\n",
            "iter 199: loss 0.0001, time 3038.95ms\n",
            "step 200: train loss 0.3736, val loss 0.7541\n",
            "step val accuracy 0.9127\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.5720\n",
            "Test recall 0.5333\n",
            "Test F1 0.5023\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [4 5 1]\n",
            " [0 8 2]]\n",
            "iter 200: loss 0.0002, time 36796.94ms\n",
            "iter 201: loss 0.0001, time 3252.08ms\n",
            "iter 202: loss 0.0000, time 3066.41ms\n",
            "iter 203: loss 0.0007, time 2991.93ms\n",
            "iter 204: loss 0.0001, time 2850.60ms\n",
            "iter 205: loss 0.0000, time 2960.51ms\n",
            "iter 206: loss 3.0469, time 3353.93ms\n",
            "iter 207: loss 0.0001, time 3037.81ms\n",
            "iter 208: loss 0.0000, time 2993.51ms\n",
            "iter 209: loss 0.0109, time 3029.96ms\n",
            "iter 210: loss 0.0001, time 3189.54ms\n",
            "iter 211: loss 0.0000, time 3067.14ms\n",
            "iter 212: loss 0.0000, time 3102.99ms\n",
            "iter 213: loss 0.0000, time 2955.49ms\n",
            "iter 214: loss 0.0006, time 2992.62ms\n",
            "iter 215: loss 0.0000, time 3094.77ms\n",
            "iter 216: loss 1.5625, time 3074.11ms\n",
            "iter 217: loss 0.0019, time 2998.29ms\n",
            "iter 218: loss 0.0056, time 2993.18ms\n",
            "iter 219: loss 0.0025, time 3220.45ms\n",
            "step 220: train loss 0.3589, val loss 0.7318\n",
            "step val accuracy 0.9453\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0003, time 31743.05ms\n",
            "iter 221: loss 0.0008, time 3037.97ms\n",
            "iter 222: loss 0.0119, time 3021.67ms\n",
            "iter 223: loss 0.0019, time 3332.39ms\n",
            "iter 224: loss 0.0007, time 3044.35ms\n",
            "iter 225: loss 0.0027, time 2936.01ms\n",
            "iter 226: loss 0.0087, time 3018.80ms\n",
            "iter 227: loss 0.0186, time 3062.53ms\n",
            "iter 228: loss 0.0004, time 3089.78ms\n",
            "iter 229: loss 0.0070, time 2973.97ms\n",
            "iter 230: loss 0.0005, time 2896.34ms\n",
            "iter 231: loss 0.0771, time 3059.80ms\n",
            "iter 232: loss 0.7656, time 3214.03ms\n",
            "iter 233: loss 0.0033, time 3052.14ms\n",
            "iter 234: loss 0.0015, time 2997.84ms\n",
            "iter 235: loss 0.0001, time 3114.99ms\n",
            "iter 236: loss 0.0684, time 3128.86ms\n",
            "iter 237: loss 0.0003, time 3081.44ms\n",
            "iter 238: loss 0.0001, time 2984.57ms\n",
            "iter 239: loss 0.0001, time 2941.22ms\n",
            "step 240: train loss 0.3349, val loss 0.7058\n",
            "step val accuracy 0.9408\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 35979.92ms\n",
            "iter 241: loss 0.0001, time 2873.57ms\n",
            "iter 242: loss 0.0309, time 2973.22ms\n",
            "iter 243: loss 0.0000, time 3189.10ms\n",
            "iter 244: loss 0.0001, time 3030.96ms\n",
            "iter 245: loss 0.0000, time 3074.74ms\n",
            "iter 246: loss 0.0002, time 2947.88ms\n",
            "iter 247: loss 0.0001, time 3165.43ms\n",
            "iter 248: loss 0.0001, time 3118.97ms\n",
            "iter 249: loss 0.0000, time 3019.17ms\n",
            "iter 250: loss 0.0000, time 3052.14ms\n",
            "iter 251: loss 0.0002, time 3072.43ms\n",
            "iter 252: loss 0.0001, time 3207.49ms\n",
            "iter 253: loss 0.0001, time 3026.02ms\n",
            "iter 254: loss 0.0000, time 2981.08ms\n",
            "iter 255: loss 0.0005, time 3055.40ms\n",
            "iter 256: loss 0.0000, time 3225.40ms\n",
            "iter 257: loss 0.0012, time 3021.22ms\n",
            "iter 258: loss 0.0000, time 3019.94ms\n",
            "iter 259: loss 0.0000, time 2875.08ms\n",
            "step 260: train loss 0.3185, val loss 0.7041\n",
            "step val accuracy 0.8743\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0020, time 30575.77ms\n",
            "iter 261: loss 0.0000, time 3161.89ms\n",
            "iter 262: loss 0.0000, time 2916.96ms\n",
            "iter 263: loss 0.0005, time 2860.36ms\n",
            "iter 264: loss 0.0011, time 2868.33ms\n",
            "iter 265: loss 0.0000, time 3171.05ms\n",
            "iter 266: loss 0.0003, time 3163.39ms\n",
            "iter 267: loss 0.0000, time 3043.52ms\n",
            "iter 268: loss 0.0002, time 2983.36ms\n",
            "iter 269: loss 0.0011, time 3107.56ms\n",
            "iter 270: loss 0.0005, time 3206.91ms\n",
            "iter 271: loss 0.0004, time 3040.49ms\n",
            "iter 272: loss 0.0006, time 3100.95ms\n",
            "iter 273: loss 0.0004, time 2951.04ms\n",
            "iter 274: loss 0.0000, time 3129.73ms\n",
            "iter 275: loss 0.0002, time 3024.25ms\n",
            "iter 276: loss 0.0164, time 3035.57ms\n",
            "iter 277: loss 0.0008, time 3029.22ms\n",
            "iter 278: loss 0.0000, time 3173.76ms\n",
            "iter 279: loss 0.0002, time 3194.77ms\n",
            "step 280: train loss 0.3027, val loss 0.6592\n",
            "step val accuracy 0.9586\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 36046.64ms\n",
            "iter 281: loss 0.0006, time 3023.80ms\n",
            "iter 282: loss 0.0003, time 2833.99ms\n",
            "iter 283: loss 0.0000, time 2942.18ms\n",
            "iter 284: loss 0.0000, time 2849.41ms\n",
            "iter 285: loss 0.0003, time 3198.55ms\n",
            "iter 286: loss 0.0000, time 3023.39ms\n",
            "iter 287: loss 0.0000, time 3108.33ms\n",
            "iter 288: loss 0.0000, time 2986.23ms\n",
            "iter 289: loss 0.0000, time 3085.70ms\n",
            "iter 290: loss 0.0003, time 3358.49ms\n",
            "iter 291: loss 0.0017, time 2923.94ms\n",
            "iter 292: loss 1.9609, time 3017.22ms\n",
            "iter 293: loss 0.0001, time 2948.05ms\n",
            "iter 294: loss 0.0004, time 3268.83ms\n",
            "iter 295: loss 0.0001, time 3013.23ms\n",
            "iter 296: loss 0.0002, time 2966.08ms\n",
            "iter 297: loss 0.0298, time 2907.69ms\n",
            "iter 298: loss 0.0000, time 3222.66ms\n",
            "iter 299: loss 0.0002, time 3975.82ms\n",
            "step 300: train loss 0.2856, val loss 0.6517\n",
            "step val accuracy 0.9601\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5667\n",
            "Test precision 0.5727\n",
            "Test recall 0.5667\n",
            "Test F1 0.5410\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 6 2]\n",
            " [0 8 2]]\n",
            "iter 300: loss 0.0001, time 30879.13ms\n",
            "iter 301: loss 0.0001, time 2955.63ms\n",
            "iter 302: loss 0.0016, time 3053.20ms\n",
            "iter 303: loss 0.0002, time 3160.90ms\n",
            "iter 304: loss 0.0001, time 2910.97ms\n",
            "iter 305: loss 0.0000, time 2951.34ms\n",
            "iter 306: loss 0.0000, time 2909.43ms\n",
            "iter 307: loss 0.0593, time 3257.04ms\n",
            "iter 308: loss 0.0003, time 3064.49ms\n",
            "iter 309: loss 0.0009, time 2933.84ms\n",
            "iter 310: loss 0.0005, time 3055.37ms\n",
            "iter 311: loss 0.0002, time 3067.43ms\n",
            "iter 312: loss 0.0013, time 3195.03ms\n",
            "iter 313: loss 0.8086, time 3011.22ms\n",
            "iter 314: loss 0.0000, time 2935.24ms\n",
            "iter 315: loss 0.0001, time 2951.14ms\n",
            "iter 316: loss 0.0001, time 3138.58ms\n",
            "iter 317: loss 0.0068, time 3025.39ms\n",
            "iter 318: loss 0.0000, time 3139.25ms\n",
            "iter 319: loss 0.0006, time 2972.41ms\n",
            "step 320: train loss 0.2700, val loss 0.6516\n",
            "step val accuracy 0.9275\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 37572.76ms\n",
            "iter 321: loss 0.0004, time 3085.41ms\n",
            "iter 322: loss 0.0117, time 3207.53ms\n",
            "iter 323: loss 0.0000, time 3110.22ms\n",
            "iter 324: loss 0.0046, time 2940.46ms\n",
            "iter 325: loss 0.0004, time 3043.06ms\n",
            "iter 326: loss 0.0002, time 2951.98ms\n",
            "iter 327: loss 0.0000, time 3338.21ms\n",
            "iter 328: loss 0.0041, time 2957.05ms\n",
            "iter 329: loss 0.0000, time 3063.76ms\n",
            "iter 330: loss 0.0000, time 2950.00ms\n",
            "iter 331: loss 0.0001, time 3205.54ms\n",
            "iter 332: loss 0.0000, time 3180.89ms\n",
            "iter 333: loss 0.0000, time 2857.39ms\n",
            "iter 334: loss 0.0001, time 3155.40ms\n",
            "iter 335: loss 0.0001, time 3045.10ms\n",
            "iter 336: loss 0.0000, time 3290.36ms\n",
            "iter 337: loss 0.0000, time 2955.53ms\n",
            "iter 338: loss 0.0000, time 3037.81ms\n",
            "iter 339: loss 0.0004, time 2924.16ms\n",
            "step 340: train loss 0.2573, val loss 0.6573\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 32652.44ms\n",
            "iter 341: loss 0.0000, time 2989.24ms\n",
            "iter 342: loss 0.0002, time 2988.58ms\n",
            "iter 343: loss 0.0000, time 2958.33ms\n",
            "iter 344: loss 0.0000, time 3212.00ms\n",
            "iter 345: loss 0.0000, time 3092.92ms\n",
            "iter 346: loss 0.0000, time 3101.47ms\n",
            "iter 347: loss 0.0000, time 3068.54ms\n",
            "iter 348: loss 0.0001, time 3323.87ms\n",
            "iter 349: loss 0.0000, time 3177.53ms\n",
            "iter 350: loss 0.0003, time 3004.30ms\n",
            "iter 351: loss 0.0000, time 3035.98ms\n",
            "iter 352: loss 0.0000, time 3127.02ms\n",
            "iter 353: loss 0.0000, time 3347.54ms\n",
            "iter 354: loss 0.0000, time 3072.39ms\n",
            "iter 355: loss 0.0000, time 3015.75ms\n",
            "iter 356: loss 0.0000, time 3088.02ms\n",
            "iter 357: loss 0.0008, time 3265.96ms\n",
            "iter 358: loss 0.0000, time 3230.17ms\n",
            "iter 359: loss 0.0000, time 2953.61ms\n",
            "step 360: train loss 0.2448, val loss 0.6635\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 36784.61ms\n",
            "iter 361: loss 0.0001, time 2892.40ms\n",
            "iter 362: loss 0.0000, time 2998.91ms\n",
            "iter 363: loss 0.0000, time 2961.24ms\n",
            "iter 364: loss 0.0001, time 3376.70ms\n",
            "iter 365: loss 0.0003, time 3060.76ms\n",
            "iter 366: loss 0.0000, time 3055.68ms\n",
            "iter 367: loss 0.0000, time 2933.39ms\n",
            "iter 368: loss 0.0000, time 3154.87ms\n",
            "iter 369: loss 0.0031, time 3151.17ms\n",
            "iter 370: loss 0.0000, time 3065.10ms\n",
            "iter 371: loss 0.0002, time 3058.30ms\n",
            "iter 372: loss 0.0016, time 2975.59ms\n",
            "iter 373: loss 0.0000, time 3144.60ms\n",
            "iter 374: loss 0.0000, time 2885.56ms\n",
            "iter 375: loss 0.0001, time 3011.55ms\n",
            "iter 376: loss 0.0001, time 2959.03ms\n",
            "iter 377: loss 0.0000, time 3111.53ms\n",
            "iter 378: loss 0.0000, time 3247.30ms\n",
            "iter 379: loss 0.0000, time 3055.21ms\n",
            "step 380: train loss 0.2328, val loss 0.6715\n",
            "step val accuracy 0.9586\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 37220.20ms\n",
            "iter 381: loss 0.0000, time 2805.64ms\n",
            "iter 382: loss 0.0003, time 2935.36ms\n",
            "iter 383: loss 0.0000, time 3024.89ms\n",
            "iter 384: loss 0.0000, time 3299.96ms\n",
            "iter 385: loss 0.0000, time 3090.11ms\n",
            "iter 386: loss 0.0000, time 2946.78ms\n",
            "iter 387: loss 0.0000, time 2979.44ms\n",
            "iter 388: loss 0.0000, time 2932.93ms\n",
            "iter 389: loss 0.0000, time 3395.04ms\n",
            "iter 390: loss 0.0008, time 3079.33ms\n",
            "iter 391: loss 0.0000, time 3143.58ms\n",
            "iter 392: loss 0.0000, time 3052.50ms\n",
            "iter 393: loss 0.0000, time 3281.81ms\n",
            "iter 394: loss 0.0000, time 2941.22ms\n",
            "iter 395: loss 0.0000, time 2905.40ms\n",
            "iter 396: loss 0.0000, time 2920.96ms\n",
            "iter 397: loss 0.0000, time 3202.78ms\n",
            "iter 398: loss 0.0000, time 3116.73ms\n",
            "iter 399: loss 0.0007, time 2999.94ms\n",
            "step 400: train loss 0.2220, val loss 0.6740\n",
            "step val accuracy 0.9527\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.4024\n",
            "Test recall 0.5333\n",
            "Test F1 0.4524\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 2  7  1]\n",
            " [ 0 10  0]]\n",
            "iter 400: loss 0.0001, time 30790.42ms\n",
            "iter 401: loss 0.0000, time 2988.81ms\n",
            "iter 402: loss 0.0008, time 3260.28ms\n",
            "iter 403: loss 0.0001, time 2936.27ms\n",
            "iter 404: loss 0.0000, time 2992.87ms\n",
            "iter 405: loss 0.0002, time 2983.20ms\n",
            "iter 406: loss 0.0000, time 3428.92ms\n",
            "iter 407: loss 0.0001, time 3052.22ms\n",
            "iter 408: loss 0.0000, time 3090.16ms\n",
            "iter 409: loss 0.0001, time 2888.58ms\n",
            "iter 410: loss 0.0000, time 3291.92ms\n",
            "iter 411: loss 0.0001, time 3104.91ms\n",
            "iter 412: loss 0.0000, time 2899.59ms\n",
            "iter 413: loss 0.0000, time 3038.09ms\n",
            "iter 414: loss 0.0000, time 2982.89ms\n",
            "iter 415: loss 0.0000, time 3156.59ms\n",
            "iter 416: loss 0.0006, time 2989.81ms\n",
            "iter 417: loss 0.0001, time 2932.71ms\n",
            "iter 418: loss 0.0001, time 3549.01ms\n",
            "iter 419: loss 0.0000, time 3160.09ms\n",
            "step 420: train loss 0.2121, val loss 0.6810\n",
            "step val accuracy 0.9527\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0002, time 34750.09ms\n",
            "iter 421: loss 0.0000, time 2978.04ms\n",
            "iter 422: loss 0.0001, time 3243.82ms\n",
            "iter 423: loss 0.0011, time 3177.48ms\n",
            "iter 424: loss 0.0003, time 2994.57ms\n",
            "iter 425: loss 0.0001, time 3063.87ms\n",
            "iter 426: loss 0.0295, time 2996.36ms\n",
            "iter 427: loss 0.0000, time 3310.31ms\n",
            "iter 428: loss 0.0000, time 2865.30ms\n",
            "iter 429: loss 0.0000, time 2985.12ms\n",
            "iter 430: loss 0.0000, time 3142.98ms\n",
            "iter 431: loss 0.0003, time 3226.79ms\n",
            "iter 432: loss 0.0010, time 2976.83ms\n",
            "iter 433: loss 0.0000, time 3139.46ms\n",
            "iter 434: loss 0.0017, time 3055.64ms\n",
            "iter 435: loss 0.0000, time 3091.15ms\n",
            "iter 436: loss 0.0000, time 3188.56ms\n",
            "iter 437: loss 0.0000, time 2992.41ms\n",
            "iter 438: loss 0.0369, time 2939.86ms\n",
            "iter 439: loss 0.0007, time 2791.52ms\n",
            "step 440: train loss 0.2050, val loss 0.6887\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0007, time 31420.23ms\n",
            "iter 441: loss 0.0000, time 2921.16ms\n",
            "iter 442: loss 0.0000, time 2841.58ms\n",
            "iter 443: loss 0.0000, time 2978.65ms\n",
            "iter 444: loss 0.0000, time 3212.09ms\n",
            "iter 445: loss 0.0001, time 3080.18ms\n",
            "iter 446: loss 0.0000, time 3012.98ms\n",
            "iter 447: loss 0.0000, time 2950.80ms\n",
            "iter 448: loss 0.0000, time 2956.51ms\n",
            "iter 449: loss 0.0000, time 3322.66ms\n",
            "iter 450: loss 0.0056, time 2975.37ms\n",
            "iter 451: loss 0.0001, time 3207.71ms\n",
            "iter 452: loss 0.0001, time 3057.55ms\n",
            "iter 453: loss 0.0000, time 3147.39ms\n",
            "iter 454: loss 0.0000, time 3084.46ms\n",
            "iter 455: loss 0.0000, time 2966.21ms\n",
            "iter 456: loss 0.0002, time 2941.05ms\n",
            "iter 457: loss 0.0000, time 3025.55ms\n",
            "iter 458: loss 0.0001, time 3253.57ms\n",
            "iter 459: loss 0.0000, time 3002.16ms\n",
            "step 460: train loss 0.1968, val loss 0.6948\n",
            "step val accuracy 0.9571\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0002, time 38740.06ms\n",
            "iter 461: loss 0.0000, time 2921.77ms\n",
            "iter 462: loss 0.0000, time 2983.77ms\n",
            "iter 463: loss 0.0001, time 3043.68ms\n",
            "iter 464: loss 0.0000, time 3127.35ms\n",
            "iter 465: loss 0.0059, time 3055.53ms\n",
            "iter 466: loss 0.0000, time 2964.62ms\n",
            "iter 467: loss 0.0000, time 2949.39ms\n",
            "iter 468: loss 0.0000, time 2998.37ms\n",
            "iter 469: loss 0.0001, time 3157.73ms\n",
            "iter 470: loss 0.0000, time 3008.82ms\n",
            "iter 471: loss 0.0002, time 2994.26ms\n",
            "iter 472: loss 0.0000, time 3046.63ms\n",
            "iter 473: loss 0.0000, time 3163.14ms\n",
            "iter 474: loss 0.0000, time 3208.33ms\n",
            "iter 475: loss 0.0000, time 3072.56ms\n",
            "iter 476: loss 0.0000, time 3049.24ms\n",
            "iter 477: loss 0.0000, time 3001.83ms\n",
            "iter 478: loss 0.0000, time 3234.72ms\n",
            "iter 479: loss 0.0000, time 3002.07ms\n",
            "step 480: train loss 0.1896, val loss 0.6935\n",
            "step val accuracy 0.9660\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0001, time 30256.53ms\n",
            "iter 481: loss 0.0000, time 2968.97ms\n",
            "iter 482: loss 0.0000, time 3237.35ms\n",
            "iter 483: loss 0.0000, time 3069.42ms\n",
            "iter 484: loss 0.0000, time 3072.10ms\n",
            "iter 485: loss 0.0000, time 2957.04ms\n",
            "iter 486: loss 0.0117, time 3090.77ms\n",
            "iter 487: loss 0.0000, time 3098.51ms\n",
            "iter 488: loss 0.0000, time 2931.10ms\n",
            "iter 489: loss 0.0000, time 3007.28ms\n",
            "iter 490: loss 0.0000, time 2847.99ms\n",
            "iter 491: loss 0.0000, time 3003.22ms\n",
            "iter 492: loss 0.0000, time 2965.08ms\n",
            "iter 493: loss 0.0000, time 2920.70ms\n",
            "iter 494: loss 0.0000, time 2910.22ms\n",
            "iter 495: loss 0.0000, time 3078.53ms\n",
            "iter 496: loss 0.0000, time 3171.30ms\n",
            "iter 497: loss 0.0000, time 2988.62ms\n",
            "iter 498: loss 0.0000, time 3147.98ms\n",
            "iter 499: loss 0.0000, time 2920.23ms\n",
            "step 500: train loss 0.1820, val loss 0.6914\n",
            "step val accuracy 0.9689\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.4296\n",
            "Test recall 0.5333\n",
            "Test F1 0.4667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 1  7  2]\n",
            " [ 0 10  0]]\n",
            "iter 500: loss 0.0000, time 39295.67ms\n"
          ]
        }
      ],
      "source": [
        "# Training script for Scratch and Pretrained init\n",
        "\"\"\"\n",
        "This training script can be run both on a single gpu in debug mode,\n",
        "and also in a larger training run with distributed data parallel (ddp).\n",
        "\n",
        "To run on a single GPU, example:\n",
        "$ python train.py --batch_size=32 --compile=False\n",
        "\n",
        "To run with DDP on 4 gpus on 1 node, example:\n",
        "$ torchrun --standalone --nproc_per_node=4 train.py\n",
        "\n",
        "To run with DDP on 4 gpus across 2 nodes, example:\n",
        "- Run on the first (master) node with example IP 123.456.123.456:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "- Run on the worker node:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "from model import GPTConfig, GPT\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import numpy as np\n",
        "from torchmetrics import MeanMetric\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from transformers import GPT2ForSequenceClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def configure_optimizers(model, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in model.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 20\n",
        "eval_test_interval = 100\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "if_SMOTE = True\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "#init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "init_from=\"scratch\"\n",
        "#init_from = 'scratch'\n",
        "# wandb logging\n",
        "wandb_log = True # disabled by default\n",
        "wandb_project = 'DI725-HW1'\n",
        "wandb_run_name = 'gpt2' + \"_\" + init_from # 'run' + str(time.time())\n",
        "# data\n",
        "#dataset = 'openwebtext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 1 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 500 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "learning_rate=3e-5\n",
        "decay_lr = False # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "#device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "device = \"cuda\"\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', \"random\")\n",
        "\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model_gpt = GPT(gptconf)\n",
        "    model = GPTSentiment(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model_gpt = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model_gpt.load_state_dict(state_dict)\n",
        "    model = GPTSentiment(model_gpt)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model_gpt = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model_gpt.config, k)\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPTSentiment(gptconf)\n",
        "    model.load_state_dict(model_gpt.state_dict(),strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# crop down the model block size if desired, using model surgery\n",
        "#if block_size < model.config.block_size:\n",
        "#    model.crop_block_size(block_size)\n",
        "#    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = configure_optimizers(model,weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "# training loop\n",
        "\n",
        "\n",
        "# Assuming 'filepath' is the path to your CSV file\n",
        "train_df = pd.read_csv('./data/customer_service/train.csv')\n",
        "test_df = pd.read_csv('./data/customer_service/test.csv')\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize tokenizers, datasets, and dataloaders\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "if if_SMOTE:\n",
        "    sm = RandomOverSampler(random_state=42)\n",
        "    X_train_smote, y_train_smote = sm.fit_resample(train_df[[\"conversation\"]],train_df[\"customer_sentiment\"])\n",
        "    train_df_smoted = pd.concat([X_train_smote, y_train_smote.reset_index(drop=True)], axis=1)\n",
        "train_dataset = SentimentDataset(train_df_smoted, tokenizer, max_length=512,padding=False,augment=True,take_only_customer=False,if_preprocess=True)\n",
        "val_dataset = SentimentDataset(val_df, tokenizer, max_length=512,padding=False,take_only_customer=False,if_preprocess=True)\n",
        "test_dataset = SentimentDataset(test_df, tokenizer, max_length=512,padding=False,take_only_customer=False,if_preprocess=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, x_labels, y_labels):\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    cax = ax.matshow(conf_matrix, cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticklabels([''] + x_labels)\n",
        "    ax.set_yticklabels([''] + y_labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    # Adding text to each cell\n",
        "    for (i, j), val in np.ndenumerate(conf_matrix):\n",
        "        ax.text(j, i, f'{val}', ha='center', va='center', color='red')\n",
        "\n",
        "    plt.close(fig)  # Prevents the figure from being displayed in the notebook\n",
        "\n",
        "    # Convert to PIL Image\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    img = Image.open(buf)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def estimate_loss(train_loader_iter,val_loader_iter,num_train,num_val):\n",
        "    out = {\"train\":{},\"val\":{}}\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for split in ['train', 'val']:\n",
        "\n",
        "\n",
        "\n",
        "        if split==\"train\":\n",
        "          if train_loader.batch_size == 1:\n",
        "              iter_num= num_train\n",
        "          else:\n",
        "              iter_num = min( num_train // train_loader.batch_size + 1  , eval_iters // train_loader.batch_size + 1)\n",
        "        else:\n",
        "          if val_loader.batch_size == 1:\n",
        "              iter_num = num_val\n",
        "          else:\n",
        "              iter_num = min( num_val // val_loader.batch_size + 1  , eval_iters // val_loader.batch_size + 1)\n",
        "\n",
        "        losses = torch.zeros(iter_num)\n",
        "        for k in range(iter_num):\n",
        "            if split == \"train\":\n",
        "                 try:\n",
        "                    batch = next(train_loader_iter)\n",
        "                    X = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    Y = batch['labels'].to(device)\n",
        "                 except:\n",
        "                  continue\n",
        "            else:\n",
        "                 try:\n",
        "                    batch = next(val_loader_iter)\n",
        "                    X = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    Y = batch['labels'].to(device)\n",
        "                 except:\n",
        "                  continue\n",
        "            with ctx:\n",
        "                logits, loss = model(X,Y)\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(Y.cpu().numpy())\n",
        "            losses[k] = loss.item()\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "        out[split][\"loss\"] = losses.mean()\n",
        "        out[split][\"accuracy\"] = accuracy\n",
        "        out[split][\"precision\"] = precision\n",
        "        out[split][\"recall\"] = recall\n",
        "        out[split][\"f1\"] = f1\n",
        "        out[split][\"conf_matrix\"] = conf_matrix\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "def estimate_loss_test(test_loader,num_samples):\n",
        "    out = {\"test\":{}}\n",
        "    test_loader_iter = iter(test_loader)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for split in ['test']:\n",
        "        if test_loader.batch_size == 1:\n",
        "          iter_num = num_samples\n",
        "        else:\n",
        "          iter_num = min( num_samples // test_loader.batch_size + 1  , eval_iters // test_loader.batch_size + 1)\n",
        "        losses = torch.zeros(iter_num)\n",
        "        print(\"Number of Samples Test = \",iter_num)\n",
        "        for k in range(iter_num):\n",
        "            if split == \"test\":\n",
        "                 try:\n",
        "                  batch = next(test_loader_iter)\n",
        "                  X = batch['input_ids'].to(device)\n",
        "                  attention_mask = batch['attention_mask'].to(device)\n",
        "                  Y = batch['labels'].to(device)\n",
        "                 except:\n",
        "                  continue\n",
        "            with ctx:\n",
        "                logits,loss = model(X,Y)\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(Y.cpu().numpy())\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        #print(\"All Labels\",all_labels)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "        out[split][\"loss\"] = losses.mean()\n",
        "        out[split][\"accuracy\"] = accuracy\n",
        "        out[split][\"precision\"] = precision\n",
        "        out[split][\"recall\"] = recall\n",
        "        out[split][\"f1\"] = f1\n",
        "        out[split][\"conf_matrix\"] = conf_matrix\n",
        "\n",
        "    model.train()\n",
        "    return out,all_labels,all_preds\n",
        "\n",
        "train_loader_iter = iter(train_loader)\n",
        "test_loader_iter = iter(test_loader)\n",
        "val_loader_iter = iter(val_loader)\n",
        "batch = next(train_loader_iter)\n",
        "X = batch['input_ids'].to(device)\n",
        "attention_mask = batch['attention_mask'].to(device)\n",
        "Y = batch['labels'].to(device)\n",
        "\n",
        "train_loss_metric = MeanMetric()\n",
        "val_loss_metric = MeanMetric()\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        out_res = estimate_loss(train_loader_iter,val_loader_iter,len(train_dataset), len(val_dataset))\n",
        "        print(f\"step {iter_num}: train loss {train_loss_metric.compute():.4f}, val loss {val_loss_metric.compute():.4f}\")\n",
        "        print(f\"step val accuracy {out_res['val']['accuracy']:.4f}\")\n",
        "\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": train_loss_metric.compute(),\n",
        "                \"val/loss\": val_loss_metric.compute(),\n",
        "                \"lr\": lr,\n",
        "                \"val/acc\":out_res['val']['accuracy'] ,\n",
        "                #\"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "\n",
        "        if out_res['val'][\"loss\"] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = out_res['val'][\"loss\"]\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num % eval_test_interval == 0:\n",
        "        out_res,all_labels,all_preds = estimate_loss_test(test_loader,len(test_dataset))\n",
        "        test_acc = out_res['test']['accuracy']\n",
        "        test_precision = out_res['test']['precision']\n",
        "        test_recall = out_res['test']['recall']\n",
        "        test_f1 = out_res['test']['f1']\n",
        "        print(f\"Test accuracy {test_acc:.4f}\")\n",
        "        print(f\"Test precision {test_precision:.4f}\")\n",
        "        print(f\"Test recall {test_recall:.4f}\")\n",
        "        print(f\"Test F1 {test_f1:.4f}\")\n",
        "        print(\"Confusion Matrix\")\n",
        "        print(\"------------------\")\n",
        "        print(out_res['test']['conf_matrix'])\n",
        "\n",
        "        if wandb_log:\n",
        "            wandb.log({\"Test_Accuracy\": test_acc, \"Test_Precision\": test_precision, \"Test_Recall\": test_recall, \"Test_F1_Score\": test_f1})\n",
        "            x_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "            y_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "            #wandb.log({\"Confusion Matrix\": wandb.plots.HeatMap(\n",
        "            #x_labels, y_labels, out_res['test']['conf_matrix'], show_text=True\n",
        "            #)})\n",
        "            #conf_matrix_img = plot_confusion_matrix(out_res['test']['conf_matrix'], x_labels, y_labels)\n",
        "            #print(\"All labels and all preds\", all_labels, all_preds)\n",
        "            wandb.log({\"Confusion Matrix\":wandb.plot.confusion_matrix(probs=None,\n",
        "                                            y_true=all_labels, preds=all_preds,\n",
        "                                            class_names=[\"negative\", \"neutral\", \"positive\"])})\n",
        "\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X,Y)\n",
        "            #print(logits)\n",
        "            #print(Y)\n",
        "            #print(loss)\n",
        "            train_loss_metric.update(loss.item())\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        try:\n",
        "          batch = next(train_loader_iter)\n",
        "        except:\n",
        "          train_loader_iter = iter(train_loader)\n",
        "          batch = next(train_loader_iter)\n",
        "        X = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        Y = batch['labels'].to(device)\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "          batch_val = next(val_loader_iter)\n",
        "        except:\n",
        "          val_loader_iter= iter(val_loader)\n",
        "          batch_val = next(val_loader_iter)\n",
        "\n",
        "        X_val = batch_val['input_ids'].to(device)\n",
        "        attention_mask = batch_val['attention_mask'].to(device)\n",
        "        Y_val = batch_val['labels'].to(device)\n",
        "        logits_val , loss_val = model(X_val,Y_val)\n",
        "        val_loss_metric.update(loss_val.item())\n",
        "    model.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            #mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            mfu = 0\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W0DD-JS21SX4",
        "outputId": "84d9f878-cddd-4014-fba2-cb30c7e4f8d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 9urflm9m\n",
            "Sweep URL: https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tzdtbbf4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maliyigitbasaran\u001b[0m (\u001b[33maliyigitbasaran-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (2.3s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250401_234040-tzdtbbf4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/tzdtbbf4' target=\"_blank\">sweet-sweep-1</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/tzdtbbf4' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/tzdtbbf4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3512\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.6172, time 33214.58ms\n",
            "iter 1: loss 0.9375, time 1345.72ms\n",
            "iter 2: loss 0.6367, time 1300.18ms\n",
            "iter 3: loss 2.0781, time 1277.03ms\n",
            "iter 4: loss 1.0234, time 1343.48ms\n",
            "iter 5: loss 0.9727, time 1341.86ms\n",
            "iter 6: loss 0.9063, time 1316.47ms\n",
            "iter 7: loss 0.8516, time 1303.82ms\n",
            "iter 8: loss 0.8438, time 1359.57ms\n",
            "iter 9: loss 1.1250, time 1371.80ms\n",
            "iter 10: loss 0.9883, time 1384.72ms\n",
            "iter 11: loss 0.9180, time 1365.68ms\n",
            "iter 12: loss 0.9023, time 1688.21ms\n",
            "iter 13: loss 0.6055, time 1609.16ms\n",
            "iter 14: loss 0.6445, time 1445.91ms\n",
            "iter 15: loss 0.5156, time 1424.98ms\n",
            "iter 16: loss 0.2949, time 1430.21ms\n",
            "iter 17: loss 0.2324, time 1365.05ms\n",
            "iter 18: loss 0.0659, time 1330.18ms\n",
            "iter 19: loss 0.0854, time 1414.27ms\n",
            "step 20: train loss 1.0956, val loss 0.8711\n",
            "step val accuracy 0.6834\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.0300, time 14155.48ms\n",
            "iter 21: loss 0.0476, time 1437.48ms\n",
            "iter 22: loss 3.4063, time 1344.90ms\n",
            "iter 23: loss 0.7461, time 1409.85ms\n",
            "iter 24: loss 0.9922, time 1396.36ms\n",
            "iter 25: loss 1.2109, time 1414.24ms\n",
            "iter 26: loss 0.3828, time 1373.54ms\n",
            "iter 27: loss 0.0396, time 1364.29ms\n",
            "iter 28: loss 0.3125, time 1384.87ms\n",
            "iter 29: loss 0.0137, time 1314.66ms\n",
            "iter 30: loss 2.3750, time 1367.54ms\n",
            "iter 31: loss 0.0383, time 1348.57ms\n",
            "iter 32: loss 0.3887, time 1457.13ms\n",
            "iter 33: loss 0.1904, time 1395.20ms\n",
            "iter 34: loss 0.3594, time 1301.86ms\n",
            "iter 35: loss 2.3750, time 1346.76ms\n",
            "iter 36: loss 1.0391, time 1434.80ms\n",
            "iter 37: loss 0.0688, time 1285.85ms\n",
            "iter 38: loss 0.4648, time 1347.07ms\n",
            "iter 39: loss 0.3594, time 1437.62ms\n",
            "step 40: train loss 0.8107, val loss 0.6093\n",
            "step val accuracy 0.8151\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0354, time 15206.44ms\n",
            "iter 41: loss 0.0114, time 1290.55ms\n",
            "iter 42: loss 0.9844, time 1311.26ms\n",
            "iter 43: loss 0.7461, time 1325.42ms\n",
            "iter 44: loss 0.0107, time 1325.53ms\n",
            "iter 45: loss 0.0197, time 1308.30ms\n",
            "iter 46: loss 0.0146, time 1308.60ms\n",
            "iter 47: loss 0.3457, time 1333.46ms\n",
            "iter 48: loss 0.7227, time 1348.42ms\n",
            "iter 49: loss 0.0791, time 1388.40ms\n",
            "iter 50: loss 0.0167, time 1346.13ms\n",
            "iter 51: loss 0.3027, time 1316.98ms\n",
            "iter 52: loss 0.2930, time 1333.11ms\n",
            "iter 53: loss 0.0645, time 1307.22ms\n",
            "iter 54: loss 0.1924, time 1302.87ms\n",
            "iter 55: loss 0.0010, time 1395.25ms\n",
            "iter 56: loss 0.0017, time 1310.00ms\n",
            "iter 57: loss 0.0007, time 1356.63ms\n",
            "iter 58: loss 0.0008, time 1301.11ms\n",
            "iter 59: loss 0.0009, time 1289.71ms\n",
            "step 60: train loss 0.6148, val loss 0.5041\n",
            "step val accuracy 0.8728\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0008, time 15022.21ms\n",
            "iter 61: loss 1.1797, time 1442.49ms\n",
            "iter 62: loss 0.0052, time 1433.65ms\n",
            "iter 63: loss 0.0006, time 1337.15ms\n",
            "iter 64: loss 0.0013, time 1311.23ms\n",
            "iter 65: loss 0.4648, time 1422.03ms\n",
            "iter 66: loss 0.0280, time 1363.22ms\n",
            "iter 67: loss 0.0220, time 1360.07ms\n",
            "iter 68: loss 0.0007, time 1422.68ms\n",
            "iter 69: loss 0.0003, time 1459.68ms\n",
            "iter 70: loss 0.0006, time 1416.51ms\n",
            "iter 71: loss 0.0013, time 1377.06ms\n",
            "iter 72: loss 0.0058, time 1367.67ms\n",
            "iter 73: loss 0.0008, time 1323.28ms\n",
            "iter 74: loss 0.0005, time 1471.19ms\n",
            "iter 75: loss 0.1299, time 1413.70ms\n",
            "iter 76: loss 0.0063, time 1327.25ms\n",
            "iter 77: loss 0.7500, time 1449.55ms\n",
            "iter 78: loss 0.0013, time 1410.66ms\n",
            "iter 79: loss 0.0004, time 1375.70ms\n",
            "step 80: train loss 0.4969, val loss 0.5298\n",
            "step val accuracy 0.9305\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0352, time 13792.28ms\n",
            "iter 81: loss 2.7500, time 2057.60ms\n",
            "iter 82: loss 0.0019, time 2186.32ms\n",
            "iter 83: loss 0.0302, time 2039.21ms\n",
            "iter 84: loss 0.0013, time 1436.39ms\n",
            "iter 85: loss 0.0017, time 1324.45ms\n",
            "iter 86: loss 0.0003, time 1348.11ms\n",
            "iter 87: loss 0.3047, time 1352.93ms\n",
            "iter 88: loss 0.0540, time 1320.85ms\n",
            "iter 89: loss 0.0090, time 1344.41ms\n",
            "iter 90: loss 0.0006, time 1306.59ms\n",
            "iter 91: loss 0.0074, time 1318.92ms\n",
            "iter 92: loss 0.0004, time 1334.71ms\n",
            "iter 93: loss 0.0396, time 1338.91ms\n",
            "iter 94: loss 0.0036, time 1298.77ms\n",
            "iter 95: loss 0.3613, time 1358.69ms\n",
            "iter 96: loss 0.2695, time 1671.37ms\n",
            "iter 97: loss 0.0004, time 1962.02ms\n",
            "iter 98: loss 0.0164, time 1428.47ms\n",
            "iter 99: loss 0.0009, time 1548.32ms\n",
            "step 100: train loss 0.4204, val loss 0.5379\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6000\n",
            "Test precision 0.6481\n",
            "Test recall 0.6000\n",
            "Test F1 0.5820\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[8 2 0]\n",
            " [0 8 2]\n",
            " [0 8 2]]\n",
            "iter 100: loss 0.0002, time 18533.70ms\n",
            "iter 101: loss 0.0396, time 1384.94ms\n",
            "iter 102: loss 0.0002, time 1350.82ms\n",
            "iter 103: loss 0.0102, time 1371.63ms\n",
            "iter 104: loss 0.0012, time 1363.30ms\n",
            "iter 105: loss 0.0017, time 1369.64ms\n",
            "iter 106: loss 0.0008, time 1407.86ms\n",
            "iter 107: loss 0.0006, time 1384.79ms\n",
            "iter 108: loss 0.0004, time 1351.52ms\n",
            "iter 109: loss 0.0009, time 1343.96ms\n",
            "iter 110: loss 0.0023, time 1357.32ms\n",
            "iter 111: loss 0.0007, time 1324.26ms\n",
            "iter 112: loss 0.0001, time 1369.05ms\n",
            "iter 113: loss 0.0003, time 1370.30ms\n",
            "iter 114: loss 0.0006, time 1425.06ms\n",
            "iter 115: loss 0.0005, time 1375.80ms\n",
            "iter 116: loss 0.0003, time 1332.73ms\n",
            "iter 117: loss 0.0728, time 1401.81ms\n",
            "iter 118: loss 0.0051, time 1340.58ms\n",
            "iter 119: loss 0.0001, time 1379.77ms\n",
            "step 120: train loss 0.3628, val loss 0.5033\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0039, time 14414.25ms\n",
            "iter 121: loss 0.0013, time 1413.82ms\n",
            "iter 122: loss 0.0020, time 1451.86ms\n",
            "iter 123: loss 0.0044, time 1429.08ms\n",
            "iter 124: loss 0.0013, time 1351.76ms\n",
            "iter 125: loss 0.0001, time 1416.20ms\n",
            "iter 126: loss 0.0000, time 1337.83ms\n",
            "iter 127: loss 0.0038, time 1563.07ms\n",
            "iter 128: loss 0.0002, time 1493.16ms\n",
            "iter 129: loss 0.0021, time 1497.04ms\n",
            "iter 130: loss 0.0025, time 1721.39ms\n",
            "iter 131: loss 0.0010, time 1375.53ms\n",
            "iter 132: loss 0.0004, time 1331.40ms\n",
            "iter 133: loss 0.0006, time 1351.92ms\n",
            "iter 134: loss 0.0004, time 1403.63ms\n",
            "iter 135: loss 0.0004, time 1420.51ms\n",
            "iter 136: loss 0.0015, time 1524.61ms\n",
            "iter 137: loss 0.0005, time 1426.51ms\n",
            "iter 138: loss 0.0320, time 1604.03ms\n",
            "iter 139: loss 0.0010, time 1483.22ms\n",
            "step 140: train loss 0.3197, val loss 0.5185\n",
            "step val accuracy 0.9660\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0001, time 15347.91ms\n",
            "iter 141: loss 0.0000, time 1347.65ms\n",
            "iter 142: loss 0.0025, time 1521.32ms\n",
            "iter 143: loss 0.0000, time 1439.64ms\n",
            "iter 144: loss 0.0013, time 1358.79ms\n",
            "iter 145: loss 0.0008, time 1406.26ms\n",
            "iter 146: loss 0.0007, time 1398.03ms\n",
            "iter 147: loss 0.0000, time 1388.66ms\n",
            "iter 148: loss 0.0003, time 1375.75ms\n",
            "iter 149: loss 0.0006, time 1406.68ms\n",
            "iter 150: loss 0.0005, time 1390.91ms\n",
            "iter 151: loss 0.0009, time 1417.32ms\n",
            "iter 152: loss 0.0109, time 1358.35ms\n",
            "iter 153: loss 0.0005, time 1387.20ms\n",
            "iter 154: loss 0.0007, time 1322.76ms\n",
            "iter 155: loss 0.0442, time 1346.05ms\n",
            "iter 156: loss 0.0002, time 1404.19ms\n",
            "iter 157: loss 0.0001, time 1355.60ms\n",
            "iter 158: loss 0.0001, time 1387.36ms\n",
            "iter 159: loss 0.0139, time 1295.62ms\n",
            "step 160: train loss 0.2845, val loss 0.5337\n",
            "step val accuracy 0.9601\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0023, time 14046.20ms\n",
            "iter 161: loss 0.0160, time 1351.65ms\n",
            "iter 162: loss 0.0005, time 1291.69ms\n",
            "iter 163: loss 0.0018, time 1303.42ms\n",
            "iter 164: loss 0.0000, time 1373.68ms\n",
            "iter 165: loss 0.0020, time 1360.80ms\n",
            "iter 166: loss 0.0000, time 1315.80ms\n",
            "iter 167: loss 0.0001, time 1317.55ms\n",
            "iter 168: loss 0.0001, time 1407.13ms\n",
            "iter 169: loss 0.0003, time 1387.85ms\n",
            "iter 170: loss 0.0002, time 1377.20ms\n",
            "iter 171: loss 0.0081, time 1329.51ms\n",
            "iter 172: loss 0.0058, time 1337.83ms\n",
            "iter 173: loss 0.0001, time 1339.52ms\n",
            "iter 174: loss 0.0001, time 1362.16ms\n",
            "iter 175: loss 0.0000, time 1324.24ms\n",
            "iter 176: loss 0.0029, time 1813.63ms\n",
            "iter 177: loss 0.0000, time 1740.97ms\n",
            "iter 178: loss 0.0007, time 1542.16ms\n",
            "iter 179: loss 0.0007, time 1650.59ms\n",
            "step 180: train loss 0.2565, val loss 0.5025\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 13850.92ms\n",
            "iter 181: loss 0.0006, time 1378.17ms\n",
            "iter 182: loss 0.0000, time 1328.17ms\n",
            "iter 183: loss 0.0000, time 1564.83ms\n",
            "iter 184: loss 0.0003, time 1338.47ms\n",
            "iter 185: loss 0.0003, time 1324.44ms\n",
            "iter 186: loss 0.0000, time 1303.45ms\n",
            "iter 187: loss 0.0001, time 1338.95ms\n",
            "iter 188: loss 0.0000, time 1271.04ms\n",
            "iter 189: loss 0.0000, time 1341.16ms\n",
            "iter 190: loss 0.0001, time 1300.81ms\n",
            "iter 191: loss 0.0003, time 1301.59ms\n",
            "iter 192: loss 0.0000, time 1309.78ms\n",
            "iter 193: loss 0.0000, time 1342.27ms\n",
            "iter 194: loss 0.0000, time 1418.53ms\n",
            "iter 195: loss 0.0001, time 1496.47ms\n",
            "iter 196: loss 0.0004, time 1613.47ms\n",
            "iter 197: loss 0.0000, time 1335.57ms\n",
            "iter 198: loss 0.0011, time 1368.65ms\n",
            "iter 199: loss 0.0005, time 1337.54ms\n",
            "step 200: train loss 0.2334, val loss 0.5041\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6000\n",
            "Test precision 0.7356\n",
            "Test recall 0.6000\n",
            "Test F1 0.5299\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  7  0]\n",
            " [ 0  9  1]]\n",
            "iter 200: loss 0.0004, time 17940.77ms\n",
            "iter 201: loss 0.0001, time 1708.33ms\n",
            "iter 202: loss 0.0059, time 1609.72ms\n",
            "iter 203: loss 0.0034, time 1291.10ms\n",
            "iter 204: loss 0.0000, time 1293.94ms\n",
            "iter 205: loss 0.0001, time 1380.77ms\n",
            "iter 206: loss 0.0001, time 1409.83ms\n",
            "iter 207: loss 0.0001, time 1407.99ms\n",
            "iter 208: loss 0.0004, time 1439.86ms\n",
            "iter 209: loss 0.0000, time 1422.95ms\n",
            "iter 210: loss 0.0042, time 1327.96ms\n",
            "iter 211: loss 0.0001, time 1420.67ms\n",
            "iter 212: loss 0.0000, time 1334.28ms\n",
            "iter 213: loss 0.0000, time 1386.61ms\n",
            "iter 214: loss 0.0000, time 1374.09ms\n",
            "iter 215: loss 0.0000, time 1345.23ms\n",
            "iter 216: loss 0.0006, time 1314.19ms\n",
            "iter 217: loss 0.0001, time 1343.29ms\n",
            "iter 218: loss 0.0000, time 1447.80ms\n",
            "iter 219: loss 0.0000, time 1436.61ms\n",
            "step 220: train loss 0.2134, val loss 0.5123\n",
            "step val accuracy 0.9675\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 14227.80ms\n",
            "iter 221: loss 0.0002, time 1721.38ms\n",
            "iter 222: loss 0.0000, time 1743.26ms\n",
            "iter 223: loss 0.0008, time 1741.56ms\n",
            "iter 224: loss 0.0087, time 1639.62ms\n",
            "iter 225: loss 0.0001, time 1316.08ms\n",
            "iter 226: loss 0.0018, time 1318.65ms\n",
            "iter 227: loss 0.0001, time 1683.51ms\n",
            "iter 228: loss 0.0001, time 1343.17ms\n",
            "iter 229: loss 0.0000, time 1328.60ms\n",
            "iter 230: loss 0.0001, time 1379.31ms\n",
            "iter 231: loss 0.0000, time 1333.51ms\n",
            "iter 232: loss 0.0000, time 1321.49ms\n",
            "iter 233: loss 0.0000, time 1366.56ms\n",
            "iter 234: loss 0.0000, time 1303.80ms\n",
            "iter 235: loss 0.0002, time 1271.76ms\n",
            "iter 236: loss 0.0000, time 1387.14ms\n",
            "iter 237: loss 0.0015, time 1286.55ms\n",
            "iter 238: loss 0.0001, time 1313.49ms\n",
            "iter 239: loss 0.0001, time 1335.27ms\n",
            "step 240: train loss 0.1969, val loss 0.4962\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0002, time 13877.70ms\n",
            "iter 241: loss 0.0000, time 1317.77ms\n",
            "iter 242: loss 0.0002, time 1332.47ms\n",
            "iter 243: loss 0.0000, time 1331.17ms\n",
            "iter 244: loss 0.0001, time 1283.39ms\n",
            "iter 245: loss 0.0000, time 1335.31ms\n",
            "iter 246: loss 0.0000, time 1326.98ms\n",
            "iter 247: loss 0.0001, time 1272.31ms\n",
            "iter 248: loss 0.0000, time 1349.18ms\n",
            "iter 249: loss 0.0000, time 1276.35ms\n",
            "iter 250: loss 0.0014, time 1268.36ms\n",
            "iter 251: loss 0.0000, time 1276.60ms\n",
            "iter 252: loss 0.0000, time 1277.79ms\n",
            "iter 253: loss 0.0000, time 1304.55ms\n",
            "iter 254: loss 0.0000, time 1271.69ms\n",
            "iter 255: loss 0.0000, time 1277.95ms\n",
            "iter 256: loss 0.0001, time 1426.68ms\n",
            "iter 257: loss 0.0000, time 1270.58ms\n",
            "iter 258: loss 0.0000, time 1343.00ms\n",
            "iter 259: loss 0.0000, time 1337.58ms\n",
            "step 260: train loss 0.1846, val loss 0.4868\n",
            "step val accuracy 0.9675\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 13740.64ms\n",
            "iter 261: loss 0.0007, time 1388.04ms\n",
            "iter 262: loss 0.0017, time 1294.06ms\n",
            "iter 263: loss 0.0000, time 1309.54ms\n",
            "iter 264: loss 0.0000, time 1306.37ms\n",
            "iter 265: loss 0.0001, time 1296.53ms\n",
            "iter 266: loss 0.0001, time 1299.16ms\n",
            "iter 267: loss 0.0002, time 1282.99ms\n",
            "iter 268: loss 0.0001, time 1367.87ms\n",
            "iter 269: loss 0.0002, time 1296.93ms\n",
            "iter 270: loss 0.0000, time 1326.34ms\n",
            "iter 271: loss 0.0000, time 1293.34ms\n",
            "iter 272: loss 0.0000, time 1678.66ms\n",
            "iter 273: loss 0.0001, time 1407.21ms\n",
            "iter 274: loss 0.0001, time 1305.67ms\n",
            "iter 275: loss 0.0001, time 1360.88ms\n",
            "iter 276: loss 7.5938, time 1337.83ms\n",
            "iter 277: loss 0.0000, time 1315.39ms\n",
            "iter 278: loss 0.0000, time 1321.04ms\n",
            "iter 279: loss 0.0001, time 1293.99ms\n",
            "step 280: train loss 0.1732, val loss 0.4800\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0051, time 13243.58ms\n",
            "iter 281: loss 0.0000, time 1356.60ms\n",
            "iter 282: loss 0.0014, time 1408.01ms\n",
            "iter 283: loss 0.0000, time 1322.44ms\n",
            "iter 284: loss 0.0001, time 1349.13ms\n",
            "iter 285: loss 0.0000, time 1353.13ms\n",
            "iter 286: loss 0.0084, time 1424.64ms\n",
            "iter 287: loss 0.0000, time 1419.08ms\n",
            "iter 288: loss 0.0000, time 1404.75ms\n",
            "iter 289: loss 0.0001, time 1336.43ms\n",
            "iter 290: loss 5.3438, time 1362.52ms\n",
            "iter 291: loss 0.0001, time 1443.15ms\n",
            "iter 292: loss 0.0004, time 1452.90ms\n",
            "iter 293: loss 0.0000, time 1332.17ms\n",
            "iter 294: loss 0.0001, time 1341.17ms\n",
            "iter 295: loss 0.0000, time 1336.32ms\n",
            "iter 296: loss 0.0000, time 1446.70ms\n",
            "iter 297: loss 0.0000, time 1356.63ms\n",
            "iter 298: loss 0.0000, time 1348.36ms\n",
            "iter 299: loss 0.0001, time 1341.40ms\n",
            "step 300: train loss 0.1624, val loss 0.4716\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6667\n",
            "Test precision 0.6897\n",
            "Test recall 0.6667\n",
            "Test F1 0.6495\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  6  1]\n",
            " [ 0  6  4]]\n",
            "iter 300: loss 0.0000, time 15733.65ms\n",
            "iter 301: loss 0.0001, time 1396.58ms\n",
            "iter 302: loss 0.0000, time 1341.04ms\n",
            "iter 303: loss 0.0000, time 1459.58ms\n",
            "iter 304: loss 0.0000, time 1353.29ms\n",
            "iter 305: loss 0.0000, time 1329.36ms\n",
            "iter 306: loss 0.0101, time 1322.38ms\n",
            "iter 307: loss 0.0000, time 1296.35ms\n",
            "iter 308: loss 0.0001, time 1315.37ms\n",
            "iter 309: loss 0.0000, time 1332.30ms\n",
            "iter 310: loss 0.0000, time 1344.70ms\n",
            "iter 311: loss 0.0001, time 1342.03ms\n",
            "iter 312: loss 0.0000, time 1336.71ms\n",
            "iter 313: loss 0.0014, time 1363.11ms\n",
            "iter 314: loss 0.0001, time 1359.54ms\n",
            "iter 315: loss 0.0001, time 1379.05ms\n",
            "iter 316: loss 0.0000, time 1319.19ms\n",
            "iter 317: loss 0.0087, time 1334.24ms\n",
            "iter 318: loss 0.0002, time 1337.78ms\n",
            "iter 319: loss 0.0001, time 1666.36ms\n",
            "step 320: train loss 0.1530, val loss 0.4435\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 14028.88ms\n",
            "iter 321: loss 0.0005, time 1387.86ms\n",
            "iter 322: loss 0.0000, time 1356.55ms\n",
            "iter 323: loss 0.0000, time 1306.08ms\n",
            "iter 324: loss 0.0000, time 1414.74ms\n",
            "iter 325: loss 0.0002, time 1308.51ms\n",
            "iter 326: loss 0.0000, time 1373.20ms\n",
            "iter 327: loss 0.0000, time 1300.51ms\n",
            "iter 328: loss 0.0001, time 1324.34ms\n",
            "iter 329: loss 0.0002, time 1332.36ms\n",
            "iter 330: loss 0.0000, time 1258.80ms\n",
            "iter 331: loss 0.0000, time 1338.25ms\n",
            "iter 332: loss 0.0000, time 1323.82ms\n",
            "iter 333: loss 0.0000, time 1405.31ms\n",
            "iter 334: loss 0.0000, time 1338.62ms\n",
            "iter 335: loss 0.0000, time 1342.61ms\n",
            "iter 336: loss 0.0000, time 1351.94ms\n",
            "iter 337: loss 0.0000, time 1380.88ms\n",
            "iter 338: loss 0.0000, time 1395.90ms\n",
            "iter 339: loss 0.0000, time 1327.50ms\n",
            "step 340: train loss 0.1445, val loss 0.4448\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0001, time 14440.18ms\n",
            "iter 341: loss 0.0000, time 1384.10ms\n",
            "iter 342: loss 0.0000, time 1399.02ms\n",
            "iter 343: loss 0.0000, time 1453.23ms\n",
            "iter 344: loss 0.0000, time 1492.61ms\n",
            "iter 345: loss 0.0000, time 1531.04ms\n",
            "iter 346: loss 0.0000, time 1403.53ms\n",
            "iter 347: loss 0.0000, time 1357.47ms\n",
            "iter 348: loss 0.0000, time 1313.94ms\n",
            "iter 349: loss 0.0000, time 1331.14ms\n",
            "iter 350: loss 0.0000, time 1347.93ms\n",
            "iter 351: loss 0.0000, time 1306.63ms\n",
            "iter 352: loss 0.0000, time 1306.24ms\n",
            "iter 353: loss 0.0000, time 1332.50ms\n",
            "iter 354: loss 0.0021, time 1291.15ms\n",
            "iter 355: loss 0.0000, time 1334.90ms\n",
            "iter 356: loss 0.0000, time 1513.02ms\n",
            "iter 357: loss 0.0000, time 1301.49ms\n",
            "iter 358: loss 0.0001, time 1317.62ms\n",
            "iter 359: loss 0.0000, time 1305.36ms\n",
            "step 360: train loss 0.1369, val loss 0.4484\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0003, time 13679.71ms\n",
            "iter 361: loss 0.0000, time 1316.23ms\n",
            "iter 362: loss 0.0000, time 1303.56ms\n",
            "iter 363: loss 0.0000, time 1307.33ms\n",
            "iter 364: loss 0.0420, time 1407.95ms\n",
            "iter 365: loss 0.0000, time 1369.16ms\n",
            "iter 366: loss 0.0000, time 1460.44ms\n",
            "iter 367: loss 0.0000, time 1380.11ms\n",
            "iter 368: loss 0.0000, time 1378.13ms\n",
            "iter 369: loss 0.0008, time 1314.50ms\n",
            "iter 370: loss 0.0002, time 1383.84ms\n",
            "iter 371: loss 0.0000, time 1460.79ms\n",
            "iter 372: loss 0.0001, time 1356.16ms\n",
            "iter 373: loss 0.0001, time 1326.42ms\n",
            "iter 374: loss 0.1069, time 1313.11ms\n",
            "iter 375: loss 0.0000, time 1358.36ms\n",
            "iter 376: loss 0.0000, time 1329.58ms\n",
            "iter 377: loss 0.0564, time 1348.54ms\n",
            "iter 378: loss 0.0001, time 1360.37ms\n",
            "iter 379: loss 0.0005, time 1403.59ms\n",
            "step 380: train loss 0.1309, val loss 0.4590\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0001, time 17311.01ms\n",
            "iter 381: loss 0.0000, time 1594.19ms\n",
            "iter 382: loss 0.0001, time 1554.54ms\n",
            "iter 383: loss 0.0000, time 1428.07ms\n",
            "iter 384: loss 0.0007, time 1370.89ms\n",
            "iter 385: loss 0.0000, time 1395.57ms\n",
            "iter 386: loss 0.0000, time 1362.64ms\n",
            "iter 387: loss 0.0009, time 1326.41ms\n",
            "iter 388: loss 0.0081, time 1393.36ms\n",
            "iter 389: loss 0.0000, time 1348.79ms\n",
            "iter 390: loss 0.0000, time 1411.80ms\n",
            "iter 391: loss 0.0000, time 1354.47ms\n",
            "iter 392: loss 0.0000, time 1349.77ms\n",
            "iter 393: loss 0.0001, time 1434.50ms\n",
            "iter 394: loss 0.0005, time 1372.01ms\n",
            "iter 395: loss 0.0000, time 1368.76ms\n",
            "iter 396: loss 0.0000, time 1391.86ms\n",
            "iter 397: loss 0.0001, time 1532.67ms\n",
            "iter 398: loss 0.0000, time 1500.27ms\n",
            "iter 399: loss 0.0000, time 1446.59ms\n",
            "step 400: train loss 0.1249, val loss 0.4653\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6000\n",
            "Test precision 0.4848\n",
            "Test recall 0.6000\n",
            "Test F1 0.5046\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 8  2  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 400: loss 0.0000, time 17515.17ms\n",
            "iter 401: loss 0.0001, time 1279.65ms\n",
            "iter 402: loss 0.0005, time 1308.03ms\n",
            "iter 403: loss 0.0000, time 1361.17ms\n",
            "iter 404: loss 0.0000, time 1372.61ms\n",
            "iter 405: loss 0.0000, time 1327.42ms\n",
            "iter 406: loss 0.0000, time 1352.39ms\n",
            "iter 407: loss 0.0022, time 1307.44ms\n",
            "iter 408: loss 0.0000, time 1370.76ms\n",
            "iter 409: loss 0.0012, time 1332.65ms\n",
            "iter 410: loss 0.0000, time 1383.16ms\n",
            "iter 411: loss 0.0000, time 1370.02ms\n",
            "iter 412: loss 0.0000, time 1285.22ms\n",
            "iter 413: loss 0.0000, time 1352.60ms\n",
            "iter 414: loss 0.0000, time 1292.63ms\n",
            "iter 415: loss 0.0000, time 1448.93ms\n",
            "iter 416: loss 0.0001, time 1334.96ms\n",
            "iter 417: loss 0.0001, time 1303.13ms\n",
            "iter 418: loss 0.0000, time 1275.06ms\n",
            "iter 419: loss 0.0415, time 1338.94ms\n",
            "step 420: train loss 0.1194, val loss 0.4911\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 13596.29ms\n",
            "iter 421: loss 0.0000, time 1270.48ms\n",
            "iter 422: loss 0.0000, time 1329.55ms\n",
            "iter 423: loss 0.0000, time 1297.85ms\n",
            "iter 424: loss 0.0000, time 1303.97ms\n",
            "iter 425: loss 0.0000, time 1402.65ms\n",
            "iter 426: loss 0.0000, time 1391.31ms\n",
            "iter 427: loss 0.0000, time 1425.92ms\n",
            "iter 428: loss 0.0000, time 1380.63ms\n",
            "iter 429: loss 0.0000, time 1346.72ms\n",
            "iter 430: loss 0.0000, time 1423.62ms\n",
            "iter 431: loss 0.0000, time 1319.16ms\n",
            "iter 432: loss 0.0000, time 1316.82ms\n",
            "iter 433: loss 0.0000, time 1390.29ms\n",
            "iter 434: loss 0.0000, time 1695.01ms\n",
            "iter 435: loss 0.0000, time 1806.29ms\n",
            "iter 436: loss 0.0000, time 1446.44ms\n",
            "iter 437: loss 0.0000, time 1656.32ms\n",
            "iter 438: loss 0.0000, time 1552.60ms\n",
            "iter 439: loss 0.0000, time 1522.84ms\n",
            "step 440: train loss 0.1141, val loss 0.4830\n",
            "step val accuracy 0.9808\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 14157.07ms\n",
            "iter 441: loss 0.0000, time 1449.65ms\n",
            "iter 442: loss 0.0021, time 1492.76ms\n",
            "iter 443: loss 0.0000, time 1370.83ms\n",
            "iter 444: loss 0.0000, time 1599.54ms\n",
            "iter 445: loss 0.0000, time 1517.93ms\n",
            "iter 446: loss 0.0003, time 1698.70ms\n",
            "iter 447: loss 0.0000, time 1364.43ms\n",
            "iter 448: loss 0.0000, time 1417.88ms\n",
            "iter 449: loss 0.0000, time 1394.54ms\n",
            "iter 450: loss 0.0000, time 1404.49ms\n",
            "iter 451: loss 0.0000, time 1474.01ms\n",
            "iter 452: loss 0.0000, time 1383.08ms\n",
            "iter 453: loss 0.0000, time 1354.41ms\n",
            "iter 454: loss 0.0000, time 1452.59ms\n",
            "iter 455: loss 0.0000, time 1474.60ms\n",
            "iter 456: loss 0.0000, time 1349.21ms\n",
            "iter 457: loss 0.0000, time 1359.84ms\n",
            "iter 458: loss 0.0000, time 1380.82ms\n",
            "iter 459: loss 0.0000, time 1437.27ms\n",
            "step 460: train loss 0.1092, val loss 0.4810\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 14070.37ms\n",
            "iter 461: loss 0.0000, time 1389.92ms\n",
            "iter 462: loss 0.0001, time 1404.14ms\n",
            "iter 463: loss 0.0000, time 1426.23ms\n",
            "iter 464: loss 0.0000, time 1362.26ms\n",
            "iter 465: loss 0.0000, time 1442.82ms\n",
            "iter 466: loss 0.0000, time 1378.78ms\n",
            "iter 467: loss 0.0000, time 1313.40ms\n",
            "iter 468: loss 0.0000, time 1356.94ms\n",
            "iter 469: loss 0.0000, time 1419.08ms\n",
            "iter 470: loss 0.0000, time 1441.76ms\n",
            "iter 471: loss 0.0001, time 1408.19ms\n",
            "iter 472: loss 0.0000, time 1345.17ms\n",
            "iter 473: loss 0.0000, time 1413.79ms\n",
            "iter 474: loss 0.0000, time 1639.08ms\n",
            "iter 475: loss 0.0000, time 1401.21ms\n",
            "iter 476: loss 0.0000, time 1532.97ms\n",
            "iter 477: loss 0.0000, time 1645.83ms\n",
            "iter 478: loss 0.0000, time 1347.31ms\n",
            "iter 479: loss 0.0000, time 1340.57ms\n",
            "step 480: train loss 0.1051, val loss 0.5159\n",
            "step val accuracy 0.9497\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 13111.70ms\n",
            "iter 481: loss 0.0001, time 1306.86ms\n",
            "iter 482: loss 0.0000, time 1384.65ms\n",
            "iter 483: loss 0.0000, time 1399.95ms\n",
            "iter 484: loss 0.0001, time 1358.60ms\n",
            "iter 485: loss 0.0000, time 1375.12ms\n",
            "iter 486: loss 0.0000, time 1568.36ms\n",
            "iter 487: loss 0.0000, time 1519.61ms\n",
            "iter 488: loss 0.0003, time 1530.18ms\n",
            "iter 489: loss 0.0000, time 1549.76ms\n",
            "iter 490: loss 0.0000, time 1484.64ms\n",
            "iter 491: loss 0.0000, time 1434.56ms\n",
            "iter 492: loss 0.0000, time 1635.70ms\n",
            "iter 493: loss 0.0000, time 1360.63ms\n",
            "iter 494: loss 0.2852, time 1319.55ms\n",
            "iter 495: loss 0.0000, time 1347.41ms\n",
            "iter 496: loss 0.0000, time 1334.70ms\n",
            "iter 497: loss 0.0000, time 1371.94ms\n",
            "iter 498: loss 0.0000, time 1394.33ms\n",
            "iter 499: loss 0.0000, time 1354.39ms\n",
            "step 500: train loss 0.1012, val loss 0.5164\n",
            "step val accuracy 0.9704\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7765\n",
            "Test recall 0.7000\n",
            "Test F1 0.6963\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[8 2 0]\n",
            " [0 9 1]\n",
            " [0 6 4]]\n",
            "iter 500: loss 0.0000, time 18386.71ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▅▇▅█</td></tr><tr><td>Test_F1_Score</td><td>▁▅▄▇▄█</td></tr><tr><td>Test_Precision</td><td>▁▆▇▇▃█</td></tr><tr><td>Test_Recall</td><td>▁▅▅▇▅█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▆▇▇█████████████████████</td></tr><tr><td>val/loss</td><td> █▄▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.7</td></tr><tr><td>Test_F1_Score</td><td>0.6963</td></tr><tr><td>Test_Precision</td><td>0.77647</td></tr><tr><td>Test_Recall</td><td>0.7</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.10124</td></tr><tr><td>val/acc</td><td>0.97041</td></tr><tr><td>val/loss</td><td>0.51636</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sweet-sweep-1</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/tzdtbbf4' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/tzdtbbf4</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250401_234040-tzdtbbf4\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0tw1kx4l with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "creating run (4.5s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_000030-0tw1kx4l</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/0tw1kx4l' target=\"_blank\">wild-sweep-2</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/0tw1kx4l' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/0tw1kx4l</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3666\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.5977, time 26697.90ms\n",
            "iter 1: loss 1.0547, time 1326.90ms\n",
            "iter 2: loss 2.1250, time 1371.58ms\n",
            "iter 3: loss 0.5508, time 1380.72ms\n",
            "iter 4: loss 0.8594, time 1401.14ms\n",
            "iter 5: loss 1.2031, time 1386.42ms\n",
            "iter 6: loss 0.8984, time 1355.85ms\n",
            "iter 7: loss 0.8867, time 1427.44ms\n",
            "iter 8: loss 1.0156, time 1430.01ms\n",
            "iter 9: loss 1.3516, time 1346.03ms\n",
            "iter 10: loss 0.5820, time 1364.88ms\n",
            "iter 11: loss 0.8945, time 1327.94ms\n",
            "iter 12: loss 1.4297, time 1417.50ms\n",
            "iter 13: loss 0.6836, time 1364.92ms\n",
            "iter 14: loss 0.7070, time 1361.81ms\n",
            "iter 15: loss 0.5273, time 1357.97ms\n",
            "iter 16: loss 0.2490, time 1377.11ms\n",
            "iter 17: loss 0.1445, time 1313.91ms\n",
            "iter 18: loss 0.5898, time 1657.88ms\n",
            "iter 19: loss 0.1104, time 1334.04ms\n",
            "step 20: train loss 1.0946, val loss 1.0954\n",
            "step val accuracy 0.8151\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.1113, time 13339.23ms\n",
            "iter 21: loss 0.1108, time 1359.66ms\n",
            "iter 22: loss 0.2061, time 1321.46ms\n",
            "iter 23: loss 0.0143, time 1373.78ms\n",
            "iter 24: loss 0.0479, time 1295.32ms\n",
            "iter 25: loss 0.6016, time 1393.42ms\n",
            "iter 26: loss 0.2988, time 1338.70ms\n",
            "iter 27: loss 0.0033, time 1459.85ms\n",
            "iter 28: loss 0.0972, time 1439.68ms\n",
            "iter 29: loss 0.0113, time 1337.03ms\n",
            "iter 30: loss 0.0398, time 1474.07ms\n",
            "iter 31: loss 0.0121, time 1994.64ms\n",
            "iter 32: loss 0.0120, time 1967.84ms\n",
            "iter 33: loss 0.1216, time 2003.35ms\n",
            "iter 34: loss 0.0077, time 2035.87ms\n",
            "iter 35: loss 0.0014, time 1998.51ms\n",
            "iter 36: loss 0.0063, time 1933.32ms\n",
            "iter 37: loss 0.0010, time 1955.78ms\n",
            "iter 38: loss 0.0237, time 1935.68ms\n",
            "iter 39: loss 0.3770, time 1964.68ms\n",
            "step 40: train loss 0.6616, val loss 0.7311\n",
            "step val accuracy 0.8609\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0505, time 20809.60ms\n",
            "iter 41: loss 0.0034, time 2002.20ms\n",
            "iter 42: loss 1.8125, time 1990.14ms\n",
            "iter 43: loss 2.4531, time 1926.14ms\n",
            "iter 44: loss 0.0007, time 1896.26ms\n",
            "iter 45: loss 0.0003, time 1918.50ms\n",
            "iter 46: loss 0.0003, time 2072.24ms\n",
            "iter 47: loss 3.1406, time 1971.34ms\n",
            "iter 48: loss 0.1494, time 2058.53ms\n",
            "iter 49: loss 0.0006, time 1954.97ms\n",
            "iter 50: loss 0.0171, time 1952.01ms\n",
            "iter 51: loss 0.2793, time 1955.28ms\n",
            "iter 52: loss 0.0703, time 1935.50ms\n",
            "iter 53: loss 0.0630, time 1987.49ms\n",
            "iter 54: loss 0.1523, time 1984.04ms\n",
            "iter 55: loss 0.0005, time 2433.34ms\n",
            "iter 56: loss 0.0008, time 2896.90ms\n",
            "iter 57: loss 1.5078, time 2964.88ms\n",
            "iter 58: loss 0.0023, time 2919.13ms\n",
            "iter 59: loss 0.0010, time 2860.84ms\n",
            "step 60: train loss 0.5125, val loss 0.7494\n",
            "step val accuracy 0.9275\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0007, time 21992.62ms\n",
            "iter 61: loss 0.4043, time 2085.11ms\n",
            "iter 62: loss 0.0005, time 2258.49ms\n",
            "iter 63: loss 0.0007, time 2059.88ms\n",
            "iter 64: loss 0.0266, time 1921.28ms\n",
            "iter 65: loss 0.2061, time 1980.48ms\n",
            "iter 66: loss 0.0132, time 2010.03ms\n",
            "iter 67: loss 0.0011, time 1954.96ms\n",
            "iter 68: loss 0.0055, time 2012.27ms\n",
            "iter 69: loss 0.0068, time 1961.46ms\n",
            "iter 70: loss 0.0131, time 1930.14ms\n",
            "iter 71: loss 0.0041, time 1973.21ms\n",
            "iter 72: loss 0.0001, time 2019.62ms\n",
            "iter 73: loss 0.0007, time 1932.78ms\n",
            "iter 74: loss 0.0011, time 2021.85ms\n",
            "iter 75: loss 0.0131, time 1916.51ms\n",
            "iter 76: loss 0.5117, time 2001.62ms\n",
            "iter 77: loss 0.0253, time 2056.52ms\n",
            "iter 78: loss 0.0693, time 1987.17ms\n",
            "iter 79: loss 0.0208, time 1992.13ms\n",
            "step 80: train loss 0.4175, val loss 0.6704\n",
            "step val accuracy 0.8876\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0015, time 27907.94ms\n",
            "iter 81: loss 0.0928, time 2771.54ms\n",
            "iter 82: loss 0.0002, time 2481.81ms\n",
            "iter 83: loss 0.0006, time 2504.09ms\n",
            "iter 84: loss 0.0005, time 2663.59ms\n",
            "iter 85: loss 0.0007, time 2465.37ms\n",
            "iter 86: loss 0.0001, time 1987.15ms\n",
            "iter 87: loss 0.0165, time 1969.01ms\n",
            "iter 88: loss 0.0001, time 1888.11ms\n",
            "iter 89: loss 0.0029, time 1988.37ms\n",
            "iter 90: loss 0.0006, time 1971.56ms\n",
            "iter 91: loss 0.0004, time 1923.22ms\n",
            "iter 92: loss 0.0000, time 1985.34ms\n",
            "iter 93: loss 0.0021, time 2001.67ms\n",
            "iter 94: loss 0.0107, time 1926.81ms\n",
            "iter 95: loss 0.0034, time 1999.70ms\n",
            "iter 96: loss 4.1250, time 1985.54ms\n",
            "iter 97: loss 0.0002, time 2051.21ms\n",
            "iter 98: loss 0.0099, time 1944.58ms\n",
            "iter 99: loss 0.0000, time 2002.33ms\n",
            "step 100: train loss 0.3503, val loss 0.5959\n",
            "step val accuracy 0.9556\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8387\n",
            "Test recall 0.8333\n",
            "Test F1 0.8347\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 8 1]\n",
            " [0 2 8]]\n",
            "iter 100: loss 0.0008, time 23747.64ms\n",
            "iter 101: loss 0.0028, time 1982.00ms\n",
            "iter 102: loss 0.0001, time 1962.27ms\n",
            "iter 103: loss 0.0002, time 1978.50ms\n",
            "iter 104: loss 0.0757, time 2351.71ms\n",
            "iter 105: loss 0.0014, time 2859.10ms\n",
            "iter 106: loss 0.0011, time 2095.69ms\n",
            "iter 107: loss 0.0020, time 2033.67ms\n",
            "iter 108: loss 0.0001, time 1961.68ms\n",
            "iter 109: loss 0.0003, time 2001.80ms\n",
            "iter 110: loss 0.0010, time 1961.30ms\n",
            "iter 111: loss 0.0001, time 1975.35ms\n",
            "iter 112: loss 0.0051, time 1981.75ms\n",
            "iter 113: loss 0.0182, time 1952.27ms\n",
            "iter 114: loss 0.0002, time 1923.86ms\n",
            "iter 115: loss 0.0001, time 1925.76ms\n",
            "iter 116: loss 0.0006, time 1946.91ms\n",
            "iter 117: loss 0.0009, time 1964.17ms\n",
            "iter 118: loss 0.0004, time 1927.31ms\n",
            "iter 119: loss 1.2266, time 1925.60ms\n",
            "step 120: train loss 0.3035, val loss 0.5019\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0001, time 20203.89ms\n",
            "iter 121: loss 0.0045, time 1949.28ms\n",
            "iter 122: loss 0.0001, time 1970.46ms\n",
            "iter 123: loss 0.0017, time 1937.20ms\n",
            "iter 124: loss 0.0001, time 1928.52ms\n",
            "iter 125: loss 0.0000, time 2011.34ms\n",
            "iter 126: loss 0.0000, time 1929.89ms\n",
            "iter 127: loss 0.0007, time 1969.58ms\n",
            "iter 128: loss 0.0003, time 1925.19ms\n",
            "iter 129: loss 0.0000, time 1979.42ms\n",
            "iter 130: loss 0.0016, time 1971.07ms\n",
            "iter 131: loss 0.0002, time 1958.56ms\n",
            "iter 132: loss 0.0001, time 1994.32ms\n",
            "iter 133: loss 0.0003, time 1955.54ms\n",
            "iter 134: loss 0.0025, time 1917.00ms\n",
            "iter 135: loss 0.0087, time 1957.09ms\n",
            "iter 136: loss 0.0001, time 1939.40ms\n",
            "iter 137: loss 0.0001, time 1953.81ms\n",
            "iter 138: loss 0.0001, time 1946.81ms\n",
            "iter 139: loss 0.0003, time 1893.79ms\n",
            "step 140: train loss 0.2683, val loss 0.4881\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0002, time 20247.19ms\n",
            "iter 141: loss 0.0000, time 1941.56ms\n",
            "iter 142: loss 0.0066, time 1914.55ms\n",
            "iter 143: loss 0.0000, time 1973.04ms\n",
            "iter 144: loss 0.0005, time 1940.75ms\n",
            "iter 145: loss 0.0001, time 1978.14ms\n",
            "iter 146: loss 0.0002, time 1938.52ms\n",
            "iter 147: loss 0.0000, time 1979.20ms\n",
            "iter 148: loss 0.0004, time 1973.02ms\n",
            "iter 149: loss 0.0001, time 2015.01ms\n",
            "iter 150: loss 0.0002, time 2266.70ms\n",
            "iter 151: loss 0.0040, time 2288.95ms\n",
            "iter 152: loss 0.0000, time 1988.85ms\n",
            "iter 153: loss 0.0004, time 2086.16ms\n",
            "iter 154: loss 0.0005, time 1977.48ms\n",
            "iter 155: loss 0.0074, time 1955.09ms\n",
            "iter 156: loss 0.0001, time 1957.85ms\n",
            "iter 157: loss 0.0001, time 1942.04ms\n",
            "iter 158: loss 0.0014, time 2049.35ms\n",
            "iter 159: loss 0.0219, time 2010.18ms\n",
            "step 160: train loss 0.2409, val loss 0.4690\n",
            "step val accuracy 0.9808\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0005, time 20461.33ms\n",
            "iter 161: loss 0.0000, time 1928.56ms\n",
            "iter 162: loss 0.0000, time 1977.00ms\n",
            "iter 163: loss 0.0005, time 1930.54ms\n",
            "iter 164: loss 0.0000, time 1938.01ms\n",
            "iter 165: loss 0.0000, time 1983.13ms\n",
            "iter 166: loss 0.0000, time 1915.38ms\n",
            "iter 167: loss 0.0000, time 1940.08ms\n",
            "iter 168: loss 0.0000, time 1912.91ms\n",
            "iter 169: loss 0.0000, time 1930.29ms\n",
            "iter 170: loss 0.0000, time 1934.35ms\n",
            "iter 171: loss 0.0000, time 1949.19ms\n",
            "iter 172: loss 0.0270, time 1946.39ms\n",
            "iter 173: loss 0.0000, time 1929.55ms\n",
            "iter 174: loss 0.0004, time 1938.31ms\n",
            "iter 175: loss 0.0000, time 1956.23ms\n",
            "iter 176: loss 0.0003, time 1930.06ms\n",
            "iter 177: loss 0.0000, time 1914.71ms\n",
            "iter 178: loss 0.0000, time 1961.61ms\n",
            "iter 179: loss 0.0000, time 1957.35ms\n",
            "step 180: train loss 0.2163, val loss 0.4756\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 20330.11ms\n",
            "iter 181: loss 0.0383, time 1983.84ms\n",
            "iter 182: loss 0.0000, time 1934.85ms\n",
            "iter 183: loss 0.0000, time 1914.97ms\n",
            "iter 184: loss 0.0815, time 1934.64ms\n",
            "iter 185: loss 0.0002, time 1993.47ms\n",
            "iter 186: loss 0.0003, time 1970.03ms\n",
            "iter 187: loss 0.0002, time 1958.82ms\n",
            "iter 188: loss 0.0000, time 1970.82ms\n",
            "iter 189: loss 0.0007, time 1975.75ms\n",
            "iter 190: loss 0.0004, time 1933.65ms\n",
            "iter 191: loss 0.0000, time 1930.95ms\n",
            "iter 192: loss 0.0000, time 1950.84ms\n",
            "iter 193: loss 0.0000, time 1950.84ms\n",
            "iter 194: loss 0.0000, time 1926.92ms\n",
            "iter 195: loss 0.0000, time 1950.46ms\n",
            "iter 196: loss 0.0002, time 1987.14ms\n",
            "iter 197: loss 0.0000, time 2290.57ms\n",
            "iter 198: loss 0.0018, time 1916.68ms\n",
            "iter 199: loss 0.0000, time 1894.75ms\n",
            "step 200: train loss 0.1971, val loss 0.5171\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8750\n",
            "Test recall 0.8000\n",
            "Test F1 0.7944\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  5  5]]\n",
            "iter 200: loss 0.0004, time 23309.66ms\n",
            "iter 201: loss 0.0052, time 1964.69ms\n",
            "iter 202: loss 0.0005, time 1899.57ms\n",
            "iter 203: loss 0.0001, time 1951.65ms\n",
            "iter 204: loss 0.0000, time 1913.53ms\n",
            "iter 205: loss 0.0000, time 1996.05ms\n",
            "iter 206: loss 0.0004, time 1941.02ms\n",
            "iter 207: loss 0.0001, time 1963.44ms\n",
            "iter 208: loss 0.0001, time 1934.59ms\n",
            "iter 209: loss 0.0000, time 1930.51ms\n",
            "iter 210: loss 0.0005, time 1916.34ms\n",
            "iter 211: loss 0.0000, time 1934.60ms\n",
            "iter 212: loss 0.0001, time 1934.58ms\n",
            "iter 213: loss 0.0000, time 1996.75ms\n",
            "iter 214: loss 0.0000, time 1937.40ms\n",
            "iter 215: loss 0.0000, time 1969.46ms\n",
            "iter 216: loss 0.0000, time 1977.85ms\n",
            "iter 217: loss 0.0000, time 1942.07ms\n",
            "iter 218: loss 0.0001, time 1951.95ms\n",
            "iter 219: loss 0.0000, time 1953.22ms\n",
            "step 220: train loss 0.1809, val loss 0.5083\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 20070.75ms\n",
            "iter 221: loss 0.0000, time 1948.35ms\n",
            "iter 222: loss 0.0000, time 1912.12ms\n",
            "iter 223: loss 0.0000, time 1922.83ms\n",
            "iter 224: loss 0.0000, time 1978.25ms\n",
            "iter 225: loss 0.0000, time 1955.18ms\n",
            "iter 226: loss 0.0001, time 1935.44ms\n",
            "iter 227: loss 0.0006, time 1999.06ms\n",
            "iter 228: loss 0.0000, time 1958.45ms\n",
            "iter 229: loss 0.0000, time 1946.19ms\n",
            "iter 230: loss 0.0000, time 1913.83ms\n",
            "iter 231: loss 0.0000, time 1944.15ms\n",
            "iter 232: loss 0.0000, time 1968.16ms\n",
            "iter 233: loss 0.0000, time 1964.27ms\n",
            "iter 234: loss 0.0000, time 1975.22ms\n",
            "iter 235: loss 0.0000, time 1925.77ms\n",
            "iter 236: loss 0.0000, time 1923.36ms\n",
            "iter 237: loss 0.0000, time 1976.74ms\n",
            "iter 238: loss 0.0000, time 1977.56ms\n",
            "iter 239: loss 0.0001, time 1923.48ms\n",
            "step 240: train loss 0.1680, val loss 0.4842\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 20563.75ms\n",
            "iter 241: loss 0.0000, time 1908.26ms\n",
            "iter 242: loss 0.0000, time 1914.80ms\n",
            "iter 243: loss 0.0000, time 1966.92ms\n",
            "iter 244: loss 0.0000, time 1982.43ms\n",
            "iter 245: loss 0.0000, time 1907.52ms\n",
            "iter 246: loss 0.0000, time 1890.86ms\n",
            "iter 247: loss 0.0000, time 1950.15ms\n",
            "iter 248: loss 0.0046, time 1955.95ms\n",
            "iter 249: loss 0.0005, time 1959.26ms\n",
            "iter 250: loss 0.0000, time 1927.82ms\n",
            "iter 251: loss 0.0000, time 1895.99ms\n",
            "iter 252: loss 0.0000, time 1963.96ms\n",
            "iter 253: loss 0.0000, time 1924.83ms\n",
            "iter 254: loss 0.0000, time 1933.50ms\n",
            "iter 255: loss 0.0001, time 1900.37ms\n",
            "iter 256: loss 0.0000, time 1996.23ms\n",
            "iter 257: loss 0.0000, time 1934.51ms\n",
            "iter 258: loss 0.0000, time 1918.19ms\n",
            "iter 259: loss 0.0000, time 1994.77ms\n",
            "step 260: train loss 0.1568, val loss 0.4518\n",
            "step val accuracy 0.9704\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 20400.87ms\n",
            "iter 261: loss 0.0002, time 1959.00ms\n",
            "iter 262: loss 0.0025, time 1904.12ms\n",
            "iter 263: loss 0.0000, time 1927.33ms\n",
            "iter 264: loss 0.0000, time 1930.59ms\n",
            "iter 265: loss 0.0000, time 1956.69ms\n",
            "iter 266: loss 0.0003, time 1968.84ms\n",
            "iter 267: loss 0.0003, time 1975.52ms\n",
            "iter 268: loss 0.7734, time 1924.22ms\n",
            "iter 269: loss 0.0001, time 1950.84ms\n",
            "iter 270: loss 0.0000, time 1977.81ms\n",
            "iter 271: loss 0.0024, time 1949.84ms\n",
            "iter 272: loss 0.0008, time 1962.09ms\n",
            "iter 273: loss 0.0001, time 1978.09ms\n",
            "iter 274: loss 0.0000, time 1971.60ms\n",
            "iter 275: loss 0.0000, time 1945.79ms\n",
            "iter 276: loss 0.0000, time 1929.73ms\n",
            "iter 277: loss 0.0000, time 1940.91ms\n",
            "iter 278: loss 0.0000, time 1919.46ms\n",
            "iter 279: loss 0.0000, time 1957.09ms\n",
            "step 280: train loss 0.1469, val loss 0.4345\n",
            "step val accuracy 0.9896\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.1069, time 20580.78ms\n",
            "iter 281: loss 0.0000, time 1971.96ms\n",
            "iter 282: loss 0.0001, time 1925.51ms\n",
            "iter 283: loss 0.0000, time 1950.41ms\n",
            "iter 284: loss 0.0002, time 1912.78ms\n",
            "iter 285: loss 0.0000, time 1932.06ms\n",
            "iter 286: loss 0.0000, time 1992.83ms\n",
            "iter 287: loss 0.0000, time 1936.77ms\n",
            "iter 288: loss 0.0002, time 1946.32ms\n",
            "iter 289: loss 0.0000, time 1911.34ms\n",
            "iter 290: loss 0.0000, time 1943.78ms\n",
            "iter 291: loss 0.0000, time 1960.16ms\n",
            "iter 292: loss 0.0001, time 1993.11ms\n",
            "iter 293: loss 0.0000, time 1963.24ms\n",
            "iter 294: loss 0.0000, time 1937.02ms\n",
            "iter 295: loss 0.0000, time 1952.71ms\n",
            "iter 296: loss 0.0000, time 1944.44ms\n",
            "iter 297: loss 0.0001, time 1928.50ms\n",
            "iter 298: loss 0.0000, time 1981.22ms\n",
            "iter 299: loss 0.0003, time 1921.58ms\n",
            "step 300: train loss 0.1376, val loss 0.4123\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.7419\n",
            "Test recall 0.6333\n",
            "Test F1 0.5853\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [2 6 2]]\n",
            "iter 300: loss 0.0000, time 23464.43ms\n",
            "iter 301: loss 0.0003, time 1920.28ms\n",
            "iter 302: loss 0.0000, time 1892.24ms\n",
            "iter 303: loss 0.0000, time 1982.91ms\n",
            "iter 304: loss 0.0002, time 1909.16ms\n",
            "iter 305: loss 0.0000, time 1969.68ms\n",
            "iter 306: loss 0.1436, time 1979.29ms\n",
            "iter 307: loss 0.0000, time 1930.24ms\n",
            "iter 308: loss 0.0000, time 1911.24ms\n",
            "iter 309: loss 0.0000, time 1941.79ms\n",
            "iter 310: loss 0.0001, time 1958.04ms\n",
            "iter 311: loss 0.0000, time 1974.14ms\n",
            "iter 312: loss 0.0001, time 1970.72ms\n",
            "iter 313: loss 0.0000, time 1938.36ms\n",
            "iter 314: loss 0.0001, time 1932.23ms\n",
            "iter 315: loss 0.0000, time 1922.80ms\n",
            "iter 316: loss 0.0001, time 1932.81ms\n",
            "iter 317: loss 0.0000, time 2002.53ms\n",
            "iter 318: loss 0.0000, time 1950.95ms\n",
            "iter 319: loss 0.0007, time 1930.81ms\n",
            "step 320: train loss 0.1323, val loss 0.4200\n",
            "step val accuracy 0.9675\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 20552.70ms\n",
            "iter 321: loss 0.0000, time 1978.65ms\n",
            "iter 322: loss 0.0000, time 1981.53ms\n",
            "iter 323: loss 0.0000, time 1950.61ms\n",
            "iter 324: loss 0.0000, time 1986.38ms\n",
            "iter 325: loss 0.0000, time 2002.53ms\n",
            "iter 326: loss 0.0000, time 1945.71ms\n",
            "iter 327: loss 0.0000, time 1966.59ms\n",
            "iter 328: loss 0.0000, time 2294.37ms\n",
            "iter 329: loss 0.0000, time 1942.77ms\n",
            "iter 330: loss 0.0000, time 1884.48ms\n",
            "iter 331: loss 0.0001, time 1959.86ms\n",
            "iter 332: loss 0.0000, time 1933.81ms\n",
            "iter 333: loss 0.0000, time 1898.64ms\n",
            "iter 334: loss 0.0000, time 1957.21ms\n",
            "iter 335: loss 0.0000, time 1992.14ms\n",
            "iter 336: loss 0.0018, time 1940.48ms\n",
            "iter 337: loss 0.0000, time 1962.75ms\n",
            "iter 338: loss 0.0000, time 1972.63ms\n",
            "iter 339: loss 0.0001, time 1941.32ms\n",
            "step 340: train loss 0.1256, val loss 0.4283\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 20174.15ms\n",
            "iter 341: loss 0.0004, time 1945.38ms\n",
            "iter 342: loss 0.0000, time 1916.70ms\n",
            "iter 343: loss 0.0000, time 1941.44ms\n",
            "iter 344: loss 0.0540, time 1929.44ms\n",
            "iter 345: loss 0.0000, time 1929.63ms\n",
            "iter 346: loss 0.0000, time 1920.38ms\n",
            "iter 347: loss 0.0000, time 1916.43ms\n",
            "iter 348: loss 0.0000, time 1956.33ms\n",
            "iter 349: loss 0.0000, time 1977.40ms\n",
            "iter 350: loss 0.0005, time 1940.27ms\n",
            "iter 351: loss 0.0000, time 1985.94ms\n",
            "iter 352: loss 0.0000, time 2046.17ms\n",
            "iter 353: loss 0.0000, time 2137.67ms\n",
            "iter 354: loss 0.0000, time 1940.51ms\n",
            "iter 355: loss 0.0000, time 1904.66ms\n",
            "iter 356: loss 0.0001, time 1939.67ms\n",
            "iter 357: loss 0.0000, time 1949.56ms\n",
            "iter 358: loss 0.0000, time 1951.67ms\n",
            "iter 359: loss 0.0000, time 1968.70ms\n",
            "step 360: train loss 0.1190, val loss 0.4222\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 20362.49ms\n",
            "iter 361: loss 0.0000, time 1982.27ms\n",
            "iter 362: loss 0.0000, time 1947.81ms\n",
            "iter 363: loss 0.0000, time 1972.46ms\n",
            "iter 364: loss 0.0000, time 1923.88ms\n",
            "iter 365: loss 0.0000, time 1917.56ms\n",
            "iter 366: loss 0.0000, time 2000.35ms\n",
            "iter 367: loss 0.0000, time 1922.00ms\n",
            "iter 368: loss 0.0000, time 1939.89ms\n",
            "iter 369: loss 0.0000, time 1975.66ms\n",
            "iter 370: loss 0.0000, time 1977.59ms\n",
            "iter 371: loss 0.0000, time 1932.09ms\n",
            "iter 372: loss 0.0000, time 1932.31ms\n",
            "iter 373: loss 0.0000, time 1930.39ms\n",
            "iter 374: loss 0.0000, time 1983.91ms\n",
            "iter 375: loss 0.0000, time 1997.00ms\n",
            "iter 376: loss 0.0000, time 1948.89ms\n",
            "iter 377: loss 0.0000, time 1952.06ms\n",
            "iter 378: loss 0.0000, time 1906.69ms\n",
            "iter 379: loss 0.0002, time 2290.12ms\n",
            "step 380: train loss 0.1129, val loss 0.4126\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 20181.71ms\n",
            "iter 381: loss 0.0000, time 2250.55ms\n",
            "iter 382: loss 0.0000, time 1952.51ms\n",
            "iter 383: loss 0.0000, time 1919.36ms\n",
            "iter 384: loss 0.0000, time 1930.59ms\n",
            "iter 385: loss 0.0000, time 1948.94ms\n",
            "iter 386: loss 0.0037, time 1990.65ms\n",
            "iter 387: loss 0.0000, time 1945.24ms\n",
            "iter 388: loss 0.0000, time 1911.56ms\n",
            "iter 389: loss 0.0000, time 1941.67ms\n",
            "iter 390: loss 0.0000, time 1953.81ms\n",
            "iter 391: loss 0.0000, time 1969.61ms\n",
            "iter 392: loss 0.0000, time 1939.43ms\n",
            "iter 393: loss 0.0000, time 1925.26ms\n",
            "iter 394: loss 0.0000, time 1938.41ms\n",
            "iter 395: loss 0.0000, time 1929.39ms\n",
            "iter 396: loss 0.0000, time 1991.74ms\n",
            "iter 397: loss 0.0001, time 1939.95ms\n",
            "iter 398: loss 0.0000, time 1939.33ms\n",
            "iter 399: loss 0.0001, time 1955.85ms\n",
            "step 400: train loss 0.1093, val loss 0.4308\n",
            "step val accuracy 0.9660\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6667\n",
            "Test precision 0.8000\n",
            "Test recall 0.6667\n",
            "Test F1 0.6254\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 8 2]]\n",
            "iter 400: loss 0.0000, time 20981.31ms\n",
            "iter 401: loss 0.0000, time 2013.56ms\n",
            "iter 402: loss 0.0000, time 1958.91ms\n",
            "iter 403: loss 0.0000, time 1936.32ms\n",
            "iter 404: loss 0.0002, time 1951.05ms\n",
            "iter 405: loss 0.0000, time 1908.89ms\n",
            "iter 406: loss 0.0001, time 1934.64ms\n",
            "iter 407: loss 0.0011, time 1942.04ms\n",
            "iter 408: loss 0.0004, time 1971.14ms\n",
            "iter 409: loss 0.0002, time 1948.62ms\n",
            "iter 410: loss 0.0004, time 1934.43ms\n",
            "iter 411: loss 0.0000, time 1961.77ms\n",
            "iter 412: loss 0.0000, time 1952.33ms\n",
            "iter 413: loss 0.0000, time 1944.84ms\n",
            "iter 414: loss 0.0000, time 1925.64ms\n",
            "iter 415: loss 0.0000, time 1972.01ms\n",
            "iter 416: loss 0.0001, time 1952.85ms\n",
            "iter 417: loss 0.0000, time 1921.59ms\n",
            "iter 418: loss 0.0000, time 2015.38ms\n",
            "iter 419: loss 0.0000, time 1947.21ms\n",
            "step 420: train loss 0.1041, val loss 0.4309\n",
            "step val accuracy 0.9675\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0001, time 20601.33ms\n",
            "iter 421: loss 0.0011, time 1925.46ms\n",
            "iter 422: loss 0.0000, time 1968.89ms\n",
            "iter 423: loss 0.0000, time 1943.37ms\n",
            "iter 424: loss 0.0000, time 1965.62ms\n",
            "iter 425: loss 0.0000, time 1950.19ms\n",
            "iter 426: loss 0.0000, time 1950.08ms\n",
            "iter 427: loss 9.4375, time 1961.23ms\n",
            "iter 428: loss 0.0000, time 1969.29ms\n",
            "iter 429: loss 0.0011, time 1916.87ms\n",
            "iter 430: loss 0.0000, time 1981.17ms\n",
            "iter 431: loss 0.0000, time 1918.51ms\n",
            "iter 432: loss 0.0000, time 1883.87ms\n",
            "iter 433: loss 0.0000, time 1932.37ms\n",
            "iter 434: loss 0.0000, time 1930.53ms\n",
            "iter 435: loss 0.0000, time 1943.49ms\n",
            "iter 436: loss 0.0000, time 1955.24ms\n",
            "iter 437: loss 0.0000, time 1959.17ms\n",
            "iter 438: loss 0.0001, time 2012.18ms\n",
            "iter 439: loss 0.0012, time 2095.07ms\n",
            "step 440: train loss 0.1006, val loss 0.4152\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 20288.59ms\n",
            "iter 441: loss 0.0000, time 1923.56ms\n",
            "iter 442: loss 0.0000, time 1920.95ms\n",
            "iter 443: loss 0.0000, time 1937.88ms\n",
            "iter 444: loss 0.0000, time 2003.96ms\n",
            "iter 445: loss 0.0001, time 1920.66ms\n",
            "iter 446: loss 0.0000, time 1913.45ms\n",
            "iter 447: loss 0.0000, time 1934.77ms\n",
            "iter 448: loss 0.0000, time 1957.15ms\n",
            "iter 449: loss 0.0000, time 1976.93ms\n",
            "iter 450: loss 0.0000, time 1949.44ms\n",
            "iter 451: loss 0.0000, time 1964.27ms\n",
            "iter 452: loss 0.0000, time 1936.88ms\n",
            "iter 453: loss 0.0000, time 1928.95ms\n",
            "iter 454: loss 0.0000, time 1923.60ms\n",
            "iter 455: loss 0.0000, time 1924.54ms\n",
            "iter 456: loss 0.0000, time 1898.12ms\n",
            "iter 457: loss 0.0000, time 1913.09ms\n",
            "iter 458: loss 0.0000, time 1919.57ms\n",
            "iter 459: loss 0.0000, time 1933.24ms\n",
            "step 460: train loss 0.0967, val loss 0.4208\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 20395.74ms\n",
            "iter 461: loss 0.0000, time 2297.44ms\n",
            "iter 462: loss 0.0000, time 1892.37ms\n",
            "iter 463: loss 0.0000, time 1888.76ms\n",
            "iter 464: loss 0.0000, time 1893.71ms\n",
            "iter 465: loss 0.0000, time 1929.12ms\n",
            "iter 466: loss 0.0000, time 2117.75ms\n",
            "iter 467: loss 0.0000, time 2036.54ms\n",
            "iter 468: loss 0.0000, time 1911.56ms\n",
            "iter 469: loss 0.0000, time 1922.77ms\n",
            "iter 470: loss 0.0000, time 1995.54ms\n",
            "iter 471: loss 0.0000, time 1954.56ms\n",
            "iter 472: loss 0.0000, time 1948.51ms\n",
            "iter 473: loss 0.0000, time 1940.40ms\n",
            "iter 474: loss 0.0000, time 1915.52ms\n",
            "iter 475: loss 0.0000, time 1960.14ms\n",
            "iter 476: loss 0.0009, time 1891.87ms\n",
            "iter 477: loss 0.0000, time 2026.48ms\n",
            "iter 478: loss 0.0000, time 1893.27ms\n",
            "iter 479: loss 0.0000, time 2133.20ms\n",
            "step 480: train loss 0.0933, val loss 0.4599\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 21188.58ms\n",
            "iter 481: loss 0.0000, time 1892.47ms\n",
            "iter 482: loss 0.0000, time 1962.78ms\n",
            "iter 483: loss 0.0000, time 1895.12ms\n",
            "iter 484: loss 0.0000, time 1933.57ms\n",
            "iter 485: loss 0.0000, time 2046.91ms\n",
            "iter 486: loss 0.0000, time 2037.65ms\n",
            "iter 487: loss 0.0000, time 1974.90ms\n",
            "iter 488: loss 0.0000, time 1940.50ms\n",
            "iter 489: loss 0.0000, time 1993.04ms\n",
            "iter 490: loss 0.0000, time 1927.60ms\n",
            "iter 491: loss 0.0000, time 1969.89ms\n",
            "iter 492: loss 0.0000, time 1929.29ms\n",
            "iter 493: loss 0.0000, time 1933.12ms\n",
            "iter 494: loss 0.0000, time 1924.15ms\n",
            "iter 495: loss 0.0000, time 1911.75ms\n",
            "iter 496: loss 0.0000, time 1940.35ms\n",
            "iter 497: loss 0.0000, time 1968.16ms\n",
            "iter 498: loss 0.0000, time 1920.83ms\n",
            "iter 499: loss 0.0000, time 1935.23ms\n",
            "step 500: train loss 0.0901, val loss 0.4426\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8627\n",
            "Test recall 0.7667\n",
            "Test F1 0.7532\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  6  4]]\n",
            "iter 500: loss 0.0000, time 21252.12ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁█▇▄▅▇</td></tr><tr><td>Test_F1_Score</td><td>▁█▇▄▅▇</td></tr><tr><td>Test_Precision</td><td>▁██▆▇█</td></tr><tr><td>Test_Recall</td><td>▁█▇▄▅▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▇▇█████████████████████</td></tr><tr><td>val/loss</td><td> █▄▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.76667</td></tr><tr><td>Test_F1_Score</td><td>0.75318</td></tr><tr><td>Test_Precision</td><td>0.86275</td></tr><tr><td>Test_Recall</td><td>0.76667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.09006</td></tr><tr><td>val/acc</td><td>0.97485</td></tr><tr><td>val/loss</td><td>0.44256</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wild-sweep-2</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/0tw1kx4l' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/0tw1kx4l</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_000030-0tw1kx4l\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b2b0kdmj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_002652-b2b0kdmj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/b2b0kdmj' target=\"_blank\">winter-sweep-3</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/b2b0kdmj' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/b2b0kdmj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.2322\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 8.0625, time 29158.93ms\n",
            "iter 1: loss 0.5781, time 2098.39ms\n",
            "iter 2: loss 3.1406, time 2001.59ms\n",
            "iter 3: loss 1.0234, time 2070.84ms\n",
            "iter 4: loss 1.9297, time 2245.28ms\n",
            "iter 5: loss 3.7344, time 2092.16ms\n",
            "iter 6: loss 0.9648, time 1978.98ms\n",
            "iter 7: loss 0.2207, time 2332.99ms\n",
            "iter 8: loss 0.3086, time 2031.86ms\n",
            "iter 9: loss 1.5313, time 2021.90ms\n",
            "iter 10: loss 0.9453, time 2047.27ms\n",
            "iter 11: loss 1.5703, time 2083.40ms\n",
            "iter 12: loss 0.1377, time 1991.67ms\n",
            "iter 13: loss 0.2695, time 2012.73ms\n",
            "iter 14: loss 0.7656, time 2000.11ms\n",
            "iter 15: loss 2.2813, time 2028.17ms\n",
            "iter 16: loss 1.0547, time 2019.64ms\n",
            "iter 17: loss 0.0571, time 1982.99ms\n",
            "iter 18: loss 0.0425, time 2106.27ms\n",
            "iter 19: loss 0.1196, time 2057.39ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 1.1860, val loss 0.3480\n",
            "step val accuracy 0.7095\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.6133, time 28854.62ms\n",
            "iter 21: loss 0.0688, time 2059.68ms\n",
            "iter 22: loss 0.0593, time 2010.61ms\n",
            "iter 23: loss 0.1484, time 1992.07ms\n",
            "iter 24: loss 0.2197, time 1978.84ms\n",
            "iter 25: loss 0.0334, time 2025.83ms\n",
            "iter 26: loss 0.0713, time 2015.72ms\n",
            "iter 27: loss 0.1230, time 2037.17ms\n",
            "iter 28: loss 0.3672, time 2012.02ms\n",
            "iter 29: loss 0.1592, time 2006.04ms\n",
            "iter 30: loss 0.1699, time 2012.84ms\n",
            "iter 31: loss 0.2832, time 2037.50ms\n",
            "iter 32: loss 0.0361, time 2002.05ms\n",
            "iter 33: loss 0.1797, time 1985.22ms\n",
            "iter 34: loss 0.0264, time 1996.41ms\n",
            "iter 35: loss 0.0104, time 1985.88ms\n",
            "iter 36: loss 0.0096, time 2024.33ms\n",
            "iter 37: loss 0.0471, time 2321.46ms\n",
            "iter 38: loss 0.0013, time 2011.79ms\n",
            "iter 39: loss 4.6563, time 2044.70ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.8563, val loss 0.4779\n",
            "step val accuracy 0.7743\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0014, time 27570.99ms\n",
            "iter 41: loss 0.0012, time 2022.41ms\n",
            "iter 42: loss 0.2354, time 1996.65ms\n",
            "iter 43: loss 0.0166, time 2013.75ms\n",
            "iter 44: loss 1.5313, time 1985.54ms\n",
            "iter 45: loss 0.0479, time 1996.64ms\n",
            "iter 46: loss 0.0361, time 1971.13ms\n",
            "iter 47: loss 0.1240, time 2013.99ms\n",
            "iter 48: loss 0.0908, time 2028.87ms\n",
            "iter 49: loss 0.0005, time 2052.66ms\n",
            "iter 50: loss 0.3770, time 1965.16ms\n",
            "iter 51: loss 0.0537, time 1959.44ms\n",
            "iter 52: loss 0.1128, time 2055.42ms\n",
            "iter 53: loss 0.0067, time 2053.43ms\n",
            "iter 54: loss 0.0967, time 1997.54ms\n",
            "iter 55: loss 0.0284, time 1993.99ms\n",
            "iter 56: loss 0.3848, time 1983.04ms\n",
            "iter 57: loss 0.9961, time 2009.06ms\n",
            "iter 58: loss 2.1719, time 2023.42ms\n",
            "iter 59: loss 0.0068, time 2046.23ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.7227, val loss 0.5936\n",
            "step val accuracy 0.9320\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0256, time 27370.79ms\n",
            "iter 61: loss 0.2393, time 2093.66ms\n",
            "iter 62: loss 0.0075, time 2062.87ms\n",
            "iter 63: loss 0.0205, time 1992.00ms\n",
            "iter 64: loss 0.0193, time 2001.29ms\n",
            "iter 65: loss 0.0128, time 2368.55ms\n",
            "iter 66: loss 0.0131, time 2055.31ms\n",
            "iter 67: loss 0.0089, time 2170.07ms\n",
            "iter 68: loss 0.0010, time 2002.93ms\n",
            "iter 69: loss 0.0005, time 2008.64ms\n",
            "iter 70: loss 1.1328, time 2005.34ms\n",
            "iter 71: loss 0.0020, time 1981.17ms\n",
            "iter 72: loss 0.1030, time 2025.10ms\n",
            "iter 73: loss 0.0229, time 1979.95ms\n",
            "iter 74: loss 0.0425, time 1991.57ms\n",
            "iter 75: loss 0.0033, time 2037.83ms\n",
            "iter 76: loss 0.1211, time 2013.46ms\n",
            "iter 77: loss 0.1377, time 2027.56ms\n",
            "iter 78: loss 0.0306, time 1983.39ms\n",
            "iter 79: loss 0.0126, time 2073.37ms\n",
            "step 80: train loss 0.5913, val loss 0.6030\n",
            "step val accuracy 0.9438\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0267, time 27470.77ms\n",
            "iter 81: loss 0.0022, time 2020.89ms\n",
            "iter 82: loss 0.0723, time 2051.43ms\n",
            "iter 83: loss 0.0074, time 2024.21ms\n",
            "iter 84: loss 0.1016, time 1958.45ms\n",
            "iter 85: loss 0.2598, time 2016.60ms\n",
            "iter 86: loss 0.0067, time 1959.26ms\n",
            "iter 87: loss 0.0006, time 1952.82ms\n",
            "iter 88: loss 0.2256, time 1953.83ms\n",
            "iter 89: loss 0.1211, time 1963.34ms\n",
            "iter 90: loss 0.0079, time 1955.80ms\n",
            "iter 91: loss 0.0018, time 1933.93ms\n",
            "iter 92: loss 0.0085, time 1991.54ms\n",
            "iter 93: loss 0.0284, time 2034.65ms\n",
            "iter 94: loss 0.0013, time 1953.20ms\n",
            "iter 95: loss 0.1021, time 1988.87ms\n",
            "iter 96: loss 0.0425, time 1964.06ms\n",
            "iter 97: loss 0.0005, time 2015.27ms\n",
            "iter 98: loss 0.0087, time 2025.39ms\n",
            "iter 99: loss 0.0122, time 1950.62ms\n",
            "step 100: train loss 0.5031, val loss 0.6089\n",
            "step val accuracy 0.8985\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3393\n",
            "Test recall 0.5000\n",
            "Test F1 0.4038\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [4 6 0]\n",
            " [1 9 0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 100: loss 0.0145, time 28798.59ms\n",
            "iter 101: loss 0.0811, time 2038.36ms\n",
            "iter 102: loss 0.0006, time 2028.85ms\n",
            "iter 103: loss 0.0020, time 2007.87ms\n",
            "iter 104: loss 0.0019, time 1976.94ms\n",
            "iter 105: loss 0.0011, time 1998.34ms\n",
            "iter 106: loss 0.0012, time 1980.16ms\n",
            "iter 107: loss 0.0449, time 2006.65ms\n",
            "iter 108: loss 0.0162, time 2009.17ms\n",
            "iter 109: loss 0.0024, time 2043.49ms\n",
            "iter 110: loss 0.0187, time 2246.19ms\n",
            "iter 111: loss 0.0087, time 2168.51ms\n",
            "iter 112: loss 0.0018, time 1987.13ms\n",
            "iter 113: loss 0.0004, time 2063.00ms\n",
            "iter 114: loss 0.0004, time 2055.21ms\n",
            "iter 115: loss 0.0041, time 2045.52ms\n",
            "iter 116: loss 0.0003, time 2026.09ms\n",
            "iter 117: loss 0.0073, time 1992.98ms\n",
            "iter 118: loss 0.0001, time 2013.01ms\n",
            "iter 119: loss 0.0030, time 1974.59ms\n",
            "step 120: train loss 0.4315, val loss 0.5607\n",
            "step val accuracy 0.9741\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0002, time 27651.91ms\n",
            "iter 121: loss 0.0903, time 1992.21ms\n",
            "iter 122: loss 0.4355, time 2005.98ms\n",
            "iter 123: loss 0.0020, time 1961.62ms\n",
            "iter 124: loss 0.0012, time 2047.43ms\n",
            "iter 125: loss 0.0386, time 2041.93ms\n",
            "iter 126: loss 0.4453, time 2053.32ms\n",
            "iter 127: loss 0.0012, time 2038.87ms\n",
            "iter 128: loss 0.0013, time 1956.63ms\n",
            "iter 129: loss 0.0052, time 1955.08ms\n",
            "iter 130: loss 0.0002, time 1958.94ms\n",
            "iter 131: loss 0.0001, time 1999.28ms\n",
            "iter 132: loss 0.0004, time 2008.02ms\n",
            "iter 133: loss 0.0013, time 1954.04ms\n",
            "iter 134: loss 0.0004, time 1969.32ms\n",
            "iter 135: loss 0.0024, time 2024.17ms\n",
            "iter 136: loss 0.0005, time 1967.99ms\n",
            "iter 137: loss 0.0001, time 1965.40ms\n",
            "iter 138: loss 0.0015, time 1950.07ms\n",
            "iter 139: loss 0.0099, time 1930.50ms\n",
            "step 140: train loss 0.3800, val loss 0.4893\n",
            "step val accuracy 0.9611\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0002, time 27770.97ms\n",
            "iter 141: loss 0.0008, time 1951.58ms\n",
            "iter 142: loss 0.0022, time 1996.20ms\n",
            "iter 143: loss 0.0036, time 2003.29ms\n",
            "iter 144: loss 0.0005, time 1980.49ms\n",
            "iter 145: loss 0.0007, time 1949.02ms\n",
            "iter 146: loss 0.0037, time 1950.23ms\n",
            "iter 147: loss 0.0001, time 1977.38ms\n",
            "iter 148: loss 0.0001, time 1985.65ms\n",
            "iter 149: loss 0.0002, time 2033.34ms\n",
            "iter 150: loss 0.0002, time 1951.30ms\n",
            "iter 151: loss 0.0003, time 1939.10ms\n",
            "iter 152: loss 0.0000, time 1971.52ms\n",
            "iter 153: loss 0.0015, time 1968.09ms\n",
            "iter 154: loss 0.0000, time 1989.26ms\n",
            "iter 155: loss 0.0001, time 2009.93ms\n",
            "iter 156: loss 0.3164, time 1989.07ms\n",
            "iter 157: loss 0.0034, time 1982.04ms\n",
            "iter 158: loss 0.0012, time 2037.87ms\n",
            "iter 159: loss 0.0001, time 2018.80ms\n",
            "step 160: train loss 0.3349, val loss 0.4650\n",
            "step val accuracy 0.9676\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 27633.09ms\n",
            "iter 161: loss 0.0001, time 2002.63ms\n",
            "iter 162: loss 0.0001, time 1964.23ms\n",
            "iter 163: loss 0.0003, time 2014.81ms\n",
            "iter 164: loss 0.0010, time 1971.58ms\n",
            "iter 165: loss 0.0000, time 1978.67ms\n",
            "iter 166: loss 0.0000, time 2006.80ms\n",
            "iter 167: loss 0.0000, time 1974.45ms\n",
            "iter 168: loss 0.0004, time 1931.97ms\n",
            "iter 169: loss 0.0000, time 1976.81ms\n",
            "iter 170: loss 0.0002, time 1965.26ms\n",
            "iter 171: loss 0.0001, time 1972.03ms\n",
            "iter 172: loss 0.0001, time 2334.99ms\n",
            "iter 173: loss 0.0479, time 1976.12ms\n",
            "iter 174: loss 0.0003, time 1956.66ms\n",
            "iter 175: loss 0.0041, time 1953.92ms\n",
            "iter 176: loss 0.0000, time 1992.06ms\n",
            "iter 177: loss 0.0000, time 2028.12ms\n",
            "iter 178: loss 0.0000, time 1954.35ms\n",
            "iter 179: loss 0.0000, time 1956.91ms\n",
            "step 180: train loss 0.2989, val loss 0.4600\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0216, time 27829.50ms\n",
            "iter 181: loss 0.0013, time 2033.87ms\n",
            "iter 182: loss 0.0002, time 1997.88ms\n",
            "iter 183: loss 0.0001, time 2031.75ms\n",
            "iter 184: loss 0.0001, time 1992.32ms\n",
            "iter 185: loss 0.0001, time 1961.22ms\n",
            "iter 186: loss 0.0004, time 2181.90ms\n",
            "iter 187: loss 0.0001, time 1974.45ms\n",
            "iter 188: loss 0.0000, time 1965.91ms\n",
            "iter 189: loss 0.0001, time 1961.26ms\n",
            "iter 190: loss 0.0000, time 1964.36ms\n",
            "iter 191: loss 0.0000, time 1958.14ms\n",
            "iter 192: loss 0.0007, time 1936.03ms\n",
            "iter 193: loss 0.0001, time 2026.47ms\n",
            "iter 194: loss 0.0000, time 2027.44ms\n",
            "iter 195: loss 0.0001, time 2022.96ms\n",
            "iter 196: loss 0.0175, time 2007.78ms\n",
            "iter 197: loss 0.0001, time 1986.08ms\n",
            "iter 198: loss 0.0000, time 2034.57ms\n",
            "iter 199: loss 0.0000, time 2004.63ms\n",
            "step 200: train loss 0.2697, val loss 0.4339\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(2), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7444\n",
            "Test recall 0.7000\n",
            "Test F1 0.6911\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 8 1]\n",
            " [0 6 4]]\n",
            "iter 200: loss 0.0056, time 28773.25ms\n",
            "iter 201: loss 0.0000, time 2297.87ms\n",
            "iter 202: loss 0.0000, time 1971.09ms\n",
            "iter 203: loss 0.0000, time 1992.31ms\n",
            "iter 204: loss 0.0000, time 1943.38ms\n",
            "iter 205: loss 0.0000, time 1944.03ms\n",
            "iter 206: loss 0.0000, time 1993.50ms\n",
            "iter 207: loss 0.0000, time 1990.83ms\n",
            "iter 208: loss 0.0004, time 1951.44ms\n",
            "iter 209: loss 0.0000, time 1994.52ms\n",
            "iter 210: loss 0.0000, time 1977.33ms\n",
            "iter 211: loss 0.0000, time 1977.61ms\n",
            "iter 212: loss 0.0002, time 2004.91ms\n",
            "iter 213: loss 0.0002, time 2041.85ms\n",
            "iter 214: loss 0.0000, time 2032.30ms\n",
            "iter 215: loss 0.0007, time 2041.32ms\n",
            "iter 216: loss 0.0070, time 1912.35ms\n",
            "iter 217: loss 0.0000, time 1995.14ms\n",
            "iter 218: loss 0.9766, time 1958.85ms\n",
            "iter 219: loss 0.0000, time 1990.52ms\n",
            "step 220: train loss 0.2462, val loss 0.4244\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 27335.60ms\n",
            "iter 221: loss 0.0000, time 1964.59ms\n",
            "iter 222: loss 0.0001, time 2005.75ms\n",
            "iter 223: loss 0.0008, time 1969.13ms\n",
            "iter 224: loss 0.0000, time 1936.86ms\n",
            "iter 225: loss 0.0000, time 1970.77ms\n",
            "iter 226: loss 0.0000, time 1985.52ms\n",
            "iter 227: loss 0.0996, time 2016.93ms\n",
            "iter 228: loss 0.0000, time 1984.90ms\n",
            "iter 229: loss 0.0000, time 1957.52ms\n",
            "iter 230: loss 0.0001, time 1938.01ms\n",
            "iter 231: loss 0.0000, time 1968.65ms\n",
            "iter 232: loss 0.0000, time 2008.99ms\n",
            "iter 233: loss 0.0001, time 2022.59ms\n",
            "iter 234: loss 0.0001, time 1991.71ms\n",
            "iter 235: loss 0.0001, time 1962.35ms\n",
            "iter 236: loss 0.0001, time 1999.08ms\n",
            "iter 237: loss 0.0001, time 1970.45ms\n",
            "iter 238: loss 0.0061, time 2006.78ms\n",
            "iter 239: loss 0.0092, time 1997.02ms\n",
            "step 240: train loss 0.2265, val loss 0.4237\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 27816.41ms\n",
            "iter 241: loss 0.0002, time 1987.20ms\n",
            "iter 242: loss 0.0000, time 1912.50ms\n",
            "iter 243: loss 0.0000, time 1970.76ms\n",
            "iter 244: loss 0.0009, time 2024.52ms\n",
            "iter 245: loss 0.2441, time 2002.41ms\n",
            "iter 246: loss 0.0001, time 2024.28ms\n",
            "iter 247: loss 0.0001, time 1981.22ms\n",
            "iter 248: loss 0.0004, time 1994.68ms\n",
            "iter 249: loss 0.0001, time 2029.57ms\n",
            "iter 250: loss 0.0000, time 2052.36ms\n",
            "iter 251: loss 0.0002, time 1971.92ms\n",
            "iter 252: loss 0.0000, time 1979.28ms\n",
            "iter 253: loss 0.0004, time 1990.36ms\n",
            "iter 254: loss 0.0000, time 1978.80ms\n",
            "iter 255: loss 0.0001, time 1989.44ms\n",
            "iter 256: loss 0.0000, time 1943.99ms\n",
            "iter 257: loss 0.0001, time 1950.45ms\n",
            "iter 258: loss 0.0000, time 1990.61ms\n",
            "iter 259: loss 0.0000, time 1976.95ms\n",
            "step 260: train loss 0.2126, val loss 0.4153\n",
            "step val accuracy 0.9730\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 27423.49ms\n",
            "iter 261: loss 0.0004, time 1932.69ms\n",
            "iter 262: loss 0.0000, time 1986.52ms\n",
            "iter 263: loss 0.0000, time 2023.89ms\n",
            "iter 264: loss 0.0000, time 1916.01ms\n",
            "iter 265: loss 0.0000, time 1967.51ms\n",
            "iter 266: loss 0.0000, time 1963.58ms\n",
            "iter 267: loss 0.0000, time 2064.63ms\n",
            "iter 268: loss 0.0000, time 1975.58ms\n",
            "iter 269: loss 0.0000, time 1947.31ms\n",
            "iter 270: loss 0.0000, time 1940.24ms\n",
            "iter 271: loss 0.0008, time 1990.32ms\n",
            "iter 272: loss 0.0000, time 1963.35ms\n",
            "iter 273: loss 0.0001, time 1972.38ms\n",
            "iter 274: loss 0.0001, time 1987.92ms\n",
            "iter 275: loss 0.0000, time 1975.23ms\n",
            "iter 276: loss 0.0000, time 2009.30ms\n",
            "iter 277: loss 0.0000, time 2062.54ms\n",
            "iter 278: loss 0.0000, time 1996.42ms\n",
            "iter 279: loss 0.0000, time 2270.54ms\n",
            "step 280: train loss 0.1976, val loss 0.4204\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0001, time 27402.21ms\n",
            "iter 281: loss 0.0000, time 1984.04ms\n",
            "iter 282: loss 0.0000, time 2017.35ms\n",
            "iter 283: loss 0.0000, time 1960.21ms\n",
            "iter 284: loss 0.0001, time 2036.65ms\n",
            "iter 285: loss 0.0000, time 2040.58ms\n",
            "iter 286: loss 0.0000, time 1957.63ms\n",
            "iter 287: loss 0.0001, time 1991.84ms\n",
            "iter 288: loss 0.0000, time 1937.82ms\n",
            "iter 289: loss 0.0076, time 1954.46ms\n",
            "iter 290: loss 0.0000, time 1951.86ms\n",
            "iter 291: loss 0.0000, time 1961.17ms\n",
            "iter 292: loss 0.0000, time 1938.31ms\n",
            "iter 293: loss 0.0000, time 1998.20ms\n",
            "iter 294: loss 0.0000, time 2025.37ms\n",
            "iter 295: loss 0.0000, time 2084.18ms\n",
            "iter 296: loss 0.0000, time 2024.35ms\n",
            "iter 297: loss 0.0000, time 1971.47ms\n",
            "iter 298: loss 0.0000, time 2002.00ms\n",
            "iter 299: loss 0.0000, time 2024.98ms\n",
            "step 300: train loss 0.1846, val loss 0.4074\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.6783\n",
            "Test recall 0.6333\n",
            "Test F1 0.6152\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 7 1]\n",
            " [0 7 3]]\n",
            "iter 300: loss 0.0002, time 28513.15ms\n",
            "iter 301: loss 0.0000, time 2024.81ms\n",
            "iter 302: loss 0.0000, time 1986.88ms\n",
            "iter 303: loss 0.0000, time 1943.17ms\n",
            "iter 304: loss 0.0000, time 1983.26ms\n",
            "iter 305: loss 0.0000, time 1954.29ms\n",
            "iter 306: loss 0.0136, time 1930.32ms\n",
            "iter 307: loss 0.0000, time 1950.37ms\n",
            "iter 308: loss 0.0021, time 2049.86ms\n",
            "iter 309: loss 0.0010, time 1983.94ms\n",
            "iter 310: loss 0.0104, time 1956.60ms\n",
            "iter 311: loss 0.0002, time 2342.18ms\n",
            "iter 312: loss 0.0001, time 2079.87ms\n",
            "iter 313: loss 0.0000, time 2007.31ms\n",
            "iter 314: loss 0.0000, time 1994.34ms\n",
            "iter 315: loss 0.0000, time 2064.23ms\n",
            "iter 316: loss 0.0000, time 2055.23ms\n",
            "iter 317: loss 0.0000, time 1977.28ms\n",
            "iter 318: loss 0.0000, time 1952.23ms\n",
            "iter 319: loss 0.0000, time 1967.22ms\n",
            "step 320: train loss 0.1735, val loss 0.4043\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 27899.01ms\n",
            "iter 321: loss 0.0000, time 1972.01ms\n",
            "iter 322: loss 0.0000, time 1967.51ms\n",
            "iter 323: loss 0.0000, time 1979.13ms\n",
            "iter 324: loss 0.0000, time 2031.81ms\n",
            "iter 325: loss 0.0001, time 2528.70ms\n",
            "iter 326: loss 0.0000, time 2323.69ms\n",
            "iter 327: loss 0.0000, time 2387.85ms\n",
            "iter 328: loss 0.0000, time 2439.91ms\n",
            "iter 329: loss 0.0000, time 2378.47ms\n",
            "iter 330: loss 0.0000, time 2382.23ms\n",
            "iter 331: loss 0.0000, time 2429.48ms\n",
            "iter 332: loss 0.0000, time 2349.69ms\n",
            "iter 333: loss 0.0000, time 2346.70ms\n",
            "iter 334: loss 0.0000, time 2350.96ms\n",
            "iter 335: loss 0.0000, time 2029.79ms\n",
            "iter 336: loss 0.0000, time 1999.20ms\n",
            "iter 337: loss 0.0000, time 1953.93ms\n",
            "iter 338: loss 0.0000, time 1948.42ms\n",
            "iter 339: loss 0.0000, time 2042.95ms\n",
            "step 340: train loss 0.1634, val loss 0.4113\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 27498.81ms\n",
            "iter 341: loss 0.0000, time 1974.17ms\n",
            "iter 342: loss 0.0000, time 2332.41ms\n",
            "iter 343: loss 0.0000, time 1999.90ms\n",
            "iter 344: loss 0.0000, time 1980.37ms\n",
            "iter 345: loss 0.0003, time 1957.66ms\n",
            "iter 346: loss 0.0000, time 1989.62ms\n",
            "iter 347: loss 0.0000, time 1973.17ms\n",
            "iter 348: loss 0.0000, time 2000.51ms\n",
            "iter 349: loss 0.0001, time 2033.11ms\n",
            "iter 350: loss 0.0001, time 1951.42ms\n",
            "iter 351: loss 0.0003, time 1967.26ms\n",
            "iter 352: loss 0.0000, time 1991.29ms\n",
            "iter 353: loss 0.0000, time 1959.25ms\n",
            "iter 354: loss 0.0000, time 1962.14ms\n",
            "iter 355: loss 0.0003, time 1961.38ms\n",
            "iter 356: loss 0.0000, time 1963.24ms\n",
            "iter 357: loss 0.0000, time 1944.43ms\n",
            "iter 358: loss 0.0000, time 1993.15ms\n",
            "iter 359: loss 0.0000, time 2011.90ms\n",
            "step 360: train loss 0.1544, val loss 0.4179\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 27457.08ms\n",
            "iter 361: loss 0.0000, time 2010.14ms\n",
            "iter 362: loss 0.0000, time 2365.03ms\n",
            "iter 363: loss 0.0000, time 1966.19ms\n",
            "iter 364: loss 0.0000, time 1942.44ms\n",
            "iter 365: loss 0.0000, time 2024.37ms\n",
            "iter 366: loss 0.0000, time 2064.04ms\n",
            "iter 367: loss 0.0000, time 1945.45ms\n",
            "iter 368: loss 0.0000, time 2022.83ms\n",
            "iter 369: loss 0.0000, time 1962.40ms\n",
            "iter 370: loss 0.0000, time 1992.01ms\n",
            "iter 371: loss 0.0000, time 1965.16ms\n",
            "iter 372: loss 0.0000, time 1987.19ms\n",
            "iter 373: loss 0.0000, time 1951.73ms\n",
            "iter 374: loss 0.0001, time 1986.49ms\n",
            "iter 375: loss 0.0000, time 1945.95ms\n",
            "iter 376: loss 0.0000, time 2044.77ms\n",
            "iter 377: loss 0.0023, time 1980.59ms\n",
            "iter 378: loss 0.0002, time 2002.94ms\n",
            "iter 379: loss 0.0000, time 2007.03ms\n",
            "step 380: train loss 0.1474, val loss 0.4247\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 28162.24ms\n",
            "iter 381: loss 0.0000, time 1980.49ms\n",
            "iter 382: loss 0.0001, time 2000.14ms\n",
            "iter 383: loss 0.0001, time 2090.45ms\n",
            "iter 384: loss 0.0000, time 1994.65ms\n",
            "iter 385: loss 0.0000, time 1957.30ms\n",
            "iter 386: loss 0.0008, time 1923.49ms\n",
            "iter 387: loss 0.0032, time 1960.44ms\n",
            "iter 388: loss 0.0004, time 1948.07ms\n",
            "iter 389: loss 0.0002, time 1979.11ms\n",
            "iter 390: loss 0.0000, time 1968.42ms\n",
            "iter 391: loss 0.0000, time 1991.89ms\n",
            "iter 392: loss 0.0000, time 1992.60ms\n",
            "iter 393: loss 0.0003, time 2048.69ms\n",
            "iter 394: loss 0.0000, time 1987.69ms\n",
            "iter 395: loss 0.0000, time 1981.86ms\n",
            "iter 396: loss 0.0000, time 2021.16ms\n",
            "iter 397: loss 0.0000, time 2004.74ms\n",
            "iter 398: loss 0.0001, time 2020.54ms\n",
            "iter 399: loss 0.0000, time 1978.11ms\n",
            "step 400: train loss 0.1402, val loss 0.4369\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6667\n",
            "Test precision 0.7727\n",
            "Test recall 0.6667\n",
            "Test F1 0.6447\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 7 3]]\n",
            "iter 400: loss 0.0000, time 28647.88ms\n",
            "iter 401: loss 0.0003, time 1964.19ms\n",
            "iter 402: loss 0.0000, time 1952.09ms\n",
            "iter 403: loss 0.0001, time 1995.09ms\n",
            "iter 404: loss 0.0000, time 1989.24ms\n",
            "iter 405: loss 0.0000, time 1991.48ms\n",
            "iter 406: loss 0.0000, time 1965.59ms\n",
            "iter 407: loss 0.0000, time 1977.22ms\n",
            "iter 408: loss 0.0000, time 1972.40ms\n",
            "iter 409: loss 0.0000, time 1980.49ms\n",
            "iter 410: loss 0.0000, time 1960.28ms\n",
            "iter 411: loss 0.0020, time 1948.76ms\n",
            "iter 412: loss 0.0000, time 1986.40ms\n",
            "iter 413: loss 0.0000, time 1974.20ms\n",
            "iter 414: loss 0.0000, time 1987.40ms\n",
            "iter 415: loss 0.0002, time 2062.55ms\n",
            "iter 416: loss 0.0000, time 2008.75ms\n",
            "iter 417: loss 0.0000, time 1986.86ms\n",
            "iter 418: loss 0.0000, time 1965.11ms\n",
            "iter 419: loss 0.0000, time 2026.96ms\n",
            "step 420: train loss 0.1343, val loss 0.4591\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 27984.39ms\n",
            "iter 421: loss 0.0000, time 1978.26ms\n",
            "iter 422: loss 0.0000, time 2022.98ms\n",
            "iter 423: loss 0.0000, time 1982.95ms\n",
            "iter 424: loss 0.0000, time 2008.89ms\n",
            "iter 425: loss 0.0000, time 1976.24ms\n",
            "iter 426: loss 0.0001, time 2000.53ms\n",
            "iter 427: loss 0.0000, time 2030.71ms\n",
            "iter 428: loss 0.0002, time 2003.69ms\n",
            "iter 429: loss 0.0001, time 1975.04ms\n",
            "iter 430: loss 0.0000, time 2010.28ms\n",
            "iter 431: loss 0.0000, time 1995.74ms\n",
            "iter 432: loss 0.0000, time 2015.35ms\n",
            "iter 433: loss 0.0000, time 1976.59ms\n",
            "iter 434: loss 0.0000, time 1970.56ms\n",
            "iter 435: loss 0.0000, time 1975.87ms\n",
            "iter 436: loss 0.0000, time 1945.65ms\n",
            "iter 437: loss 0.0000, time 1960.05ms\n",
            "iter 438: loss 0.0000, time 1978.35ms\n",
            "iter 439: loss 0.0001, time 2002.03ms\n",
            "step 440: train loss 0.1285, val loss 0.4466\n",
            "step val accuracy 0.9752\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 27735.84ms\n",
            "iter 441: loss 0.0000, time 2010.09ms\n",
            "iter 442: loss 0.0000, time 1987.46ms\n",
            "iter 443: loss 0.0013, time 1971.17ms\n",
            "iter 444: loss 0.0000, time 1997.54ms\n",
            "iter 445: loss 0.0001, time 2002.09ms\n",
            "iter 446: loss 0.0002, time 1977.94ms\n",
            "iter 447: loss 0.0000, time 1978.48ms\n",
            "iter 448: loss 0.0000, time 1966.56ms\n",
            "iter 449: loss 0.0000, time 2029.63ms\n",
            "iter 450: loss 0.0017, time 1991.76ms\n",
            "iter 451: loss 0.0003, time 1930.83ms\n",
            "iter 452: loss 0.0000, time 1954.21ms\n",
            "iter 453: loss 0.0000, time 1983.91ms\n",
            "iter 454: loss 0.0000, time 2023.77ms\n",
            "iter 455: loss 0.0000, time 2314.11ms\n",
            "iter 456: loss 0.0000, time 1965.22ms\n",
            "iter 457: loss 0.0000, time 2007.45ms\n",
            "iter 458: loss 0.0001, time 1978.00ms\n",
            "iter 459: loss 0.0000, time 1992.14ms\n",
            "step 460: train loss 0.1230, val loss 0.4548\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 27592.71ms\n",
            "iter 461: loss 0.0008, time 1994.22ms\n",
            "iter 462: loss 0.0001, time 1992.61ms\n",
            "iter 463: loss 0.0001, time 1990.94ms\n",
            "iter 464: loss 0.0000, time 2011.21ms\n",
            "iter 465: loss 0.0000, time 1968.05ms\n",
            "iter 466: loss 0.0001, time 2016.40ms\n",
            "iter 467: loss 0.0001, time 2011.61ms\n",
            "iter 468: loss 0.0000, time 2127.91ms\n",
            "iter 469: loss 0.0000, time 2149.60ms\n",
            "iter 470: loss 0.0000, time 1993.82ms\n",
            "iter 471: loss 0.0000, time 1992.19ms\n",
            "iter 472: loss 0.0000, time 1958.09ms\n",
            "iter 473: loss 0.0000, time 2082.49ms\n",
            "iter 474: loss 0.0000, time 1979.04ms\n",
            "iter 475: loss 0.0000, time 1944.84ms\n",
            "iter 476: loss 0.0000, time 1971.20ms\n",
            "iter 477: loss 0.0000, time 1976.88ms\n",
            "iter 478: loss 0.0000, time 2092.29ms\n",
            "iter 479: loss 0.0000, time 1969.17ms\n",
            "step 480: train loss 0.1181, val loss 0.4500\n",
            "step val accuracy 0.9730\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 28604.25ms\n",
            "iter 481: loss 0.0000, time 1976.87ms\n",
            "iter 482: loss 0.0000, time 2006.27ms\n",
            "iter 483: loss 0.0000, time 2029.35ms\n",
            "iter 484: loss 0.0000, time 2009.12ms\n",
            "iter 485: loss 0.0000, time 2307.10ms\n",
            "iter 486: loss 0.0000, time 1968.63ms\n",
            "iter 487: loss 0.0000, time 1976.46ms\n",
            "iter 488: loss 0.0002, time 1962.93ms\n",
            "iter 489: loss 0.0000, time 1951.17ms\n",
            "iter 490: loss 0.0000, time 1939.70ms\n",
            "iter 491: loss 0.0000, time 1993.15ms\n",
            "iter 492: loss 0.0000, time 2060.95ms\n",
            "iter 493: loss 0.0001, time 2030.69ms\n",
            "iter 494: loss 0.0000, time 1995.52ms\n",
            "iter 495: loss 0.0000, time 1963.59ms\n",
            "iter 496: loss 0.0000, time 2003.81ms\n",
            "iter 497: loss 0.0000, time 2041.40ms\n",
            "iter 498: loss 0.0000, time 2057.35ms\n",
            "iter 499: loss 0.0001, time 2011.55ms\n",
            "step 500: train loss 0.1143, val loss 0.4574\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8364\n",
            "Test recall 0.7667\n",
            "Test F1 0.7479\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 1  9  0]\n",
            " [ 0  6  4]]\n",
            "iter 500: loss 0.0000, time 28505.71ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▁▆▅▅█</td></tr><tr><td>Test_F1_Score</td><td>▁▁▇▅▆█</td></tr><tr><td>Test_Precision</td><td>▁▁▇▆▇█</td></tr><tr><td>Test_Recall</td><td>▁▁▆▅▅█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▆▇█▇████████████████████</td></tr><tr><td>val/loss</td><td> ▁▄███▇▅▄▄▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.76667</td></tr><tr><td>Test_F1_Score</td><td>0.74794</td></tr><tr><td>Test_Precision</td><td>0.83636</td></tr><tr><td>Test_Recall</td><td>0.76667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.11434</td></tr><tr><td>val/acc</td><td>0.98704</td></tr><tr><td>val/loss</td><td>0.45739</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">winter-sweep-3</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/b2b0kdmj' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/b2b0kdmj</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_002652-b2b0kdmj\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h16bnel2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_005652-h16bnel2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/h16bnel2' target=\"_blank\">dazzling-sweep-4</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/h16bnel2' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/h16bnel2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.2343\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 8.0625, time 29030.80ms\n",
            "iter 1: loss 0.5156, time 2093.23ms\n",
            "iter 2: loss 0.1553, time 2049.25ms\n",
            "iter 3: loss 1.2109, time 2072.13ms\n",
            "iter 4: loss 0.7227, time 2038.79ms\n",
            "iter 5: loss 0.1602, time 2038.60ms\n",
            "iter 6: loss 0.7422, time 1983.20ms\n",
            "iter 7: loss 1.1875, time 2067.34ms\n",
            "iter 8: loss 0.4766, time 2060.69ms\n",
            "iter 9: loss 1.2109, time 2030.86ms\n",
            "iter 10: loss 0.2773, time 2043.60ms\n",
            "iter 11: loss 0.7227, time 2036.96ms\n",
            "iter 12: loss 0.5156, time 2389.26ms\n",
            "iter 13: loss 0.8633, time 2048.81ms\n",
            "iter 14: loss 0.5664, time 2116.43ms\n",
            "iter 15: loss 0.6641, time 2011.81ms\n",
            "iter 16: loss 0.3457, time 2038.89ms\n",
            "iter 17: loss 0.9453, time 2118.14ms\n",
            "iter 18: loss 0.0396, time 2083.38ms\n",
            "iter 19: loss 0.0977, time 2025.23ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 1.0023, val loss 0.7375\n",
            "step val accuracy 0.7527\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.0845, time 28782.68ms\n",
            "iter 21: loss 0.4141, time 2042.53ms\n",
            "iter 22: loss 0.0525, time 2043.72ms\n",
            "iter 23: loss 1.5000, time 2039.22ms\n",
            "iter 24: loss 0.0986, time 2025.78ms\n",
            "iter 25: loss 0.0171, time 2060.54ms\n",
            "iter 26: loss 1.1406, time 2049.69ms\n",
            "iter 27: loss 0.0148, time 2064.74ms\n",
            "iter 28: loss 1.1484, time 2083.42ms\n",
            "iter 29: loss 0.0693, time 2056.21ms\n",
            "iter 30: loss 0.3867, time 2014.55ms\n",
            "iter 31: loss 0.9727, time 2071.69ms\n",
            "iter 32: loss 0.0540, time 2022.64ms\n",
            "iter 33: loss 0.1836, time 2144.12ms\n",
            "iter 34: loss 0.0256, time 2058.80ms\n",
            "iter 35: loss 0.0161, time 2066.53ms\n",
            "iter 36: loss 0.0143, time 2024.21ms\n",
            "iter 37: loss 0.0056, time 2044.17ms\n",
            "iter 38: loss 0.0023, time 2053.66ms\n",
            "iter 39: loss 2.2813, time 2057.76ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.7182, val loss 0.5946\n",
            "step val accuracy 0.9125\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0747, time 28242.73ms\n",
            "iter 41: loss 0.0327, time 2057.64ms\n",
            "iter 42: loss 0.1270, time 2024.07ms\n",
            "iter 43: loss 0.0187, time 1998.11ms\n",
            "iter 44: loss 0.1904, time 2375.65ms\n",
            "iter 45: loss 0.0679, time 2099.90ms\n",
            "iter 46: loss 0.5781, time 2032.84ms\n",
            "iter 47: loss 0.0527, time 2088.14ms\n",
            "iter 48: loss 0.3789, time 2039.33ms\n",
            "iter 49: loss 0.0162, time 2100.45ms\n",
            "iter 50: loss 0.3066, time 2073.14ms\n",
            "iter 51: loss 0.5078, time 1998.27ms\n",
            "iter 52: loss 0.0046, time 2044.63ms\n",
            "iter 53: loss 0.1060, time 2053.47ms\n",
            "iter 54: loss 0.0031, time 2062.63ms\n",
            "iter 55: loss 0.0029, time 2043.90ms\n",
            "iter 56: loss 0.0278, time 2024.95ms\n",
            "iter 57: loss 0.0452, time 2052.58ms\n",
            "iter 58: loss 0.2246, time 2018.67ms\n",
            "iter 59: loss 0.0003, time 2110.87ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.5676, val loss 0.5387\n",
            "step val accuracy 0.9406\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0322, time 27940.28ms\n",
            "iter 61: loss 0.0806, time 2123.50ms\n",
            "iter 62: loss 0.0002, time 2129.92ms\n",
            "iter 63: loss 0.0045, time 2030.86ms\n",
            "iter 64: loss 0.0045, time 2071.48ms\n",
            "iter 65: loss 0.0088, time 2057.91ms\n",
            "iter 66: loss 0.0003, time 2135.65ms\n",
            "iter 67: loss 0.0001, time 2117.74ms\n",
            "iter 68: loss 0.0145, time 2039.41ms\n",
            "iter 69: loss 0.0065, time 2036.63ms\n",
            "iter 70: loss 0.2773, time 2065.30ms\n",
            "iter 71: loss 0.0272, time 2037.32ms\n",
            "iter 72: loss 0.0266, time 2046.38ms\n",
            "iter 73: loss 0.0471, time 2029.95ms\n",
            "iter 74: loss 0.0001, time 1992.31ms\n",
            "iter 75: loss 0.0076, time 2050.90ms\n",
            "iter 76: loss 0.0192, time 2062.06ms\n",
            "iter 77: loss 0.0066, time 2091.29ms\n",
            "iter 78: loss 0.0017, time 2020.28ms\n",
            "iter 79: loss 0.0000, time 2059.84ms\n",
            "step 80: train loss 0.4697, val loss 0.5406\n",
            "step val accuracy 0.9039\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.7500, time 28280.39ms\n",
            "iter 81: loss 0.0513, time 2067.42ms\n",
            "iter 82: loss 0.0659, time 2085.84ms\n",
            "iter 83: loss 0.0087, time 2032.26ms\n",
            "iter 84: loss 0.0048, time 1986.32ms\n",
            "iter 85: loss 0.0269, time 2019.81ms\n",
            "iter 86: loss 0.0003, time 1949.47ms\n",
            "iter 87: loss 0.0659, time 1968.61ms\n",
            "iter 88: loss 0.0002, time 1996.11ms\n",
            "iter 89: loss 0.0366, time 1970.72ms\n",
            "iter 90: loss 0.0664, time 2001.09ms\n",
            "iter 91: loss 0.0092, time 1980.67ms\n",
            "iter 92: loss 0.0035, time 2007.39ms\n",
            "iter 93: loss 0.0017, time 2036.99ms\n",
            "iter 94: loss 0.0009, time 1970.95ms\n",
            "iter 95: loss 0.0000, time 1995.02ms\n",
            "iter 96: loss 1.1875, time 1958.30ms\n",
            "iter 97: loss 0.0000, time 2033.08ms\n",
            "iter 98: loss 0.0000, time 2014.53ms\n",
            "iter 99: loss 0.0014, time 2000.27ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 100: train loss 0.4021, val loss 0.4845\n",
            "step val accuracy 0.9579\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6000\n",
            "Test precision 0.4500\n",
            "Test recall 0.6000\n",
            "Test F1 0.5000\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 1  9  0]\n",
            " [ 0 10  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 100: loss 0.0001, time 29053.69ms\n",
            "iter 101: loss 1.9453, time 2075.70ms\n",
            "iter 102: loss 0.0000, time 2024.10ms\n",
            "iter 103: loss 0.0002, time 2080.52ms\n",
            "iter 104: loss 0.0034, time 2042.75ms\n",
            "iter 105: loss 0.0002, time 2016.24ms\n",
            "iter 106: loss 0.0002, time 2041.62ms\n",
            "iter 107: loss 0.0011, time 2083.74ms\n",
            "iter 108: loss 0.0006, time 2062.16ms\n",
            "iter 109: loss 0.0007, time 2095.81ms\n",
            "iter 110: loss 0.0659, time 2076.18ms\n",
            "iter 111: loss 0.0003, time 2071.74ms\n",
            "iter 112: loss 0.0001, time 2004.58ms\n",
            "iter 113: loss 0.0000, time 2060.55ms\n",
            "iter 114: loss 0.0000, time 2086.48ms\n",
            "iter 115: loss 0.0037, time 2115.73ms\n",
            "iter 116: loss 0.0000, time 2044.85ms\n",
            "iter 117: loss 0.0044, time 2022.64ms\n",
            "iter 118: loss 0.0001, time 2075.42ms\n",
            "iter 119: loss 0.0000, time 2036.97ms\n",
            "step 120: train loss 0.3489, val loss 0.4700\n",
            "step val accuracy 0.9773\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0000, time 28331.20ms\n",
            "iter 121: loss 0.0003, time 2058.62ms\n",
            "iter 122: loss 0.0908, time 2035.78ms\n",
            "iter 123: loss 0.0000, time 2025.73ms\n",
            "iter 124: loss 0.0021, time 2049.56ms\n",
            "iter 125: loss 0.0104, time 2089.50ms\n",
            "iter 126: loss 0.0183, time 2062.65ms\n",
            "iter 127: loss 0.0104, time 2041.58ms\n",
            "iter 128: loss 0.0090, time 2023.88ms\n",
            "iter 129: loss 0.0081, time 1994.10ms\n",
            "iter 130: loss 0.0004, time 2054.01ms\n",
            "iter 131: loss 0.0000, time 2079.30ms\n",
            "iter 132: loss 0.0000, time 2032.60ms\n",
            "iter 133: loss 0.0001, time 1977.21ms\n",
            "iter 134: loss 0.0000, time 1991.23ms\n",
            "iter 135: loss 0.0001, time 2007.66ms\n",
            "iter 136: loss 0.0002, time 1990.44ms\n",
            "iter 137: loss 0.0000, time 1985.18ms\n",
            "iter 138: loss 0.0000, time 1995.73ms\n",
            "iter 139: loss 0.0001, time 1970.53ms\n",
            "step 140: train loss 0.3038, val loss 0.4298\n",
            "step val accuracy 0.9622\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0000, time 27876.99ms\n",
            "iter 141: loss 0.0002, time 1995.99ms\n",
            "iter 142: loss 0.0014, time 2032.07ms\n",
            "iter 143: loss 0.0013, time 2039.47ms\n",
            "iter 144: loss 0.0000, time 2014.22ms\n",
            "iter 145: loss 0.0004, time 1981.81ms\n",
            "iter 146: loss 0.0013, time 1994.96ms\n",
            "iter 147: loss 0.0000, time 2067.30ms\n",
            "iter 148: loss 0.0000, time 2076.39ms\n",
            "iter 149: loss 0.0000, time 2006.71ms\n",
            "iter 150: loss 0.0005, time 2007.86ms\n",
            "iter 151: loss 0.0003, time 1982.78ms\n",
            "iter 152: loss 0.0000, time 2009.43ms\n",
            "iter 153: loss 0.0001, time 1962.09ms\n",
            "iter 154: loss 0.0000, time 1975.93ms\n",
            "iter 155: loss 0.0066, time 2063.75ms\n",
            "iter 156: loss 0.0000, time 2023.88ms\n",
            "iter 157: loss 0.0007, time 2353.06ms\n",
            "iter 158: loss 0.0001, time 2096.01ms\n",
            "iter 159: loss 0.0354, time 2029.84ms\n",
            "step 160: train loss 0.2696, val loss 0.4127\n",
            "step val accuracy 0.9741\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 27959.67ms\n",
            "iter 161: loss 0.0002, time 1995.47ms\n",
            "iter 162: loss 0.0576, time 2019.14ms\n",
            "iter 163: loss 0.0000, time 2094.06ms\n",
            "iter 164: loss 0.0002, time 2051.54ms\n",
            "iter 165: loss 0.0000, time 2019.50ms\n",
            "iter 166: loss 0.0001, time 1996.06ms\n",
            "iter 167: loss 0.0002, time 2027.84ms\n",
            "iter 168: loss 0.0001, time 1974.48ms\n",
            "iter 169: loss 0.0000, time 2036.07ms\n",
            "iter 170: loss 0.0003, time 1995.21ms\n",
            "iter 171: loss 0.0000, time 2048.48ms\n",
            "iter 172: loss 0.0000, time 1997.62ms\n",
            "iter 173: loss 0.1279, time 2044.85ms\n",
            "iter 174: loss 0.0002, time 2044.85ms\n",
            "iter 175: loss 0.0020, time 1997.80ms\n",
            "iter 176: loss 0.0002, time 2042.67ms\n",
            "iter 177: loss 0.0000, time 2006.02ms\n",
            "iter 178: loss 0.0000, time 1999.01ms\n",
            "iter 179: loss 0.0003, time 2006.49ms\n",
            "step 180: train loss 0.2413, val loss 0.4250\n",
            "step val accuracy 0.9773\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0220, time 28576.29ms\n",
            "iter 181: loss 0.0013, time 2067.01ms\n",
            "iter 182: loss 0.0000, time 2023.20ms\n",
            "iter 183: loss 0.0000, time 1985.97ms\n",
            "iter 184: loss 0.0000, time 1989.25ms\n",
            "iter 185: loss 0.0000, time 2031.29ms\n",
            "iter 186: loss 0.0000, time 2059.18ms\n",
            "iter 187: loss 0.0000, time 2392.02ms\n",
            "iter 188: loss 0.0000, time 2023.52ms\n",
            "iter 189: loss 0.0000, time 2008.70ms\n",
            "iter 190: loss 0.0003, time 2013.09ms\n",
            "iter 191: loss 0.0000, time 2004.69ms\n",
            "iter 192: loss 0.0001, time 1972.88ms\n",
            "iter 193: loss 0.0003, time 2025.60ms\n",
            "iter 194: loss 0.0000, time 2034.71ms\n",
            "iter 195: loss 0.0000, time 2088.48ms\n",
            "iter 196: loss 0.0236, time 2057.29ms\n",
            "iter 197: loss 0.0000, time 1990.36ms\n",
            "iter 198: loss 0.0000, time 2022.14ms\n",
            "iter 199: loss 0.0001, time 2018.49ms\n",
            "step 200: train loss 0.2185, val loss 0.4047\n",
            "step val accuracy 0.9687\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5667\n",
            "Test precision 0.7206\n",
            "Test recall 0.5667\n",
            "Test F1 0.5062\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [3 7 0]\n",
            " [0 9 1]]\n",
            "iter 200: loss 0.0003, time 29215.50ms\n",
            "iter 201: loss 0.0004, time 1948.42ms\n",
            "iter 202: loss 0.0027, time 2168.47ms\n",
            "iter 203: loss 0.0000, time 2200.85ms\n",
            "iter 204: loss 0.0000, time 2003.05ms\n",
            "iter 205: loss 0.0001, time 1989.64ms\n",
            "iter 206: loss 0.0000, time 1999.88ms\n",
            "iter 207: loss 0.0008, time 2002.01ms\n",
            "iter 208: loss 0.0435, time 1965.50ms\n",
            "iter 209: loss 0.0000, time 1977.55ms\n",
            "iter 210: loss 0.0001, time 1934.51ms\n",
            "iter 211: loss 0.0000, time 2017.06ms\n",
            "iter 212: loss 0.0564, time 2101.35ms\n",
            "iter 213: loss 0.0000, time 2021.74ms\n",
            "iter 214: loss 0.0000, time 1978.97ms\n",
            "iter 215: loss 0.0000, time 1956.57ms\n",
            "iter 216: loss 0.0000, time 1964.40ms\n",
            "iter 217: loss 0.0000, time 2051.00ms\n",
            "iter 218: loss 0.0000, time 1987.68ms\n",
            "iter 219: loss 0.0000, time 2018.11ms\n",
            "step 220: train loss 0.2050, val loss 0.3990\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 28761.91ms\n",
            "iter 221: loss 0.0000, time 2009.99ms\n",
            "iter 222: loss 0.0001, time 2054.50ms\n",
            "iter 223: loss 0.0000, time 2020.12ms\n",
            "iter 224: loss 0.0001, time 1972.07ms\n",
            "iter 225: loss 0.0000, time 2050.69ms\n",
            "iter 226: loss 0.0000, time 1987.99ms\n",
            "iter 227: loss 0.0001, time 2042.21ms\n",
            "iter 228: loss 0.0000, time 2021.92ms\n",
            "iter 229: loss 0.0000, time 2056.40ms\n",
            "iter 230: loss 0.0000, time 1983.84ms\n",
            "iter 231: loss 0.0000, time 1958.93ms\n",
            "iter 232: loss 0.0000, time 2003.60ms\n",
            "iter 233: loss 0.0000, time 2075.62ms\n",
            "iter 234: loss 0.0000, time 2064.27ms\n",
            "iter 235: loss 0.0000, time 2011.66ms\n",
            "iter 236: loss 0.0000, time 1998.07ms\n",
            "iter 237: loss 0.0000, time 1989.60ms\n",
            "iter 238: loss 0.0001, time 2033.68ms\n",
            "iter 239: loss 0.0000, time 2024.81ms\n",
            "step 240: train loss 0.1910, val loss 0.3927\n",
            "step val accuracy 0.9752\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 27960.62ms\n",
            "iter 241: loss 0.0000, time 2085.37ms\n",
            "iter 242: loss 0.0008, time 1998.91ms\n",
            "iter 243: loss 0.0039, time 1989.05ms\n",
            "iter 244: loss 0.0000, time 2066.84ms\n",
            "iter 245: loss 0.0000, time 2111.03ms\n",
            "iter 246: loss 0.0000, time 2112.44ms\n",
            "iter 247: loss 0.0052, time 2045.69ms\n",
            "iter 248: loss 0.0001, time 2047.90ms\n",
            "iter 249: loss 0.0000, time 2035.71ms\n",
            "iter 250: loss 0.0000, time 2077.02ms\n",
            "iter 251: loss 0.0000, time 2049.23ms\n",
            "iter 252: loss 0.0000, time 2044.75ms\n",
            "iter 253: loss 0.0000, time 2021.74ms\n",
            "iter 254: loss 0.0000, time 2013.92ms\n",
            "iter 255: loss 0.0000, time 2003.04ms\n",
            "iter 256: loss 0.0000, time 1980.35ms\n",
            "iter 257: loss 0.4531, time 1995.58ms\n",
            "iter 258: loss 0.0000, time 1978.86ms\n",
            "iter 259: loss 0.0000, time 2013.39ms\n",
            "step 260: train loss 0.1774, val loss 0.4456\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0002, time 28569.73ms\n",
            "iter 261: loss 0.0023, time 2006.49ms\n",
            "iter 262: loss 0.0024, time 2025.95ms\n",
            "iter 263: loss 0.0000, time 2035.31ms\n",
            "iter 264: loss 0.0154, time 1950.12ms\n",
            "iter 265: loss 0.0000, time 2007.25ms\n",
            "iter 266: loss 0.0000, time 2004.88ms\n",
            "iter 267: loss 0.0000, time 2061.58ms\n",
            "iter 268: loss 0.0000, time 2060.53ms\n",
            "iter 269: loss 0.0000, time 2006.94ms\n",
            "iter 270: loss 0.0000, time 2047.26ms\n",
            "iter 271: loss 0.0002, time 2037.40ms\n",
            "iter 272: loss 0.0000, time 2016.95ms\n",
            "iter 273: loss 0.0010, time 1989.93ms\n",
            "iter 274: loss 0.0000, time 2039.76ms\n",
            "iter 275: loss 0.0000, time 2064.47ms\n",
            "iter 276: loss 0.0075, time 2067.64ms\n",
            "iter 277: loss 0.0001, time 2066.62ms\n",
            "iter 278: loss 0.0000, time 2042.55ms\n",
            "iter 279: loss 0.0000, time 1967.40ms\n",
            "step 280: train loss 0.1686, val loss 0.4363\n",
            "step val accuracy 0.9752\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 28052.01ms\n",
            "iter 281: loss 0.0000, time 2020.56ms\n",
            "iter 282: loss 0.0000, time 2006.19ms\n",
            "iter 283: loss 0.0000, time 2055.21ms\n",
            "iter 284: loss 0.0000, time 2047.49ms\n",
            "iter 285: loss 0.0000, time 2010.13ms\n",
            "iter 286: loss 0.0000, time 1998.96ms\n",
            "iter 287: loss 0.0000, time 1985.96ms\n",
            "iter 288: loss 0.0000, time 2021.65ms\n",
            "iter 289: loss 0.0000, time 1986.87ms\n",
            "iter 290: loss 0.0000, time 2057.05ms\n",
            "iter 291: loss 0.0001, time 2010.73ms\n",
            "iter 292: loss 0.0000, time 1992.89ms\n",
            "iter 293: loss 0.0000, time 2100.71ms\n",
            "iter 294: loss 0.0000, time 2056.09ms\n",
            "iter 295: loss 0.0000, time 2044.16ms\n",
            "iter 296: loss 0.0000, time 1988.95ms\n",
            "iter 297: loss 0.0000, time 1991.18ms\n",
            "iter 298: loss 0.0000, time 2065.01ms\n",
            "iter 299: loss 0.0000, time 2034.23ms\n",
            "step 300: train loss 0.1577, val loss 0.4927\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.7629\n",
            "Test recall 0.6333\n",
            "Test F1 0.5944\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 8 2]]\n",
            "iter 300: loss 0.0000, time 29248.51ms\n",
            "iter 301: loss 0.0000, time 2012.98ms\n",
            "iter 302: loss 0.0000, time 2032.62ms\n",
            "iter 303: loss 0.0000, time 2002.43ms\n",
            "iter 304: loss 0.0000, time 2055.63ms\n",
            "iter 305: loss 0.0000, time 2000.74ms\n",
            "iter 306: loss 0.0000, time 1983.00ms\n",
            "iter 307: loss 0.0000, time 1999.20ms\n",
            "iter 308: loss 0.0002, time 2045.47ms\n",
            "iter 309: loss 0.0000, time 2046.31ms\n",
            "iter 310: loss 0.1826, time 2054.72ms\n",
            "iter 311: loss 0.0001, time 2015.26ms\n",
            "iter 312: loss 0.0000, time 2054.14ms\n",
            "iter 313: loss 0.0001, time 2010.64ms\n",
            "iter 314: loss 0.0000, time 2063.04ms\n",
            "iter 315: loss 0.0000, time 2052.11ms\n",
            "iter 316: loss 0.0000, time 2028.70ms\n",
            "iter 317: loss 0.0000, time 2009.13ms\n",
            "iter 318: loss 0.0000, time 2016.07ms\n",
            "iter 319: loss 0.0000, time 2045.63ms\n",
            "step 320: train loss 0.1494, val loss 0.4910\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 28005.96ms\n",
            "iter 321: loss 0.0012, time 2023.86ms\n",
            "iter 322: loss 0.0000, time 1949.38ms\n",
            "iter 323: loss 0.0000, time 1972.88ms\n",
            "iter 324: loss 0.0001, time 1999.41ms\n",
            "iter 325: loss 0.0000, time 2023.13ms\n",
            "iter 326: loss 0.0001, time 1984.10ms\n",
            "iter 327: loss 0.0000, time 2017.99ms\n",
            "iter 328: loss 0.0000, time 2045.31ms\n",
            "iter 329: loss 0.0000, time 1974.52ms\n",
            "iter 330: loss 0.0000, time 2011.03ms\n",
            "iter 331: loss 0.0001, time 2380.74ms\n",
            "iter 332: loss 0.0000, time 2053.31ms\n",
            "iter 333: loss 0.0000, time 2013.78ms\n",
            "iter 334: loss 0.0000, time 1963.11ms\n",
            "iter 335: loss 0.0000, time 1985.21ms\n",
            "iter 336: loss 0.0002, time 2005.16ms\n",
            "iter 337: loss 0.0004, time 2014.80ms\n",
            "iter 338: loss 0.0000, time 2030.79ms\n",
            "iter 339: loss 0.0000, time 2025.91ms\n",
            "step 340: train loss 0.1414, val loss 0.5268\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 28201.77ms\n",
            "iter 341: loss 0.0000, time 2013.08ms\n",
            "iter 342: loss 0.0001, time 2101.57ms\n",
            "iter 343: loss 0.0002, time 2088.09ms\n",
            "iter 344: loss 0.0000, time 2059.33ms\n",
            "iter 345: loss 0.0000, time 2030.42ms\n",
            "iter 346: loss 0.0000, time 2040.67ms\n",
            "iter 347: loss 0.0000, time 2002.75ms\n",
            "iter 348: loss 0.0000, time 2039.33ms\n",
            "iter 349: loss 0.0001, time 2019.55ms\n",
            "iter 350: loss 0.0005, time 2009.97ms\n",
            "iter 351: loss 0.0000, time 2031.88ms\n",
            "iter 352: loss 0.0000, time 2014.07ms\n",
            "iter 353: loss 0.0000, time 2019.69ms\n",
            "iter 354: loss 0.0000, time 2000.03ms\n",
            "iter 355: loss 0.0000, time 2025.82ms\n",
            "iter 356: loss 0.0000, time 2017.94ms\n",
            "iter 357: loss 0.0000, time 2007.75ms\n",
            "iter 358: loss 0.0000, time 2072.00ms\n",
            "iter 359: loss 0.0000, time 2072.47ms\n",
            "step 360: train loss 0.1355, val loss 0.5329\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 28085.01ms\n",
            "iter 361: loss 0.0000, time 1973.56ms\n",
            "iter 362: loss 0.0005, time 2357.31ms\n",
            "iter 363: loss 0.0001, time 1999.92ms\n",
            "iter 364: loss 0.0000, time 2014.68ms\n",
            "iter 365: loss 0.0000, time 2065.24ms\n",
            "iter 366: loss 0.0020, time 2034.80ms\n",
            "iter 367: loss 0.0000, time 2003.01ms\n",
            "iter 368: loss 0.0000, time 2041.82ms\n",
            "iter 369: loss 0.0000, time 2013.52ms\n",
            "iter 370: loss 0.0000, time 2038.23ms\n",
            "iter 371: loss 0.0000, time 2008.09ms\n",
            "iter 372: loss 0.0000, time 2006.96ms\n",
            "iter 373: loss 0.0000, time 2023.21ms\n",
            "iter 374: loss 0.0001, time 2062.31ms\n",
            "iter 375: loss 0.0000, time 1997.16ms\n",
            "iter 376: loss 0.0114, time 2033.67ms\n",
            "iter 377: loss 0.0001, time 2010.41ms\n",
            "iter 378: loss 0.0014, time 2016.37ms\n",
            "iter 379: loss 0.0000, time 2040.85ms\n",
            "step 380: train loss 0.1292, val loss 0.5263\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 28256.22ms\n",
            "iter 381: loss 0.0007, time 2069.17ms\n",
            "iter 382: loss 0.0000, time 2046.65ms\n",
            "iter 383: loss 0.0000, time 2050.94ms\n",
            "iter 384: loss 0.0000, time 2027.38ms\n",
            "iter 385: loss 0.0000, time 1987.15ms\n",
            "iter 386: loss 0.0001, time 2005.67ms\n",
            "iter 387: loss 0.0000, time 2020.84ms\n",
            "iter 388: loss 0.0001, time 2014.68ms\n",
            "iter 389: loss 0.0000, time 1979.74ms\n",
            "iter 390: loss 0.0000, time 2003.75ms\n",
            "iter 391: loss 0.0000, time 2039.23ms\n",
            "iter 392: loss 0.0000, time 2078.30ms\n",
            "iter 393: loss 0.0000, time 2044.48ms\n",
            "iter 394: loss 0.0004, time 2004.93ms\n",
            "iter 395: loss 0.0000, time 2055.17ms\n",
            "iter 396: loss 0.0001, time 2073.69ms\n",
            "iter 397: loss 0.0000, time 2053.43ms\n",
            "iter 398: loss 0.0000, time 2008.84ms\n",
            "iter 399: loss 0.0000, time 2011.60ms\n",
            "step 400: train loss 0.1232, val loss 0.5259\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.8254\n",
            "Test recall 0.6333\n",
            "Test F1 0.5720\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 8  2  0]\n",
            " [ 0 10  0]\n",
            " [ 0  9  1]]\n",
            "iter 400: loss 0.0000, time 29156.80ms\n",
            "iter 401: loss 0.0000, time 2025.99ms\n",
            "iter 402: loss 0.0000, time 1997.94ms\n",
            "iter 403: loss 0.0000, time 2012.19ms\n",
            "iter 404: loss 0.0000, time 2060.62ms\n",
            "iter 405: loss 0.0000, time 2084.59ms\n",
            "iter 406: loss 0.0000, time 1984.74ms\n",
            "iter 407: loss 0.0024, time 2035.35ms\n",
            "iter 408: loss 0.0000, time 2006.64ms\n",
            "iter 409: loss 0.0000, time 1974.18ms\n",
            "iter 410: loss 0.0001, time 1995.99ms\n",
            "iter 411: loss 0.0001, time 2020.94ms\n",
            "iter 412: loss 0.0000, time 2030.86ms\n",
            "iter 413: loss 0.0000, time 2028.99ms\n",
            "iter 414: loss 0.0000, time 2037.36ms\n",
            "iter 415: loss 0.0000, time 2020.95ms\n",
            "iter 416: loss 0.0000, time 2032.86ms\n",
            "iter 417: loss 0.0000, time 2002.71ms\n",
            "iter 418: loss 0.0001, time 2051.84ms\n",
            "iter 419: loss 0.0000, time 2013.28ms\n",
            "step 420: train loss 0.1174, val loss 0.5347\n",
            "step val accuracy 0.9881\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 27803.52ms\n",
            "iter 421: loss 0.0000, time 2023.64ms\n",
            "iter 422: loss 0.0000, time 2026.41ms\n",
            "iter 423: loss 0.0000, time 2044.20ms\n",
            "iter 424: loss 0.0000, time 2106.61ms\n",
            "iter 425: loss 0.0000, time 2097.65ms\n",
            "iter 426: loss 0.0000, time 2014.33ms\n",
            "iter 427: loss 0.0006, time 2037.19ms\n",
            "iter 428: loss 0.0000, time 2058.88ms\n",
            "iter 429: loss 0.0000, time 2028.43ms\n",
            "iter 430: loss 0.0000, time 2093.05ms\n",
            "iter 431: loss 0.0000, time 1999.09ms\n",
            "iter 432: loss 0.0000, time 2014.06ms\n",
            "iter 433: loss 0.0000, time 2006.80ms\n",
            "iter 434: loss 0.0000, time 2004.17ms\n",
            "iter 435: loss 0.0000, time 2012.30ms\n",
            "iter 436: loss 0.0000, time 1982.62ms\n",
            "iter 437: loss 0.0000, time 1978.27ms\n",
            "iter 438: loss 0.0000, time 2038.84ms\n",
            "iter 439: loss 0.0000, time 2059.60ms\n",
            "step 440: train loss 0.1120, val loss 0.5465\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 28565.24ms\n",
            "iter 441: loss 0.0000, time 2016.76ms\n",
            "iter 442: loss 0.0000, time 2009.15ms\n",
            "iter 443: loss 0.0000, time 1998.38ms\n",
            "iter 444: loss 0.0000, time 1975.40ms\n",
            "iter 445: loss 0.0000, time 1989.09ms\n",
            "iter 446: loss 0.0000, time 2001.13ms\n",
            "iter 447: loss 0.0000, time 2001.73ms\n",
            "iter 448: loss 0.0000, time 2020.77ms\n",
            "iter 449: loss 0.0000, time 2013.09ms\n",
            "iter 450: loss 0.0000, time 2010.97ms\n",
            "iter 451: loss 0.0032, time 1942.08ms\n",
            "iter 452: loss 0.0000, time 2042.68ms\n",
            "iter 453: loss 0.0000, time 1985.58ms\n",
            "iter 454: loss 0.0000, time 2028.56ms\n",
            "iter 455: loss 0.0000, time 2019.82ms\n",
            "iter 456: loss 0.0000, time 2021.94ms\n",
            "iter 457: loss 0.0000, time 2039.05ms\n",
            "iter 458: loss 0.0000, time 1964.96ms\n",
            "iter 459: loss 0.0000, time 1949.47ms\n",
            "step 460: train loss 0.1076, val loss 0.5476\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 28054.49ms\n",
            "iter 461: loss 0.0000, time 2018.98ms\n",
            "iter 462: loss 0.0002, time 2038.88ms\n",
            "iter 463: loss 0.0000, time 2063.75ms\n",
            "iter 464: loss 0.0000, time 2067.98ms\n",
            "iter 465: loss 0.0000, time 2005.42ms\n",
            "iter 466: loss 0.0000, time 1965.39ms\n",
            "iter 467: loss 0.0000, time 2018.36ms\n",
            "iter 468: loss 0.0000, time 2014.30ms\n",
            "iter 469: loss 0.0000, time 2010.94ms\n",
            "iter 470: loss 0.0000, time 2047.12ms\n",
            "iter 471: loss 0.0000, time 2006.75ms\n",
            "iter 472: loss 0.0002, time 1981.31ms\n",
            "iter 473: loss 0.0000, time 2116.43ms\n",
            "iter 474: loss 0.0000, time 2069.00ms\n",
            "iter 475: loss 0.0000, time 2088.64ms\n",
            "iter 476: loss 0.0000, time 2321.82ms\n",
            "iter 477: loss 0.0000, time 2236.68ms\n",
            "iter 478: loss 0.0000, time 2107.28ms\n",
            "iter 479: loss 0.0000, time 2043.65ms\n",
            "step 480: train loss 0.1041, val loss 0.5545\n",
            "step val accuracy 0.9773\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 27996.38ms\n",
            "iter 481: loss 0.0000, time 2018.16ms\n",
            "iter 482: loss 0.0000, time 2035.80ms\n",
            "iter 483: loss 0.0000, time 2031.95ms\n",
            "iter 484: loss 0.0000, time 2020.29ms\n",
            "iter 485: loss 0.0000, time 1992.09ms\n",
            "iter 486: loss 0.0000, time 2031.71ms\n",
            "iter 487: loss 0.0000, time 2022.35ms\n",
            "iter 488: loss 0.0000, time 2024.82ms\n",
            "iter 489: loss 0.0000, time 1975.87ms\n",
            "iter 490: loss 0.0000, time 2027.79ms\n",
            "iter 491: loss 0.0000, time 2057.01ms\n",
            "iter 492: loss 0.0000, time 2033.67ms\n",
            "iter 493: loss 0.0000, time 2047.65ms\n",
            "iter 494: loss 0.0000, time 2056.72ms\n",
            "iter 495: loss 0.0000, time 2058.67ms\n",
            "iter 496: loss 0.0000, time 2029.57ms\n",
            "iter 497: loss 0.0000, time 2036.78ms\n",
            "iter 498: loss 0.0000, time 2048.67ms\n",
            "iter 499: loss 0.0001, time 2006.99ms\n",
            "step 500: train loss 0.1006, val loss 0.5409\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.7629\n",
            "Test recall 0.6333\n",
            "Test F1 0.5944\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 8 2]]\n",
            "iter 500: loss 0.0000, time 28954.76ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▆▅███</td></tr><tr><td>Test_F1_Score</td><td>▁▅▅█▇█</td></tr><tr><td>Test_Precision</td><td>▁▃▆▇█▇</td></tr><tr><td>Test_Recall</td><td>▁▆▅███</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇█▇█████████████████████</td></tr><tr><td>val/loss</td><td> █▅▄▄▃▃▂▁▂▁▁▁▂▂▃▃▄▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.63333</td></tr><tr><td>Test_F1_Score</td><td>0.59436</td></tr><tr><td>Test_Precision</td><td>0.76292</td></tr><tr><td>Test_Recall</td><td>0.63333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.10056</td></tr><tr><td>val/acc</td><td>0.98056</td></tr><tr><td>val/loss</td><td>0.54094</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dazzling-sweep-4</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/h16bnel2' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/h16bnel2</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_005652-h16bnel2\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dfydkbq7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_012712-dfydkbq7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/dfydkbq7' target=\"_blank\">stilted-sweep-5</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/dfydkbq7' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/dfydkbq7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3445\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.5313, time 54147.81ms\n",
            "iter 1: loss 0.7031, time 2340.47ms\n",
            "iter 2: loss 0.8398, time 2358.61ms\n",
            "iter 3: loss 3.0312, time 2312.31ms\n",
            "iter 4: loss 2.5000, time 2292.46ms\n",
            "iter 5: loss 1.2422, time 2313.34ms\n",
            "iter 6: loss 0.8281, time 2493.74ms\n",
            "iter 7: loss 0.7891, time 2690.73ms\n",
            "iter 8: loss 3.5938, time 2331.68ms\n",
            "iter 9: loss 0.6641, time 2315.24ms\n",
            "iter 10: loss 3.1094, time 2348.59ms\n",
            "iter 11: loss 0.7305, time 2376.37ms\n",
            "iter 12: loss 1.8672, time 2369.76ms\n",
            "iter 13: loss 0.9805, time 2363.77ms\n",
            "iter 14: loss 1.1094, time 2334.70ms\n",
            "iter 15: loss 1.1016, time 2396.44ms\n",
            "iter 16: loss 0.7656, time 2312.87ms\n",
            "iter 17: loss 0.6641, time 2261.93ms\n",
            "iter 18: loss 1.2344, time 2331.42ms\n",
            "iter 19: loss 0.5937, time 2653.39ms\n",
            "step 20: train loss 1.3759, val loss 1.2923\n",
            "step val accuracy 0.6834\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.6016, time 26947.14ms\n",
            "iter 21: loss 0.6406, time 2336.10ms\n",
            "iter 22: loss 1.6172, time 2258.51ms\n",
            "iter 23: loss 0.7031, time 2282.97ms\n",
            "iter 24: loss 0.6406, time 2304.88ms\n",
            "iter 25: loss 0.5937, time 2358.38ms\n",
            "iter 26: loss 0.3945, time 2423.63ms\n",
            "iter 27: loss 0.7461, time 2371.60ms\n",
            "iter 28: loss 0.3086, time 2316.13ms\n",
            "iter 29: loss 0.5313, time 2241.56ms\n",
            "iter 30: loss 1.3359, time 2372.30ms\n",
            "iter 31: loss 0.6016, time 2317.22ms\n",
            "iter 32: loss 0.9375, time 2607.11ms\n",
            "iter 33: loss 0.0752, time 2285.81ms\n",
            "iter 34: loss 0.4199, time 2250.60ms\n",
            "iter 35: loss 0.5859, time 2266.44ms\n",
            "iter 36: loss 0.5234, time 2302.02ms\n",
            "iter 37: loss 0.1875, time 2285.36ms\n",
            "iter 38: loss 0.1328, time 2327.68ms\n",
            "iter 39: loss 0.0859, time 2332.79ms\n",
            "step 40: train loss 1.0288, val loss 0.9718\n",
            "step val accuracy 0.8743\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.1807, time 25429.77ms\n",
            "iter 41: loss 0.1289, time 2210.17ms\n",
            "iter 42: loss 0.3164, time 2368.34ms\n",
            "iter 43: loss 0.4512, time 2266.14ms\n",
            "iter 44: loss 0.0209, time 2266.36ms\n",
            "iter 45: loss 0.1807, time 2225.42ms\n",
            "iter 46: loss 0.0496, time 2340.81ms\n",
            "iter 47: loss 0.3730, time 2610.41ms\n",
            "iter 48: loss 0.0557, time 2342.45ms\n",
            "iter 49: loss 0.0211, time 2264.47ms\n",
            "iter 50: loss 0.0845, time 2304.48ms\n",
            "iter 51: loss 0.9375, time 2264.85ms\n",
            "iter 52: loss 0.0258, time 2199.97ms\n",
            "iter 53: loss 0.0275, time 2312.73ms\n",
            "iter 54: loss 0.0703, time 2277.48ms\n",
            "iter 55: loss 0.0114, time 2344.59ms\n",
            "iter 56: loss 0.0179, time 2268.52ms\n",
            "iter 57: loss 0.0063, time 2356.56ms\n",
            "iter 58: loss 0.0152, time 2273.56ms\n",
            "iter 59: loss 0.0082, time 2254.91ms\n",
            "step 60: train loss 0.7585, val loss 0.7631\n",
            "step val accuracy 0.9290\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0291, time 25976.45ms\n",
            "iter 61: loss 0.0559, time 2252.76ms\n",
            "iter 62: loss 0.0014, time 2354.27ms\n",
            "iter 63: loss 0.0025, time 2236.39ms\n",
            "iter 64: loss 0.0118, time 2236.49ms\n",
            "iter 65: loss 0.3066, time 2305.98ms\n",
            "iter 66: loss 0.0933, time 2308.20ms\n",
            "iter 67: loss 0.6523, time 2238.66ms\n",
            "iter 68: loss 0.0022, time 2276.93ms\n",
            "iter 69: loss 0.0009, time 2273.98ms\n",
            "iter 70: loss 0.0010, time 2291.36ms\n",
            "iter 71: loss 0.0023, time 2276.58ms\n",
            "iter 72: loss 0.0017, time 2320.60ms\n",
            "iter 73: loss 0.0986, time 2648.77ms\n",
            "iter 74: loss 0.0008, time 2315.02ms\n",
            "iter 75: loss 0.0098, time 2309.01ms\n",
            "iter 76: loss 0.0276, time 2314.74ms\n",
            "iter 77: loss 0.0032, time 2299.20ms\n",
            "iter 78: loss 0.2813, time 2279.77ms\n",
            "iter 79: loss 0.2695, time 2305.49ms\n",
            "step 80: train loss 0.6147, val loss 0.6211\n",
            "step val accuracy 0.9556\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0050, time 25676.60ms\n",
            "iter 81: loss 0.8281, time 2279.85ms\n",
            "iter 82: loss 0.0024, time 2284.09ms\n",
            "iter 83: loss 0.0008, time 2278.43ms\n",
            "iter 84: loss 0.0034, time 2371.70ms\n",
            "iter 85: loss 0.0008, time 2248.65ms\n",
            "iter 86: loss 0.0011, time 2662.84ms\n",
            "iter 87: loss 0.0037, time 2341.36ms\n",
            "iter 88: loss 0.0005, time 2204.08ms\n",
            "iter 89: loss 0.0059, time 2245.09ms\n",
            "iter 90: loss 0.0001, time 2291.12ms\n",
            "iter 91: loss 0.0042, time 2320.97ms\n",
            "iter 92: loss 0.0011, time 2264.16ms\n",
            "iter 93: loss 0.0068, time 2295.13ms\n",
            "iter 94: loss 0.0003, time 2219.48ms\n",
            "iter 95: loss 0.1377, time 2322.24ms\n",
            "iter 96: loss 0.0349, time 2285.07ms\n",
            "iter 97: loss 0.0001, time 2308.97ms\n",
            "iter 98: loss 0.0835, time 2243.57ms\n",
            "iter 99: loss 0.0000, time 2636.76ms\n",
            "step 100: train loss 0.5033, val loss 0.5565\n",
            "step val accuracy 0.9482\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8283\n",
            "Test recall 0.8000\n",
            "Test F1 0.8026\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 3 7]]\n",
            "iter 100: loss 0.0001, time 26202.25ms\n",
            "iter 101: loss 0.0025, time 2598.26ms\n",
            "iter 102: loss 0.0002, time 2302.02ms\n",
            "iter 103: loss 0.0010, time 2264.43ms\n",
            "iter 104: loss 0.0053, time 2361.14ms\n",
            "iter 105: loss 0.0003, time 2373.02ms\n",
            "iter 106: loss 0.0000, time 2351.59ms\n",
            "iter 107: loss 0.0354, time 2285.91ms\n",
            "iter 108: loss 0.0000, time 2276.99ms\n",
            "iter 109: loss 0.0006, time 2338.29ms\n",
            "iter 110: loss 0.0000, time 2259.52ms\n",
            "iter 111: loss 0.0000, time 2279.90ms\n",
            "iter 112: loss 0.0002, time 2348.87ms\n",
            "iter 113: loss 0.0004, time 2304.60ms\n",
            "iter 114: loss 0.0063, time 2629.82ms\n",
            "iter 115: loss 0.0013, time 2318.32ms\n",
            "iter 116: loss 0.0003, time 2253.26ms\n",
            "iter 117: loss 0.0052, time 2317.81ms\n",
            "iter 118: loss 0.0085, time 2265.37ms\n",
            "iter 119: loss 0.0086, time 2343.02ms\n",
            "step 120: train loss 0.4335, val loss 0.6102\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0044, time 25298.73ms\n",
            "iter 121: loss 0.0317, time 2283.98ms\n",
            "iter 122: loss 0.0006, time 2291.55ms\n",
            "iter 123: loss 0.0036, time 2281.21ms\n",
            "iter 124: loss 0.0006, time 2245.05ms\n",
            "iter 125: loss 0.0008, time 2291.52ms\n",
            "iter 126: loss 0.0000, time 2244.19ms\n",
            "iter 127: loss 0.0002, time 2322.97ms\n",
            "iter 128: loss 0.0000, time 2538.35ms\n",
            "iter 129: loss 0.0001, time 2322.96ms\n",
            "iter 130: loss 0.0003, time 2372.98ms\n",
            "iter 131: loss 0.0001, time 2384.16ms\n",
            "iter 132: loss 0.0000, time 2318.97ms\n",
            "iter 133: loss 0.0002, time 2288.99ms\n",
            "iter 134: loss 0.0000, time 2269.41ms\n",
            "iter 135: loss 0.0002, time 2322.83ms\n",
            "iter 136: loss 0.0000, time 2297.41ms\n",
            "iter 137: loss 0.0000, time 2318.40ms\n",
            "iter 138: loss 0.0000, time 2244.34ms\n",
            "iter 139: loss 0.0723, time 2253.26ms\n",
            "step 140: train loss 0.3781, val loss 0.6168\n",
            "step val accuracy 0.9571\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0002, time 25603.95ms\n",
            "iter 141: loss 0.0000, time 2303.13ms\n",
            "iter 142: loss 0.0003, time 2588.35ms\n",
            "iter 143: loss 0.0009, time 2269.75ms\n",
            "iter 144: loss 0.0002, time 2259.45ms\n",
            "iter 145: loss 0.0000, time 2336.17ms\n",
            "iter 146: loss 0.0000, time 2296.43ms\n",
            "iter 147: loss 0.0000, time 2350.61ms\n",
            "iter 148: loss 0.0001, time 2240.91ms\n",
            "iter 149: loss 0.0000, time 2343.16ms\n",
            "iter 150: loss 0.0001, time 2277.42ms\n",
            "iter 151: loss 0.0001, time 2356.98ms\n",
            "iter 152: loss 0.0035, time 2298.90ms\n",
            "iter 153: loss 0.0001, time 2290.89ms\n",
            "iter 154: loss 0.0002, time 2278.75ms\n",
            "iter 155: loss 0.0001, time 2275.85ms\n",
            "iter 156: loss 0.0003, time 2620.82ms\n",
            "iter 157: loss 0.0002, time 2263.30ms\n",
            "iter 158: loss 0.0000, time 2335.84ms\n",
            "iter 159: loss 0.0005, time 2289.49ms\n",
            "step 160: train loss 0.3411, val loss 0.5736\n",
            "step val accuracy 0.9393\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0113, time 25498.80ms\n",
            "iter 161: loss 0.0002, time 2296.55ms\n",
            "iter 162: loss 0.0002, time 2283.12ms\n",
            "iter 163: loss 0.0027, time 2361.99ms\n",
            "iter 164: loss 0.0001, time 2237.52ms\n",
            "iter 165: loss 0.0080, time 2324.88ms\n",
            "iter 166: loss 0.0000, time 2212.92ms\n",
            "iter 167: loss 0.0003, time 2330.36ms\n",
            "iter 168: loss 0.0004, time 2278.22ms\n",
            "iter 169: loss 0.0004, time 2287.10ms\n",
            "iter 170: loss 0.0002, time 2602.95ms\n",
            "iter 171: loss 0.0001, time 2323.00ms\n",
            "iter 172: loss 0.0035, time 2239.22ms\n",
            "iter 173: loss 0.0003, time 2282.36ms\n",
            "iter 174: loss 0.0000, time 2281.13ms\n",
            "iter 175: loss 0.0000, time 2328.32ms\n",
            "iter 176: loss 0.0000, time 2333.53ms\n",
            "iter 177: loss 0.0001, time 2294.89ms\n",
            "iter 178: loss 0.0001, time 2326.41ms\n",
            "iter 179: loss 0.0003, time 2301.53ms\n",
            "step 180: train loss 0.3057, val loss 0.5730\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 25480.89ms\n",
            "iter 181: loss 0.0003, time 2314.32ms\n",
            "iter 182: loss 0.0025, time 2296.75ms\n",
            "iter 183: loss 0.0006, time 2345.95ms\n",
            "iter 184: loss 0.0003, time 2279.24ms\n",
            "iter 185: loss 0.0000, time 2630.73ms\n",
            "iter 186: loss 0.0000, time 2299.07ms\n",
            "iter 187: loss 0.0000, time 2333.02ms\n",
            "iter 188: loss 0.0000, time 2293.34ms\n",
            "iter 189: loss 0.0000, time 2414.98ms\n",
            "iter 190: loss 0.0089, time 2285.12ms\n",
            "iter 191: loss 0.0000, time 2298.15ms\n",
            "iter 192: loss 0.0000, time 2371.63ms\n",
            "iter 193: loss 0.0000, time 2292.00ms\n",
            "iter 194: loss 0.0000, time 2244.57ms\n",
            "iter 195: loss 0.0005, time 2306.93ms\n",
            "iter 196: loss 0.0000, time 2369.67ms\n",
            "iter 197: loss 0.0000, time 2336.95ms\n",
            "iter 198: loss 0.0001, time 2582.52ms\n",
            "iter 199: loss 0.0000, time 2252.91ms\n",
            "step 200: train loss 0.2782, val loss 0.5522\n",
            "step val accuracy 0.9704\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8027\n",
            "Test recall 0.8000\n",
            "Test F1 0.7928\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  6  1]\n",
            " [ 0  2  8]]\n",
            "iter 200: loss 0.0000, time 26457.16ms\n",
            "iter 201: loss 0.0000, time 2257.36ms\n",
            "iter 202: loss 0.0017, time 2191.10ms\n",
            "iter 203: loss 0.0020, time 2336.74ms\n",
            "iter 204: loss 0.0000, time 2305.10ms\n",
            "iter 205: loss 0.0002, time 2283.92ms\n",
            "iter 206: loss 0.0000, time 2374.70ms\n",
            "iter 207: loss 0.0011, time 2357.78ms\n",
            "iter 208: loss 0.0001, time 2300.87ms\n",
            "iter 209: loss 0.0000, time 2313.16ms\n",
            "iter 210: loss 0.0143, time 2257.56ms\n",
            "iter 211: loss 0.0000, time 2343.48ms\n",
            "iter 212: loss 0.0000, time 2396.40ms\n",
            "iter 213: loss 0.0000, time 2882.81ms\n",
            "iter 214: loss 0.0000, time 2286.86ms\n",
            "iter 215: loss 0.0000, time 2287.08ms\n",
            "iter 216: loss 0.0001, time 2318.23ms\n",
            "iter 217: loss 0.0000, time 2310.69ms\n",
            "iter 218: loss 0.0001, time 2255.23ms\n",
            "iter 219: loss 0.0002, time 2317.98ms\n",
            "step 220: train loss 0.2540, val loss 0.5560\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0001, time 25347.14ms\n",
            "iter 221: loss 0.0000, time 2299.40ms\n",
            "iter 222: loss 0.0000, time 2232.85ms\n",
            "iter 223: loss 0.0000, time 2278.16ms\n",
            "iter 224: loss 0.0000, time 2327.21ms\n",
            "iter 225: loss 0.0000, time 2319.21ms\n",
            "iter 226: loss 0.0002, time 2631.16ms\n",
            "iter 227: loss 0.0002, time 2387.21ms\n",
            "iter 228: loss 0.0001, time 2324.14ms\n",
            "iter 229: loss 0.0000, time 2307.02ms\n",
            "iter 230: loss 0.0000, time 2256.30ms\n",
            "iter 231: loss 0.0000, time 2364.70ms\n",
            "iter 232: loss 0.0000, time 2309.97ms\n",
            "iter 233: loss 0.0000, time 2317.96ms\n",
            "iter 234: loss 0.0000, time 2265.31ms\n",
            "iter 235: loss 0.0000, time 2266.25ms\n",
            "iter 236: loss 0.0004, time 2330.88ms\n",
            "iter 237: loss 0.0000, time 2315.79ms\n",
            "iter 238: loss 0.0000, time 2635.05ms\n",
            "iter 239: loss 0.0000, time 2281.76ms\n",
            "step 240: train loss 0.2335, val loss 0.5386\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 25509.76ms\n",
            "iter 241: loss 0.0000, time 2222.09ms\n",
            "iter 242: loss 0.0000, time 2321.89ms\n",
            "iter 243: loss 0.0000, time 2315.81ms\n",
            "iter 244: loss 0.0000, time 2262.42ms\n",
            "iter 245: loss 0.0002, time 2276.91ms\n",
            "iter 246: loss 0.0003, time 2225.55ms\n",
            "iter 247: loss 0.0009, time 2299.25ms\n",
            "iter 248: loss 0.0000, time 2233.72ms\n",
            "iter 249: loss 0.0019, time 2328.80ms\n",
            "iter 250: loss 0.0000, time 2325.45ms\n",
            "iter 251: loss 0.0000, time 2249.58ms\n",
            "iter 252: loss 0.0001, time 2732.98ms\n",
            "iter 253: loss 0.0000, time 2289.35ms\n",
            "iter 254: loss 0.0000, time 2223.79ms\n",
            "iter 255: loss 0.0001, time 2242.61ms\n",
            "iter 256: loss 0.0041, time 2381.22ms\n",
            "iter 257: loss 0.0000, time 2312.07ms\n",
            "iter 258: loss 0.0000, time 2265.35ms\n",
            "iter 259: loss 0.0000, time 2304.23ms\n",
            "step 260: train loss 0.2166, val loss 0.5345\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 25740.10ms\n",
            "iter 261: loss 0.0000, time 2264.79ms\n",
            "iter 262: loss 0.0210, time 2274.57ms\n",
            "iter 263: loss 0.0000, time 2267.59ms\n",
            "iter 264: loss 0.0000, time 2315.23ms\n",
            "iter 265: loss 0.0000, time 2275.51ms\n",
            "iter 266: loss 0.0000, time 2653.68ms\n",
            "iter 267: loss 0.0000, time 2298.02ms\n",
            "iter 268: loss 0.0000, time 2321.98ms\n",
            "iter 269: loss 0.0000, time 2314.95ms\n",
            "iter 270: loss 0.0001, time 2298.39ms\n",
            "iter 271: loss 0.0000, time 2279.86ms\n",
            "iter 272: loss 0.0000, time 2361.94ms\n",
            "iter 273: loss 0.0000, time 2359.45ms\n",
            "iter 274: loss 0.0000, time 2284.50ms\n",
            "iter 275: loss 0.0001, time 2310.55ms\n",
            "iter 276: loss 0.1162, time 2291.96ms\n",
            "iter 277: loss 0.0000, time 2292.49ms\n",
            "iter 278: loss 0.0000, time 2321.12ms\n",
            "iter 279: loss 0.0000, time 2285.07ms\n",
            "step 280: train loss 0.2018, val loss 0.5196\n",
            "step val accuracy 0.9882\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 26003.20ms\n",
            "iter 281: loss 0.0000, time 2234.86ms\n",
            "iter 282: loss 0.0041, time 2264.99ms\n",
            "iter 283: loss 0.0000, time 2330.98ms\n",
            "iter 284: loss 0.0000, time 2314.54ms\n",
            "iter 285: loss 0.0000, time 2290.54ms\n",
            "iter 286: loss 0.0003, time 2342.63ms\n",
            "iter 287: loss 0.0003, time 2320.09ms\n",
            "iter 288: loss 0.1260, time 2369.31ms\n",
            "iter 289: loss 0.0000, time 2259.54ms\n",
            "iter 290: loss 0.0001, time 2266.06ms\n",
            "iter 291: loss 0.0000, time 2297.36ms\n",
            "iter 292: loss 0.0001, time 2279.93ms\n",
            "iter 293: loss 0.0000, time 2291.11ms\n",
            "iter 294: loss 0.0000, time 2655.41ms\n",
            "iter 295: loss 0.0000, time 2310.30ms\n",
            "iter 296: loss 0.0000, time 2378.56ms\n",
            "iter 297: loss 0.0000, time 2262.60ms\n",
            "iter 298: loss 0.0000, time 2493.18ms\n",
            "iter 299: loss 0.0001, time 2270.72ms\n",
            "step 300: train loss 0.1885, val loss 0.5233\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8671\n",
            "Test recall 0.8333\n",
            "Test F1 0.8283\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 1  9  0]\n",
            " [ 0  4  6]]\n",
            "iter 300: loss 0.0000, time 26886.98ms\n",
            "iter 301: loss 0.0313, time 2249.84ms\n",
            "iter 302: loss 0.0000, time 2215.21ms\n",
            "iter 303: loss 0.0000, time 2343.88ms\n",
            "iter 304: loss 0.0000, time 2282.93ms\n",
            "iter 305: loss 0.0000, time 2284.54ms\n",
            "iter 306: loss 0.0032, time 2281.34ms\n",
            "iter 307: loss 0.0000, time 2251.06ms\n",
            "iter 308: loss 0.0000, time 2592.46ms\n",
            "iter 309: loss 0.0000, time 2232.39ms\n",
            "iter 310: loss 0.0000, time 2238.34ms\n",
            "iter 311: loss 0.0000, time 2331.27ms\n",
            "iter 312: loss 0.0000, time 2419.66ms\n",
            "iter 313: loss 0.0000, time 2325.67ms\n",
            "iter 314: loss 0.0000, time 2272.46ms\n",
            "iter 315: loss 0.0000, time 2294.82ms\n",
            "iter 316: loss 0.0000, time 2288.85ms\n",
            "iter 317: loss 0.0000, time 2310.90ms\n",
            "iter 318: loss 0.0000, time 2269.80ms\n",
            "iter 319: loss 0.0000, time 2309.00ms\n",
            "step 320: train loss 0.1769, val loss 0.5330\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 25828.24ms\n",
            "iter 321: loss 0.0000, time 2273.13ms\n",
            "iter 322: loss 0.0000, time 2644.76ms\n",
            "iter 323: loss 0.0000, time 2286.26ms\n",
            "iter 324: loss 0.0000, time 2350.50ms\n",
            "iter 325: loss 0.0000, time 2347.99ms\n",
            "iter 326: loss 0.0000, time 2272.21ms\n",
            "iter 327: loss 0.0000, time 2300.45ms\n",
            "iter 328: loss 0.0000, time 2338.47ms\n",
            "iter 329: loss 0.0000, time 2291.09ms\n",
            "iter 330: loss 0.0688, time 2248.54ms\n",
            "iter 331: loss 0.0000, time 2278.51ms\n",
            "iter 332: loss 0.0000, time 2248.53ms\n",
            "iter 333: loss 0.0018, time 2207.20ms\n",
            "iter 334: loss 0.0000, time 2308.95ms\n",
            "iter 335: loss 0.0000, time 2362.80ms\n",
            "iter 336: loss 0.0000, time 2281.30ms\n",
            "iter 337: loss 0.0000, time 2648.80ms\n",
            "iter 338: loss 0.0000, time 2340.97ms\n",
            "iter 339: loss 0.0000, time 2272.85ms\n",
            "step 340: train loss 0.1673, val loss 0.5159\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 25856.58ms\n",
            "iter 341: loss 0.0000, time 2293.93ms\n",
            "iter 342: loss 0.0000, time 2210.71ms\n",
            "iter 343: loss 0.0001, time 2371.20ms\n",
            "iter 344: loss 0.0000, time 2312.60ms\n",
            "iter 345: loss 0.0000, time 2272.30ms\n",
            "iter 346: loss 0.0000, time 2271.86ms\n",
            "iter 347: loss 0.0000, time 2298.19ms\n",
            "iter 348: loss 0.0000, time 2318.78ms\n",
            "iter 349: loss 0.0000, time 2312.13ms\n",
            "iter 350: loss 0.0058, time 2649.17ms\n",
            "iter 351: loss 0.0000, time 2316.40ms\n",
            "iter 352: loss 0.0000, time 2282.77ms\n",
            "iter 353: loss 0.0000, time 2252.03ms\n",
            "iter 354: loss 0.0000, time 2281.39ms\n",
            "iter 355: loss 0.0000, time 2326.50ms\n",
            "iter 356: loss 0.0000, time 2316.90ms\n",
            "iter 357: loss 0.0007, time 2315.92ms\n",
            "iter 358: loss 0.0000, time 2315.80ms\n",
            "iter 359: loss 0.0000, time 2289.18ms\n",
            "step 360: train loss 0.1584, val loss 0.4904\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0001, time 26026.27ms\n",
            "iter 361: loss 0.0000, time 2315.28ms\n",
            "iter 362: loss 0.0000, time 2328.63ms\n",
            "iter 363: loss 0.0000, time 2296.78ms\n",
            "iter 364: loss 0.0000, time 2592.52ms\n",
            "iter 365: loss 0.0000, time 2282.78ms\n",
            "iter 366: loss 0.0000, time 2269.38ms\n",
            "iter 367: loss 0.0000, time 2354.38ms\n",
            "iter 368: loss 0.0000, time 2344.65ms\n",
            "iter 369: loss 0.0000, time 2301.97ms\n",
            "iter 370: loss 0.0001, time 2295.40ms\n",
            "iter 371: loss 0.0000, time 2307.68ms\n",
            "iter 372: loss 0.0000, time 2300.91ms\n",
            "iter 373: loss 0.0001, time 2318.12ms\n",
            "iter 374: loss 0.0000, time 2297.05ms\n",
            "iter 375: loss 0.0000, time 2371.26ms\n",
            "iter 376: loss 0.0000, time 2292.58ms\n",
            "iter 377: loss 0.0000, time 2637.91ms\n",
            "iter 378: loss 0.0000, time 2257.19ms\n",
            "iter 379: loss 0.0000, time 2319.92ms\n",
            "step 380: train loss 0.1503, val loss 0.5025\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 25793.94ms\n",
            "iter 381: loss 0.0000, time 2299.80ms\n",
            "iter 382: loss 0.0000, time 2275.94ms\n",
            "iter 383: loss 0.0000, time 2331.73ms\n",
            "iter 384: loss 0.0000, time 2315.06ms\n",
            "iter 385: loss 0.0000, time 2329.46ms\n",
            "iter 386: loss 0.0000, time 2264.87ms\n",
            "iter 387: loss 0.0001, time 2226.91ms\n",
            "iter 388: loss 0.0000, time 2305.27ms\n",
            "iter 389: loss 0.0000, time 2363.24ms\n",
            "iter 390: loss 0.0000, time 2249.11ms\n",
            "iter 391: loss 0.0000, time 2636.26ms\n",
            "iter 392: loss 0.0000, time 2315.56ms\n",
            "iter 393: loss 0.0000, time 2283.55ms\n",
            "iter 394: loss 0.0000, time 2310.14ms\n",
            "iter 395: loss 0.0000, time 2271.19ms\n",
            "iter 396: loss 0.0000, time 2385.58ms\n",
            "iter 397: loss 0.0000, time 2347.41ms\n",
            "iter 398: loss 0.0000, time 2279.22ms\n",
            "iter 399: loss 0.0081, time 2242.03ms\n",
            "step 400: train loss 0.1435, val loss 0.5367\n",
            "step val accuracy 0.9453\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.7714\n",
            "Test recall 0.7333\n",
            "Test F1 0.7278\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 4  6  0]\n",
            " [ 0  4  6]]\n",
            "iter 400: loss 0.0000, time 26331.35ms\n",
            "iter 401: loss 0.0001, time 2317.76ms\n",
            "iter 402: loss 0.0000, time 2270.28ms\n",
            "iter 403: loss 0.0000, time 2314.77ms\n",
            "iter 404: loss 0.0000, time 2356.01ms\n",
            "iter 405: loss 0.0000, time 2616.43ms\n",
            "iter 406: loss 0.0000, time 2290.26ms\n",
            "iter 407: loss 0.0031, time 2378.23ms\n",
            "iter 408: loss 0.0000, time 2303.13ms\n",
            "iter 409: loss 0.0175, time 2325.71ms\n",
            "iter 410: loss 0.0000, time 2320.93ms\n",
            "iter 411: loss 0.0000, time 2304.12ms\n",
            "iter 412: loss 0.0000, time 2310.30ms\n",
            "iter 413: loss 0.0000, time 2299.30ms\n",
            "iter 414: loss 0.0000, time 2258.17ms\n",
            "iter 415: loss 0.0000, time 2350.76ms\n",
            "iter 416: loss 0.0000, time 2249.93ms\n",
            "iter 417: loss 0.0001, time 2270.85ms\n",
            "iter 418: loss 0.0001, time 2305.50ms\n",
            "iter 419: loss 0.0000, time 2643.87ms\n",
            "step 420: train loss 0.1376, val loss 0.5217\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 25830.50ms\n",
            "iter 421: loss 0.0002, time 2261.21ms\n",
            "iter 422: loss 0.0000, time 2376.92ms\n",
            "iter 423: loss 0.0000, time 2324.63ms\n",
            "iter 424: loss 0.0000, time 2334.92ms\n",
            "iter 425: loss 0.0000, time 2247.61ms\n",
            "iter 426: loss 0.0000, time 2292.96ms\n",
            "iter 427: loss 0.0002, time 2388.81ms\n",
            "iter 428: loss 0.0000, time 2324.66ms\n",
            "iter 429: loss 0.0000, time 2292.01ms\n",
            "iter 430: loss 0.0000, time 2335.65ms\n",
            "iter 431: loss 0.0000, time 2246.20ms\n",
            "iter 432: loss 0.0000, time 2239.74ms\n",
            "iter 433: loss 0.0000, time 2672.68ms\n",
            "iter 434: loss 0.0000, time 2266.53ms\n",
            "iter 435: loss 0.0000, time 2317.13ms\n",
            "iter 436: loss 0.0000, time 2432.61ms\n",
            "iter 437: loss 0.0000, time 2293.45ms\n",
            "iter 438: loss 0.0005, time 2312.90ms\n",
            "iter 439: loss 0.0002, time 2356.87ms\n",
            "step 440: train loss 0.1322, val loss 0.5304\n",
            "step val accuracy 0.9808\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 25564.91ms\n",
            "iter 441: loss 0.0009, time 2238.25ms\n",
            "iter 442: loss 0.0000, time 2271.02ms\n",
            "iter 443: loss 0.0000, time 2373.85ms\n",
            "iter 444: loss 0.0001, time 2390.14ms\n",
            "iter 445: loss 0.0000, time 2252.19ms\n",
            "iter 446: loss 0.0001, time 2325.97ms\n",
            "iter 447: loss 0.0000, time 2593.44ms\n",
            "iter 448: loss 0.0000, time 2264.65ms\n",
            "iter 449: loss 0.0000, time 2274.35ms\n",
            "iter 450: loss 0.0000, time 2278.69ms\n",
            "iter 451: loss 0.0001, time 2395.21ms\n",
            "iter 452: loss 0.0002, time 2346.79ms\n",
            "iter 453: loss 0.0069, time 2245.62ms\n",
            "iter 454: loss 0.0000, time 2264.73ms\n",
            "iter 455: loss 0.0000, time 2317.51ms\n",
            "iter 456: loss 0.0000, time 2324.21ms\n",
            "iter 457: loss 0.0001, time 2277.33ms\n",
            "iter 458: loss 0.0000, time 2227.28ms\n",
            "iter 459: loss 0.0000, time 2269.78ms\n",
            "step 460: train loss 0.1266, val loss 0.5442\n",
            "step val accuracy 0.9911\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 25763.79ms\n",
            "iter 461: loss 0.0000, time 2693.74ms\n",
            "iter 462: loss 0.0064, time 2216.27ms\n",
            "iter 463: loss 0.0000, time 2238.13ms\n",
            "iter 464: loss 0.0000, time 2271.78ms\n",
            "iter 465: loss 0.0000, time 2301.91ms\n",
            "iter 466: loss 0.0000, time 2339.25ms\n",
            "iter 467: loss 0.0000, time 2395.13ms\n",
            "iter 468: loss 0.0000, time 2325.74ms\n",
            "iter 469: loss 0.0000, time 2312.29ms\n",
            "iter 470: loss 0.0001, time 2539.40ms\n",
            "iter 471: loss 0.0037, time 2404.92ms\n",
            "iter 472: loss 0.0000, time 2346.14ms\n",
            "iter 473: loss 0.0000, time 2267.17ms\n",
            "iter 474: loss 0.0000, time 2602.51ms\n",
            "iter 475: loss 0.0000, time 2362.61ms\n",
            "iter 476: loss 0.0000, time 2261.40ms\n",
            "iter 477: loss 0.0000, time 2287.66ms\n",
            "iter 478: loss 0.0000, time 2272.93ms\n",
            "iter 479: loss 0.0000, time 2288.98ms\n",
            "step 480: train loss 0.1218, val loss 0.5542\n",
            "step val accuracy 0.9911\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 25701.05ms\n",
            "iter 481: loss 0.0000, time 2200.50ms\n",
            "iter 482: loss 0.0000, time 2309.84ms\n",
            "iter 483: loss 0.0000, time 2274.21ms\n",
            "iter 484: loss 0.0000, time 2314.87ms\n",
            "iter 485: loss 0.0002, time 2238.18ms\n",
            "iter 486: loss 0.0000, time 2282.90ms\n",
            "iter 487: loss 0.0000, time 2311.12ms\n",
            "iter 488: loss 0.0001, time 2350.54ms\n",
            "iter 489: loss 0.0000, time 2651.58ms\n",
            "iter 490: loss 0.0000, time 2278.44ms\n",
            "iter 491: loss 0.0000, time 2273.08ms\n",
            "iter 492: loss 0.0000, time 2244.99ms\n",
            "iter 493: loss 0.0000, time 2319.28ms\n",
            "iter 494: loss 0.0075, time 2281.36ms\n",
            "iter 495: loss 0.0000, time 2288.20ms\n",
            "iter 496: loss 1.6250, time 2313.59ms\n",
            "iter 497: loss 0.0000, time 2332.32ms\n",
            "iter 498: loss 0.0000, time 2241.43ms\n",
            "iter 499: loss 0.0000, time 2307.04ms\n",
            "step 500: train loss 0.1172, val loss 0.5613\n",
            "step val accuracy 0.9896\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8333\n",
            "Test recall 0.7667\n",
            "Test F1 0.7622\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 5 5]]\n",
            "iter 500: loss 0.0000, time 26817.96ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁███▇▇</td></tr><tr><td>Test_F1_Score</td><td>▁███▇▇</td></tr><tr><td>Test_Precision</td><td>▁█▇█▇█</td></tr><tr><td>Test_Recall</td><td>▁███▇▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▇▇████▇█████████████████</td></tr><tr><td>val/loss</td><td> █▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.76667</td></tr><tr><td>Test_F1_Score</td><td>0.76222</td></tr><tr><td>Test_Precision</td><td>0.83333</td></tr><tr><td>Test_Recall</td><td>0.76667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.11723</td></tr><tr><td>val/acc</td><td>0.98964</td></tr><tr><td>val/loss</td><td>0.56133</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stilted-sweep-5</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/dfydkbq7' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/dfydkbq7</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_012712-dfydkbq7\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wvae9uls with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_015730-wvae9uls</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/wvae9uls' target=\"_blank\">rural-sweep-6</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/wvae9uls' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/wvae9uls</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3438\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.5117, time 54357.89ms\n",
            "iter 1: loss 1.0078, time 2354.94ms\n",
            "iter 2: loss 5.0625, time 2300.43ms\n",
            "iter 3: loss 3.0469, time 2290.98ms\n",
            "iter 4: loss 1.0156, time 2327.49ms\n",
            "iter 5: loss 1.4609, time 2387.85ms\n",
            "iter 6: loss 0.8477, time 2278.09ms\n",
            "iter 7: loss 1.0391, time 2309.13ms\n",
            "iter 8: loss 0.6758, time 2721.02ms\n",
            "iter 9: loss 0.9063, time 2386.79ms\n",
            "iter 10: loss 1.6016, time 2355.03ms\n",
            "iter 11: loss 0.5664, time 2325.46ms\n",
            "iter 12: loss 2.0000, time 2354.24ms\n",
            "iter 13: loss 0.7969, time 2355.60ms\n",
            "iter 14: loss 0.6719, time 2510.83ms\n",
            "iter 15: loss 0.8242, time 2285.82ms\n",
            "iter 16: loss 0.7305, time 2339.28ms\n",
            "iter 17: loss 0.5625, time 2305.13ms\n",
            "iter 18: loss 0.2197, time 2411.86ms\n",
            "iter 19: loss 0.3203, time 2272.31ms\n",
            "step 20: train loss 1.2635, val loss 1.2323\n",
            "step val accuracy 0.7456\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.3164, time 25938.42ms\n",
            "iter 21: loss 0.2598, time 2576.61ms\n",
            "iter 22: loss 2.8438, time 2649.88ms\n",
            "iter 23: loss 0.3867, time 2315.55ms\n",
            "iter 24: loss 0.6250, time 2358.19ms\n",
            "iter 25: loss 0.0571, time 2358.38ms\n",
            "iter 26: loss 0.4336, time 2365.61ms\n",
            "iter 27: loss 0.1582, time 2322.02ms\n",
            "iter 28: loss 0.2813, time 2333.65ms\n",
            "iter 29: loss 0.1895, time 2288.33ms\n",
            "iter 30: loss 0.7656, time 2440.83ms\n",
            "iter 31: loss 0.3691, time 2297.03ms\n",
            "iter 32: loss 0.6445, time 2356.52ms\n",
            "iter 33: loss 0.0137, time 2388.22ms\n",
            "iter 34: loss 0.1680, time 2282.79ms\n",
            "iter 35: loss 0.1357, time 2771.05ms\n",
            "iter 36: loss 0.4492, time 2336.51ms\n",
            "iter 37: loss 0.1680, time 2303.03ms\n",
            "iter 38: loss 0.1699, time 2271.19ms\n",
            "iter 39: loss 0.1650, time 2332.60ms\n",
            "step 40: train loss 0.8701, val loss 0.8358\n",
            "step val accuracy 0.9157\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0172, time 25440.53ms\n",
            "iter 41: loss 0.0020, time 2267.61ms\n",
            "iter 42: loss 0.9961, time 2307.57ms\n",
            "iter 43: loss 0.5820, time 2268.77ms\n",
            "iter 44: loss 0.0007, time 2254.09ms\n",
            "iter 45: loss 0.0305, time 2267.31ms\n",
            "iter 46: loss 0.0005, time 2216.80ms\n",
            "iter 47: loss 1.2578, time 2256.24ms\n",
            "iter 48: loss 0.0942, time 2381.35ms\n",
            "iter 49: loss 0.0102, time 2391.94ms\n",
            "iter 50: loss 0.0013, time 2312.14ms\n",
            "iter 51: loss 0.0131, time 2627.34ms\n",
            "iter 52: loss 0.0361, time 2238.48ms\n",
            "iter 53: loss 0.0049, time 2371.29ms\n",
            "iter 54: loss 0.0564, time 2266.04ms\n",
            "iter 55: loss 0.0011, time 2341.46ms\n",
            "iter 56: loss 0.0461, time 2269.63ms\n",
            "iter 57: loss 0.0005, time 2346.50ms\n",
            "iter 58: loss 0.0016, time 2258.80ms\n",
            "iter 59: loss 0.0051, time 2223.05ms\n",
            "step 60: train loss 0.6311, val loss 0.7503\n",
            "step val accuracy 0.8299\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0153, time 25900.30ms\n",
            "iter 61: loss 0.0068, time 2233.67ms\n",
            "iter 62: loss 0.0004, time 2298.58ms\n",
            "iter 63: loss 0.0003, time 2243.83ms\n",
            "iter 64: loss 0.0085, time 2298.73ms\n",
            "iter 65: loss 1.1719, time 2399.18ms\n",
            "iter 66: loss 0.0127, time 2703.23ms\n",
            "iter 67: loss 0.0077, time 2228.85ms\n",
            "iter 68: loss 0.0006, time 2336.96ms\n",
            "iter 69: loss 0.0002, time 2341.49ms\n",
            "iter 70: loss 0.0008, time 2353.47ms\n",
            "iter 71: loss 0.0017, time 2211.03ms\n",
            "iter 72: loss 0.0000, time 2358.67ms\n",
            "iter 73: loss 0.0000, time 2295.26ms\n",
            "iter 74: loss 0.0029, time 2371.65ms\n",
            "iter 75: loss 0.0454, time 2309.31ms\n",
            "iter 76: loss 0.2344, time 2335.29ms\n",
            "iter 77: loss 0.0908, time 2332.92ms\n",
            "iter 78: loss 0.0021, time 2731.55ms\n",
            "iter 79: loss 0.0007, time 2321.81ms\n",
            "step 80: train loss 0.5079, val loss 0.7030\n",
            "step val accuracy 0.9601\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0023, time 25588.05ms\n",
            "iter 81: loss 4.6250, time 2310.22ms\n",
            "iter 82: loss 0.0001, time 2331.14ms\n",
            "iter 83: loss 0.0009, time 2276.69ms\n",
            "iter 84: loss 0.0057, time 2399.06ms\n",
            "iter 85: loss 0.0001, time 2289.79ms\n",
            "iter 86: loss 0.0005, time 2309.84ms\n",
            "iter 87: loss 0.0300, time 2339.19ms\n",
            "iter 88: loss 0.0026, time 2198.77ms\n",
            "iter 89: loss 0.0210, time 2252.49ms\n",
            "iter 90: loss 0.0092, time 2231.14ms\n",
            "iter 91: loss 0.0117, time 2298.44ms\n",
            "iter 92: loss 0.0001, time 2276.62ms\n",
            "iter 93: loss 0.0007, time 2750.86ms\n",
            "iter 94: loss 0.0035, time 2254.75ms\n",
            "iter 95: loss 0.0723, time 2267.71ms\n",
            "iter 96: loss 2.3906, time 2280.97ms\n",
            "iter 97: loss 0.0003, time 2346.77ms\n",
            "iter 98: loss 0.0001, time 2230.07ms\n",
            "iter 99: loss 0.0000, time 2259.35ms\n",
            "step 100: train loss 0.4217, val loss 0.6213\n",
            "step val accuracy 0.9482\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8476\n",
            "Test recall 0.8000\n",
            "Test F1 0.8000\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 4 6]]\n",
            "iter 100: loss 0.0011, time 26480.55ms\n",
            "iter 101: loss 0.0001, time 2566.24ms\n",
            "iter 102: loss 0.0000, time 2273.06ms\n",
            "iter 103: loss 0.0029, time 2324.74ms\n",
            "iter 104: loss 0.0007, time 2330.68ms\n",
            "iter 105: loss 0.0000, time 2381.22ms\n",
            "iter 106: loss 0.0000, time 2275.96ms\n",
            "iter 107: loss 0.0004, time 2637.12ms\n",
            "iter 108: loss 0.0000, time 2360.35ms\n",
            "iter 109: loss 0.0003, time 2456.26ms\n",
            "iter 110: loss 0.0000, time 2321.66ms\n",
            "iter 111: loss 0.0000, time 2243.74ms\n",
            "iter 112: loss 0.0000, time 2428.32ms\n",
            "iter 113: loss 0.0000, time 2332.34ms\n",
            "iter 114: loss 0.0000, time 2286.25ms\n",
            "iter 115: loss 0.0002, time 2265.68ms\n",
            "iter 116: loss 0.0031, time 2291.38ms\n",
            "iter 117: loss 5.0625, time 2342.09ms\n",
            "iter 118: loss 0.0001, time 2313.47ms\n",
            "iter 119: loss 0.0006, time 2627.91ms\n",
            "step 120: train loss 0.3732, val loss 0.6222\n",
            "step val accuracy 0.9305\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0001, time 25792.04ms\n",
            "iter 121: loss 0.0012, time 2322.79ms\n",
            "iter 122: loss 0.0012, time 2328.53ms\n",
            "iter 123: loss 0.0003, time 2227.62ms\n",
            "iter 124: loss 0.0000, time 2470.32ms\n",
            "iter 125: loss 0.0001, time 2293.48ms\n",
            "iter 126: loss 0.0000, time 2254.02ms\n",
            "iter 127: loss 0.0003, time 2359.59ms\n",
            "iter 128: loss 0.0005, time 2312.09ms\n",
            "iter 129: loss 0.0001, time 2351.50ms\n",
            "iter 130: loss 0.0011, time 2297.29ms\n",
            "iter 131: loss 0.0000, time 2329.13ms\n",
            "iter 132: loss 0.0001, time 2320.26ms\n",
            "iter 133: loss 0.0591, time 2287.91ms\n",
            "iter 134: loss 0.0020, time 2629.18ms\n",
            "iter 135: loss 0.0046, time 2300.65ms\n",
            "iter 136: loss 0.0000, time 2347.46ms\n",
            "iter 137: loss 0.0000, time 2377.09ms\n",
            "iter 138: loss 0.0000, time 2278.28ms\n",
            "iter 139: loss 0.0505, time 2272.94ms\n",
            "step 140: train loss 0.3285, val loss 0.5662\n",
            "step val accuracy 0.9497\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0001, time 25597.28ms\n",
            "iter 141: loss 0.0000, time 2295.15ms\n",
            "iter 142: loss 0.0004, time 2284.90ms\n",
            "iter 143: loss 0.0033, time 2331.33ms\n",
            "iter 144: loss 0.0003, time 2341.61ms\n",
            "iter 145: loss 0.0005, time 2377.33ms\n",
            "iter 146: loss 0.0006, time 2291.93ms\n",
            "iter 147: loss 0.0000, time 2319.69ms\n",
            "iter 148: loss 0.0001, time 2266.56ms\n",
            "iter 149: loss 0.0024, time 2672.75ms\n",
            "iter 150: loss 0.0017, time 2285.77ms\n",
            "iter 151: loss 0.0000, time 2269.38ms\n",
            "iter 152: loss 0.0520, time 2342.83ms\n",
            "iter 153: loss 0.0000, time 2369.13ms\n",
            "iter 154: loss 0.0002, time 2306.48ms\n",
            "iter 155: loss 0.0000, time 2281.45ms\n",
            "iter 156: loss 0.0000, time 2299.53ms\n",
            "iter 157: loss 0.0000, time 2352.62ms\n",
            "iter 158: loss 0.0000, time 2308.44ms\n",
            "iter 159: loss 0.0356, time 2253.49ms\n",
            "step 160: train loss 0.2963, val loss 0.5087\n",
            "step val accuracy 0.9112\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 25958.33ms\n",
            "iter 161: loss 0.0000, time 2217.48ms\n",
            "iter 162: loss 0.0000, time 2274.86ms\n",
            "iter 163: loss 0.0005, time 2260.50ms\n",
            "iter 164: loss 0.0000, time 2567.25ms\n",
            "iter 165: loss 0.0000, time 2251.19ms\n",
            "iter 166: loss 0.0001, time 2259.37ms\n",
            "iter 167: loss 0.0000, time 2226.32ms\n",
            "iter 168: loss 0.0000, time 2320.77ms\n",
            "iter 169: loss 0.0000, time 2305.05ms\n",
            "iter 170: loss 0.0000, time 2315.82ms\n",
            "iter 171: loss 0.0000, time 2310.71ms\n",
            "iter 172: loss 0.0018, time 2232.41ms\n",
            "iter 173: loss 0.0053, time 2341.72ms\n",
            "iter 174: loss 0.0001, time 2258.55ms\n",
            "iter 175: loss 0.0000, time 2324.32ms\n",
            "iter 176: loss 0.0000, time 2278.74ms\n",
            "iter 177: loss 0.0000, time 2289.83ms\n",
            "iter 178: loss 0.0000, time 2668.62ms\n",
            "iter 179: loss 0.0000, time 2289.13ms\n",
            "step 180: train loss 0.2689, val loss 0.5474\n",
            "step val accuracy 0.9689\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 26004.43ms\n",
            "iter 181: loss 0.0011, time 2296.51ms\n",
            "iter 182: loss 0.0000, time 2274.52ms\n",
            "iter 183: loss 0.0000, time 2321.87ms\n",
            "iter 184: loss 0.0000, time 2361.63ms\n",
            "iter 185: loss 0.0000, time 2349.10ms\n",
            "iter 186: loss 0.0000, time 2288.19ms\n",
            "iter 187: loss 0.0000, time 2362.35ms\n",
            "iter 188: loss 0.0000, time 2320.50ms\n",
            "iter 189: loss 0.0014, time 2319.31ms\n",
            "iter 190: loss 0.0020, time 2317.93ms\n",
            "iter 191: loss 0.0000, time 2266.00ms\n",
            "iter 192: loss 0.0000, time 2695.99ms\n",
            "iter 193: loss 0.0000, time 2292.51ms\n",
            "iter 194: loss 0.0000, time 2280.83ms\n",
            "iter 195: loss 0.0001, time 2284.77ms\n",
            "iter 196: loss 0.0000, time 2341.04ms\n",
            "iter 197: loss 0.0000, time 2420.21ms\n",
            "iter 198: loss 0.0001, time 2308.63ms\n",
            "iter 199: loss 0.0077, time 2261.91ms\n",
            "step 200: train loss 0.2450, val loss 0.5313\n",
            "step val accuracy 0.9615\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8069\n",
            "Test recall 0.8000\n",
            "Test F1 0.7839\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  5  2]\n",
            " [ 0  1  9]]\n",
            "iter 200: loss 0.0000, time 27537.21ms\n",
            "iter 201: loss 0.0000, time 2346.05ms\n",
            "iter 202: loss 0.0001, time 2335.67ms\n",
            "iter 203: loss 0.0000, time 2431.11ms\n",
            "iter 204: loss 0.0000, time 2363.69ms\n",
            "iter 205: loss 0.0000, time 2510.63ms\n",
            "iter 206: loss 0.0002, time 2648.20ms\n",
            "iter 207: loss 0.0000, time 2398.13ms\n",
            "iter 208: loss 0.0000, time 2259.29ms\n",
            "iter 209: loss 0.0017, time 2298.80ms\n",
            "iter 210: loss 2.6719, time 2234.97ms\n",
            "iter 211: loss 0.0000, time 2340.51ms\n",
            "iter 212: loss 0.0000, time 2396.06ms\n",
            "iter 213: loss 0.0010, time 2316.46ms\n",
            "iter 214: loss 0.0002, time 2257.39ms\n",
            "iter 215: loss 0.0000, time 2279.65ms\n",
            "iter 216: loss 0.0004, time 2380.91ms\n",
            "iter 217: loss 0.0000, time 2302.26ms\n",
            "iter 218: loss 0.0002, time 2314.88ms\n",
            "iter 219: loss 0.0000, time 2298.33ms\n",
            "step 220: train loss 0.2241, val loss 0.4871\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 26261.43ms\n",
            "iter 221: loss 0.0000, time 2653.47ms\n",
            "iter 222: loss 0.0000, time 2303.77ms\n",
            "iter 223: loss 0.0002, time 2399.42ms\n",
            "iter 224: loss 0.0001, time 2533.07ms\n",
            "iter 225: loss 0.0000, time 2315.29ms\n",
            "iter 226: loss 0.0000, time 2317.19ms\n",
            "iter 227: loss 0.0001, time 2590.64ms\n",
            "iter 228: loss 0.0000, time 2972.21ms\n",
            "iter 229: loss 0.0000, time 3035.21ms\n",
            "iter 230: loss 0.0000, time 2973.69ms\n",
            "iter 231: loss 0.0000, time 2989.41ms\n",
            "iter 232: loss 0.0000, time 3029.56ms\n",
            "iter 233: loss 0.0000, time 3055.25ms\n",
            "iter 234: loss 0.0000, time 2462.38ms\n",
            "iter 235: loss 0.0000, time 2638.81ms\n",
            "iter 236: loss 0.0000, time 2303.45ms\n",
            "iter 237: loss 0.0000, time 2278.34ms\n",
            "iter 238: loss 0.0000, time 2345.35ms\n",
            "iter 239: loss 0.0000, time 2272.62ms\n",
            "step 240: train loss 0.2060, val loss 0.4563\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 25878.48ms\n",
            "iter 241: loss 0.0000, time 2290.52ms\n",
            "iter 242: loss 0.0000, time 2343.65ms\n",
            "iter 243: loss 0.0000, time 2285.35ms\n",
            "iter 244: loss 0.0000, time 2271.49ms\n",
            "iter 245: loss 0.0000, time 2311.70ms\n",
            "iter 246: loss 0.0000, time 2276.09ms\n",
            "iter 247: loss 0.0000, time 2269.67ms\n",
            "iter 248: loss 0.0000, time 2265.13ms\n",
            "iter 249: loss 0.0000, time 2747.87ms\n",
            "iter 250: loss 0.0011, time 2314.00ms\n",
            "iter 251: loss 0.0000, time 2276.69ms\n",
            "iter 252: loss 0.0000, time 2303.15ms\n",
            "iter 253: loss 0.0000, time 2232.39ms\n",
            "iter 254: loss 0.0000, time 2267.26ms\n",
            "iter 255: loss 0.0000, time 2233.99ms\n",
            "iter 256: loss 0.0000, time 2314.52ms\n",
            "iter 257: loss 0.0001, time 2292.14ms\n",
            "iter 258: loss 0.0000, time 2342.04ms\n",
            "iter 259: loss 0.0001, time 2278.30ms\n",
            "step 260: train loss 0.1913, val loss 0.4679\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 25949.60ms\n",
            "iter 261: loss 0.0032, time 2339.47ms\n",
            "iter 262: loss 0.0026, time 2576.09ms\n",
            "iter 263: loss 0.0000, time 2260.55ms\n",
            "iter 264: loss 0.0000, time 2333.50ms\n",
            "iter 265: loss 0.0000, time 2341.15ms\n",
            "iter 266: loss 0.0000, time 2324.16ms\n",
            "iter 267: loss 0.0000, time 2234.60ms\n",
            "iter 268: loss 0.0000, time 2303.25ms\n",
            "iter 269: loss 0.0000, time 2305.98ms\n",
            "iter 270: loss 0.0000, time 2291.16ms\n",
            "iter 271: loss 0.0000, time 2238.34ms\n",
            "iter 272: loss 0.0000, time 2303.59ms\n",
            "iter 273: loss 0.0000, time 2377.70ms\n",
            "iter 274: loss 0.0000, time 2315.94ms\n",
            "iter 275: loss 0.0000, time 2619.69ms\n",
            "iter 276: loss 0.0000, time 2345.50ms\n",
            "iter 277: loss 0.0000, time 2343.27ms\n",
            "iter 278: loss 0.0000, time 2357.71ms\n",
            "iter 279: loss 0.0001, time 2323.71ms\n",
            "step 280: train loss 0.1781, val loss 0.4871\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.1299, time 25724.61ms\n",
            "iter 281: loss 0.0000, time 2309.01ms\n",
            "iter 282: loss 0.0000, time 2303.30ms\n",
            "iter 283: loss 0.0000, time 2347.12ms\n",
            "iter 284: loss 0.0000, time 2312.16ms\n",
            "iter 285: loss 0.0000, time 2301.54ms\n",
            "iter 286: loss 0.0000, time 2368.71ms\n",
            "iter 287: loss 0.0000, time 2291.49ms\n",
            "iter 288: loss 0.0000, time 2394.17ms\n",
            "iter 289: loss 0.0000, time 2673.17ms\n",
            "iter 290: loss 0.0000, time 2333.21ms\n",
            "iter 291: loss 0.0000, time 2264.98ms\n",
            "iter 292: loss 0.0000, time 2307.62ms\n",
            "iter 293: loss 0.0000, time 2351.16ms\n",
            "iter 294: loss 0.0007, time 2382.71ms\n",
            "iter 295: loss 0.0000, time 2429.65ms\n",
            "iter 296: loss 0.0000, time 2292.87ms\n",
            "iter 297: loss 0.0000, time 2277.88ms\n",
            "iter 298: loss 0.0000, time 2367.76ms\n",
            "iter 299: loss 0.0000, time 2307.30ms\n",
            "step 300: train loss 0.1666, val loss 0.4910\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8112\n",
            "Test recall 0.7667\n",
            "Test F1 0.7676\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 4 6]]\n",
            "iter 300: loss 0.0000, time 26577.81ms\n",
            "iter 301: loss 0.0001, time 2360.24ms\n",
            "iter 302: loss 0.0000, time 2255.80ms\n",
            "iter 303: loss 0.0000, time 2685.45ms\n",
            "iter 304: loss 0.0000, time 2373.78ms\n",
            "iter 305: loss 0.0000, time 2333.34ms\n",
            "iter 306: loss 0.0000, time 2312.16ms\n",
            "iter 307: loss 0.0000, time 2288.31ms\n",
            "iter 308: loss 0.0000, time 2293.57ms\n",
            "iter 309: loss 0.0000, time 2284.15ms\n",
            "iter 310: loss 0.0000, time 2277.33ms\n",
            "iter 311: loss 0.0000, time 2306.13ms\n",
            "iter 312: loss 0.0000, time 2318.05ms\n",
            "iter 313: loss 0.0000, time 2326.28ms\n",
            "iter 314: loss 0.0000, time 2274.08ms\n",
            "iter 315: loss 0.0000, time 2294.09ms\n",
            "iter 316: loss 0.0000, time 2275.08ms\n",
            "iter 317: loss 0.0000, time 2691.69ms\n",
            "iter 318: loss 0.0000, time 2331.56ms\n",
            "iter 319: loss 0.0000, time 2335.14ms\n",
            "step 320: train loss 0.1563, val loss 0.5018\n",
            "step val accuracy 0.9808\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 26088.76ms\n",
            "iter 321: loss 0.0000, time 2365.02ms\n",
            "iter 322: loss 0.0000, time 2272.37ms\n",
            "iter 323: loss 0.0000, time 2345.81ms\n",
            "iter 324: loss 0.0056, time 2363.49ms\n",
            "iter 325: loss 0.0000, time 2315.52ms\n",
            "iter 326: loss 0.0000, time 2281.21ms\n",
            "iter 327: loss 0.0000, time 2275.98ms\n",
            "iter 328: loss 0.0000, time 2284.32ms\n",
            "iter 329: loss 0.0005, time 2298.78ms\n",
            "iter 330: loss 0.0000, time 2190.57ms\n",
            "iter 331: loss 0.0001, time 2610.24ms\n",
            "iter 332: loss 0.0000, time 2249.65ms\n",
            "iter 333: loss 0.0000, time 2330.09ms\n",
            "iter 334: loss 0.0000, time 2349.75ms\n",
            "iter 335: loss 0.0000, time 2368.71ms\n",
            "iter 336: loss 0.0000, time 2301.43ms\n",
            "iter 337: loss 0.0000, time 2327.34ms\n",
            "iter 338: loss 0.0000, time 2459.80ms\n",
            "iter 339: loss 0.0000, time 2278.32ms\n",
            "step 340: train loss 0.1471, val loss 0.5103\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 26080.86ms\n",
            "iter 341: loss 0.0000, time 2295.51ms\n",
            "iter 342: loss 0.0001, time 2254.31ms\n",
            "iter 343: loss 0.0008, time 2326.56ms\n",
            "iter 344: loss 0.0009, time 2351.40ms\n",
            "iter 345: loss 0.0000, time 2292.95ms\n",
            "iter 346: loss 0.0000, time 2639.97ms\n",
            "iter 347: loss 0.0000, time 2261.64ms\n",
            "iter 348: loss 0.0000, time 2287.69ms\n",
            "iter 349: loss 0.0000, time 2399.84ms\n",
            "iter 350: loss 0.0000, time 2321.77ms\n",
            "iter 351: loss 0.0000, time 2349.62ms\n",
            "iter 352: loss 0.0000, time 2571.07ms\n",
            "iter 353: loss 0.0000, time 2352.48ms\n",
            "iter 354: loss 0.0002, time 2297.79ms\n",
            "iter 355: loss 0.0000, time 2291.06ms\n",
            "iter 356: loss 0.0000, time 2337.08ms\n",
            "iter 357: loss 0.0000, time 2277.98ms\n",
            "iter 358: loss 0.0000, time 2701.74ms\n",
            "iter 359: loss 0.0001, time 2270.69ms\n",
            "step 360: train loss 0.1401, val loss 0.5441\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 26013.42ms\n",
            "iter 361: loss 0.0000, time 2303.26ms\n",
            "iter 362: loss 0.0000, time 2246.21ms\n",
            "iter 363: loss 0.0007, time 2238.17ms\n",
            "iter 364: loss 0.0000, time 2295.54ms\n",
            "iter 365: loss 0.0000, time 2309.33ms\n",
            "iter 366: loss 0.0000, time 2344.54ms\n",
            "iter 367: loss 0.0000, time 2317.75ms\n",
            "iter 368: loss 0.0000, time 2421.07ms\n",
            "iter 369: loss 0.0000, time 2400.97ms\n",
            "iter 370: loss 0.0000, time 2359.28ms\n",
            "iter 371: loss 0.0000, time 2300.13ms\n",
            "iter 372: loss 0.0000, time 2315.25ms\n",
            "iter 373: loss 0.0000, time 2794.37ms\n",
            "iter 374: loss 0.0000, time 2311.83ms\n",
            "iter 375: loss 0.0001, time 2438.35ms\n",
            "iter 376: loss 0.0000, time 2302.63ms\n",
            "iter 377: loss 0.0000, time 2347.69ms\n",
            "iter 378: loss 0.0000, time 2307.90ms\n",
            "iter 379: loss 0.0000, time 2313.91ms\n",
            "step 380: train loss 0.1334, val loss 0.5343\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0001, time 25608.13ms\n",
            "iter 381: loss 0.0000, time 2327.98ms\n",
            "iter 382: loss 0.0000, time 2293.53ms\n",
            "iter 383: loss 0.0000, time 2279.13ms\n",
            "iter 384: loss 0.0000, time 2416.12ms\n",
            "iter 385: loss 0.0000, time 2345.82ms\n",
            "iter 386: loss 0.0000, time 2240.22ms\n",
            "iter 387: loss 0.0000, time 2212.66ms\n",
            "iter 388: loss 0.0000, time 2365.00ms\n",
            "iter 389: loss 0.0000, time 2746.73ms\n",
            "iter 390: loss 0.0001, time 2247.27ms\n",
            "iter 391: loss 0.0000, time 2305.47ms\n",
            "iter 392: loss 0.0000, time 2359.69ms\n",
            "iter 393: loss 0.0000, time 2379.86ms\n",
            "iter 394: loss 0.0000, time 2304.25ms\n",
            "iter 395: loss 0.0000, time 2281.17ms\n",
            "iter 396: loss 0.0000, time 2372.19ms\n",
            "iter 397: loss 0.0000, time 2391.75ms\n",
            "iter 398: loss 0.0003, time 2362.25ms\n",
            "iter 399: loss 0.0000, time 2294.07ms\n",
            "step 400: train loss 0.1271, val loss 0.5803\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8889\n",
            "Test recall 0.8333\n",
            "Test F1 0.8325\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  4  6]]\n",
            "iter 400: loss 0.0000, time 26757.96ms\n",
            "iter 401: loss 0.0000, time 2361.53ms\n",
            "iter 402: loss 0.0000, time 2250.50ms\n",
            "iter 403: loss 0.0000, time 2349.67ms\n",
            "iter 404: loss 0.0000, time 2654.96ms\n",
            "iter 405: loss 0.0000, time 2213.16ms\n",
            "iter 406: loss 0.0000, time 2294.33ms\n",
            "iter 407: loss 0.0009, time 2327.32ms\n",
            "iter 408: loss 0.0000, time 2315.95ms\n",
            "iter 409: loss 0.0004, time 2353.52ms\n",
            "iter 410: loss 0.0000, time 2405.13ms\n",
            "iter 411: loss 0.0000, time 2285.86ms\n",
            "iter 412: loss 0.0000, time 2277.79ms\n",
            "iter 413: loss 0.0000, time 2321.96ms\n",
            "iter 414: loss 0.0000, time 2356.44ms\n",
            "iter 415: loss 0.0000, time 2398.30ms\n",
            "iter 416: loss 0.0000, time 2639.19ms\n",
            "iter 417: loss 0.0000, time 2237.98ms\n",
            "iter 418: loss 0.0029, time 2323.58ms\n",
            "iter 419: loss 0.0000, time 2328.49ms\n",
            "step 420: train loss 0.1211, val loss 0.5954\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 25746.63ms\n",
            "iter 421: loss 0.0000, time 2264.78ms\n",
            "iter 422: loss 0.0000, time 2335.56ms\n",
            "iter 423: loss 0.0000, time 2347.30ms\n",
            "iter 424: loss 0.0000, time 2387.01ms\n",
            "iter 425: loss 0.0000, time 2305.15ms\n",
            "iter 426: loss 0.0000, time 2459.89ms\n",
            "iter 427: loss 0.0000, time 2770.76ms\n",
            "iter 428: loss 0.0000, time 2361.38ms\n",
            "iter 429: loss 0.0000, time 2330.11ms\n",
            "iter 430: loss 0.0000, time 2320.95ms\n",
            "iter 431: loss 0.0000, time 2418.98ms\n",
            "iter 432: loss 0.0000, time 2613.26ms\n",
            "iter 433: loss 0.0000, time 2315.91ms\n",
            "iter 434: loss 0.0000, time 2265.81ms\n",
            "iter 435: loss 0.0062, time 2334.48ms\n",
            "iter 436: loss 0.0000, time 2421.94ms\n",
            "iter 437: loss 0.0000, time 2283.02ms\n",
            "iter 438: loss 0.0000, time 2321.98ms\n",
            "iter 439: loss 0.0000, time 2402.73ms\n",
            "step 440: train loss 0.1160, val loss 0.6131\n",
            "step val accuracy 0.9660\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0033, time 25652.71ms\n",
            "iter 441: loss 0.0000, time 2315.67ms\n",
            "iter 442: loss 0.0000, time 2292.41ms\n",
            "iter 443: loss 0.0000, time 2404.49ms\n",
            "iter 444: loss 0.0000, time 2378.88ms\n",
            "iter 445: loss 0.0000, time 2274.21ms\n",
            "iter 446: loss 0.0000, time 2313.44ms\n",
            "iter 447: loss 0.0000, time 2687.10ms\n",
            "iter 448: loss 0.0000, time 2376.95ms\n",
            "iter 449: loss 0.0000, time 2259.61ms\n",
            "iter 450: loss 0.0000, time 2317.35ms\n",
            "iter 451: loss 0.0000, time 2374.78ms\n",
            "iter 452: loss 0.0000, time 2368.55ms\n",
            "iter 453: loss 0.0001, time 2302.87ms\n",
            "iter 454: loss 0.0015, time 2326.69ms\n",
            "iter 455: loss 0.0000, time 2326.60ms\n",
            "iter 456: loss 0.0000, time 2329.93ms\n",
            "iter 457: loss 0.0000, time 2319.03ms\n",
            "iter 458: loss 0.0000, time 2226.22ms\n",
            "iter 459: loss 0.0000, time 2276.96ms\n",
            "step 460: train loss 0.1113, val loss 0.6054\n",
            "step val accuracy 0.9808\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 26537.38ms\n",
            "iter 461: loss 0.0000, time 2684.60ms\n",
            "iter 462: loss 0.0327, time 2268.33ms\n",
            "iter 463: loss 0.0003, time 2224.11ms\n",
            "iter 464: loss 0.0000, time 2224.76ms\n",
            "iter 465: loss 0.0000, time 2270.75ms\n",
            "iter 466: loss 0.0000, time 2333.77ms\n",
            "iter 467: loss 0.0000, time 2371.75ms\n",
            "iter 468: loss 0.0000, time 2313.46ms\n",
            "iter 469: loss 0.0000, time 2328.10ms\n",
            "iter 470: loss 0.0000, time 2390.80ms\n",
            "iter 471: loss 0.0000, time 2331.88ms\n",
            "iter 472: loss 0.0000, time 2334.58ms\n",
            "iter 473: loss 0.0000, time 2311.32ms\n",
            "iter 474: loss 0.0000, time 2630.26ms\n",
            "iter 475: loss 0.0001, time 2371.74ms\n",
            "iter 476: loss 0.0000, time 2278.08ms\n",
            "iter 477: loss 0.0000, time 2338.15ms\n",
            "iter 478: loss 0.0000, time 2284.31ms\n",
            "iter 479: loss 0.0000, time 2310.46ms\n",
            "step 480: train loss 0.1067, val loss 0.6184\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 25768.04ms\n",
            "iter 481: loss 0.0000, time 2208.91ms\n",
            "iter 482: loss 0.0000, time 2396.34ms\n",
            "iter 483: loss 0.2813, time 2322.67ms\n",
            "iter 484: loss 0.0000, time 2348.11ms\n",
            "iter 485: loss 0.0000, time 2269.97ms\n",
            "iter 486: loss 0.0000, time 2280.11ms\n",
            "iter 487: loss 0.0000, time 2290.56ms\n",
            "iter 488: loss 0.0000, time 2325.95ms\n",
            "iter 489: loss 0.0029, time 2670.25ms\n",
            "iter 490: loss 0.0000, time 2331.53ms\n",
            "iter 491: loss 0.0000, time 2336.50ms\n",
            "iter 492: loss 0.0000, time 2276.58ms\n",
            "iter 493: loss 0.0000, time 2284.64ms\n",
            "iter 494: loss 3.0312, time 2292.72ms\n",
            "iter 495: loss 0.0000, time 2383.91ms\n",
            "iter 496: loss 0.0000, time 2346.92ms\n",
            "iter 497: loss 0.0000, time 2382.63ms\n",
            "iter 498: loss 0.0000, time 2324.47ms\n",
            "iter 499: loss 0.0000, time 2318.40ms\n",
            "step 500: train loss 0.1031, val loss 0.6303\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8498\n",
            "Test recall 0.8333\n",
            "Test F1 0.8295\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 0  9  1]\n",
            " [ 0  4  6]]\n",
            "iter 500: loss 0.0000, time 27064.21ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁██▇██</td></tr><tr><td>Test_F1_Score</td><td>▁█▇▇██</td></tr><tr><td>Test_Precision</td><td>▁█▇▇██</td></tr><tr><td>Test_Recall</td><td>▁██▇██</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▇▆██▇█▇█████████████████</td></tr><tr><td>val/loss</td><td> █▄▄▃▂▂▂▁▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.83333</td></tr><tr><td>Test_F1_Score</td><td>0.8295</td></tr><tr><td>Test_Precision</td><td>0.84982</td></tr><tr><td>Test_Recall</td><td>0.83333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.1031</td></tr><tr><td>val/acc</td><td>0.97337</td></tr><tr><td>val/loss</td><td>0.63028</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rural-sweep-6</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/wvae9uls' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/wvae9uls</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_015730-wvae9uls\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1tioxpto with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_022806-1tioxpto</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/1tioxpto' target=\"_blank\">pious-sweep-7</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/1tioxpto' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/1tioxpto</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.4231\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 7.5625, time 36321.05ms\n",
            "iter 1: loss 0.4824, time 2387.09ms\n",
            "iter 2: loss 1.7734, time 2301.76ms\n",
            "iter 3: loss 0.4727, time 2427.55ms\n",
            "iter 4: loss 1.9219, time 2446.32ms\n",
            "iter 5: loss 0.5625, time 2389.86ms\n",
            "iter 6: loss 0.4336, time 2563.78ms\n",
            "iter 7: loss 0.4453, time 2611.79ms\n",
            "iter 8: loss 0.4766, time 2735.93ms\n",
            "iter 9: loss 0.2539, time 2363.56ms\n",
            "iter 10: loss 1.0000, time 2433.19ms\n",
            "iter 11: loss 0.6406, time 2433.89ms\n",
            "iter 12: loss 0.2871, time 2379.09ms\n",
            "iter 13: loss 0.7109, time 2346.95ms\n",
            "iter 14: loss 0.5937, time 2403.74ms\n",
            "iter 15: loss 0.9766, time 2392.60ms\n",
            "iter 16: loss 0.3945, time 2424.01ms\n",
            "iter 17: loss 0.0894, time 2346.37ms\n",
            "iter 18: loss 0.0674, time 2429.29ms\n",
            "iter 19: loss 0.0654, time 2381.78ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 1.0439, val loss 0.5306\n",
            "step val accuracy 0.8402\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.2393, time 36480.18ms\n",
            "iter 21: loss 0.2402, time 2355.02ms\n",
            "iter 22: loss 0.0243, time 2393.45ms\n",
            "iter 23: loss 0.9609, time 2325.57ms\n",
            "iter 24: loss 0.1367, time 2311.53ms\n",
            "iter 25: loss 0.0080, time 2404.70ms\n",
            "iter 26: loss 0.0417, time 2504.84ms\n",
            "iter 27: loss 0.0276, time 2798.13ms\n",
            "iter 28: loss 0.0698, time 2398.07ms\n",
            "iter 29: loss 0.0615, time 2372.36ms\n",
            "iter 30: loss 0.0283, time 2296.64ms\n",
            "iter 31: loss 0.2891, time 2350.68ms\n",
            "iter 32: loss 0.1377, time 2289.52ms\n",
            "iter 33: loss 0.2363, time 2321.34ms\n",
            "iter 34: loss 0.0013, time 2354.73ms\n",
            "iter 35: loss 0.0013, time 2407.92ms\n",
            "iter 36: loss 0.0537, time 2336.33ms\n",
            "iter 37: loss 0.0069, time 2364.20ms\n",
            "iter 38: loss 0.0110, time 2331.24ms\n",
            "iter 39: loss 1.1328, time 2824.53ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.7233, val loss 0.6668\n",
            "step val accuracy 0.9050\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.1748, time 35956.07ms\n",
            "iter 41: loss 0.0072, time 2397.91ms\n",
            "iter 42: loss 0.0259, time 2356.26ms\n",
            "iter 43: loss 0.0012, time 2310.16ms\n",
            "iter 44: loss 0.0408, time 2351.78ms\n",
            "iter 45: loss 0.0110, time 2786.32ms\n",
            "iter 46: loss 0.4023, time 2376.54ms\n",
            "iter 47: loss 0.0840, time 2342.76ms\n",
            "iter 48: loss 0.0439, time 2406.37ms\n",
            "iter 49: loss 0.0099, time 2398.20ms\n",
            "iter 50: loss 0.6680, time 2419.60ms\n",
            "iter 51: loss 0.2080, time 2341.08ms\n",
            "iter 52: loss 0.0952, time 2401.79ms\n",
            "iter 53: loss 0.0101, time 2467.20ms\n",
            "iter 54: loss 0.0320, time 2381.91ms\n",
            "iter 55: loss 0.0334, time 2426.60ms\n",
            "iter 56: loss 0.1021, time 2367.89ms\n",
            "iter 57: loss 0.0947, time 2447.37ms\n",
            "iter 58: loss 1.5469, time 2760.03ms\n",
            "iter 59: loss 0.0112, time 2441.17ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.5733, val loss 0.7139\n",
            "step val accuracy 0.9460\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.2148, time 35817.84ms\n",
            "iter 61: loss 0.7070, time 2587.74ms\n",
            "iter 62: loss 0.0129, time 2453.71ms\n",
            "iter 63: loss 0.0071, time 2347.19ms\n",
            "iter 64: loss 0.0155, time 2719.36ms\n",
            "iter 65: loss 0.0148, time 2325.53ms\n",
            "iter 66: loss 0.0073, time 2348.11ms\n",
            "iter 67: loss 0.0052, time 2360.83ms\n",
            "iter 68: loss 0.0003, time 2359.01ms\n",
            "iter 69: loss 0.0024, time 2325.42ms\n",
            "iter 70: loss 0.0659, time 2375.78ms\n",
            "iter 71: loss 0.0164, time 2310.37ms\n",
            "iter 72: loss 0.0371, time 2362.54ms\n",
            "iter 73: loss 0.0124, time 2351.60ms\n",
            "iter 74: loss 0.0090, time 2322.79ms\n",
            "iter 75: loss 0.0009, time 2363.50ms\n",
            "iter 76: loss 0.0513, time 2643.87ms\n",
            "iter 77: loss 0.0156, time 2433.16ms\n",
            "iter 78: loss 0.0005, time 2352.02ms\n",
            "iter 79: loss 0.0016, time 2428.71ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 80: train loss 0.4758, val loss 0.7211\n",
            "step val accuracy 0.8996\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.8984, time 36036.44ms\n",
            "iter 81: loss 0.0133, time 2446.11ms\n",
            "iter 82: loss 0.0093, time 2788.92ms\n",
            "iter 83: loss 0.0022, time 2346.84ms\n",
            "iter 84: loss 0.0004, time 2388.01ms\n",
            "iter 85: loss 0.0009, time 2451.04ms\n",
            "iter 86: loss 0.0064, time 2363.12ms\n",
            "iter 87: loss 0.0033, time 2350.73ms\n",
            "iter 88: loss 0.0121, time 2362.04ms\n",
            "iter 89: loss 0.0703, time 2377.17ms\n",
            "iter 90: loss 0.0003, time 2371.60ms\n",
            "iter 91: loss 0.0001, time 2337.70ms\n",
            "iter 92: loss 0.0996, time 2445.27ms\n",
            "iter 93: loss 0.0247, time 2429.10ms\n",
            "iter 94: loss 0.0057, time 2733.32ms\n",
            "iter 95: loss 0.0141, time 2389.48ms\n",
            "iter 96: loss 1.1328, time 2441.34ms\n",
            "iter 97: loss 0.0107, time 2468.29ms\n",
            "iter 98: loss 0.0894, time 2346.01ms\n",
            "iter 99: loss 0.0065, time 2389.13ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 100: train loss 0.4086, val loss 0.6974\n",
            "step val accuracy 0.9708\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6000\n",
            "Test precision 0.4306\n",
            "Test recall 0.6000\n",
            "Test F1 0.4926\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [1 9 0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 100: loss 0.0133, time 36757.86ms\n",
            "iter 101: loss 0.0093, time 2847.25ms\n",
            "iter 102: loss 0.0012, time 2392.54ms\n",
            "iter 103: loss 0.0112, time 2473.80ms\n",
            "iter 104: loss 0.0010, time 2411.63ms\n",
            "iter 105: loss 0.0002, time 2314.78ms\n",
            "iter 106: loss 0.0635, time 2327.74ms\n",
            "iter 107: loss 0.0277, time 2460.06ms\n",
            "iter 108: loss 0.0679, time 2395.98ms\n",
            "iter 109: loss 0.0000, time 2377.11ms\n",
            "iter 110: loss 0.0005, time 2423.53ms\n",
            "iter 111: loss 0.0010, time 2372.82ms\n",
            "iter 112: loss 0.0012, time 2376.20ms\n",
            "iter 113: loss 0.0009, time 2391.32ms\n",
            "iter 114: loss 0.0053, time 2708.41ms\n",
            "iter 115: loss 0.0001, time 2414.29ms\n",
            "iter 116: loss 0.0042, time 2501.36ms\n",
            "iter 117: loss 0.0124, time 2366.62ms\n",
            "iter 118: loss 0.0005, time 2403.15ms\n",
            "iter 119: loss 0.0026, time 2360.92ms\n",
            "step 120: train loss 0.3544, val loss 0.6909\n",
            "step val accuracy 0.9568\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0006, time 35982.58ms\n",
            "iter 121: loss 0.1089, time 2472.41ms\n",
            "iter 122: loss 0.0327, time 2398.06ms\n",
            "iter 123: loss 0.0000, time 2329.66ms\n",
            "iter 124: loss 0.0038, time 2390.01ms\n",
            "iter 125: loss 0.0908, time 2448.14ms\n",
            "iter 126: loss 0.0757, time 2360.58ms\n",
            "iter 127: loss 0.0002, time 2440.61ms\n",
            "iter 128: loss 0.0002, time 2377.81ms\n",
            "iter 129: loss 0.0003, time 2367.46ms\n",
            "iter 130: loss 0.0000, time 2318.16ms\n",
            "iter 131: loss 0.0007, time 2763.95ms\n",
            "iter 132: loss 0.0000, time 2391.47ms\n",
            "iter 133: loss 0.0004, time 2372.34ms\n",
            "iter 134: loss 0.0015, time 2342.31ms\n",
            "iter 135: loss 0.0001, time 2363.27ms\n",
            "iter 136: loss 0.0028, time 2370.59ms\n",
            "iter 137: loss 0.0000, time 2402.10ms\n",
            "iter 138: loss 0.0000, time 2379.37ms\n",
            "iter 139: loss 0.0001, time 2376.27ms\n",
            "step 140: train loss 0.3089, val loss 0.6810\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0001, time 36076.14ms\n",
            "iter 141: loss 0.0006, time 2352.99ms\n",
            "iter 142: loss 0.0023, time 2405.01ms\n",
            "iter 143: loss 0.0009, time 2363.66ms\n",
            "iter 144: loss 0.0015, time 2313.66ms\n",
            "iter 145: loss 0.0009, time 2336.72ms\n",
            "iter 146: loss 0.9414, time 2381.92ms\n",
            "iter 147: loss 0.0000, time 2311.41ms\n",
            "iter 148: loss 0.0004, time 2288.35ms\n",
            "iter 149: loss 0.0015, time 2717.50ms\n",
            "iter 150: loss 0.0011, time 2354.58ms\n",
            "iter 151: loss 0.0003, time 2359.85ms\n",
            "iter 152: loss 0.0064, time 2369.78ms\n",
            "iter 153: loss 0.0002, time 2255.49ms\n",
            "iter 154: loss 0.0001, time 2340.53ms\n",
            "iter 155: loss 0.0000, time 2448.97ms\n",
            "iter 156: loss 0.0000, time 2372.80ms\n",
            "iter 157: loss 0.0001, time 2599.93ms\n",
            "iter 158: loss 0.0002, time 2485.79ms\n",
            "iter 159: loss 0.0000, time 2277.39ms\n",
            "step 160: train loss 0.2751, val loss 0.6790\n",
            "step val accuracy 0.9568\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 36074.90ms\n",
            "iter 161: loss 0.0001, time 2383.51ms\n",
            "iter 162: loss 0.0001, time 2422.14ms\n",
            "iter 163: loss 0.0000, time 2401.36ms\n",
            "iter 164: loss 0.0019, time 2394.62ms\n",
            "iter 165: loss 0.0000, time 2358.00ms\n",
            "iter 166: loss 0.0000, time 2409.51ms\n",
            "iter 167: loss 0.0000, time 2785.42ms\n",
            "iter 168: loss 0.0007, time 2320.54ms\n",
            "iter 169: loss 0.0001, time 2419.00ms\n",
            "iter 170: loss 0.0030, time 2338.27ms\n",
            "iter 171: loss 0.0001, time 2354.91ms\n",
            "iter 172: loss 0.0002, time 2400.68ms\n",
            "iter 173: loss 0.1367, time 2421.99ms\n",
            "iter 174: loss 0.0403, time 2384.90ms\n",
            "iter 175: loss 0.0002, time 2326.12ms\n",
            "iter 176: loss 0.0001, time 2329.42ms\n",
            "iter 177: loss 0.0001, time 2636.20ms\n",
            "iter 178: loss 0.0008, time 2848.46ms\n",
            "iter 179: loss 0.0000, time 2343.95ms\n",
            "step 180: train loss 0.2470, val loss 0.6513\n",
            "step val accuracy 0.9924\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0041, time 36139.98ms\n",
            "iter 181: loss 0.0003, time 2343.79ms\n",
            "iter 182: loss 0.0000, time 2380.62ms\n",
            "iter 183: loss 0.0009, time 2366.57ms\n",
            "iter 184: loss 0.0001, time 2696.54ms\n",
            "iter 185: loss 0.0000, time 2369.59ms\n",
            "iter 186: loss 0.0009, time 2380.22ms\n",
            "iter 187: loss 0.0077, time 2348.32ms\n",
            "iter 188: loss 0.0000, time 2421.84ms\n",
            "iter 189: loss 0.0000, time 2346.13ms\n",
            "iter 190: loss 0.0000, time 2389.29ms\n",
            "iter 191: loss 0.0000, time 2376.26ms\n",
            "iter 192: loss 0.0000, time 2266.63ms\n",
            "iter 193: loss 0.0000, time 2318.92ms\n",
            "iter 194: loss 0.0000, time 2382.23ms\n",
            "iter 195: loss 0.0003, time 2355.11ms\n",
            "iter 196: loss 0.0801, time 2389.68ms\n",
            "iter 197: loss 0.0027, time 2721.04ms\n",
            "iter 198: loss 0.0000, time 2417.61ms\n",
            "iter 199: loss 0.0000, time 2355.51ms\n",
            "step 200: train loss 0.2228, val loss 0.6521\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.8098\n",
            "Test recall 0.7000\n",
            "Test F1 0.6761\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 7 3]]\n",
            "iter 200: loss 0.0000, time 36686.27ms\n",
            "iter 201: loss 0.0000, time 2402.67ms\n",
            "iter 202: loss 0.0000, time 2380.82ms\n",
            "iter 203: loss 0.3184, time 2389.28ms\n",
            "iter 204: loss 0.0000, time 2689.39ms\n",
            "iter 205: loss 0.0000, time 2293.44ms\n",
            "iter 206: loss 0.0000, time 2381.15ms\n",
            "iter 207: loss 0.0000, time 2426.80ms\n",
            "iter 208: loss 0.0001, time 2368.28ms\n",
            "iter 209: loss 0.0000, time 2335.07ms\n",
            "iter 210: loss 0.0001, time 2291.10ms\n",
            "iter 211: loss 0.0000, time 2457.50ms\n",
            "iter 212: loss 0.0000, time 2494.43ms\n",
            "iter 213: loss 0.0004, time 2374.66ms\n",
            "iter 214: loss 0.0000, time 2406.39ms\n",
            "iter 215: loss 0.0002, time 2384.26ms\n",
            "iter 216: loss 0.0003, time 2296.88ms\n",
            "iter 217: loss 0.0009, time 2753.47ms\n",
            "iter 218: loss 0.0000, time 2353.21ms\n",
            "iter 219: loss 0.0000, time 2447.38ms\n",
            "step 220: train loss 0.2030, val loss 0.6964\n",
            "step val accuracy 0.9892\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 35999.03ms\n",
            "iter 221: loss 0.0000, time 2334.32ms\n",
            "iter 222: loss 0.0000, time 2772.52ms\n",
            "iter 223: loss 0.0000, time 2404.82ms\n",
            "iter 224: loss 0.0000, time 2343.17ms\n",
            "iter 225: loss 0.0000, time 2377.69ms\n",
            "iter 226: loss 0.0000, time 2364.69ms\n",
            "iter 227: loss 0.0000, time 2370.00ms\n",
            "iter 228: loss 0.0000, time 2335.79ms\n",
            "iter 229: loss 0.0000, time 2385.73ms\n",
            "iter 230: loss 0.0001, time 2299.38ms\n",
            "iter 231: loss 0.0000, time 2402.03ms\n",
            "iter 232: loss 0.0000, time 2355.83ms\n",
            "iter 233: loss 0.0000, time 2401.04ms\n",
            "iter 234: loss 0.0000, time 2383.89ms\n",
            "iter 235: loss 0.0000, time 2789.39ms\n",
            "iter 236: loss 0.0000, time 2326.53ms\n",
            "iter 237: loss 0.0000, time 2320.72ms\n",
            "iter 238: loss 0.0000, time 2348.84ms\n",
            "iter 239: loss 0.0000, time 2447.12ms\n",
            "step 240: train loss 0.1863, val loss 0.6971\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 35597.89ms\n",
            "iter 241: loss 0.0000, time 2819.10ms\n",
            "iter 242: loss 0.0000, time 2350.93ms\n",
            "iter 243: loss 0.0038, time 2344.49ms\n",
            "iter 244: loss 0.0000, time 2384.54ms\n",
            "iter 245: loss 0.0000, time 2421.16ms\n",
            "iter 246: loss 0.0000, time 2459.16ms\n",
            "iter 247: loss 0.0000, time 2396.90ms\n",
            "iter 248: loss 0.0000, time 2356.49ms\n",
            "iter 249: loss 0.0000, time 2390.97ms\n",
            "iter 250: loss 0.0000, time 2503.94ms\n",
            "iter 251: loss 0.0000, time 2369.18ms\n",
            "iter 252: loss 0.0000, time 2432.25ms\n",
            "iter 253: loss 0.0002, time 2745.12ms\n",
            "iter 254: loss 0.0000, time 2396.43ms\n",
            "iter 255: loss 0.0000, time 2388.76ms\n",
            "iter 256: loss 0.0000, time 2344.64ms\n",
            "iter 257: loss 0.0053, time 2283.89ms\n",
            "iter 258: loss 0.0000, time 2350.21ms\n",
            "iter 259: loss 0.0000, time 2369.47ms\n",
            "step 260: train loss 0.1725, val loss 0.6879\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 36091.60ms\n",
            "iter 261: loss 0.0000, time 2325.61ms\n",
            "iter 262: loss 0.0000, time 2320.09ms\n",
            "iter 263: loss 0.0000, time 2400.84ms\n",
            "iter 264: loss 0.0123, time 2292.11ms\n",
            "iter 265: loss 0.0000, time 2332.12ms\n",
            "iter 266: loss 0.0024, time 2356.29ms\n",
            "iter 267: loss 0.0000, time 2361.21ms\n",
            "iter 268: loss 0.0000, time 2362.75ms\n",
            "iter 269: loss 0.0000, time 2292.67ms\n",
            "iter 270: loss 0.0000, time 2346.08ms\n",
            "iter 271: loss 0.0011, time 2371.50ms\n",
            "iter 272: loss 0.0001, time 2682.40ms\n",
            "iter 273: loss 0.0000, time 2311.98ms\n",
            "iter 274: loss 0.0001, time 2374.97ms\n",
            "iter 275: loss 0.0000, time 2365.21ms\n",
            "iter 276: loss 0.0000, time 2410.71ms\n",
            "iter 277: loss 0.0000, time 2400.04ms\n",
            "iter 278: loss 0.0000, time 2369.28ms\n",
            "iter 279: loss 0.0000, time 2320.34ms\n",
            "step 280: train loss 0.1621, val loss 0.6743\n",
            "step val accuracy 0.9903\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 36219.01ms\n",
            "iter 281: loss 0.0000, time 2443.05ms\n",
            "iter 282: loss 0.0000, time 2357.62ms\n",
            "iter 283: loss 0.0000, time 2344.26ms\n",
            "iter 284: loss 0.0000, time 2382.21ms\n",
            "iter 285: loss 0.0000, time 2355.80ms\n",
            "iter 286: loss 0.0000, time 2333.47ms\n",
            "iter 287: loss 0.0000, time 2354.33ms\n",
            "iter 288: loss 0.0000, time 2354.04ms\n",
            "iter 289: loss 0.0464, time 2380.19ms\n",
            "iter 290: loss 0.0000, time 2332.87ms\n",
            "iter 291: loss 0.0000, time 2674.26ms\n",
            "iter 292: loss 0.0000, time 2297.71ms\n",
            "iter 293: loss 0.0000, time 2417.66ms\n",
            "iter 294: loss 0.0000, time 2417.48ms\n",
            "iter 295: loss 0.0000, time 2402.25ms\n",
            "iter 296: loss 0.0001, time 2364.60ms\n",
            "iter 297: loss 0.0000, time 2332.20ms\n",
            "iter 298: loss 0.0000, time 2421.14ms\n",
            "iter 299: loss 0.0000, time 2353.47ms\n",
            "step 300: train loss 0.1527, val loss 0.6972\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.8519\n",
            "Test recall 0.7333\n",
            "Test F1 0.7077\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  7  3]]\n",
            "iter 300: loss 0.0000, time 37590.25ms\n",
            "iter 301: loss 0.0000, time 2296.74ms\n",
            "iter 302: loss 0.0056, time 2345.02ms\n",
            "iter 303: loss 0.0000, time 2382.53ms\n",
            "iter 304: loss 0.0004, time 2466.94ms\n",
            "iter 305: loss 0.0000, time 2329.36ms\n",
            "iter 306: loss 0.0000, time 2385.31ms\n",
            "iter 307: loss 0.0000, time 2345.72ms\n",
            "iter 308: loss 0.0000, time 2438.77ms\n",
            "iter 309: loss 0.0001, time 2371.62ms\n",
            "iter 310: loss 0.0001, time 2705.56ms\n",
            "iter 311: loss 0.0001, time 2355.71ms\n",
            "iter 312: loss 0.0000, time 2445.90ms\n",
            "iter 313: loss 0.0000, time 2358.00ms\n",
            "iter 314: loss 0.0000, time 2412.44ms\n",
            "iter 315: loss 0.0002, time 2386.39ms\n",
            "iter 316: loss 0.0000, time 2455.60ms\n",
            "iter 317: loss 0.0000, time 2363.21ms\n",
            "iter 318: loss 0.0000, time 2369.99ms\n",
            "iter 319: loss 0.0000, time 2370.92ms\n",
            "step 320: train loss 0.1450, val loss 0.7226\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 36255.53ms\n",
            "iter 321: loss 0.0003, time 2390.78ms\n",
            "iter 322: loss 0.0010, time 2336.84ms\n",
            "iter 323: loss 0.0000, time 2348.95ms\n",
            "iter 324: loss 0.0000, time 2403.16ms\n",
            "iter 325: loss 0.0000, time 2354.32ms\n",
            "iter 326: loss 0.0000, time 2320.37ms\n",
            "iter 327: loss 0.0000, time 2372.01ms\n",
            "iter 328: loss 0.0000, time 2478.63ms\n",
            "iter 329: loss 0.0000, time 2337.06ms\n",
            "iter 330: loss 0.0000, time 2787.19ms\n",
            "iter 331: loss 0.0000, time 2373.92ms\n",
            "iter 332: loss 0.0001, time 2352.53ms\n",
            "iter 333: loss 0.0000, time 2322.98ms\n",
            "iter 334: loss 0.0000, time 2321.14ms\n",
            "iter 335: loss 0.0000, time 2394.44ms\n",
            "iter 336: loss 0.0000, time 2423.96ms\n",
            "iter 337: loss 0.0000, time 2306.49ms\n",
            "iter 338: loss 0.0000, time 2350.59ms\n",
            "iter 339: loss 0.0002, time 2400.80ms\n",
            "step 340: train loss 0.1375, val loss 0.7071\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 36076.48ms\n",
            "iter 341: loss 0.0000, time 2321.91ms\n",
            "iter 342: loss 0.0000, time 2399.62ms\n",
            "iter 343: loss 0.0000, time 2414.18ms\n",
            "iter 344: loss 0.0000, time 2379.47ms\n",
            "iter 345: loss 0.0000, time 2390.77ms\n",
            "iter 346: loss 0.0004, time 2389.82ms\n",
            "iter 347: loss 0.0000, time 2365.12ms\n",
            "iter 348: loss 0.0081, time 2319.83ms\n",
            "iter 349: loss 0.0006, time 2728.35ms\n",
            "iter 350: loss 0.0000, time 2353.35ms\n",
            "iter 351: loss 0.0000, time 2445.67ms\n",
            "iter 352: loss 0.0000, time 2394.56ms\n",
            "iter 353: loss 0.0000, time 2376.19ms\n",
            "iter 354: loss 0.0000, time 2333.73ms\n",
            "iter 355: loss 0.0000, time 2421.62ms\n",
            "iter 356: loss 0.0001, time 2388.77ms\n",
            "iter 357: loss 0.0000, time 2358.41ms\n",
            "iter 358: loss 0.0000, time 2317.97ms\n",
            "iter 359: loss 0.0001, time 2364.11ms\n",
            "step 360: train loss 0.1302, val loss 0.7228\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0001, time 36288.38ms\n",
            "iter 361: loss 0.0000, time 2316.87ms\n",
            "iter 362: loss 0.0000, time 2397.55ms\n",
            "iter 363: loss 0.0000, time 2403.84ms\n",
            "iter 364: loss 0.0000, time 2326.08ms\n",
            "iter 365: loss 0.0000, time 2395.44ms\n",
            "iter 366: loss 0.0000, time 2418.16ms\n",
            "iter 367: loss 0.0000, time 2355.89ms\n",
            "iter 368: loss 0.0000, time 2799.04ms\n",
            "iter 369: loss 0.0000, time 2394.04ms\n",
            "iter 370: loss 0.0000, time 2401.87ms\n",
            "iter 371: loss 0.0000, time 2373.87ms\n",
            "iter 372: loss 0.0000, time 2358.56ms\n",
            "iter 373: loss 0.0000, time 2313.39ms\n",
            "iter 374: loss 0.0000, time 2454.25ms\n",
            "iter 375: loss 0.0025, time 2366.80ms\n",
            "iter 376: loss 0.0000, time 2457.66ms\n",
            "iter 377: loss 0.0003, time 2474.87ms\n",
            "iter 378: loss 0.0001, time 2393.98ms\n",
            "iter 379: loss 0.0065, time 2454.65ms\n",
            "step 380: train loss 0.1238, val loss 0.7211\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0001, time 36269.31ms\n",
            "iter 381: loss 0.0194, time 2347.40ms\n",
            "iter 382: loss 0.0003, time 2332.86ms\n",
            "iter 383: loss 0.0000, time 2396.46ms\n",
            "iter 384: loss 0.0000, time 2346.93ms\n",
            "iter 385: loss 0.0000, time 2304.36ms\n",
            "iter 386: loss 0.0001, time 2663.55ms\n",
            "iter 387: loss 0.0000, time 2323.72ms\n",
            "iter 388: loss 0.0001, time 2365.64ms\n",
            "iter 389: loss 0.0000, time 2331.50ms\n",
            "iter 390: loss 0.0000, time 2322.11ms\n",
            "iter 391: loss 0.0000, time 2321.97ms\n",
            "iter 392: loss 0.0000, time 2345.55ms\n",
            "iter 393: loss 0.0001, time 2381.93ms\n",
            "iter 394: loss 0.0000, time 2351.67ms\n",
            "iter 395: loss 0.0000, time 2395.37ms\n",
            "iter 396: loss 0.0000, time 2366.32ms\n",
            "iter 397: loss 0.0000, time 2556.85ms\n",
            "iter 398: loss 0.0000, time 2475.51ms\n",
            "iter 399: loss 0.0000, time 2726.62ms\n",
            "step 400: train loss 0.1181, val loss 0.7084\n",
            "step val accuracy 0.9892\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6667\n",
            "Test precision 0.7727\n",
            "Test recall 0.6667\n",
            "Test F1 0.6447\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 7 3]]\n",
            "iter 400: loss 0.0000, time 37143.75ms\n",
            "iter 401: loss 0.0000, time 2323.73ms\n",
            "iter 402: loss 0.0000, time 2344.09ms\n",
            "iter 403: loss 0.0000, time 2316.83ms\n",
            "iter 404: loss 0.0003, time 2385.26ms\n",
            "iter 405: loss 0.0000, time 2776.31ms\n",
            "iter 406: loss 0.0000, time 2275.92ms\n",
            "iter 407: loss 3.8750, time 2343.63ms\n",
            "iter 408: loss 0.0000, time 2351.45ms\n",
            "iter 409: loss 0.0000, time 2345.05ms\n",
            "iter 410: loss 0.0001, time 2278.36ms\n",
            "iter 411: loss 0.0148, time 2320.48ms\n",
            "iter 412: loss 0.0000, time 2412.24ms\n",
            "iter 413: loss 0.0000, time 2382.51ms\n",
            "iter 414: loss 0.0000, time 2326.33ms\n",
            "iter 415: loss 0.0002, time 2410.22ms\n",
            "iter 416: loss 0.0000, time 2413.63ms\n",
            "iter 417: loss 0.0000, time 2696.91ms\n",
            "iter 418: loss 0.0002, time 2371.61ms\n",
            "iter 419: loss 0.0000, time 2343.13ms\n",
            "step 420: train loss 0.1129, val loss 0.7288\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 35925.72ms\n",
            "iter 421: loss 0.0000, time 2340.88ms\n",
            "iter 422: loss 0.0000, time 2330.33ms\n",
            "iter 423: loss 0.0000, time 2743.43ms\n",
            "iter 424: loss 0.0000, time 2425.35ms\n",
            "iter 425: loss 0.0000, time 2315.14ms\n",
            "iter 426: loss 0.0000, time 2347.06ms\n",
            "iter 427: loss 0.0000, time 2393.24ms\n",
            "iter 428: loss 0.0000, time 2360.33ms\n",
            "iter 429: loss 0.0000, time 2341.16ms\n",
            "iter 430: loss 0.0000, time 2337.57ms\n",
            "iter 431: loss 0.0000, time 2286.91ms\n",
            "iter 432: loss 0.0000, time 2382.63ms\n",
            "iter 433: loss 0.0000, time 2336.36ms\n",
            "iter 434: loss 0.0000, time 2324.98ms\n",
            "iter 435: loss 0.0000, time 2649.90ms\n",
            "iter 436: loss 0.0000, time 2288.35ms\n",
            "iter 437: loss 0.2891, time 2332.61ms\n",
            "iter 438: loss 0.0000, time 2373.55ms\n",
            "iter 439: loss 0.0000, time 2340.95ms\n",
            "step 440: train loss 0.1085, val loss 0.7084\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 35853.77ms\n",
            "iter 441: loss 0.0000, time 2665.09ms\n",
            "iter 442: loss 0.0000, time 2381.74ms\n",
            "iter 443: loss 0.0000, time 2391.60ms\n",
            "iter 444: loss 0.0000, time 2383.01ms\n",
            "iter 445: loss 0.0000, time 2530.43ms\n",
            "iter 446: loss 0.0000, time 2358.78ms\n",
            "iter 447: loss 0.0000, time 2436.64ms\n",
            "iter 448: loss 0.0000, time 2367.32ms\n",
            "iter 449: loss 0.0000, time 2337.45ms\n",
            "iter 450: loss 0.0000, time 2364.97ms\n",
            "iter 451: loss 0.0003, time 2324.05ms\n",
            "iter 452: loss 0.0000, time 2320.42ms\n",
            "iter 453: loss 0.0000, time 2724.42ms\n",
            "iter 454: loss 0.0000, time 2394.91ms\n",
            "iter 455: loss 0.0000, time 2304.28ms\n",
            "iter 456: loss 0.0000, time 2381.81ms\n",
            "iter 457: loss 0.0000, time 2399.41ms\n",
            "iter 458: loss 0.0000, time 2282.12ms\n",
            "iter 459: loss 0.0000, time 2304.56ms\n",
            "step 460: train loss 0.1053, val loss 0.7127\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 36307.57ms\n",
            "iter 461: loss 0.0005, time 2350.31ms\n",
            "iter 462: loss 0.0001, time 2292.32ms\n",
            "iter 463: loss 0.0000, time 2301.42ms\n",
            "iter 464: loss 0.0000, time 2361.50ms\n",
            "iter 465: loss 0.0000, time 2325.53ms\n",
            "iter 466: loss 0.0007, time 2285.26ms\n",
            "iter 467: loss 0.0019, time 2558.84ms\n",
            "iter 468: loss 0.0000, time 2303.25ms\n",
            "iter 469: loss 0.0000, time 2336.39ms\n",
            "iter 470: loss 0.0000, time 2333.77ms\n",
            "iter 471: loss 0.0000, time 2732.24ms\n",
            "iter 472: loss 0.0006, time 2319.13ms\n",
            "iter 473: loss 0.0000, time 2442.08ms\n",
            "iter 474: loss 0.0000, time 2371.79ms\n",
            "iter 475: loss 0.0071, time 2373.38ms\n",
            "iter 476: loss 0.0002, time 2290.05ms\n",
            "iter 477: loss 0.0000, time 2256.45ms\n",
            "iter 478: loss 0.0000, time 2294.40ms\n",
            "iter 479: loss 0.0000, time 2332.29ms\n",
            "step 480: train loss 0.1011, val loss 0.6869\n",
            "step val accuracy 0.9946\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 36174.13ms\n",
            "iter 481: loss 0.0000, time 2333.36ms\n",
            "iter 482: loss 0.0000, time 2356.87ms\n",
            "iter 483: loss 0.0000, time 2334.45ms\n",
            "iter 484: loss 0.0000, time 2451.99ms\n",
            "iter 485: loss 0.0000, time 2287.32ms\n",
            "iter 486: loss 0.0000, time 2392.99ms\n",
            "iter 487: loss 0.0000, time 2383.83ms\n",
            "iter 488: loss 0.0000, time 2668.73ms\n",
            "iter 489: loss 0.0000, time 2311.13ms\n",
            "iter 490: loss 0.0000, time 2351.95ms\n",
            "iter 491: loss 0.0000, time 2448.76ms\n",
            "iter 492: loss 0.0015, time 2362.71ms\n",
            "iter 493: loss 0.0000, time 2353.25ms\n",
            "iter 494: loss 0.0000, time 2390.22ms\n",
            "iter 495: loss 0.0000, time 2384.25ms\n",
            "iter 496: loss 0.0000, time 2400.81ms\n",
            "iter 497: loss 0.0000, time 2416.20ms\n",
            "iter 498: loss 0.0000, time 2376.57ms\n",
            "iter 499: loss 0.0000, time 2347.45ms\n",
            "step 500: train loss 0.0971, val loss 0.6809\n",
            "step val accuracy 0.9924\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8750\n",
            "Test recall 0.8000\n",
            "Test F1 0.7802\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  6  4]]\n",
            "iter 500: loss 0.0000, time 37305.88ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▆▇▆█</td></tr><tr><td>Test_F1_Score</td><td>▁▅▇▇▆█</td></tr><tr><td>Test_Precision</td><td>▁▄▇█▇█</td></tr><tr><td>Test_Recall</td><td>▁▅▆▇▆█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▇▇█████████████████████</td></tr><tr><td>val/loss</td><td> ▁▆▇█▇▇▆▆▅▅▇▇▇▆▇█▇██▇█▇▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.8</td></tr><tr><td>Test_F1_Score</td><td>0.78022</td></tr><tr><td>Test_Precision</td><td>0.875</td></tr><tr><td>Test_Recall</td><td>0.8</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.09708</td></tr><tr><td>val/acc</td><td>0.99244</td></tr><tr><td>val/loss</td><td>0.68094</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pious-sweep-7</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/1tioxpto' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/1tioxpto</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_022806-1tioxpto\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o70d8jq5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_030311-o70d8jq5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/o70d8jq5' target=\"_blank\">pretty-sweep-8</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/o70d8jq5' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/o70d8jq5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.4231\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 7.6250, time 36812.36ms\n",
            "iter 1: loss 0.6484, time 2374.23ms\n",
            "iter 2: loss 0.0021, time 2324.63ms\n",
            "iter 3: loss 0.1436, time 2394.61ms\n",
            "iter 4: loss 0.6953, time 2343.90ms\n",
            "iter 5: loss 1.1250, time 2349.36ms\n",
            "iter 6: loss 0.5937, time 2313.06ms\n",
            "iter 7: loss 1.4609, time 2370.81ms\n",
            "iter 8: loss 0.1699, time 2384.97ms\n",
            "iter 9: loss 0.7383, time 2320.57ms\n",
            "iter 10: loss 0.5078, time 2384.33ms\n",
            "iter 11: loss 0.4648, time 2403.05ms\n",
            "iter 12: loss 0.3105, time 2719.26ms\n",
            "iter 13: loss 0.9141, time 2333.91ms\n",
            "iter 14: loss 0.4141, time 2365.46ms\n",
            "iter 15: loss 1.1406, time 2370.63ms\n",
            "iter 16: loss 0.3242, time 2362.13ms\n",
            "iter 17: loss 0.1338, time 2350.77ms\n",
            "iter 18: loss 0.9453, time 2396.21ms\n",
            "iter 19: loss 0.0151, time 2374.57ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 1.0645, val loss 0.7223\n",
            "step val accuracy 0.7797\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.0259, time 36386.60ms\n",
            "iter 21: loss 0.1182, time 2489.11ms\n",
            "iter 22: loss 0.0029, time 2436.73ms\n",
            "iter 23: loss 3.4688, time 2343.80ms\n",
            "iter 24: loss 0.0253, time 2345.20ms\n",
            "iter 25: loss 0.0016, time 2453.24ms\n",
            "iter 26: loss 0.2969, time 2402.79ms\n",
            "iter 27: loss 0.0107, time 2394.69ms\n",
            "iter 28: loss 0.8320, time 2389.34ms\n",
            "iter 29: loss 0.0437, time 2376.12ms\n",
            "iter 30: loss 0.1504, time 2688.90ms\n",
            "iter 31: loss 0.7969, time 2414.14ms\n",
            "iter 32: loss 0.0211, time 2308.77ms\n",
            "iter 33: loss 0.6094, time 2394.51ms\n",
            "iter 34: loss 0.0041, time 2371.63ms\n",
            "iter 35: loss 0.0045, time 2371.60ms\n",
            "iter 36: loss 0.0898, time 2383.48ms\n",
            "iter 37: loss 0.1504, time 2390.97ms\n",
            "iter 38: loss 0.0281, time 2423.82ms\n",
            "iter 39: loss 0.1641, time 2392.48ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.7978, val loss 0.9082\n",
            "step val accuracy 0.7138\n",
            "saving checkpoint to out\n",
            "iter 40: loss 1.8047, time 36108.81ms\n",
            "iter 41: loss 0.1914, time 2402.63ms\n",
            "iter 42: loss 0.3027, time 2374.17ms\n",
            "iter 43: loss 0.0356, time 2307.79ms\n",
            "iter 44: loss 0.0149, time 2326.38ms\n",
            "iter 45: loss 0.0014, time 2390.31ms\n",
            "iter 46: loss 2.7031, time 2327.24ms\n",
            "iter 47: loss 0.0254, time 2323.88ms\n",
            "iter 48: loss 0.2578, time 2378.43ms\n",
            "iter 49: loss 0.0280, time 2449.68ms\n",
            "iter 50: loss 0.6875, time 2695.58ms\n",
            "iter 51: loss 0.5000, time 2321.03ms\n",
            "iter 52: loss 0.0752, time 2375.57ms\n",
            "iter 53: loss 0.0510, time 2401.81ms\n",
            "iter 54: loss 0.0388, time 2370.27ms\n",
            "iter 55: loss 0.0486, time 2373.57ms\n",
            "iter 56: loss 0.0105, time 2313.24ms\n",
            "iter 57: loss 0.0181, time 2378.75ms\n",
            "iter 58: loss 0.7734, time 2378.22ms\n",
            "iter 59: loss 0.0030, time 2428.24ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.6561, val loss 0.7714\n",
            "step val accuracy 0.9244\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.3496, time 36641.32ms\n",
            "iter 61: loss 0.0679, time 2511.89ms\n",
            "iter 62: loss 0.0021, time 2463.22ms\n",
            "iter 63: loss 0.0089, time 2377.47ms\n",
            "iter 64: loss 0.1147, time 2408.54ms\n",
            "iter 65: loss 0.0040, time 2449.00ms\n",
            "iter 66: loss 0.0159, time 2414.60ms\n",
            "iter 67: loss 0.0010, time 2429.31ms\n",
            "iter 68: loss 0.0031, time 2808.20ms\n",
            "iter 69: loss 0.0168, time 2412.89ms\n",
            "iter 70: loss 0.2891, time 2403.76ms\n",
            "iter 71: loss 0.0001, time 2341.93ms\n",
            "iter 72: loss 0.0062, time 2484.65ms\n",
            "iter 73: loss 0.0019, time 2439.41ms\n",
            "iter 74: loss 0.0015, time 2362.12ms\n",
            "iter 75: loss 0.0070, time 2392.21ms\n",
            "iter 76: loss 0.0298, time 2403.53ms\n",
            "iter 77: loss 0.2227, time 2492.99ms\n",
            "iter 78: loss 0.0173, time 2396.98ms\n",
            "iter 79: loss 0.0002, time 2439.74ms\n",
            "step 80: train loss 0.5370, val loss 0.7104\n",
            "step val accuracy 0.9536\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0947, time 36718.61ms\n",
            "iter 81: loss 0.0020, time 2454.98ms\n",
            "iter 82: loss 0.0605, time 2469.39ms\n",
            "iter 83: loss 0.0095, time 2434.96ms\n",
            "iter 84: loss 0.0032, time 2361.14ms\n",
            "iter 85: loss 0.0129, time 2442.15ms\n",
            "iter 86: loss 0.0011, time 2351.17ms\n",
            "iter 87: loss 0.0020, time 2730.73ms\n",
            "iter 88: loss 0.0004, time 2386.43ms\n",
            "iter 89: loss 0.0510, time 2348.66ms\n",
            "iter 90: loss 0.0043, time 2376.27ms\n",
            "iter 91: loss 0.0000, time 2350.74ms\n",
            "iter 92: loss 4.2812, time 2380.98ms\n",
            "iter 93: loss 0.0037, time 2368.86ms\n",
            "iter 94: loss 0.0027, time 2374.38ms\n",
            "iter 95: loss 0.0036, time 2431.60ms\n",
            "iter 96: loss 0.1689, time 2354.50ms\n",
            "iter 97: loss 0.0015, time 2377.65ms\n",
            "iter 98: loss 0.0542, time 2346.81ms\n",
            "iter 99: loss 0.0005, time 2346.99ms\n",
            "step 100: train loss 0.4500, val loss 0.6085\n",
            "step val accuracy 0.9590\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.4921\n",
            "Test recall 0.6333\n",
            "Test F1 0.5308\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 100: loss 0.0003, time 38100.26ms\n",
            "iter 101: loss 0.0000, time 2489.34ms\n",
            "iter 102: loss 0.0001, time 2420.34ms\n",
            "iter 103: loss 0.0117, time 2434.50ms\n",
            "iter 104: loss 0.0036, time 2423.80ms\n",
            "iter 105: loss 0.0087, time 2769.48ms\n",
            "iter 106: loss 0.0000, time 2502.31ms\n",
            "iter 107: loss 0.0004, time 2438.84ms\n",
            "iter 108: loss 0.0125, time 2414.91ms\n",
            "iter 109: loss 0.0000, time 2482.61ms\n",
            "iter 110: loss 0.0019, time 2406.65ms\n",
            "iter 111: loss 0.0006, time 2438.53ms\n",
            "iter 112: loss 0.0000, time 2353.08ms\n",
            "iter 113: loss 0.0000, time 2473.50ms\n",
            "iter 114: loss 0.0000, time 2421.99ms\n",
            "iter 115: loss 0.0005, time 2424.82ms\n",
            "iter 116: loss 0.0000, time 2444.72ms\n",
            "iter 117: loss 0.0001, time 2403.50ms\n",
            "iter 118: loss 0.0002, time 2837.22ms\n",
            "iter 119: loss 0.0000, time 2353.23ms\n",
            "step 120: train loss 0.3842, val loss 0.5756\n",
            "step val accuracy 0.9773\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0000, time 35905.02ms\n",
            "iter 121: loss 0.0006, time 2435.02ms\n",
            "iter 122: loss 0.0001, time 2422.27ms\n",
            "iter 123: loss 0.0000, time 2310.37ms\n",
            "iter 124: loss 0.0000, time 2400.95ms\n",
            "iter 125: loss 0.0000, time 2421.61ms\n",
            "iter 126: loss 0.0000, time 2733.08ms\n",
            "iter 127: loss 0.0000, time 2423.59ms\n",
            "iter 128: loss 0.0000, time 2470.06ms\n",
            "iter 129: loss 0.0002, time 2381.93ms\n",
            "iter 130: loss 0.0005, time 2356.69ms\n",
            "iter 131: loss 0.0000, time 2389.46ms\n",
            "iter 132: loss 0.0000, time 2415.90ms\n",
            "iter 133: loss 0.0001, time 2379.45ms\n",
            "iter 134: loss 0.0001, time 2393.67ms\n",
            "iter 135: loss 0.0000, time 2436.36ms\n",
            "iter 136: loss 0.0000, time 2411.11ms\n",
            "iter 137: loss 0.0000, time 2842.94ms\n",
            "iter 138: loss 0.0000, time 2327.46ms\n",
            "iter 139: loss 0.0000, time 2316.07ms\n",
            "step 140: train loss 0.3309, val loss 0.6244\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0000, time 36113.19ms\n",
            "iter 141: loss 0.0019, time 2351.54ms\n",
            "iter 142: loss 0.0000, time 2436.04ms\n",
            "iter 143: loss 0.0000, time 2379.30ms\n",
            "iter 144: loss 0.0000, time 2403.17ms\n",
            "iter 145: loss 0.0000, time 2671.43ms\n",
            "iter 146: loss 0.0281, time 2408.77ms\n",
            "iter 147: loss 0.0000, time 2367.04ms\n",
            "iter 148: loss 0.0000, time 2383.73ms\n",
            "iter 149: loss 0.0000, time 2425.56ms\n",
            "iter 150: loss 0.0000, time 2362.71ms\n",
            "iter 151: loss 0.0000, time 2387.23ms\n",
            "iter 152: loss 0.0178, time 2498.67ms\n",
            "iter 153: loss 0.0000, time 2334.97ms\n",
            "iter 154: loss 0.0000, time 2342.82ms\n",
            "iter 155: loss 0.0000, time 2419.15ms\n",
            "iter 156: loss 0.0000, time 2740.94ms\n",
            "iter 157: loss 0.0000, time 2426.46ms\n",
            "iter 158: loss 0.0004, time 2449.51ms\n",
            "iter 159: loss 0.0000, time 2345.89ms\n",
            "step 160: train loss 0.2929, val loss 0.5905\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 35817.48ms\n",
            "iter 161: loss 0.0021, time 2723.68ms\n",
            "iter 162: loss 0.0046, time 2473.66ms\n",
            "iter 163: loss 0.0000, time 2453.02ms\n",
            "iter 164: loss 0.0002, time 2425.04ms\n",
            "iter 165: loss 0.0000, time 2330.02ms\n",
            "iter 166: loss 0.0000, time 2343.04ms\n",
            "iter 167: loss 0.0000, time 2379.35ms\n",
            "iter 168: loss 0.0003, time 2338.86ms\n",
            "iter 169: loss 0.0000, time 2349.66ms\n",
            "iter 170: loss 0.0000, time 2321.39ms\n",
            "iter 171: loss 0.0001, time 2434.26ms\n",
            "iter 172: loss 0.0000, time 2399.94ms\n",
            "iter 173: loss 0.0015, time 2421.31ms\n",
            "iter 174: loss 0.0618, time 2736.78ms\n",
            "iter 175: loss 0.0000, time 2493.11ms\n",
            "iter 176: loss 0.0052, time 2358.85ms\n",
            "iter 177: loss 0.0008, time 2434.21ms\n",
            "iter 178: loss 0.0001, time 2364.68ms\n",
            "iter 179: loss 0.0001, time 2321.51ms\n",
            "step 180: train loss 0.2635, val loss 0.5253\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0001, time 36504.09ms\n",
            "iter 181: loss 0.0000, time 2448.06ms\n",
            "iter 182: loss 0.0000, time 2359.30ms\n",
            "iter 183: loss 0.0000, time 2313.66ms\n",
            "iter 184: loss 0.0000, time 2282.01ms\n",
            "iter 185: loss 0.0000, time 2323.15ms\n",
            "iter 186: loss 0.0000, time 2321.08ms\n",
            "iter 187: loss 0.0000, time 2345.00ms\n",
            "iter 188: loss 0.0000, time 2324.19ms\n",
            "iter 189: loss 0.0000, time 2320.69ms\n",
            "iter 190: loss 0.0001, time 2350.90ms\n",
            "iter 191: loss 0.0000, time 2429.17ms\n",
            "iter 192: loss 0.0005, time 2303.50ms\n",
            "iter 193: loss 0.0311, time 2669.74ms\n",
            "iter 194: loss 0.0000, time 2396.04ms\n",
            "iter 195: loss 0.0000, time 2427.94ms\n",
            "iter 196: loss 0.0276, time 2380.87ms\n",
            "iter 197: loss 0.0000, time 2313.31ms\n",
            "iter 198: loss 0.0000, time 2342.31ms\n",
            "iter 199: loss 0.0004, time 2431.42ms\n",
            "step 200: train loss 0.2382, val loss 0.5586\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6667\n",
            "Test precision 0.7727\n",
            "Test recall 0.6667\n",
            "Test F1 0.6447\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 7 3]]\n",
            "iter 200: loss 0.0001, time 37343.59ms\n",
            "iter 201: loss 0.6445, time 2312.51ms\n",
            "iter 202: loss 0.0004, time 2337.73ms\n",
            "iter 203: loss 0.0006, time 2375.56ms\n",
            "iter 204: loss 0.0000, time 2279.19ms\n",
            "iter 205: loss 0.0021, time 2375.86ms\n",
            "iter 206: loss 0.0000, time 2331.72ms\n",
            "iter 207: loss 0.0000, time 2323.69ms\n",
            "iter 208: loss 0.0000, time 2295.19ms\n",
            "iter 209: loss 0.0000, time 2330.28ms\n",
            "iter 210: loss 0.0001, time 2300.97ms\n",
            "iter 211: loss 0.0000, time 2928.73ms\n",
            "iter 212: loss 0.0000, time 2361.34ms\n",
            "iter 213: loss 0.0000, time 2342.74ms\n",
            "iter 214: loss 0.0000, time 2375.22ms\n",
            "iter 215: loss 0.0000, time 2330.60ms\n",
            "iter 216: loss 0.0001, time 2235.04ms\n",
            "iter 217: loss 0.0000, time 2352.30ms\n",
            "iter 218: loss 0.0000, time 2340.88ms\n",
            "iter 219: loss 0.0000, time 2360.04ms\n",
            "step 220: train loss 0.2168, val loss 0.5787\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 36087.41ms\n",
            "iter 221: loss 0.0000, time 2472.16ms\n",
            "iter 222: loss 0.0000, time 2399.81ms\n",
            "iter 223: loss 0.0004, time 2367.90ms\n",
            "iter 224: loss 0.0000, time 2275.41ms\n",
            "iter 225: loss 0.0000, time 2344.44ms\n",
            "iter 226: loss 0.0000, time 2339.86ms\n",
            "iter 227: loss 0.0001, time 2356.64ms\n",
            "iter 228: loss 0.0000, time 2300.97ms\n",
            "iter 229: loss 0.0000, time 2765.63ms\n",
            "iter 230: loss 0.0018, time 2323.32ms\n",
            "iter 231: loss 0.0000, time 2333.84ms\n",
            "iter 232: loss 0.0000, time 2352.99ms\n",
            "iter 233: loss 0.0000, time 2474.06ms\n",
            "iter 234: loss 0.0000, time 2366.82ms\n",
            "iter 235: loss 0.0000, time 2327.72ms\n",
            "iter 236: loss 0.0000, time 2373.45ms\n",
            "iter 237: loss 0.0000, time 2332.85ms\n",
            "iter 238: loss 0.0000, time 2380.39ms\n",
            "iter 239: loss 4.0625, time 2378.48ms\n",
            "step 240: train loss 0.1995, val loss 0.5554\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 36357.73ms\n",
            "iter 241: loss 0.0000, time 2366.96ms\n",
            "iter 242: loss 0.0029, time 2245.59ms\n",
            "iter 243: loss 0.0000, time 2341.19ms\n",
            "iter 244: loss 0.0000, time 2498.75ms\n",
            "iter 245: loss 0.0002, time 2693.18ms\n",
            "iter 246: loss 0.0000, time 2325.31ms\n",
            "iter 247: loss 0.0000, time 2310.45ms\n",
            "iter 248: loss 0.0002, time 2329.75ms\n",
            "iter 249: loss 0.0000, time 2365.89ms\n",
            "iter 250: loss 0.0000, time 2273.63ms\n",
            "iter 251: loss 0.0003, time 2327.97ms\n",
            "iter 252: loss 0.0001, time 2387.71ms\n",
            "iter 253: loss 0.0000, time 2380.64ms\n",
            "iter 254: loss 0.0000, time 2309.23ms\n",
            "iter 255: loss 0.0000, time 2320.89ms\n",
            "iter 256: loss 0.0000, time 2321.69ms\n",
            "iter 257: loss 0.0033, time 2675.26ms\n",
            "iter 258: loss 0.0000, time 2285.78ms\n",
            "iter 259: loss 0.0003, time 2265.57ms\n",
            "step 260: train loss 0.1856, val loss 0.5645\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0002, time 35705.77ms\n",
            "iter 261: loss 0.0000, time 2330.04ms\n",
            "iter 262: loss 0.0001, time 2360.77ms\n",
            "iter 263: loss 0.0000, time 2782.42ms\n",
            "iter 264: loss 0.0001, time 2338.38ms\n",
            "iter 265: loss 0.0001, time 2290.69ms\n",
            "iter 266: loss 0.0001, time 2338.92ms\n",
            "iter 267: loss 0.0000, time 2401.74ms\n",
            "iter 268: loss 0.0000, time 2405.56ms\n",
            "iter 269: loss 0.0000, time 2353.13ms\n",
            "iter 270: loss 0.0000, time 2305.41ms\n",
            "iter 271: loss 0.0017, time 2350.76ms\n",
            "iter 272: loss 0.1226, time 2427.24ms\n",
            "iter 273: loss 0.0000, time 2353.17ms\n",
            "iter 274: loss 0.0000, time 2401.98ms\n",
            "iter 275: loss 0.0000, time 2408.31ms\n",
            "iter 276: loss 0.0000, time 2838.74ms\n",
            "iter 277: loss 0.0000, time 2405.02ms\n",
            "iter 278: loss 0.0000, time 2376.76ms\n",
            "iter 279: loss 0.0000, time 2324.80ms\n",
            "step 280: train loss 0.1730, val loss 0.5430\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 36007.43ms\n",
            "iter 281: loss 0.0003, time 2332.44ms\n",
            "iter 282: loss 0.0000, time 2779.41ms\n",
            "iter 283: loss 0.0000, time 2299.94ms\n",
            "iter 284: loss 0.0000, time 2381.22ms\n",
            "iter 285: loss 0.0000, time 2367.37ms\n",
            "iter 286: loss 0.0000, time 2316.24ms\n",
            "iter 287: loss 0.0000, time 2334.33ms\n",
            "iter 288: loss 0.0000, time 2310.57ms\n",
            "iter 289: loss 0.0688, time 2309.05ms\n",
            "iter 290: loss 0.0000, time 2270.11ms\n",
            "iter 291: loss 0.0000, time 2353.27ms\n",
            "iter 292: loss 0.0000, time 2303.08ms\n",
            "iter 293: loss 0.0000, time 2398.91ms\n",
            "iter 294: loss 0.0002, time 2323.77ms\n",
            "iter 295: loss 0.0000, time 2770.59ms\n",
            "iter 296: loss 0.0000, time 2406.41ms\n",
            "iter 297: loss 0.0000, time 2290.15ms\n",
            "iter 298: loss 0.0000, time 2390.44ms\n",
            "iter 299: loss 0.0000, time 2322.94ms\n",
            "step 300: train loss 0.1617, val loss 0.5521\n",
            "step val accuracy 0.9773\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.7714\n",
            "Test recall 0.7333\n",
            "Test F1 0.7278\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 4  6  0]\n",
            " [ 0  4  6]]\n",
            "iter 300: loss 0.0002, time 36907.53ms\n",
            "iter 301: loss 0.0000, time 2700.19ms\n",
            "iter 302: loss 0.0000, time 2449.41ms\n",
            "iter 303: loss 0.0000, time 2369.52ms\n",
            "iter 304: loss 0.0000, time 2374.96ms\n",
            "iter 305: loss 0.0000, time 2318.41ms\n",
            "iter 306: loss 0.0000, time 2322.39ms\n",
            "iter 307: loss 0.0000, time 2301.66ms\n",
            "iter 308: loss 0.0173, time 2358.20ms\n",
            "iter 309: loss 0.0000, time 2376.22ms\n",
            "iter 310: loss 0.0000, time 2360.21ms\n",
            "iter 311: loss 0.0000, time 2399.41ms\n",
            "iter 312: loss 0.0000, time 2418.86ms\n",
            "iter 313: loss 0.0000, time 2709.25ms\n",
            "iter 314: loss 0.0000, time 2429.46ms\n",
            "iter 315: loss 0.0000, time 2420.48ms\n",
            "iter 316: loss 0.0000, time 2410.86ms\n",
            "iter 317: loss 0.0000, time 2374.11ms\n",
            "iter 318: loss 0.0000, time 2320.53ms\n",
            "iter 319: loss 0.0000, time 2401.53ms\n",
            "step 320: train loss 0.1525, val loss 0.5344\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 37623.50ms\n",
            "iter 321: loss 0.0000, time 2385.61ms\n",
            "iter 322: loss 0.0000, time 2281.10ms\n",
            "iter 323: loss 0.0000, time 2299.91ms\n",
            "iter 324: loss 0.0000, time 2280.17ms\n",
            "iter 325: loss 0.2227, time 2472.80ms\n",
            "iter 326: loss 0.0001, time 2356.54ms\n",
            "iter 327: loss 0.0000, time 2358.35ms\n",
            "iter 328: loss 0.0000, time 2430.67ms\n",
            "iter 329: loss 0.0000, time 2348.43ms\n",
            "iter 330: loss 0.0000, time 2368.73ms\n",
            "iter 331: loss 0.0000, time 2737.63ms\n",
            "iter 332: loss 0.0001, time 2379.93ms\n",
            "iter 333: loss 0.0000, time 2325.95ms\n",
            "iter 334: loss 0.0000, time 2368.92ms\n",
            "iter 335: loss 0.0000, time 2324.65ms\n",
            "iter 336: loss 0.0000, time 2372.52ms\n",
            "iter 337: loss 0.0000, time 2321.22ms\n",
            "iter 338: loss 0.0000, time 2351.96ms\n",
            "iter 339: loss 0.0000, time 2330.85ms\n",
            "step 340: train loss 0.1444, val loss 0.5383\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 36136.91ms\n",
            "iter 341: loss 0.0000, time 2376.22ms\n",
            "iter 342: loss 0.0004, time 2377.39ms\n",
            "iter 343: loss 0.0000, time 2351.50ms\n",
            "iter 344: loss 0.0000, time 2353.51ms\n",
            "iter 345: loss 0.0000, time 2401.59ms\n",
            "iter 346: loss 0.0021, time 2411.02ms\n",
            "iter 347: loss 0.0000, time 2385.66ms\n",
            "iter 348: loss 0.0000, time 2342.05ms\n",
            "iter 349: loss 0.0000, time 2830.07ms\n",
            "iter 350: loss 0.0000, time 2346.06ms\n",
            "iter 351: loss 0.0002, time 2364.35ms\n",
            "iter 352: loss 0.0000, time 2405.57ms\n",
            "iter 353: loss 0.0000, time 2407.51ms\n",
            "iter 354: loss 0.0000, time 2340.74ms\n",
            "iter 355: loss 0.0000, time 2390.61ms\n",
            "iter 356: loss 0.0000, time 2540.03ms\n",
            "iter 357: loss 0.0000, time 2380.20ms\n",
            "iter 358: loss 0.0000, time 2382.84ms\n",
            "iter 359: loss 0.0000, time 2364.81ms\n",
            "step 360: train loss 0.1366, val loss 0.5250\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 36827.74ms\n",
            "iter 361: loss 0.0000, time 2308.22ms\n",
            "iter 362: loss 0.0266, time 2344.99ms\n",
            "iter 363: loss 0.0000, time 2401.59ms\n",
            "iter 364: loss 0.0000, time 2328.04ms\n",
            "iter 365: loss 0.0000, time 2323.16ms\n",
            "iter 366: loss 0.0000, time 2329.32ms\n",
            "iter 367: loss 0.0000, time 2309.91ms\n",
            "iter 368: loss 0.0000, time 2836.90ms\n",
            "iter 369: loss 0.0000, time 2353.18ms\n",
            "iter 370: loss 0.0000, time 2382.18ms\n",
            "iter 371: loss 0.0000, time 2576.02ms\n",
            "iter 372: loss 0.0000, time 2569.72ms\n",
            "iter 373: loss 0.0000, time 2278.31ms\n",
            "iter 374: loss 0.0000, time 2398.94ms\n",
            "iter 375: loss 0.0145, time 2362.46ms\n",
            "iter 376: loss 0.0001, time 2678.09ms\n",
            "iter 377: loss 0.0000, time 2411.54ms\n",
            "iter 378: loss 0.0000, time 2288.74ms\n",
            "iter 379: loss 0.0000, time 2420.95ms\n",
            "step 380: train loss 0.1296, val loss 0.5122\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 36427.21ms\n",
            "iter 381: loss 0.0000, time 2308.75ms\n",
            "iter 382: loss 0.0000, time 2262.83ms\n",
            "iter 383: loss 0.0001, time 2531.73ms\n",
            "iter 384: loss 0.0000, time 2301.21ms\n",
            "iter 385: loss 0.0000, time 2220.83ms\n",
            "iter 386: loss 0.0005, time 2574.73ms\n",
            "iter 387: loss 0.0000, time 2295.37ms\n",
            "iter 388: loss 0.0000, time 2199.16ms\n",
            "iter 389: loss 0.0000, time 2260.37ms\n",
            "iter 390: loss 0.0000, time 2233.04ms\n",
            "iter 391: loss 0.0000, time 2338.50ms\n",
            "iter 392: loss 0.0000, time 2382.75ms\n",
            "iter 393: loss 0.0231, time 2320.51ms\n",
            "iter 394: loss 0.0000, time 2270.14ms\n",
            "iter 395: loss 0.0000, time 2307.35ms\n",
            "iter 396: loss 0.0000, time 2503.92ms\n",
            "iter 397: loss 0.0000, time 2216.15ms\n",
            "iter 398: loss 0.0000, time 2205.42ms\n",
            "iter 399: loss 0.0000, time 2675.08ms\n",
            "step 400: train loss 0.1232, val loss 0.4867\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8364\n",
            "Test recall 0.7667\n",
            "Test F1 0.7479\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 1  9  0]\n",
            " [ 0  6  4]]\n",
            "iter 400: loss 0.0000, time 35509.67ms\n",
            "iter 401: loss 0.0000, time 2204.53ms\n",
            "iter 402: loss 0.0000, time 2252.84ms\n",
            "iter 403: loss 0.0000, time 2422.94ms\n",
            "iter 404: loss 0.0000, time 2377.55ms\n",
            "iter 405: loss 0.0008, time 2638.60ms\n",
            "iter 406: loss 0.0000, time 2183.57ms\n",
            "iter 407: loss 0.0000, time 2393.38ms\n",
            "iter 408: loss 0.0000, time 2283.42ms\n",
            "iter 409: loss 0.0000, time 2254.02ms\n",
            "iter 410: loss 0.0000, time 2191.03ms\n",
            "iter 411: loss 0.0003, time 2241.95ms\n",
            "iter 412: loss 0.0000, time 2278.04ms\n",
            "iter 413: loss 0.0000, time 2209.73ms\n",
            "iter 414: loss 0.0000, time 2216.10ms\n",
            "iter 415: loss 0.0000, time 2290.15ms\n",
            "iter 416: loss 0.0000, time 2415.15ms\n",
            "iter 417: loss 0.0000, time 2632.91ms\n",
            "iter 418: loss 0.0012, time 2243.84ms\n",
            "iter 419: loss 0.0000, time 2242.76ms\n",
            "step 420: train loss 0.1173, val loss 0.4888\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 35251.50ms\n",
            "iter 421: loss 0.0000, time 2230.54ms\n",
            "iter 422: loss 0.0000, time 2348.04ms\n",
            "iter 423: loss 0.0000, time 2296.75ms\n",
            "iter 424: loss 0.0000, time 2704.18ms\n",
            "iter 425: loss 0.0000, time 2216.36ms\n",
            "iter 426: loss 0.0000, time 2238.40ms\n",
            "iter 427: loss 0.0000, time 2389.67ms\n",
            "iter 428: loss 0.0000, time 2436.59ms\n",
            "iter 429: loss 0.0000, time 2276.22ms\n",
            "iter 430: loss 0.0000, time 2276.18ms\n",
            "iter 431: loss 0.0000, time 2256.57ms\n",
            "iter 432: loss 0.0000, time 2400.82ms\n",
            "iter 433: loss 0.0000, time 2322.23ms\n",
            "iter 434: loss 0.0000, time 2261.99ms\n",
            "iter 435: loss 0.0000, time 2262.92ms\n",
            "iter 436: loss 0.0000, time 2253.90ms\n",
            "iter 437: loss 0.0000, time 2574.69ms\n",
            "iter 438: loss 0.0000, time 2284.16ms\n",
            "iter 439: loss 0.0000, time 2279.56ms\n",
            "step 440: train loss 0.1120, val loss 0.4968\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 34955.33ms\n",
            "iter 441: loss 0.0000, time 2263.06ms\n",
            "iter 442: loss 0.0000, time 2285.01ms\n",
            "iter 443: loss 0.0000, time 2619.03ms\n",
            "iter 444: loss 0.0000, time 2367.54ms\n",
            "iter 445: loss 0.0000, time 2298.39ms\n",
            "iter 446: loss 0.0000, time 2302.73ms\n",
            "iter 447: loss 0.0000, time 2232.23ms\n",
            "iter 448: loss 0.0000, time 2217.00ms\n",
            "iter 449: loss 0.0000, time 2201.67ms\n",
            "iter 450: loss 0.0000, time 2245.38ms\n",
            "iter 451: loss 0.0000, time 2220.59ms\n",
            "iter 452: loss 0.0000, time 2308.28ms\n",
            "iter 453: loss 0.0000, time 2435.47ms\n",
            "iter 454: loss 0.0000, time 2301.96ms\n",
            "iter 455: loss 0.0000, time 2213.88ms\n",
            "iter 456: loss 0.0000, time 2616.82ms\n",
            "iter 457: loss 0.0000, time 2434.16ms\n",
            "iter 458: loss 0.0000, time 2203.67ms\n",
            "iter 459: loss 0.0000, time 2183.93ms\n",
            "step 460: train loss 0.1074, val loss 0.4996\n",
            "step val accuracy 0.9892\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 34895.23ms\n",
            "iter 461: loss 0.0000, time 2274.39ms\n",
            "iter 462: loss 0.0000, time 2582.70ms\n",
            "iter 463: loss 0.0000, time 2263.70ms\n",
            "iter 464: loss 0.0000, time 2461.32ms\n",
            "iter 465: loss 0.0000, time 2331.22ms\n",
            "iter 466: loss 0.0000, time 2193.95ms\n",
            "iter 467: loss 0.0000, time 2318.88ms\n",
            "iter 468: loss 0.0000, time 2406.02ms\n",
            "iter 469: loss 0.0000, time 2571.38ms\n",
            "iter 470: loss 0.0000, time 2371.79ms\n",
            "iter 471: loss 0.0000, time 2282.27ms\n",
            "iter 472: loss 0.0000, time 2253.09ms\n",
            "iter 473: loss 0.0000, time 2345.08ms\n",
            "iter 474: loss 0.0000, time 2617.91ms\n",
            "iter 475: loss 0.0000, time 2242.22ms\n",
            "iter 476: loss 1.7891, time 2235.65ms\n",
            "iter 477: loss 0.0000, time 2362.24ms\n",
            "iter 478: loss 0.0001, time 2339.97ms\n",
            "iter 479: loss 0.0000, time 2251.88ms\n",
            "step 480: train loss 0.1040, val loss 0.4789\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 34661.39ms\n",
            "iter 481: loss 0.0000, time 2715.27ms\n",
            "iter 482: loss 0.0000, time 2308.08ms\n",
            "iter 483: loss 0.0000, time 2215.29ms\n",
            "iter 484: loss 0.0000, time 2388.94ms\n",
            "iter 485: loss 0.0000, time 2348.67ms\n",
            "iter 486: loss 0.0000, time 2261.65ms\n",
            "iter 487: loss 0.0000, time 2281.43ms\n",
            "iter 488: loss 0.0000, time 2204.44ms\n",
            "iter 489: loss 0.0000, time 2188.71ms\n",
            "iter 490: loss 0.0000, time 2205.11ms\n",
            "iter 491: loss 0.0000, time 2299.06ms\n",
            "iter 492: loss 0.0000, time 2240.14ms\n",
            "iter 493: loss 0.0000, time 2722.52ms\n",
            "iter 494: loss 0.0000, time 2363.10ms\n",
            "iter 495: loss 0.0001, time 2210.42ms\n",
            "iter 496: loss 0.0000, time 2273.03ms\n",
            "iter 497: loss 0.0000, time 2397.97ms\n",
            "iter 498: loss 0.0000, time 2467.05ms\n",
            "iter 499: loss 0.0000, time 2265.18ms\n",
            "step 500: train loss 0.0999, val loss 0.4702\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.7714\n",
            "Test recall 0.7667\n",
            "Test F1 0.7678\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 7 2]\n",
            " [0 3 7]]\n",
            "iter 500: loss 0.0000, time 36272.07ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▆▆▇██</td></tr><tr><td>Test_F1_Score</td><td>▁▅▇███</td></tr><tr><td>Test_Precision</td><td>▁▅▇▇█▇</td></tr><tr><td>Test_Recall</td><td>▁▆▆▇██</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▅▇██████████████████████</td></tr><tr><td>val/loss</td><td> ▅█▆▅▃▃▃▃▂▂▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.76667</td></tr><tr><td>Test_F1_Score</td><td>0.76784</td></tr><tr><td>Test_Precision</td><td>0.77138</td></tr><tr><td>Test_Recall</td><td>0.76667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.09994</td></tr><tr><td>val/acc</td><td>0.98596</td></tr><tr><td>val/loss</td><td>0.47024</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pretty-sweep-8</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/o70d8jq5' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/o70d8jq5</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_030311-o70d8jq5\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bmsnzozl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_033803-bmsnzozl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/bmsnzozl' target=\"_blank\">youthful-sweep-9</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/bmsnzozl' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/bmsnzozl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3779\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.5977, time 29124.96ms\n",
            "iter 1: loss 0.9297, time 1629.87ms\n",
            "iter 2: loss 0.5898, time 1635.85ms\n",
            "iter 3: loss 1.4531, time 1628.27ms\n",
            "iter 4: loss 1.2266, time 1611.12ms\n",
            "iter 5: loss 1.2656, time 1648.59ms\n",
            "iter 6: loss 0.7305, time 1678.26ms\n",
            "iter 7: loss 0.6641, time 1669.25ms\n",
            "iter 8: loss 2.1563, time 1594.15ms\n",
            "iter 9: loss 0.9219, time 1625.88ms\n",
            "iter 10: loss 1.4453, time 1575.07ms\n",
            "iter 11: loss 0.7695, time 1551.54ms\n",
            "iter 12: loss 0.5937, time 1651.35ms\n",
            "iter 13: loss 1.0312, time 1698.64ms\n",
            "iter 14: loss 1.1797, time 1589.65ms\n",
            "iter 15: loss 0.5820, time 1579.22ms\n",
            "iter 16: loss 0.2656, time 1576.15ms\n",
            "iter 17: loss 0.3477, time 1591.15ms\n",
            "iter 18: loss 1.5781, time 1624.97ms\n",
            "iter 19: loss 0.3066, time 1585.66ms\n",
            "step 20: train loss 1.2065, val loss 1.1001\n",
            "step val accuracy 0.6494\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.3301, time 15426.91ms\n",
            "iter 21: loss 0.4805, time 1608.93ms\n",
            "iter 22: loss 1.6094, time 1631.64ms\n",
            "iter 23: loss 0.9102, time 1676.05ms\n",
            "iter 24: loss 0.6406, time 1634.81ms\n",
            "iter 25: loss 0.7227, time 1614.42ms\n",
            "iter 26: loss 0.9258, time 1601.56ms\n",
            "iter 27: loss 0.1172, time 1557.75ms\n",
            "iter 28: loss 0.6211, time 1573.61ms\n",
            "iter 29: loss 0.0527, time 1544.25ms\n",
            "iter 30: loss 0.7109, time 1524.83ms\n",
            "iter 31: loss 0.1387, time 1530.53ms\n",
            "iter 32: loss 0.1758, time 1587.78ms\n",
            "iter 33: loss 0.1060, time 1549.94ms\n",
            "iter 34: loss 0.3164, time 1565.89ms\n",
            "iter 35: loss 0.3477, time 1638.90ms\n",
            "iter 36: loss 1.1328, time 1656.94ms\n",
            "iter 37: loss 0.0669, time 1573.52ms\n",
            "iter 38: loss 0.3242, time 1531.45ms\n",
            "iter 39: loss 0.8164, time 1544.24ms\n",
            "step 40: train loss 0.8379, val loss 0.7859\n",
            "step val accuracy 0.8654\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0194, time 14968.59ms\n",
            "iter 41: loss 0.0267, time 1558.46ms\n",
            "iter 42: loss 1.7891, time 1547.23ms\n",
            "iter 43: loss 2.0156, time 1566.48ms\n",
            "iter 44: loss 0.0024, time 1550.90ms\n",
            "iter 45: loss 0.0120, time 1560.95ms\n",
            "iter 46: loss 0.0325, time 1643.76ms\n",
            "iter 47: loss 0.2100, time 1629.18ms\n",
            "iter 48: loss 0.3672, time 1563.51ms\n",
            "iter 49: loss 0.0854, time 1550.88ms\n",
            "iter 50: loss 0.0280, time 1594.66ms\n",
            "iter 51: loss 0.2363, time 1555.52ms\n",
            "iter 52: loss 0.3281, time 1647.27ms\n",
            "iter 53: loss 0.1011, time 1640.63ms\n",
            "iter 54: loss 0.1992, time 1539.57ms\n",
            "iter 55: loss 0.0017, time 1589.03ms\n",
            "iter 56: loss 0.0005, time 1541.18ms\n",
            "iter 57: loss 0.0010, time 1572.98ms\n",
            "iter 58: loss 0.0060, time 1615.79ms\n",
            "iter 59: loss 0.0051, time 1553.34ms\n",
            "step 60: train loss 0.6141, val loss 0.6181\n",
            "step val accuracy 0.9364\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0005, time 15249.81ms\n",
            "iter 61: loss 0.0242, time 1560.94ms\n",
            "iter 62: loss 0.0006, time 1605.85ms\n",
            "iter 63: loss 0.0008, time 1660.03ms\n",
            "iter 64: loss 0.0010, time 1592.34ms\n",
            "iter 65: loss 1.1172, time 1552.50ms\n",
            "iter 66: loss 0.0253, time 1563.07ms\n",
            "iter 67: loss 0.0022, time 1576.95ms\n",
            "iter 68: loss 0.0003, time 1544.00ms\n",
            "iter 69: loss 0.0002, time 1567.12ms\n",
            "iter 70: loss 0.0002, time 1555.36ms\n",
            "iter 71: loss 0.0023, time 1543.94ms\n",
            "iter 72: loss 0.0013, time 1553.96ms\n",
            "iter 73: loss 0.0118, time 1578.12ms\n",
            "iter 74: loss 0.0007, time 1616.70ms\n",
            "iter 75: loss 0.0042, time 1624.53ms\n",
            "iter 76: loss 0.0383, time 1665.88ms\n",
            "iter 77: loss 0.0072, time 1579.21ms\n",
            "iter 78: loss 0.0520, time 1573.52ms\n",
            "iter 79: loss 0.0048, time 1583.04ms\n",
            "step 80: train loss 0.4883, val loss 0.5811\n",
            "step val accuracy 0.9527\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0039, time 15065.87ms\n",
            "iter 81: loss 1.3984, time 1573.62ms\n",
            "iter 82: loss 0.0038, time 1578.39ms\n",
            "iter 83: loss 0.0019, time 1545.60ms\n",
            "iter 84: loss 0.0004, time 1554.93ms\n",
            "iter 85: loss 0.0012, time 1656.37ms\n",
            "iter 86: loss 0.0002, time 1628.04ms\n",
            "iter 87: loss 0.0002, time 1582.80ms\n",
            "iter 88: loss 0.0037, time 1580.63ms\n",
            "iter 89: loss 0.0013, time 1548.75ms\n",
            "iter 90: loss 0.0009, time 1537.68ms\n",
            "iter 91: loss 0.0022, time 1588.09ms\n",
            "iter 92: loss 0.0003, time 1653.59ms\n",
            "iter 93: loss 0.0014, time 1615.02ms\n",
            "iter 94: loss 0.0011, time 1557.95ms\n",
            "iter 95: loss 0.0026, time 1549.20ms\n",
            "iter 96: loss 0.0552, time 1548.07ms\n",
            "iter 97: loss 0.0006, time 1577.95ms\n",
            "iter 98: loss 0.0054, time 1576.58ms\n",
            "iter 99: loss 0.0001, time 1551.43ms\n",
            "step 100: train loss 0.4019, val loss 0.5449\n",
            "step val accuracy 0.9615\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.7389\n",
            "Test recall 0.6333\n",
            "Test F1 0.6132\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [3 7 0]\n",
            " [0 7 3]]\n",
            "iter 100: loss 0.0092, time 16252.13ms\n",
            "iter 101: loss 0.0064, time 1646.38ms\n",
            "iter 102: loss 0.0000, time 1692.10ms\n",
            "iter 103: loss 0.0002, time 1800.53ms\n",
            "iter 104: loss 0.0120, time 1574.87ms\n",
            "iter 105: loss 0.0006, time 1568.21ms\n",
            "iter 106: loss 0.0003, time 1578.97ms\n",
            "iter 107: loss 0.0001, time 1621.35ms\n",
            "iter 108: loss 0.0006, time 1662.13ms\n",
            "iter 109: loss 0.0011, time 1645.45ms\n",
            "iter 110: loss 0.0013, time 1567.60ms\n",
            "iter 111: loss 0.0002, time 1559.53ms\n",
            "iter 112: loss 0.0000, time 1562.00ms\n",
            "iter 113: loss 0.0000, time 1630.55ms\n",
            "iter 114: loss 0.0001, time 1558.71ms\n",
            "iter 115: loss 0.0003, time 1559.91ms\n",
            "iter 116: loss 0.0004, time 1589.83ms\n",
            "iter 117: loss 0.0127, time 1580.68ms\n",
            "iter 118: loss 0.0003, time 1534.81ms\n",
            "iter 119: loss 0.2002, time 1579.10ms\n",
            "step 120: train loss 0.3435, val loss 0.5159\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0011, time 15418.54ms\n",
            "iter 121: loss 0.0043, time 1549.06ms\n",
            "iter 122: loss 0.0000, time 1548.47ms\n",
            "iter 123: loss 0.0006, time 1583.08ms\n",
            "iter 124: loss 0.0011, time 1571.89ms\n",
            "iter 125: loss 0.0000, time 1578.33ms\n",
            "iter 126: loss 0.0000, time 1547.97ms\n",
            "iter 127: loss 0.0003, time 1579.83ms\n",
            "iter 128: loss 0.0000, time 1559.85ms\n",
            "iter 129: loss 0.0007, time 1542.07ms\n",
            "iter 130: loss 0.0006, time 1588.55ms\n",
            "iter 131: loss 0.0001, time 1659.80ms\n",
            "iter 132: loss 0.0001, time 1647.28ms\n",
            "iter 133: loss 0.0009, time 1523.89ms\n",
            "iter 134: loss 0.0006, time 1651.42ms\n",
            "iter 135: loss 0.0013, time 1568.10ms\n",
            "iter 136: loss 0.0002, time 1583.47ms\n",
            "iter 137: loss 0.0002, time 1693.44ms\n",
            "iter 138: loss 0.0002, time 1629.29ms\n",
            "iter 139: loss 0.0081, time 1536.92ms\n",
            "step 140: train loss 0.2985, val loss 0.5005\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0001, time 14994.71ms\n",
            "iter 141: loss 0.0000, time 1674.28ms\n",
            "iter 142: loss 0.0007, time 1688.62ms\n",
            "iter 143: loss 0.0000, time 1610.98ms\n",
            "iter 144: loss 0.0004, time 1553.83ms\n",
            "iter 145: loss 0.0027, time 1529.61ms\n",
            "iter 146: loss 0.0004, time 1582.67ms\n",
            "iter 147: loss 0.0000, time 1634.61ms\n",
            "iter 148: loss 0.0000, time 1669.71ms\n",
            "iter 149: loss 0.0002, time 1602.82ms\n",
            "iter 150: loss 0.0001, time 1601.21ms\n",
            "iter 151: loss 0.0001, time 1561.91ms\n",
            "iter 152: loss 0.0192, time 1552.00ms\n",
            "iter 153: loss 0.0003, time 1597.46ms\n",
            "iter 154: loss 0.0002, time 1572.85ms\n",
            "iter 155: loss 0.0000, time 1578.34ms\n",
            "iter 156: loss 0.0000, time 1563.54ms\n",
            "iter 157: loss 0.0001, time 1565.27ms\n",
            "iter 158: loss 0.0001, time 1557.62ms\n",
            "iter 159: loss 0.0000, time 1554.42ms\n",
            "step 160: train loss 0.2635, val loss 0.5147\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0001, time 15489.92ms\n",
            "iter 161: loss 0.0001, time 1580.13ms\n",
            "iter 162: loss 0.0001, time 1572.33ms\n",
            "iter 163: loss 0.0000, time 1580.43ms\n",
            "iter 164: loss 0.0001, time 1542.11ms\n",
            "iter 165: loss 0.0001, time 1584.81ms\n",
            "iter 166: loss 0.0000, time 1553.81ms\n",
            "iter 167: loss 0.0003, time 1534.83ms\n",
            "iter 168: loss 0.0005, time 1548.91ms\n",
            "iter 169: loss 0.0006, time 1551.34ms\n",
            "iter 170: loss 0.0003, time 1613.25ms\n",
            "iter 171: loss 0.0001, time 1634.31ms\n",
            "iter 172: loss 0.0053, time 1585.66ms\n",
            "iter 173: loss 0.0003, time 1565.14ms\n",
            "iter 174: loss 0.0000, time 1616.41ms\n",
            "iter 175: loss 0.0000, time 1538.06ms\n",
            "iter 176: loss 0.0000, time 1601.07ms\n",
            "iter 177: loss 0.0000, time 1663.28ms\n",
            "iter 178: loss 0.0000, time 1596.13ms\n",
            "iter 179: loss 0.0001, time 1565.34ms\n",
            "step 180: train loss 0.2356, val loss 0.5597\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 14924.05ms\n",
            "iter 181: loss 0.0002, time 1693.39ms\n",
            "iter 182: loss 0.0000, time 1646.67ms\n",
            "iter 183: loss 0.0000, time 1579.93ms\n",
            "iter 184: loss 0.0000, time 1575.41ms\n",
            "iter 185: loss 0.0000, time 1583.13ms\n",
            "iter 186: loss 0.0000, time 1580.16ms\n",
            "iter 187: loss 0.0000, time 1685.88ms\n",
            "iter 188: loss 0.0000, time 1667.97ms\n",
            "iter 189: loss 0.0000, time 1565.12ms\n",
            "iter 190: loss 0.0001, time 1581.29ms\n",
            "iter 191: loss 0.0000, time 1587.95ms\n",
            "iter 192: loss 0.0000, time 1581.38ms\n",
            "iter 193: loss 0.0000, time 1589.75ms\n",
            "iter 194: loss 0.0000, time 1552.42ms\n",
            "iter 195: loss 0.0000, time 1570.31ms\n",
            "iter 196: loss 0.0000, time 1595.96ms\n",
            "iter 197: loss 0.0000, time 1547.11ms\n",
            "iter 198: loss 0.0010, time 1581.97ms\n",
            "iter 199: loss 0.0273, time 1621.38ms\n",
            "step 200: train loss 0.2126, val loss 0.5910\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.8208\n",
            "Test recall 0.7333\n",
            "Test F1 0.7212\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 6 4]]\n",
            "iter 200: loss 0.0001, time 16401.43ms\n",
            "iter 201: loss 0.0022, time 1590.31ms\n",
            "iter 202: loss 0.0001, time 1617.44ms\n",
            "iter 203: loss 0.0393, time 1563.11ms\n",
            "iter 204: loss 0.0000, time 1580.51ms\n",
            "iter 205: loss 0.0000, time 1573.95ms\n",
            "iter 206: loss 0.0000, time 1543.32ms\n",
            "iter 207: loss 0.0001, time 1565.62ms\n",
            "iter 208: loss 0.0000, time 1569.51ms\n",
            "iter 209: loss 0.0000, time 1604.62ms\n",
            "iter 210: loss 0.0002, time 1642.93ms\n",
            "iter 211: loss 0.0001, time 1576.46ms\n",
            "iter 212: loss 0.0003, time 1611.85ms\n",
            "iter 213: loss 0.0000, time 1722.40ms\n",
            "iter 214: loss 0.0000, time 1797.41ms\n",
            "iter 215: loss 0.0000, time 1617.62ms\n",
            "iter 216: loss 0.0000, time 1669.84ms\n",
            "iter 217: loss 0.0000, time 1580.72ms\n",
            "iter 218: loss 0.0000, time 1564.92ms\n",
            "iter 219: loss 0.0000, time 1554.76ms\n",
            "step 220: train loss 0.1940, val loss 0.6062\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 15334.82ms\n",
            "iter 221: loss 0.0000, time 1597.27ms\n",
            "iter 222: loss 0.0000, time 1608.16ms\n",
            "iter 223: loss 0.0001, time 1553.46ms\n",
            "iter 224: loss 0.0002, time 1587.04ms\n",
            "iter 225: loss 0.0000, time 1650.83ms\n",
            "iter 226: loss 0.0005, time 1676.12ms\n",
            "iter 227: loss 0.0000, time 1589.69ms\n",
            "iter 228: loss 0.0003, time 1564.49ms\n",
            "iter 229: loss 0.0000, time 1580.76ms\n",
            "iter 230: loss 0.0003, time 1564.06ms\n",
            "iter 231: loss 0.0000, time 1556.47ms\n",
            "iter 232: loss 0.0000, time 1566.37ms\n",
            "iter 233: loss 0.0000, time 1563.71ms\n",
            "iter 234: loss 0.0000, time 1567.83ms\n",
            "iter 235: loss 0.0002, time 1570.41ms\n",
            "iter 236: loss 0.0000, time 1559.24ms\n",
            "iter 237: loss 0.0000, time 1556.66ms\n",
            "iter 238: loss 0.0000, time 1650.07ms\n",
            "iter 239: loss 0.0000, time 1689.98ms\n",
            "step 240: train loss 0.1789, val loss 0.6314\n",
            "step val accuracy 0.9704\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 15193.12ms\n",
            "iter 241: loss 0.0000, time 1605.60ms\n",
            "iter 242: loss 0.0000, time 1554.52ms\n",
            "iter 243: loss 0.0000, time 1581.73ms\n",
            "iter 244: loss 0.0000, time 1584.94ms\n",
            "iter 245: loss 0.0000, time 1546.46ms\n",
            "iter 246: loss 0.0000, time 1543.34ms\n",
            "iter 247: loss 0.0002, time 1564.56ms\n",
            "iter 248: loss 0.0004, time 1598.73ms\n",
            "iter 249: loss 0.0000, time 1643.97ms\n",
            "iter 250: loss 0.0001, time 1625.09ms\n",
            "iter 251: loss 0.0001, time 1566.27ms\n",
            "iter 252: loss 0.0000, time 1530.26ms\n",
            "iter 253: loss 0.0000, time 1572.52ms\n",
            "iter 254: loss 0.0000, time 1557.60ms\n",
            "iter 255: loss 0.0000, time 1751.24ms\n",
            "iter 256: loss 0.0013, time 1638.27ms\n",
            "iter 257: loss 0.0000, time 1554.83ms\n",
            "iter 258: loss 0.0000, time 1554.70ms\n",
            "iter 259: loss 0.0000, time 1559.23ms\n",
            "step 260: train loss 0.1653, val loss 0.6134\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 15369.59ms\n",
            "iter 261: loss 0.0001, time 1537.47ms\n",
            "iter 262: loss 0.0043, time 1557.87ms\n",
            "iter 263: loss 0.0000, time 1623.68ms\n",
            "iter 264: loss 0.0000, time 1554.76ms\n",
            "iter 265: loss 0.0000, time 1634.87ms\n",
            "iter 266: loss 0.0001, time 1661.81ms\n",
            "iter 267: loss 0.0000, time 1577.99ms\n",
            "iter 268: loss 0.0000, time 1547.74ms\n",
            "iter 269: loss 0.0001, time 1534.56ms\n",
            "iter 270: loss 0.0000, time 1580.13ms\n",
            "iter 271: loss 0.0000, time 1542.82ms\n",
            "iter 272: loss 0.0000, time 1553.04ms\n",
            "iter 273: loss 0.0000, time 1577.06ms\n",
            "iter 274: loss 0.0000, time 1600.55ms\n",
            "iter 275: loss 0.0000, time 1557.69ms\n",
            "iter 276: loss 5.7812, time 1547.70ms\n",
            "iter 277: loss 0.0000, time 1556.53ms\n",
            "iter 278: loss 0.0000, time 1669.70ms\n",
            "iter 279: loss 0.0000, time 1650.75ms\n",
            "step 280: train loss 0.1547, val loss 0.5827\n",
            "step val accuracy 0.9808\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 15158.76ms\n",
            "iter 281: loss 0.0000, time 1573.70ms\n",
            "iter 282: loss 0.0000, time 1547.30ms\n",
            "iter 283: loss 0.0000, time 1521.69ms\n",
            "iter 284: loss 0.0000, time 1555.11ms\n",
            "iter 285: loss 0.0000, time 1549.87ms\n",
            "iter 286: loss 0.0000, time 1543.92ms\n",
            "iter 287: loss 0.0000, time 1545.23ms\n",
            "iter 288: loss 0.3926, time 1599.13ms\n",
            "iter 289: loss 0.0032, time 1645.65ms\n",
            "iter 290: loss 0.0000, time 1599.37ms\n",
            "iter 291: loss 0.0000, time 1551.43ms\n",
            "iter 292: loss 0.0003, time 1580.47ms\n",
            "iter 293: loss 0.0001, time 1585.18ms\n",
            "iter 294: loss 0.0000, time 1574.20ms\n",
            "iter 295: loss 0.0000, time 1655.39ms\n",
            "iter 296: loss 0.0000, time 1627.07ms\n",
            "iter 297: loss 0.0000, time 1532.18ms\n",
            "iter 298: loss 0.0000, time 1542.37ms\n",
            "iter 299: loss 0.0000, time 1587.45ms\n",
            "step 300: train loss 0.1450, val loss 0.5795\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.8519\n",
            "Test recall 0.7333\n",
            "Test F1 0.7249\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 8  2  0]\n",
            " [ 0 10  0]\n",
            " [ 0  6  4]]\n",
            "iter 300: loss 0.0000, time 16306.78ms\n",
            "iter 301: loss 0.0001, time 1606.87ms\n",
            "iter 302: loss 0.0000, time 1585.65ms\n",
            "iter 303: loss 0.0000, time 1543.88ms\n",
            "iter 304: loss 0.0001, time 1674.90ms\n",
            "iter 305: loss 0.0000, time 1669.07ms\n",
            "iter 306: loss 0.0001, time 1585.51ms\n",
            "iter 307: loss 0.0000, time 1546.61ms\n",
            "iter 308: loss 0.0006, time 1545.26ms\n",
            "iter 309: loss 0.0000, time 1585.34ms\n",
            "iter 310: loss 0.0000, time 1569.33ms\n",
            "iter 311: loss 0.0000, time 1532.80ms\n",
            "iter 312: loss 0.0000, time 1565.99ms\n",
            "iter 313: loss 0.0000, time 1576.84ms\n",
            "iter 314: loss 0.0000, time 1514.88ms\n",
            "iter 315: loss 0.0000, time 1556.51ms\n",
            "iter 316: loss 0.0000, time 1573.15ms\n",
            "iter 317: loss 0.0000, time 1639.87ms\n",
            "iter 318: loss 0.0000, time 1642.55ms\n",
            "iter 319: loss 0.0000, time 1584.01ms\n",
            "step 320: train loss 0.1360, val loss 0.5766\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 15217.20ms\n",
            "iter 321: loss 0.0005, time 1552.67ms\n",
            "iter 322: loss 0.0000, time 1550.88ms\n",
            "iter 323: loss 0.0000, time 1542.44ms\n",
            "iter 324: loss 0.0001, time 1581.34ms\n",
            "iter 325: loss 0.0000, time 1559.91ms\n",
            "iter 326: loss 0.0000, time 1531.11ms\n",
            "iter 327: loss 0.0000, time 1614.76ms\n",
            "iter 328: loss 0.0000, time 1663.78ms\n",
            "iter 329: loss 0.0000, time 1605.80ms\n",
            "iter 330: loss 0.0002, time 1559.04ms\n",
            "iter 331: loss 0.0003, time 1536.36ms\n",
            "iter 332: loss 0.0000, time 1581.48ms\n",
            "iter 333: loss 0.0000, time 1609.45ms\n",
            "iter 334: loss 0.0000, time 1641.86ms\n",
            "iter 335: loss 0.0000, time 1640.81ms\n",
            "iter 336: loss 0.0000, time 1556.84ms\n",
            "iter 337: loss 0.0000, time 1532.90ms\n",
            "iter 338: loss 0.0000, time 1601.71ms\n",
            "iter 339: loss 0.0000, time 1570.08ms\n",
            "step 340: train loss 0.1280, val loss 0.5904\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0001, time 15093.06ms\n",
            "iter 341: loss 0.0000, time 1535.14ms\n",
            "iter 342: loss 0.0000, time 1589.85ms\n",
            "iter 343: loss 0.0000, time 1570.16ms\n",
            "iter 344: loss 0.0001, time 1627.40ms\n",
            "iter 345: loss 0.0000, time 1661.13ms\n",
            "iter 346: loss 0.0000, time 1581.43ms\n",
            "iter 347: loss 0.0000, time 1571.27ms\n",
            "iter 348: loss 0.0000, time 1550.28ms\n",
            "iter 349: loss 0.0000, time 1580.36ms\n",
            "iter 350: loss 0.0007, time 1580.75ms\n",
            "iter 351: loss 0.0000, time 1559.12ms\n",
            "iter 352: loss 0.0000, time 1547.49ms\n",
            "iter 353: loss 0.0000, time 1566.43ms\n",
            "iter 354: loss 0.0009, time 1557.19ms\n",
            "iter 355: loss 0.0000, time 1547.05ms\n",
            "iter 356: loss 0.0000, time 1570.60ms\n",
            "iter 357: loss 0.0000, time 1698.98ms\n",
            "iter 358: loss 0.0000, time 1670.39ms\n",
            "iter 359: loss 0.0000, time 1535.65ms\n",
            "step 360: train loss 0.1214, val loss 0.6083\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0001, time 15180.87ms\n",
            "iter 361: loss 0.0000, time 1586.84ms\n",
            "iter 362: loss 0.0000, time 1543.23ms\n",
            "iter 363: loss 0.0002, time 1546.29ms\n",
            "iter 364: loss 0.0000, time 1567.47ms\n",
            "iter 365: loss 0.0000, time 1573.38ms\n",
            "iter 366: loss 0.0000, time 1547.79ms\n",
            "iter 367: loss 0.0000, time 1594.35ms\n",
            "iter 368: loss 0.0000, time 1679.45ms\n",
            "iter 369: loss 0.0000, time 1579.39ms\n",
            "iter 370: loss 0.0000, time 1549.56ms\n",
            "iter 371: loss 0.0000, time 1582.96ms\n",
            "iter 372: loss 0.0000, time 1570.16ms\n",
            "iter 373: loss 0.0000, time 1600.31ms\n",
            "iter 374: loss 0.0000, time 1652.98ms\n",
            "iter 375: loss 0.0000, time 1608.34ms\n",
            "iter 376: loss 0.0000, time 1559.20ms\n",
            "iter 377: loss 0.0000, time 1575.30ms\n",
            "iter 378: loss 0.0000, time 1559.03ms\n",
            "iter 379: loss 0.0000, time 1567.85ms\n",
            "step 380: train loss 0.1159, val loss 0.5854\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 15632.82ms\n",
            "iter 381: loss 0.0000, time 1574.50ms\n",
            "iter 382: loss 0.0002, time 1549.53ms\n",
            "iter 383: loss 0.0058, time 1593.62ms\n",
            "iter 384: loss 0.0000, time 1759.16ms\n",
            "iter 385: loss 0.0000, time 1717.01ms\n",
            "iter 386: loss 0.0000, time 1566.18ms\n",
            "iter 387: loss 0.0009, time 1564.08ms\n",
            "iter 388: loss 0.0000, time 1581.48ms\n",
            "iter 389: loss 0.0000, time 1573.26ms\n",
            "iter 390: loss 0.0000, time 1569.78ms\n",
            "iter 391: loss 0.0000, time 1577.15ms\n",
            "iter 392: loss 0.0000, time 1586.40ms\n",
            "iter 393: loss 0.0001, time 1596.75ms\n",
            "iter 394: loss 0.0000, time 1535.95ms\n",
            "iter 395: loss 0.0000, time 1577.00ms\n",
            "iter 396: loss 0.0000, time 1661.26ms\n",
            "iter 397: loss 0.0000, time 1695.03ms\n",
            "iter 398: loss 0.0000, time 1609.91ms\n",
            "iter 399: loss 0.0042, time 1577.99ms\n",
            "step 400: train loss 0.1105, val loss 0.6137\n",
            "step val accuracy 0.9689\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7838\n",
            "Test recall 0.7000\n",
            "Test F1 0.6895\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 6 4]]\n",
            "iter 400: loss 0.0000, time 16149.37ms\n",
            "iter 401: loss 0.0000, time 1562.05ms\n",
            "iter 402: loss 0.0000, time 1554.51ms\n",
            "iter 403: loss 0.0000, time 1541.70ms\n",
            "iter 404: loss 0.0001, time 1551.36ms\n",
            "iter 405: loss 0.0000, time 1581.33ms\n",
            "iter 406: loss 0.0003, time 1645.38ms\n",
            "iter 407: loss 0.0025, time 1661.65ms\n",
            "iter 408: loss 0.0000, time 1560.96ms\n",
            "iter 409: loss 0.0052, time 1532.36ms\n",
            "iter 410: loss 0.0001, time 1577.02ms\n",
            "iter 411: loss 0.0000, time 1545.07ms\n",
            "iter 412: loss 0.0000, time 1608.06ms\n",
            "iter 413: loss 0.0000, time 1676.24ms\n",
            "iter 414: loss 0.0001, time 1570.06ms\n",
            "iter 415: loss 0.0000, time 1557.60ms\n",
            "iter 416: loss 0.0000, time 1564.75ms\n",
            "iter 417: loss 0.0000, time 1577.53ms\n",
            "iter 418: loss 0.0000, time 1557.94ms\n",
            "iter 419: loss 0.0000, time 1566.23ms\n",
            "step 420: train loss 0.1053, val loss 0.6214\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 15179.24ms\n",
            "iter 421: loss 0.0000, time 1588.93ms\n",
            "iter 422: loss 0.0000, time 1571.53ms\n",
            "iter 423: loss 0.0000, time 1721.28ms\n",
            "iter 424: loss 0.0000, time 1646.45ms\n",
            "iter 425: loss 0.0000, time 1556.32ms\n",
            "iter 426: loss 0.0063, time 1576.56ms\n",
            "iter 427: loss 0.0000, time 1591.62ms\n",
            "iter 428: loss 0.0001, time 1566.44ms\n",
            "iter 429: loss 0.0000, time 1575.15ms\n",
            "iter 430: loss 0.0000, time 1560.31ms\n",
            "iter 431: loss 0.0000, time 1557.04ms\n",
            "iter 432: loss 0.0000, time 1594.74ms\n",
            "iter 433: loss 0.0000, time 1549.58ms\n",
            "iter 434: loss 0.0000, time 1567.30ms\n",
            "iter 435: loss 0.0000, time 1655.19ms\n",
            "iter 436: loss 0.0000, time 1684.54ms\n",
            "iter 437: loss 0.0000, time 1586.78ms\n",
            "iter 438: loss 0.0000, time 1602.97ms\n",
            "iter 439: loss 0.0000, time 1593.99ms\n",
            "step 440: train loss 0.1010, val loss 0.6174\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 15207.52ms\n",
            "iter 441: loss 0.0000, time 1598.34ms\n",
            "iter 442: loss 0.0000, time 1554.66ms\n",
            "iter 443: loss 0.0000, time 1550.65ms\n",
            "iter 444: loss 0.0000, time 1567.57ms\n",
            "iter 445: loss 0.0000, time 1566.34ms\n",
            "iter 446: loss 0.0000, time 1661.35ms\n",
            "iter 447: loss 0.0000, time 1646.68ms\n",
            "iter 448: loss 0.0000, time 1554.10ms\n",
            "iter 449: loss 0.0000, time 1567.60ms\n",
            "iter 450: loss 0.0000, time 1589.78ms\n",
            "iter 451: loss 0.0000, time 1552.68ms\n",
            "iter 452: loss 0.0000, time 1666.00ms\n",
            "iter 453: loss 0.0000, time 1644.50ms\n",
            "iter 454: loss 0.0000, time 1574.83ms\n",
            "iter 455: loss 0.0000, time 1560.68ms\n",
            "iter 456: loss 0.0000, time 1609.96ms\n",
            "iter 457: loss 0.0000, time 1565.68ms\n",
            "iter 458: loss 0.0000, time 1567.45ms\n",
            "iter 459: loss 0.0000, time 1545.54ms\n",
            "step 460: train loss 0.0971, val loss 0.6184\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0001, time 15077.52ms\n",
            "iter 461: loss 0.0000, time 1577.45ms\n",
            "iter 462: loss 0.0000, time 1626.04ms\n",
            "iter 463: loss 0.0000, time 1698.61ms\n",
            "iter 464: loss 0.0000, time 1638.39ms\n",
            "iter 465: loss 0.0000, time 1568.54ms\n",
            "iter 466: loss 0.0000, time 1558.61ms\n",
            "iter 467: loss 0.0000, time 1566.07ms\n",
            "iter 468: loss 0.0003, time 1573.17ms\n",
            "iter 469: loss 0.0000, time 1565.53ms\n",
            "iter 470: loss 0.0000, time 1569.40ms\n",
            "iter 471: loss 0.0000, time 1583.95ms\n",
            "iter 472: loss 0.0000, time 1589.77ms\n",
            "iter 473: loss 0.0000, time 1594.03ms\n",
            "iter 474: loss 0.0000, time 1567.54ms\n",
            "iter 475: loss 0.0000, time 1679.50ms\n",
            "iter 476: loss 0.0000, time 1689.85ms\n",
            "iter 477: loss 0.0000, time 1622.26ms\n",
            "iter 478: loss 0.0000, time 1751.81ms\n",
            "iter 479: loss 0.0000, time 1558.27ms\n",
            "step 480: train loss 0.0931, val loss 0.6262\n",
            "step val accuracy 0.9615\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 15167.81ms\n",
            "iter 481: loss 0.0000, time 1580.13ms\n",
            "iter 482: loss 0.0000, time 1546.72ms\n",
            "iter 483: loss 0.0000, time 1557.68ms\n",
            "iter 484: loss 0.0000, time 1568.75ms\n",
            "iter 485: loss 0.0001, time 1634.53ms\n",
            "iter 486: loss 0.0000, time 1660.80ms\n",
            "iter 487: loss 0.0002, time 1571.25ms\n",
            "iter 488: loss 0.0000, time 1553.88ms\n",
            "iter 489: loss 0.0000, time 1566.02ms\n",
            "iter 490: loss 0.0000, time 1534.53ms\n",
            "iter 491: loss 0.0000, time 1598.42ms\n",
            "iter 492: loss 0.0000, time 1669.29ms\n",
            "iter 493: loss 0.0000, time 1615.05ms\n",
            "iter 494: loss 0.0537, time 1548.62ms\n",
            "iter 495: loss 0.0000, time 1543.37ms\n",
            "iter 496: loss 0.0000, time 1579.53ms\n",
            "iter 497: loss 0.0000, time 1560.57ms\n",
            "iter 498: loss 0.0000, time 1550.32ms\n",
            "iter 499: loss 0.0000, time 1580.53ms\n",
            "step 500: train loss 0.0894, val loss 0.6205\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7838\n",
            "Test recall 0.7000\n",
            "Test F1 0.6895\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 8 0]\n",
            " [0 6 4]]\n",
            "iter 500: loss 0.0001, time 16414.57ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅██▇▇</td></tr><tr><td>Test_F1_Score</td><td>▁▆██▇▇</td></tr><tr><td>Test_Precision</td><td>▁▆██▇▇</td></tr><tr><td>Test_Recall</td><td>▁▅██▇▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▄▇▇██████████████████████</td></tr><tr><td>val/loss</td><td> █▄▂▂▂▁▁▁▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.7</td></tr><tr><td>Test_F1_Score</td><td>0.68952</td></tr><tr><td>Test_Precision</td><td>0.78384</td></tr><tr><td>Test_Recall</td><td>0.7</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.08942</td></tr><tr><td>val/acc</td><td>0.97781</td></tr><tr><td>val/loss</td><td>0.62053</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">youthful-sweep-9</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/bmsnzozl' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/bmsnzozl</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_033803-bmsnzozl\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q5v837mt with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_040019-q5v837mt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/q5v837mt' target=\"_blank\">mild-sweep-10</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/q5v837mt' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/q5v837mt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3779\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.5977, time 29017.55ms\n",
            "iter 1: loss 1.0391, time 1645.13ms\n",
            "iter 2: loss 2.2500, time 1621.92ms\n",
            "iter 3: loss 0.5742, time 1593.65ms\n",
            "iter 4: loss 0.5391, time 1652.22ms\n",
            "iter 5: loss 0.1631, time 1709.37ms\n",
            "iter 6: loss 0.7852, time 1628.89ms\n",
            "iter 7: loss 0.8047, time 1605.06ms\n",
            "iter 8: loss 0.2471, time 1586.63ms\n",
            "iter 9: loss 0.9297, time 1623.14ms\n",
            "iter 10: loss 3.0312, time 1640.44ms\n",
            "iter 11: loss 0.6445, time 1703.44ms\n",
            "iter 12: loss 3.4844, time 1618.23ms\n",
            "iter 13: loss 0.4707, time 1635.37ms\n",
            "iter 14: loss 0.6875, time 1631.22ms\n",
            "iter 15: loss 1.2188, time 1591.42ms\n",
            "iter 16: loss 0.9063, time 1581.78ms\n",
            "iter 17: loss 0.8789, time 1596.38ms\n",
            "iter 18: loss 0.9141, time 1587.37ms\n",
            "iter 19: loss 0.3887, time 1581.39ms\n",
            "step 20: train loss 1.5937, val loss 2.3608\n",
            "step val accuracy 0.6627\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.3145, time 15655.95ms\n",
            "iter 21: loss 0.2695, time 1723.80ms\n",
            "iter 22: loss 1.4453, time 1645.50ms\n",
            "iter 23: loss 0.2715, time 1610.70ms\n",
            "iter 24: loss 0.1152, time 1622.96ms\n",
            "iter 25: loss 0.6328, time 1579.23ms\n",
            "iter 26: loss 2.8125, time 1595.15ms\n",
            "iter 27: loss 0.0942, time 1597.61ms\n",
            "iter 28: loss 0.3984, time 1585.82ms\n",
            "iter 29: loss 0.5156, time 1554.16ms\n",
            "iter 30: loss 0.8828, time 1562.65ms\n",
            "iter 31: loss 0.1177, time 1557.86ms\n",
            "iter 32: loss 0.0151, time 1557.29ms\n",
            "iter 33: loss 0.0087, time 1631.63ms\n",
            "iter 34: loss 0.0236, time 1703.81ms\n",
            "iter 35: loss 0.0010, time 1621.38ms\n",
            "iter 36: loss 3.5469, time 1543.83ms\n",
            "iter 37: loss 0.0079, time 1566.81ms\n",
            "iter 38: loss 0.7578, time 1566.02ms\n",
            "iter 39: loss 0.0344, time 1622.91ms\n",
            "step 40: train loss 1.0110, val loss 1.3509\n",
            "step val accuracy 0.9009\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0869, time 15015.64ms\n",
            "iter 41: loss 0.0017, time 1555.54ms\n",
            "iter 42: loss 1.1484, time 1572.53ms\n",
            "iter 43: loss 0.7227, time 1545.07ms\n",
            "iter 44: loss 0.0005, time 1660.79ms\n",
            "iter 45: loss 0.0796, time 1669.01ms\n",
            "iter 46: loss 0.0020, time 1557.89ms\n",
            "iter 47: loss 0.0292, time 1549.06ms\n",
            "iter 48: loss 0.3574, time 1613.61ms\n",
            "iter 49: loss 0.1196, time 1546.27ms\n",
            "iter 50: loss 0.0046, time 1662.32ms\n",
            "iter 51: loss 0.1748, time 1617.69ms\n",
            "iter 52: loss 0.3105, time 1559.91ms\n",
            "iter 53: loss 0.0240, time 1565.48ms\n",
            "iter 54: loss 0.1680, time 1580.35ms\n",
            "iter 55: loss 0.0004, time 1541.04ms\n",
            "iter 56: loss 0.0008, time 1548.68ms\n",
            "iter 57: loss 0.0091, time 1606.03ms\n",
            "iter 58: loss 0.0002, time 1543.90ms\n",
            "iter 59: loss 0.0002, time 1553.72ms\n",
            "step 60: train loss 0.7201, val loss 1.0150\n",
            "step val accuracy 0.9320\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0005, time 15336.57ms\n",
            "iter 61: loss 0.0505, time 1787.41ms\n",
            "iter 62: loss 0.0012, time 1609.04ms\n",
            "iter 63: loss 0.0001, time 1549.76ms\n",
            "iter 64: loss 0.0001, time 1558.80ms\n",
            "iter 65: loss 0.4941, time 1594.48ms\n",
            "iter 66: loss 0.0073, time 1544.22ms\n",
            "iter 67: loss 0.0009, time 1564.02ms\n",
            "iter 68: loss 0.0006, time 1587.86ms\n",
            "iter 69: loss 0.0008, time 1572.77ms\n",
            "iter 70: loss 0.0007, time 1549.52ms\n",
            "iter 71: loss 0.0000, time 1590.21ms\n",
            "iter 72: loss 0.0000, time 1603.87ms\n",
            "iter 73: loss 0.0006, time 1687.50ms\n",
            "iter 74: loss 0.0000, time 1676.49ms\n",
            "iter 75: loss 0.0022, time 1575.23ms\n",
            "iter 76: loss 0.1099, time 1578.26ms\n",
            "iter 77: loss 0.0164, time 1566.20ms\n",
            "iter 78: loss 0.0232, time 1573.03ms\n",
            "iter 79: loss 0.0013, time 1658.83ms\n",
            "step 80: train loss 0.5715, val loss 0.8389\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0005, time 15085.84ms\n",
            "iter 81: loss 0.0265, time 1522.54ms\n",
            "iter 82: loss 0.0000, time 1523.95ms\n",
            "iter 83: loss 0.0008, time 1631.15ms\n",
            "iter 84: loss 0.0002, time 1661.88ms\n",
            "iter 85: loss 0.0001, time 1542.64ms\n",
            "iter 86: loss 0.0009, time 1563.34ms\n",
            "iter 87: loss 0.0542, time 1577.56ms\n",
            "iter 88: loss 0.0015, time 1534.50ms\n",
            "iter 89: loss 0.0020, time 1598.40ms\n",
            "iter 90: loss 0.0005, time 1662.63ms\n",
            "iter 91: loss 0.0150, time 1623.00ms\n",
            "iter 92: loss 0.0000, time 1537.82ms\n",
            "iter 93: loss 0.0015, time 1594.18ms\n",
            "iter 94: loss 0.0087, time 1571.46ms\n",
            "iter 95: loss 0.3438, time 1575.93ms\n",
            "iter 96: loss 0.0014, time 1568.62ms\n",
            "iter 97: loss 0.0117, time 1571.52ms\n",
            "iter 98: loss 0.0003, time 1588.65ms\n",
            "iter 99: loss 0.0001, time 1553.18ms\n",
            "step 100: train loss 0.4698, val loss 0.6789\n",
            "step val accuracy 0.9423\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6667\n",
            "Test precision 0.8333\n",
            "Test recall 0.6667\n",
            "Test F1 0.6296\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 8  2  0]\n",
            " [ 0 10  0]\n",
            " [ 0  8  2]]\n",
            "iter 100: loss 0.0003, time 16400.89ms\n",
            "iter 101: loss 0.0087, time 1571.13ms\n",
            "iter 102: loss 0.0002, time 1564.59ms\n",
            "iter 103: loss 0.0283, time 1553.07ms\n",
            "iter 104: loss 0.0014, time 1568.67ms\n",
            "iter 105: loss 0.0011, time 1580.58ms\n",
            "iter 106: loss 0.0002, time 1579.73ms\n",
            "iter 107: loss 0.0001, time 1542.43ms\n",
            "iter 108: loss 0.0261, time 1550.22ms\n",
            "iter 109: loss 0.0204, time 1554.19ms\n",
            "iter 110: loss 0.0001, time 1545.29ms\n",
            "iter 111: loss 0.0005, time 1576.72ms\n",
            "iter 112: loss 0.0004, time 1674.93ms\n",
            "iter 113: loss 0.0000, time 1661.81ms\n",
            "iter 114: loss 0.0001, time 1550.24ms\n",
            "iter 115: loss 0.0005, time 1565.23ms\n",
            "iter 116: loss 0.0005, time 1572.13ms\n",
            "iter 117: loss 0.0068, time 1533.81ms\n",
            "iter 118: loss 0.0011, time 1719.59ms\n",
            "iter 119: loss 0.0189, time 1710.34ms\n",
            "step 120: train loss 0.4025, val loss 0.6298\n",
            "step val accuracy 0.9630\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0001, time 14813.78ms\n",
            "iter 121: loss 0.0078, time 1552.73ms\n",
            "iter 122: loss 0.0004, time 1624.36ms\n",
            "iter 123: loss 0.0009, time 1702.87ms\n",
            "iter 124: loss 0.0003, time 1605.26ms\n",
            "iter 125: loss 0.0025, time 1572.05ms\n",
            "iter 126: loss 0.0000, time 1584.39ms\n",
            "iter 127: loss 0.0001, time 1560.11ms\n",
            "iter 128: loss 0.0004, time 1536.64ms\n",
            "iter 129: loss 0.0019, time 1714.81ms\n",
            "iter 130: loss 0.0010, time 1656.89ms\n",
            "iter 131: loss 0.0001, time 1544.50ms\n",
            "iter 132: loss 0.0012, time 1610.51ms\n",
            "iter 133: loss 0.0030, time 1562.80ms\n",
            "iter 134: loss 0.0003, time 1585.01ms\n",
            "iter 135: loss 0.0007, time 1580.77ms\n",
            "iter 136: loss 0.0001, time 1563.99ms\n",
            "iter 137: loss 0.0002, time 1566.22ms\n",
            "iter 138: loss 0.0001, time 1567.49ms\n",
            "iter 139: loss 0.0002, time 1540.50ms\n",
            "step 140: train loss 0.3492, val loss 0.6293\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0001, time 15279.39ms\n",
            "iter 141: loss 0.0000, time 1589.55ms\n",
            "iter 142: loss 0.0004, time 1602.72ms\n",
            "iter 143: loss 0.0000, time 1550.75ms\n",
            "iter 144: loss 0.0001, time 1550.83ms\n",
            "iter 145: loss 0.0000, time 1596.23ms\n",
            "iter 146: loss 0.0001, time 1579.30ms\n",
            "iter 147: loss 0.0000, time 1560.95ms\n",
            "iter 148: loss 0.0019, time 1578.41ms\n",
            "iter 149: loss 0.0000, time 1590.07ms\n",
            "iter 150: loss 0.0000, time 1559.05ms\n",
            "iter 151: loss 0.0010, time 1583.13ms\n",
            "iter 152: loss 0.0001, time 1724.82ms\n",
            "iter 153: loss 0.0006, time 1794.87ms\n",
            "iter 154: loss 0.0018, time 1776.05ms\n",
            "iter 155: loss 0.0002, time 1560.17ms\n",
            "iter 156: loss 0.0002, time 1568.32ms\n",
            "iter 157: loss 0.0000, time 1626.48ms\n",
            "iter 158: loss 0.0000, time 1696.76ms\n",
            "iter 159: loss 0.0000, time 1643.31ms\n",
            "step 160: train loss 0.3202, val loss 0.6036\n",
            "step val accuracy 0.8565\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0001, time 14824.37ms\n",
            "iter 161: loss 0.0000, time 1549.19ms\n",
            "iter 162: loss 0.0000, time 1680.15ms\n",
            "iter 163: loss 0.0002, time 1682.93ms\n",
            "iter 164: loss 0.0000, time 1568.25ms\n",
            "iter 165: loss 0.0002, time 1596.46ms\n",
            "iter 166: loss 0.0001, time 1559.69ms\n",
            "iter 167: loss 0.0005, time 1535.88ms\n",
            "iter 168: loss 0.0005, time 1694.04ms\n",
            "iter 169: loss 0.0009, time 1707.93ms\n",
            "iter 170: loss 0.0025, time 1553.86ms\n",
            "iter 171: loss 0.0000, time 1579.79ms\n",
            "iter 172: loss 0.0002, time 1590.50ms\n",
            "iter 173: loss 0.0001, time 1567.98ms\n",
            "iter 174: loss 0.0000, time 1574.68ms\n",
            "iter 175: loss 0.0000, time 1558.91ms\n",
            "iter 176: loss 0.0009, time 1580.69ms\n",
            "iter 177: loss 0.0000, time 1568.02ms\n",
            "iter 178: loss 0.0001, time 1554.22ms\n",
            "iter 179: loss 0.0001, time 1564.79ms\n",
            "step 180: train loss 0.2892, val loss 0.5998\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 15407.58ms\n",
            "iter 181: loss 0.0186, time 1599.86ms\n",
            "iter 182: loss 0.0000, time 1649.45ms\n",
            "iter 183: loss 0.0000, time 1621.28ms\n",
            "iter 184: loss 0.0000, time 1571.41ms\n",
            "iter 185: loss 0.0000, time 1588.66ms\n",
            "iter 186: loss 0.0000, time 1612.58ms\n",
            "iter 187: loss 0.0000, time 1570.22ms\n",
            "iter 188: loss 0.0000, time 1586.66ms\n",
            "iter 189: loss 0.0001, time 1741.97ms\n",
            "iter 190: loss 0.0000, time 1621.49ms\n",
            "iter 191: loss 0.0000, time 1749.66ms\n",
            "iter 192: loss 0.0000, time 1661.37ms\n",
            "iter 193: loss 0.0000, time 1558.69ms\n",
            "iter 194: loss 0.0004, time 1582.31ms\n",
            "iter 195: loss 0.0001, time 1594.85ms\n",
            "iter 196: loss 0.0000, time 1591.93ms\n",
            "iter 197: loss 0.0000, time 1825.93ms\n",
            "iter 198: loss 0.0014, time 1752.12ms\n",
            "iter 199: loss 0.0000, time 1596.06ms\n",
            "step 200: train loss 0.2622, val loss 0.5950\n",
            "step val accuracy 0.9601\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7300\n",
            "Test recall 0.7000\n",
            "Test F1 0.6969\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 7 1]\n",
            " [0 5 5]]\n",
            "iter 200: loss 0.0001, time 15887.80ms\n",
            "iter 201: loss 0.0011, time 1697.24ms\n",
            "iter 202: loss 0.0006, time 1636.34ms\n",
            "iter 203: loss 0.0001, time 1628.67ms\n",
            "iter 204: loss 0.0000, time 1574.47ms\n",
            "iter 205: loss 0.0003, time 1556.42ms\n",
            "iter 206: loss 0.0007, time 1602.32ms\n",
            "iter 207: loss 0.0000, time 1690.04ms\n",
            "iter 208: loss 0.0000, time 1620.97ms\n",
            "iter 209: loss 0.0000, time 1603.64ms\n",
            "iter 210: loss 0.0001, time 1581.90ms\n",
            "iter 211: loss 0.0000, time 1555.58ms\n",
            "iter 212: loss 0.0001, time 1584.45ms\n",
            "iter 213: loss 0.0000, time 1578.83ms\n",
            "iter 214: loss 0.0000, time 1575.14ms\n",
            "iter 215: loss 0.0000, time 1545.52ms\n",
            "iter 216: loss 0.0000, time 1618.35ms\n",
            "iter 217: loss 0.0000, time 1579.32ms\n",
            "iter 218: loss 0.0000, time 1559.15ms\n",
            "iter 219: loss 0.0000, time 1705.89ms\n",
            "step 220: train loss 0.2400, val loss 0.5494\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 15435.23ms\n",
            "iter 221: loss 0.0000, time 1625.61ms\n",
            "iter 222: loss 0.0000, time 1565.07ms\n",
            "iter 223: loss 0.0001, time 1604.17ms\n",
            "iter 224: loss 0.0000, time 1559.06ms\n",
            "iter 225: loss 0.0056, time 1575.88ms\n",
            "iter 226: loss 0.0004, time 1540.63ms\n",
            "iter 227: loss 0.0003, time 1561.34ms\n",
            "iter 228: loss 0.0000, time 1572.60ms\n",
            "iter 229: loss 0.0000, time 1631.75ms\n",
            "iter 230: loss 0.0000, time 1662.56ms\n",
            "iter 231: loss 0.0000, time 1578.22ms\n",
            "iter 232: loss 0.0000, time 1589.19ms\n",
            "iter 233: loss 0.0000, time 1599.71ms\n",
            "iter 234: loss 0.0000, time 1527.93ms\n",
            "iter 235: loss 0.0000, time 1607.75ms\n",
            "iter 236: loss 0.0032, time 1689.65ms\n",
            "iter 237: loss 0.0000, time 1646.35ms\n",
            "iter 238: loss 0.0000, time 1561.97ms\n",
            "iter 239: loss 0.0000, time 1594.97ms\n",
            "step 240: train loss 0.2205, val loss 0.5553\n",
            "step val accuracy 0.9541\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0001, time 15033.91ms\n",
            "iter 241: loss 0.0000, time 1704.98ms\n",
            "iter 242: loss 0.0000, time 1528.60ms\n",
            "iter 243: loss 0.0035, time 1575.74ms\n",
            "iter 244: loss 0.0003, time 1558.05ms\n",
            "iter 245: loss 0.0000, time 1551.79ms\n",
            "iter 246: loss 0.0000, time 1662.26ms\n",
            "iter 247: loss 0.0001, time 1681.36ms\n",
            "iter 248: loss 0.0001, time 1592.83ms\n",
            "iter 249: loss 0.0001, time 1596.84ms\n",
            "iter 250: loss 0.0006, time 1586.43ms\n",
            "iter 251: loss 0.0001, time 1581.00ms\n",
            "iter 252: loss 0.0001, time 1587.21ms\n",
            "iter 253: loss 0.0000, time 1550.70ms\n",
            "iter 254: loss 0.0000, time 1568.18ms\n",
            "iter 255: loss 0.0000, time 1599.09ms\n",
            "iter 256: loss 0.0002, time 1600.13ms\n",
            "iter 257: loss 0.0000, time 1579.55ms\n",
            "iter 258: loss 0.0000, time 1656.34ms\n",
            "iter 259: loss 0.0000, time 1703.98ms\n",
            "step 260: train loss 0.2043, val loss 0.5404\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 15218.00ms\n",
            "iter 261: loss 0.0000, time 1566.06ms\n",
            "iter 262: loss 0.0005, time 1558.96ms\n",
            "iter 263: loss 0.0000, time 1572.08ms\n",
            "iter 264: loss 0.0000, time 1579.17ms\n",
            "iter 265: loss 0.0000, time 1547.54ms\n",
            "iter 266: loss 0.0000, time 1580.35ms\n",
            "iter 267: loss 0.0000, time 1606.60ms\n",
            "iter 268: loss 0.0000, time 1558.68ms\n",
            "iter 269: loss 0.0000, time 1705.16ms\n",
            "iter 270: loss 0.0000, time 1701.48ms\n",
            "iter 271: loss 0.0000, time 1605.63ms\n",
            "iter 272: loss 0.0000, time 1556.83ms\n",
            "iter 273: loss 0.0000, time 1566.95ms\n",
            "iter 274: loss 0.0000, time 1571.74ms\n",
            "iter 275: loss 0.0000, time 1654.14ms\n",
            "iter 276: loss 0.0000, time 1689.39ms\n",
            "iter 277: loss 0.0000, time 1624.55ms\n",
            "iter 278: loss 0.0000, time 1590.53ms\n",
            "iter 279: loss 0.0001, time 1565.11ms\n",
            "step 280: train loss 0.1915, val loss 0.5317\n",
            "step val accuracy 0.9719\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 15052.25ms\n",
            "iter 281: loss 0.0000, time 1584.45ms\n",
            "iter 282: loss 0.0000, time 1602.29ms\n",
            "iter 283: loss 0.0000, time 1590.02ms\n",
            "iter 284: loss 0.0000, time 1550.56ms\n",
            "iter 285: loss 0.0000, time 1660.30ms\n",
            "iter 286: loss 0.0000, time 1676.12ms\n",
            "iter 287: loss 0.0000, time 1638.54ms\n",
            "iter 288: loss 0.0001, time 1599.17ms\n",
            "iter 289: loss 0.0000, time 1549.11ms\n",
            "iter 290: loss 0.0547, time 1597.19ms\n",
            "iter 291: loss 0.0000, time 1557.65ms\n",
            "iter 292: loss 0.0006, time 1563.97ms\n",
            "iter 293: loss 0.0000, time 1643.51ms\n",
            "iter 294: loss 0.0000, time 1589.07ms\n",
            "iter 295: loss 0.0000, time 1549.26ms\n",
            "iter 296: loss 0.0000, time 1601.01ms\n",
            "iter 297: loss 0.0000, time 1568.41ms\n",
            "iter 298: loss 0.0000, time 1671.59ms\n",
            "iter 299: loss 0.0001, time 1680.01ms\n",
            "step 300: train loss 0.1788, val loss 0.5101\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.8208\n",
            "Test recall 0.7333\n",
            "Test F1 0.7212\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 6 4]]\n",
            "iter 300: loss 0.0000, time 16243.48ms\n",
            "iter 301: loss 0.0000, time 1583.73ms\n",
            "iter 302: loss 0.0004, time 1542.56ms\n",
            "iter 303: loss 0.0000, time 1567.72ms\n",
            "iter 304: loss 0.0000, time 1572.40ms\n",
            "iter 305: loss 0.0000, time 1564.99ms\n",
            "iter 306: loss 0.0273, time 1542.79ms\n",
            "iter 307: loss 0.0000, time 1606.60ms\n",
            "iter 308: loss 0.0000, time 1649.83ms\n",
            "iter 309: loss 0.0000, time 1603.03ms\n",
            "iter 310: loss 0.0000, time 1597.27ms\n",
            "iter 311: loss 0.0000, time 1572.86ms\n",
            "iter 312: loss 0.0001, time 1518.69ms\n",
            "iter 313: loss 0.0000, time 1578.83ms\n",
            "iter 314: loss 0.0000, time 1666.88ms\n",
            "iter 315: loss 0.0003, time 1655.97ms\n",
            "iter 316: loss 0.0001, time 1567.20ms\n",
            "iter 317: loss 0.0000, time 1560.12ms\n",
            "iter 318: loss 0.0000, time 1590.21ms\n",
            "iter 319: loss 0.0000, time 1564.45ms\n",
            "step 320: train loss 0.1690, val loss 0.5050\n",
            "step val accuracy 0.9497\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 15047.68ms\n",
            "iter 321: loss 0.0002, time 1622.64ms\n",
            "iter 322: loss 0.0000, time 1635.55ms\n",
            "iter 323: loss 0.0000, time 1538.77ms\n",
            "iter 324: loss 0.0000, time 1657.25ms\n",
            "iter 325: loss 0.0000, time 1711.48ms\n",
            "iter 326: loss 0.0000, time 1663.79ms\n",
            "iter 327: loss 0.0000, time 1587.90ms\n",
            "iter 328: loss 0.0000, time 1559.57ms\n",
            "iter 329: loss 0.0000, time 1575.37ms\n",
            "iter 330: loss 0.0007, time 1576.31ms\n",
            "iter 331: loss 0.0000, time 1559.78ms\n",
            "iter 332: loss 0.0153, time 1562.70ms\n",
            "iter 333: loss 0.0001, time 1548.56ms\n",
            "iter 334: loss 0.0000, time 1576.51ms\n",
            "iter 335: loss 0.0000, time 1570.47ms\n",
            "iter 336: loss 0.0008, time 1603.32ms\n",
            "iter 337: loss 0.0000, time 1676.85ms\n",
            "iter 338: loss 0.0000, time 1678.21ms\n",
            "iter 339: loss 0.0000, time 1580.26ms\n",
            "step 340: train loss 0.1598, val loss 0.5505\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0003, time 15170.51ms\n",
            "iter 341: loss 0.0000, time 1586.68ms\n",
            "iter 342: loss 0.0000, time 1561.77ms\n",
            "iter 343: loss 0.0000, time 1530.23ms\n",
            "iter 344: loss 0.0000, time 1544.44ms\n",
            "iter 345: loss 0.0000, time 1555.35ms\n",
            "iter 346: loss 0.0000, time 1524.62ms\n",
            "iter 347: loss 0.0000, time 1607.47ms\n",
            "iter 348: loss 0.0000, time 1661.91ms\n",
            "iter 349: loss 0.0000, time 1590.35ms\n",
            "iter 350: loss 0.0001, time 1544.84ms\n",
            "iter 351: loss 0.0000, time 1574.66ms\n",
            "iter 352: loss 0.0000, time 1545.02ms\n",
            "iter 353: loss 0.0000, time 1546.77ms\n",
            "iter 354: loss 0.0002, time 1697.99ms\n",
            "iter 355: loss 0.0000, time 1637.81ms\n",
            "iter 356: loss 0.0000, time 1589.30ms\n",
            "iter 357: loss 0.0000, time 1539.73ms\n",
            "iter 358: loss 0.0000, time 1612.50ms\n",
            "iter 359: loss 0.0000, time 1565.10ms\n",
            "step 360: train loss 0.1514, val loss 0.5296\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 15010.19ms\n",
            "iter 361: loss 0.0000, time 1537.43ms\n",
            "iter 362: loss 0.0000, time 1572.17ms\n",
            "iter 363: loss 0.0000, time 1543.16ms\n",
            "iter 364: loss 0.0000, time 1664.65ms\n",
            "iter 365: loss 0.0000, time 1684.58ms\n",
            "iter 366: loss 0.0000, time 1601.87ms\n",
            "iter 367: loss 0.0000, time 1584.86ms\n",
            "iter 368: loss 0.0000, time 1564.52ms\n",
            "iter 369: loss 0.0000, time 1553.03ms\n",
            "iter 370: loss 0.0001, time 1622.04ms\n",
            "iter 371: loss 0.0000, time 1574.04ms\n",
            "iter 372: loss 0.0000, time 1529.91ms\n",
            "iter 373: loss 0.0001, time 1571.81ms\n",
            "iter 374: loss 2.8594, time 1546.19ms\n",
            "iter 375: loss 0.0000, time 1547.10ms\n",
            "iter 376: loss 0.0000, time 1564.91ms\n",
            "iter 377: loss 0.0000, time 1678.54ms\n",
            "iter 378: loss 0.0000, time 1711.25ms\n",
            "iter 379: loss 0.0000, time 1594.80ms\n",
            "step 380: train loss 0.1440, val loss 0.5493\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 15152.19ms\n",
            "iter 381: loss 0.0000, time 1615.15ms\n",
            "iter 382: loss 0.0000, time 1574.85ms\n",
            "iter 383: loss 0.0000, time 1541.00ms\n",
            "iter 384: loss 0.0000, time 1554.25ms\n",
            "iter 385: loss 0.0000, time 1572.95ms\n",
            "iter 386: loss 0.0000, time 1564.53ms\n",
            "iter 387: loss 0.0000, time 1649.80ms\n",
            "iter 388: loss 0.0000, time 1661.34ms\n",
            "iter 389: loss 0.0000, time 1636.04ms\n",
            "iter 390: loss 0.0000, time 1705.85ms\n",
            "iter 391: loss 0.0000, time 1531.18ms\n",
            "iter 392: loss 0.0000, time 1565.92ms\n",
            "iter 393: loss 0.0000, time 1634.74ms\n",
            "iter 394: loss 0.0000, time 1648.19ms\n",
            "iter 395: loss 0.0000, time 1665.21ms\n",
            "iter 396: loss 0.0000, time 1587.09ms\n",
            "iter 397: loss 0.0000, time 1547.66ms\n",
            "iter 398: loss 0.0000, time 1588.44ms\n",
            "iter 399: loss 0.0000, time 1544.83ms\n",
            "step 400: train loss 0.1369, val loss 0.5390\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8750\n",
            "Test recall 0.8000\n",
            "Test F1 0.7944\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  5  5]]\n",
            "iter 400: loss 0.0000, time 16000.08ms\n",
            "iter 401: loss 0.0000, time 1632.60ms\n",
            "iter 402: loss 0.0000, time 1584.35ms\n",
            "iter 403: loss 0.0000, time 1638.50ms\n",
            "iter 404: loss 0.0000, time 1673.34ms\n",
            "iter 405: loss 0.0000, time 1627.81ms\n",
            "iter 406: loss 0.0000, time 1580.51ms\n",
            "iter 407: loss 0.0009, time 1556.96ms\n",
            "iter 408: loss 0.0001, time 1539.90ms\n",
            "iter 409: loss 0.0239, time 1603.81ms\n",
            "iter 410: loss 0.0020, time 1565.54ms\n",
            "iter 411: loss 0.0000, time 1564.91ms\n",
            "iter 412: loss 0.0000, time 1575.24ms\n",
            "iter 413: loss 0.0000, time 1573.93ms\n",
            "iter 414: loss 0.0000, time 1548.10ms\n",
            "iter 415: loss 0.0000, time 1597.86ms\n",
            "iter 416: loss 0.0000, time 1811.46ms\n",
            "iter 417: loss 0.0000, time 1732.18ms\n",
            "iter 418: loss 0.0000, time 1590.67ms\n",
            "iter 419: loss 0.0000, time 1547.32ms\n",
            "step 420: train loss 0.1314, val loss 0.5287\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 15032.33ms\n",
            "iter 421: loss 0.0192, time 1565.92ms\n",
            "iter 422: loss 0.0000, time 1570.93ms\n",
            "iter 423: loss 0.0000, time 1536.81ms\n",
            "iter 424: loss 0.0000, time 1548.81ms\n",
            "iter 425: loss 0.0000, time 1571.02ms\n",
            "iter 426: loss 0.0000, time 1875.71ms\n",
            "iter 427: loss 0.0000, time 1694.71ms\n",
            "iter 428: loss 0.0001, time 1612.75ms\n",
            "iter 429: loss 0.0000, time 1571.74ms\n",
            "iter 430: loss 0.0000, time 1564.83ms\n",
            "iter 431: loss 0.0000, time 1528.12ms\n",
            "iter 432: loss 0.0000, time 1653.79ms\n",
            "iter 433: loss 0.0000, time 1685.35ms\n",
            "iter 434: loss 0.0000, time 1606.03ms\n",
            "iter 435: loss 0.0000, time 1562.48ms\n",
            "iter 436: loss 0.0000, time 1589.52ms\n",
            "iter 437: loss 0.0000, time 1578.12ms\n",
            "iter 438: loss 0.0000, time 1559.02ms\n",
            "iter 439: loss 0.0001, time 1550.58ms\n",
            "step 440: train loss 0.1259, val loss 0.5142\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0025, time 15382.47ms\n",
            "iter 441: loss 0.0000, time 1578.57ms\n",
            "iter 442: loss 0.0000, time 1612.01ms\n",
            "iter 443: loss 0.0006, time 1670.59ms\n",
            "iter 444: loss 0.0000, time 1725.34ms\n",
            "iter 445: loss 0.0000, time 1555.95ms\n",
            "iter 446: loss 0.0000, time 1588.08ms\n",
            "iter 447: loss 0.0000, time 1584.25ms\n",
            "iter 448: loss 0.0000, time 1709.27ms\n",
            "iter 449: loss 0.0000, time 1606.40ms\n",
            "iter 450: loss 0.0000, time 1563.11ms\n",
            "iter 451: loss 0.0000, time 1563.89ms\n",
            "iter 452: loss 0.0000, time 1580.77ms\n",
            "iter 453: loss 0.0004, time 1575.42ms\n",
            "iter 454: loss 0.0000, time 1572.77ms\n",
            "iter 455: loss 0.0001, time 1672.59ms\n",
            "iter 456: loss 0.0000, time 1695.54ms\n",
            "iter 457: loss 0.0000, time 1554.03ms\n",
            "iter 458: loss 0.0000, time 1550.53ms\n",
            "iter 459: loss 0.5703, time 1581.55ms\n",
            "step 460: train loss 0.1211, val loss 0.4990\n",
            "step val accuracy 0.9689\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 15824.10ms\n",
            "iter 461: loss 0.0000, time 1579.63ms\n",
            "iter 462: loss 0.0001, time 1580.44ms\n",
            "iter 463: loss 0.0000, time 1586.71ms\n",
            "iter 464: loss 0.0000, time 1558.48ms\n",
            "iter 465: loss 0.0000, time 1557.13ms\n",
            "iter 466: loss 0.0000, time 1586.75ms\n",
            "iter 467: loss 0.0000, time 1535.53ms\n",
            "iter 468: loss 0.0000, time 1594.17ms\n",
            "iter 469: loss 0.0000, time 1578.26ms\n",
            "iter 470: loss 0.0000, time 1566.79ms\n",
            "iter 471: loss 0.0001, time 1680.36ms\n",
            "iter 472: loss 0.0000, time 1724.70ms\n",
            "iter 473: loss 0.0000, time 1637.54ms\n",
            "iter 474: loss 0.0000, time 1550.33ms\n",
            "iter 475: loss 0.0000, time 1579.79ms\n",
            "iter 476: loss 0.0000, time 1569.88ms\n",
            "iter 477: loss 0.0000, time 1611.64ms\n",
            "iter 478: loss 0.0000, time 1681.58ms\n",
            "iter 479: loss 0.0000, time 1615.23ms\n",
            "step 480: train loss 0.1161, val loss 0.5061\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 14752.07ms\n",
            "iter 481: loss 0.0000, time 1570.86ms\n",
            "iter 482: loss 0.0000, time 1661.50ms\n",
            "iter 483: loss 0.0000, time 1685.03ms\n",
            "iter 484: loss 0.0000, time 1587.68ms\n",
            "iter 485: loss 0.0000, time 1565.70ms\n",
            "iter 486: loss 0.0000, time 1534.38ms\n",
            "iter 487: loss 0.0000, time 1583.94ms\n",
            "iter 488: loss 0.0000, time 1718.34ms\n",
            "iter 489: loss 0.0000, time 1716.04ms\n",
            "iter 490: loss 0.0000, time 1546.63ms\n",
            "iter 491: loss 0.0000, time 1597.08ms\n",
            "iter 492: loss 0.0000, time 1589.59ms\n",
            "iter 493: loss 0.0000, time 1594.56ms\n",
            "iter 494: loss 0.0000, time 1538.72ms\n",
            "iter 495: loss 0.0000, time 1560.24ms\n",
            "iter 496: loss 0.0000, time 1699.09ms\n",
            "iter 497: loss 0.0000, time 1552.35ms\n",
            "iter 498: loss 0.0000, time 1574.27ms\n",
            "iter 499: loss 0.0000, time 1575.03ms\n",
            "step 500: train loss 0.1115, val loss 0.5181\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8671\n",
            "Test recall 0.8333\n",
            "Test F1 0.8283\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 1  9  0]\n",
            " [ 0  4  6]]\n",
            "iter 500: loss 0.0000, time 16264.24ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▅▆▇█</td></tr><tr><td>Test_F1_Score</td><td>▁▅▆▆▇█</td></tr><tr><td>Test_Precision</td><td>▁▇▆▇██</td></tr><tr><td>Test_Recall</td><td>▁▅▅▆▇█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▄▇▇████▇█████████████████</td></tr><tr><td>val/loss</td><td> █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.83333</td></tr><tr><td>Test_F1_Score</td><td>0.82833</td></tr><tr><td>Test_Precision</td><td>0.86713</td></tr><tr><td>Test_Recall</td><td>0.83333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.11149</td></tr><tr><td>val/acc</td><td>0.97633</td></tr><tr><td>val/loss</td><td>0.5181</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">mild-sweep-10</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/q5v837mt' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/q5v837mt</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_040019-q5v837mt\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gq5qzwms with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_042259-gq5qzwms</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/gq5qzwms' target=\"_blank\">rose-sweep-11</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/gq5qzwms' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/gq5qzwms</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.2322\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 8.1875, time 20121.96ms\n",
            "iter 1: loss 0.5664, time 1645.87ms\n",
            "iter 2: loss 2.8281, time 1585.79ms\n",
            "iter 3: loss 0.6641, time 1652.59ms\n",
            "iter 4: loss 2.0469, time 1601.30ms\n",
            "iter 5: loss 0.5430, time 1623.19ms\n",
            "iter 6: loss 0.5078, time 1562.14ms\n",
            "iter 7: loss 0.4648, time 1614.36ms\n",
            "iter 8: loss 1.1250, time 1599.20ms\n",
            "iter 9: loss 0.8438, time 1694.26ms\n",
            "iter 10: loss 0.8672, time 1649.51ms\n",
            "iter 11: loss 0.3555, time 1652.69ms\n",
            "iter 12: loss 0.9375, time 1736.67ms\n",
            "iter 13: loss 0.6992, time 1652.90ms\n",
            "iter 14: loss 0.5000, time 1630.89ms\n",
            "iter 15: loss 0.8789, time 1638.09ms\n",
            "iter 16: loss 0.7266, time 1682.44ms\n",
            "iter 17: loss 0.6445, time 1644.38ms\n",
            "iter 18: loss 0.2393, time 1644.06ms\n",
            "iter 19: loss 0.2852, time 1673.50ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 1.1858, val loss 0.5820\n",
            "step val accuracy 0.7408\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.3750, time 19817.51ms\n",
            "iter 21: loss 0.0864, time 1651.37ms\n",
            "iter 22: loss 0.0322, time 1584.22ms\n",
            "iter 23: loss 0.2217, time 1793.39ms\n",
            "iter 24: loss 0.3750, time 1660.45ms\n",
            "iter 25: loss 0.0014, time 1588.96ms\n",
            "iter 26: loss 0.0781, time 1602.04ms\n",
            "iter 27: loss 0.0265, time 1565.78ms\n",
            "iter 28: loss 0.6289, time 1565.32ms\n",
            "iter 29: loss 0.1904, time 1608.25ms\n",
            "iter 30: loss 0.1699, time 1577.70ms\n",
            "iter 31: loss 1.0859, time 1612.88ms\n",
            "iter 32: loss 0.1172, time 1610.63ms\n",
            "iter 33: loss 0.1797, time 1604.63ms\n",
            "iter 34: loss 0.0056, time 1572.63ms\n",
            "iter 35: loss 0.0046, time 1647.50ms\n",
            "iter 36: loss 0.0801, time 1683.10ms\n",
            "iter 37: loss 0.0036, time 1630.89ms\n",
            "iter 38: loss 0.0028, time 1590.68ms\n",
            "iter 39: loss 2.5000, time 1582.68ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.7853, val loss 0.5936\n",
            "step val accuracy 0.9104\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0703, time 19528.63ms\n",
            "iter 41: loss 0.0146, time 1598.76ms\n",
            "iter 42: loss 0.0369, time 1559.05ms\n",
            "iter 43: loss 0.0013, time 1698.83ms\n",
            "iter 44: loss 0.1631, time 1687.92ms\n",
            "iter 45: loss 0.0152, time 1593.15ms\n",
            "iter 46: loss 0.2061, time 1570.78ms\n",
            "iter 47: loss 0.0376, time 1588.49ms\n",
            "iter 48: loss 0.0752, time 1617.52ms\n",
            "iter 49: loss 0.0075, time 1679.14ms\n",
            "iter 50: loss 0.1865, time 1677.21ms\n",
            "iter 51: loss 0.1396, time 1597.87ms\n",
            "iter 52: loss 0.0250, time 1570.31ms\n",
            "iter 53: loss 0.0613, time 1604.06ms\n",
            "iter 54: loss 0.0095, time 1594.47ms\n",
            "iter 55: loss 0.0251, time 1599.44ms\n",
            "iter 56: loss 0.0398, time 1632.01ms\n",
            "iter 57: loss 0.2285, time 1583.50ms\n",
            "iter 58: loss 0.4668, time 1581.46ms\n",
            "iter 59: loss 0.0008, time 1605.78ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.6123, val loss 0.6609\n",
            "step val accuracy 0.7732\n",
            "saving checkpoint to out\n",
            "iter 60: loss 1.7656, time 19843.45ms\n",
            "iter 61: loss 0.0513, time 1619.75ms\n",
            "iter 62: loss 0.0016, time 1598.69ms\n",
            "iter 63: loss 0.0056, time 1621.33ms\n",
            "iter 64: loss 0.6719, time 1696.13ms\n",
            "iter 65: loss 0.0019, time 1639.56ms\n",
            "iter 66: loss 0.0693, time 1591.16ms\n",
            "iter 67: loss 0.0011, time 1605.09ms\n",
            "iter 68: loss 0.0151, time 1621.93ms\n",
            "iter 69: loss 0.0479, time 1713.15ms\n",
            "iter 70: loss 3.0625, time 1639.69ms\n",
            "iter 71: loss 0.0245, time 1590.28ms\n",
            "iter 72: loss 0.2344, time 1629.22ms\n",
            "iter 73: loss 0.0052, time 1603.38ms\n",
            "iter 74: loss 0.0058, time 1581.63ms\n",
            "iter 75: loss 0.0032, time 1735.98ms\n",
            "iter 76: loss 2.0938, time 1683.13ms\n",
            "iter 77: loss 0.0038, time 1597.72ms\n",
            "iter 78: loss 0.0114, time 1608.29ms\n",
            "iter 79: loss 0.0159, time 1587.54ms\n",
            "step 80: train loss 0.5365, val loss 0.6185\n",
            "step val accuracy 0.9428\n",
            "saving checkpoint to out\n",
            "iter 80: loss 1.0859, time 19445.35ms\n",
            "iter 81: loss 0.1787, time 1609.99ms\n",
            "iter 82: loss 0.7773, time 1649.03ms\n",
            "iter 83: loss 0.1973, time 1677.24ms\n",
            "iter 84: loss 0.1328, time 1613.72ms\n",
            "iter 85: loss 0.0194, time 1565.89ms\n",
            "iter 86: loss 0.0056, time 1561.28ms\n",
            "iter 87: loss 0.0040, time 1588.22ms\n",
            "iter 88: loss 0.0269, time 1589.91ms\n",
            "iter 89: loss 0.1299, time 1548.88ms\n",
            "iter 90: loss 0.0014, time 1590.63ms\n",
            "iter 91: loss 0.0001, time 1563.64ms\n",
            "iter 92: loss 0.0140, time 1556.32ms\n",
            "iter 93: loss 0.0150, time 1558.63ms\n",
            "iter 94: loss 0.0031, time 1602.13ms\n",
            "iter 95: loss 0.0410, time 1660.37ms\n",
            "iter 96: loss 0.0713, time 1676.11ms\n",
            "iter 97: loss 0.0004, time 1565.49ms\n",
            "iter 98: loss 0.0010, time 1555.44ms\n",
            "iter 99: loss 0.0062, time 1542.16ms\n",
            "step 100: train loss 0.4520, val loss 0.5479\n",
            "step val accuracy 0.9590\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.3796\n",
            "Test recall 0.5333\n",
            "Test F1 0.4394\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 3  7  0]\n",
            " [ 0 10  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 100: loss 0.0046, time 20912.54ms\n",
            "iter 101: loss 0.0054, time 1637.43ms\n",
            "iter 102: loss 0.0003, time 1690.73ms\n",
            "iter 103: loss 0.0013, time 1654.33ms\n",
            "iter 104: loss 0.0090, time 1581.66ms\n",
            "iter 105: loss 0.0008, time 1651.00ms\n",
            "iter 106: loss 0.0010, time 1597.98ms\n",
            "iter 107: loss 0.0005, time 1613.22ms\n",
            "iter 108: loss 0.0003, time 1680.37ms\n",
            "iter 109: loss 1.0859, time 1686.11ms\n",
            "iter 110: loss 0.0152, time 1598.38ms\n",
            "iter 111: loss 0.0014, time 1613.52ms\n",
            "iter 112: loss 0.0002, time 1586.41ms\n",
            "iter 113: loss 0.0001, time 1664.21ms\n",
            "iter 114: loss 0.0011, time 1646.26ms\n",
            "iter 115: loss 0.0002, time 1634.30ms\n",
            "iter 116: loss 0.0001, time 1598.66ms\n",
            "iter 117: loss 0.0007, time 1619.86ms\n",
            "iter 118: loss 0.0869, time 1591.75ms\n",
            "iter 119: loss 0.0001, time 1577.21ms\n",
            "step 120: train loss 0.3873, val loss 0.5385\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0000, time 19845.67ms\n",
            "iter 121: loss 0.0085, time 1613.70ms\n",
            "iter 122: loss 0.0003, time 1667.04ms\n",
            "iter 123: loss 0.0000, time 1595.09ms\n",
            "iter 124: loss 0.0019, time 1581.02ms\n",
            "iter 125: loss 0.0004, time 1596.20ms\n",
            "iter 126: loss 0.0001, time 1602.32ms\n",
            "iter 127: loss 0.0030, time 1607.53ms\n",
            "iter 128: loss 0.0006, time 1655.23ms\n",
            "iter 129: loss 0.0000, time 1622.48ms\n",
            "iter 130: loss 0.0000, time 1558.25ms\n",
            "iter 131: loss 0.0001, time 1581.35ms\n",
            "iter 132: loss 0.0000, time 1579.70ms\n",
            "iter 133: loss 0.0006, time 1550.13ms\n",
            "iter 134: loss 0.0001, time 1692.96ms\n",
            "iter 135: loss 0.0104, time 1608.49ms\n",
            "iter 136: loss 0.0010, time 1559.18ms\n",
            "iter 137: loss 0.0000, time 1565.34ms\n",
            "iter 138: loss 0.0008, time 1580.69ms\n",
            "iter 139: loss 0.0001, time 1523.85ms\n",
            "step 140: train loss 0.3338, val loss 0.5247\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0000, time 19355.21ms\n",
            "iter 141: loss 0.0018, time 1589.98ms\n",
            "iter 142: loss 0.0006, time 1710.86ms\n",
            "iter 143: loss 0.0000, time 1604.74ms\n",
            "iter 144: loss 0.0000, time 1549.72ms\n",
            "iter 145: loss 0.0003, time 1573.28ms\n",
            "iter 146: loss 0.0029, time 1535.86ms\n",
            "iter 147: loss 0.0000, time 1561.08ms\n",
            "iter 148: loss 0.0000, time 1565.55ms\n",
            "iter 149: loss 0.0000, time 1591.81ms\n",
            "iter 150: loss 0.0000, time 1557.04ms\n",
            "iter 151: loss 0.0001, time 1585.77ms\n",
            "iter 152: loss 0.0003, time 1559.47ms\n",
            "iter 153: loss 0.0001, time 1569.11ms\n",
            "iter 154: loss 0.0001, time 1607.42ms\n",
            "iter 155: loss 0.0000, time 1653.49ms\n",
            "iter 156: loss 0.0001, time 1592.72ms\n",
            "iter 157: loss 0.0000, time 1549.95ms\n",
            "iter 158: loss 0.0002, time 1574.54ms\n",
            "iter 159: loss 0.0002, time 1571.91ms\n",
            "step 160: train loss 0.2937, val loss 0.5069\n",
            "step val accuracy 0.9492\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 19288.83ms\n",
            "iter 161: loss 0.0002, time 1526.81ms\n",
            "iter 162: loss 0.0001, time 1661.11ms\n",
            "iter 163: loss 0.0000, time 1669.00ms\n",
            "iter 164: loss 0.0000, time 1569.21ms\n",
            "iter 165: loss 0.0000, time 1589.40ms\n",
            "iter 166: loss 0.0000, time 1563.03ms\n",
            "iter 167: loss 0.0000, time 1561.91ms\n",
            "iter 168: loss 0.0001, time 1617.13ms\n",
            "iter 169: loss 0.0000, time 1641.45ms\n",
            "iter 170: loss 0.0014, time 1588.09ms\n",
            "iter 171: loss 0.0000, time 1591.67ms\n",
            "iter 172: loss 0.0001, time 1532.84ms\n",
            "iter 173: loss 0.0195, time 1576.07ms\n",
            "iter 174: loss 0.0020, time 1561.22ms\n",
            "iter 175: loss 0.0098, time 1553.08ms\n",
            "iter 176: loss 0.0000, time 1555.44ms\n",
            "iter 177: loss 0.0000, time 1577.29ms\n",
            "iter 178: loss 0.0000, time 1552.50ms\n",
            "iter 179: loss 0.0000, time 1509.50ms\n",
            "step 180: train loss 0.2632, val loss 0.4821\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0004, time 19611.58ms\n",
            "iter 181: loss 0.0003, time 1565.03ms\n",
            "iter 182: loss 0.0000, time 1620.58ms\n",
            "iter 183: loss 0.0000, time 1584.79ms\n",
            "iter 184: loss 0.0012, time 1577.06ms\n",
            "iter 185: loss 0.0000, time 1576.73ms\n",
            "iter 186: loss 0.0023, time 1546.65ms\n",
            "iter 187: loss 0.0000, time 1532.51ms\n",
            "iter 188: loss 0.0000, time 1583.43ms\n",
            "iter 189: loss 0.0000, time 1641.48ms\n",
            "iter 190: loss 0.0000, time 1652.70ms\n",
            "iter 191: loss 0.0000, time 1581.00ms\n",
            "iter 192: loss 0.0005, time 1584.14ms\n",
            "iter 193: loss 0.0001, time 1630.95ms\n",
            "iter 194: loss 0.0000, time 1568.41ms\n",
            "iter 195: loss 0.1592, time 1662.82ms\n",
            "iter 196: loss 0.0698, time 1665.84ms\n",
            "iter 197: loss 0.0000, time 1563.70ms\n",
            "iter 198: loss 0.0000, time 1544.41ms\n",
            "iter 199: loss 0.0000, time 1602.76ms\n",
            "step 200: train loss 0.2375, val loss 0.4945\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.4921\n",
            "Test recall 0.6333\n",
            "Test F1 0.5308\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 200: loss 0.0000, time 20406.38ms\n",
            "iter 201: loss 0.0000, time 1608.99ms\n",
            "iter 202: loss 0.0000, time 1686.38ms\n",
            "iter 203: loss 0.0000, time 1717.84ms\n",
            "iter 204: loss 0.0000, time 1565.56ms\n",
            "iter 205: loss 0.0000, time 1590.59ms\n",
            "iter 206: loss 0.0000, time 1639.13ms\n",
            "iter 207: loss 0.0000, time 1627.29ms\n",
            "iter 208: loss 0.0000, time 1575.03ms\n",
            "iter 209: loss 0.0000, time 1665.91ms\n",
            "iter 210: loss 0.0000, time 1598.90ms\n",
            "iter 211: loss 0.0019, time 1563.84ms\n",
            "iter 212: loss 0.0001, time 1595.30ms\n",
            "iter 213: loss 0.0000, time 1601.07ms\n",
            "iter 214: loss 0.0000, time 1630.81ms\n",
            "iter 215: loss 0.0066, time 1703.57ms\n",
            "iter 216: loss 0.0010, time 1679.32ms\n",
            "iter 217: loss 0.0000, time 1603.35ms\n",
            "iter 218: loss 0.0003, time 1618.80ms\n",
            "iter 219: loss 0.0001, time 1628.96ms\n",
            "step 220: train loss 0.2168, val loss 0.5967\n",
            "step val accuracy 0.9708\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 19569.58ms\n",
            "iter 221: loss 0.0000, time 1600.98ms\n",
            "iter 222: loss 0.0001, time 1730.75ms\n",
            "iter 223: loss 0.0000, time 1682.19ms\n",
            "iter 224: loss 0.0000, time 1586.41ms\n",
            "iter 225: loss 0.0000, time 1584.92ms\n",
            "iter 226: loss 0.0000, time 1622.54ms\n",
            "iter 227: loss 0.0012, time 1601.37ms\n",
            "iter 228: loss 0.0000, time 1618.00ms\n",
            "iter 229: loss 0.0000, time 1641.32ms\n",
            "iter 230: loss 0.0000, time 1590.37ms\n",
            "iter 231: loss 0.0002, time 1555.97ms\n",
            "iter 232: loss 0.0000, time 1562.99ms\n",
            "iter 233: loss 0.0023, time 1584.29ms\n",
            "iter 234: loss 0.0001, time 1627.05ms\n",
            "iter 235: loss 0.0000, time 1593.32ms\n",
            "iter 236: loss 0.0000, time 1554.74ms\n",
            "iter 237: loss 0.0000, time 1557.83ms\n",
            "iter 238: loss 0.0025, time 1588.45ms\n",
            "iter 239: loss 0.0000, time 1526.40ms\n",
            "step 240: train loss 0.2005, val loss 0.5774\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 19725.30ms\n",
            "iter 241: loss 0.0000, time 1562.87ms\n",
            "iter 242: loss 0.0016, time 1619.62ms\n",
            "iter 243: loss 0.0000, time 1565.09ms\n",
            "iter 244: loss 0.0011, time 1557.76ms\n",
            "iter 245: loss 0.0000, time 1576.08ms\n",
            "iter 246: loss 0.0000, time 1584.04ms\n",
            "iter 247: loss 0.0000, time 1545.82ms\n",
            "iter 248: loss 0.0001, time 1606.73ms\n",
            "iter 249: loss 0.0000, time 1680.16ms\n",
            "iter 250: loss 0.0000, time 1608.65ms\n",
            "iter 251: loss 0.0000, time 1548.37ms\n",
            "iter 252: loss 0.0000, time 1652.22ms\n",
            "iter 253: loss 0.0003, time 1550.78ms\n",
            "iter 254: loss 0.0000, time 1573.77ms\n",
            "iter 255: loss 0.0000, time 1669.19ms\n",
            "iter 256: loss 0.0000, time 1611.01ms\n",
            "iter 257: loss 0.0024, time 1605.54ms\n",
            "iter 258: loss 0.0000, time 1569.60ms\n",
            "iter 259: loss 0.0000, time 1572.96ms\n",
            "step 260: train loss 0.1851, val loss 0.5721\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 19368.25ms\n",
            "iter 261: loss 0.0000, time 1561.51ms\n",
            "iter 262: loss 0.0001, time 1662.68ms\n",
            "iter 263: loss 0.0000, time 1696.77ms\n",
            "iter 264: loss 0.0016, time 1573.09ms\n",
            "iter 265: loss 0.0000, time 1566.99ms\n",
            "iter 266: loss 0.0003, time 1549.76ms\n",
            "iter 267: loss 0.0000, time 1586.66ms\n",
            "iter 268: loss 0.0000, time 1549.20ms\n",
            "iter 269: loss 0.0000, time 1565.01ms\n",
            "iter 270: loss 0.0000, time 1581.28ms\n",
            "iter 271: loss 0.3262, time 1540.09ms\n",
            "iter 272: loss 0.1758, time 1565.72ms\n",
            "iter 273: loss 0.0007, time 1543.69ms\n",
            "iter 274: loss 0.0000, time 1565.27ms\n",
            "iter 275: loss 0.0000, time 1646.70ms\n",
            "iter 276: loss 0.0000, time 1636.31ms\n",
            "iter 277: loss 0.0001, time 1559.73ms\n",
            "iter 278: loss 0.0000, time 1629.42ms\n",
            "iter 279: loss 0.0000, time 1563.02ms\n",
            "step 280: train loss 0.1729, val loss 0.5564\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 19257.35ms\n",
            "iter 281: loss 0.0000, time 1550.25ms\n",
            "iter 282: loss 0.0000, time 1597.94ms\n",
            "iter 283: loss 0.0000, time 1651.15ms\n",
            "iter 284: loss 0.0000, time 1598.31ms\n",
            "iter 285: loss 0.0000, time 1601.41ms\n",
            "iter 286: loss 0.0000, time 1565.37ms\n",
            "iter 287: loss 0.0000, time 1554.15ms\n",
            "iter 288: loss 0.0000, time 1545.54ms\n",
            "iter 289: loss 0.0001, time 1697.56ms\n",
            "iter 290: loss 0.0000, time 1646.57ms\n",
            "iter 291: loss 0.0001, time 1558.42ms\n",
            "iter 292: loss 0.0000, time 1564.72ms\n",
            "iter 293: loss 0.0000, time 1546.61ms\n",
            "iter 294: loss 0.0007, time 1550.25ms\n",
            "iter 295: loss 0.0000, time 1570.71ms\n",
            "iter 296: loss 0.0000, time 1581.08ms\n",
            "iter 297: loss 0.0000, time 1544.30ms\n",
            "iter 298: loss 0.0001, time 1564.31ms\n",
            "iter 299: loss 0.0000, time 1556.45ms\n",
            "step 300: train loss 0.1618, val loss 0.5619\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7160\n",
            "Test recall 0.7000\n",
            "Test F1 0.6887\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  6  1]\n",
            " [ 0  5  5]]\n",
            "iter 300: loss 0.0004, time 20772.24ms\n",
            "iter 301: loss 0.0000, time 1562.04ms\n",
            "iter 302: loss 0.0000, time 1641.84ms\n",
            "iter 303: loss 0.0000, time 1588.88ms\n",
            "iter 304: loss 0.0001, time 1554.56ms\n",
            "iter 305: loss 0.0000, time 1572.21ms\n",
            "iter 306: loss 0.0000, time 1546.62ms\n",
            "iter 307: loss 0.0000, time 1547.98ms\n",
            "iter 308: loss 0.0003, time 1540.30ms\n",
            "iter 309: loss 0.0000, time 1668.37ms\n",
            "iter 310: loss 0.0151, time 1634.96ms\n",
            "iter 311: loss 0.0000, time 1573.61ms\n",
            "iter 312: loss 0.0000, time 1604.50ms\n",
            "iter 313: loss 0.0000, time 1574.02ms\n",
            "iter 314: loss 0.0000, time 1590.05ms\n",
            "iter 315: loss 0.0000, time 1641.25ms\n",
            "iter 316: loss 0.0000, time 1636.19ms\n",
            "iter 317: loss 0.0000, time 1574.18ms\n",
            "iter 318: loss 0.0000, time 1564.96ms\n",
            "iter 319: loss 0.0000, time 1551.59ms\n",
            "step 320: train loss 0.1517, val loss 0.5712\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 19671.08ms\n",
            "iter 321: loss 0.0000, time 1548.88ms\n",
            "iter 322: loss 0.0000, time 1654.77ms\n",
            "iter 323: loss 0.0000, time 1613.49ms\n",
            "iter 324: loss 0.0020, time 1576.93ms\n",
            "iter 325: loss 0.0000, time 1564.14ms\n",
            "iter 326: loss 0.0000, time 1541.65ms\n",
            "iter 327: loss 0.0000, time 1551.65ms\n",
            "iter 328: loss 0.0000, time 1585.80ms\n",
            "iter 329: loss 0.0000, time 1599.78ms\n",
            "iter 330: loss 0.0000, time 1546.99ms\n",
            "iter 331: loss 0.0000, time 1576.04ms\n",
            "iter 332: loss 0.0000, time 1536.72ms\n",
            "iter 333: loss 0.0000, time 1574.07ms\n",
            "iter 334: loss 0.0000, time 1549.87ms\n",
            "iter 335: loss 0.0000, time 1627.03ms\n",
            "iter 336: loss 0.0001, time 1679.57ms\n",
            "iter 337: loss 0.0000, time 1598.88ms\n",
            "iter 338: loss 0.0000, time 1543.98ms\n",
            "iter 339: loss 0.0000, time 1581.97ms\n",
            "step 340: train loss 0.1429, val loss 0.5724\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 19714.65ms\n",
            "iter 341: loss 0.0000, time 1573.25ms\n",
            "iter 342: loss 0.0000, time 1572.87ms\n",
            "iter 343: loss 0.0003, time 1652.16ms\n",
            "iter 344: loss 0.0000, time 1637.43ms\n",
            "iter 345: loss 0.0000, time 1605.50ms\n",
            "iter 346: loss 0.0000, time 1545.91ms\n",
            "iter 347: loss 0.0000, time 1565.12ms\n",
            "iter 348: loss 0.0000, time 1580.86ms\n",
            "iter 349: loss 0.0000, time 1631.85ms\n",
            "iter 350: loss 0.0000, time 1647.56ms\n",
            "iter 351: loss 0.0000, time 1597.79ms\n",
            "iter 352: loss 0.0000, time 1573.62ms\n",
            "iter 353: loss 0.0000, time 1549.04ms\n",
            "iter 354: loss 0.0000, time 1596.26ms\n",
            "iter 355: loss 0.0000, time 1597.11ms\n",
            "iter 356: loss 0.0000, time 1545.20ms\n",
            "iter 357: loss 0.0000, time 1563.53ms\n",
            "iter 358: loss 0.0000, time 1555.58ms\n",
            "iter 359: loss 0.0000, time 1583.11ms\n",
            "step 360: train loss 0.1349, val loss 0.5609\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 19702.08ms\n",
            "iter 361: loss 0.0000, time 1534.30ms\n",
            "iter 362: loss 0.0000, time 1581.46ms\n",
            "iter 363: loss 0.0000, time 1605.53ms\n",
            "iter 364: loss 0.0000, time 1550.44ms\n",
            "iter 365: loss 0.0000, time 1603.19ms\n",
            "iter 366: loss 0.0000, time 1568.72ms\n",
            "iter 367: loss 0.0000, time 1574.94ms\n",
            "iter 368: loss 0.0000, time 1562.08ms\n",
            "iter 369: loss 0.0001, time 1632.47ms\n",
            "iter 370: loss 0.0000, time 1891.28ms\n",
            "iter 371: loss 0.0000, time 1676.87ms\n",
            "iter 372: loss 0.0000, time 1564.79ms\n",
            "iter 373: loss 0.0000, time 1576.30ms\n",
            "iter 374: loss 0.0001, time 1577.38ms\n",
            "iter 375: loss 0.0000, time 1591.15ms\n",
            "iter 376: loss 0.0004, time 1646.39ms\n",
            "iter 377: loss 0.0000, time 1686.08ms\n",
            "iter 378: loss 0.0001, time 1594.28ms\n",
            "iter 379: loss 0.0000, time 1574.69ms\n",
            "step 380: train loss 0.1280, val loss 0.5638\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 19921.16ms\n",
            "iter 381: loss 0.0000, time 1578.68ms\n",
            "iter 382: loss 0.0001, time 1687.02ms\n",
            "iter 383: loss 0.0000, time 1683.33ms\n",
            "iter 384: loss 0.0000, time 1618.31ms\n",
            "iter 385: loss 0.0000, time 1573.37ms\n",
            "iter 386: loss 0.0002, time 1561.11ms\n",
            "iter 387: loss 0.0000, time 1537.60ms\n",
            "iter 388: loss 0.0000, time 1588.95ms\n",
            "iter 389: loss 0.0000, time 1597.10ms\n",
            "iter 390: loss 0.0000, time 1547.93ms\n",
            "iter 391: loss 0.0000, time 1591.44ms\n",
            "iter 392: loss 0.0000, time 1531.24ms\n",
            "iter 393: loss 0.0000, time 1536.11ms\n",
            "iter 394: loss 0.0000, time 1586.24ms\n",
            "iter 395: loss 0.0000, time 1593.74ms\n",
            "iter 396: loss 0.0048, time 1634.08ms\n",
            "iter 397: loss 0.0000, time 1606.71ms\n",
            "iter 398: loss 0.0000, time 1569.57ms\n",
            "iter 399: loss 0.0000, time 1564.88ms\n",
            "step 400: train loss 0.1221, val loss 0.5619\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.8421\n",
            "Test recall 0.7000\n",
            "Test F1 0.6568\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  8  2]]\n",
            "iter 400: loss 0.0000, time 20269.03ms\n",
            "iter 401: loss 0.0000, time 1555.83ms\n",
            "iter 402: loss 0.0000, time 1606.70ms\n",
            "iter 403: loss 0.0000, time 1650.36ms\n",
            "iter 404: loss 0.0075, time 1623.97ms\n",
            "iter 405: loss 0.0000, time 1572.47ms\n",
            "iter 406: loss 0.0000, time 1549.32ms\n",
            "iter 407: loss 0.0000, time 1584.61ms\n",
            "iter 408: loss 0.0000, time 1564.30ms\n",
            "iter 409: loss 0.0002, time 1617.42ms\n",
            "iter 410: loss 1.9375, time 1668.78ms\n",
            "iter 411: loss 0.0001, time 1566.29ms\n",
            "iter 412: loss 0.0000, time 1533.77ms\n",
            "iter 413: loss 0.0004, time 1561.58ms\n",
            "iter 414: loss 0.0000, time 1557.93ms\n",
            "iter 415: loss 0.0001, time 1597.18ms\n",
            "iter 416: loss 0.0004, time 1550.35ms\n",
            "iter 417: loss 0.0000, time 1566.29ms\n",
            "iter 418: loss 0.0001, time 1557.46ms\n",
            "iter 419: loss 0.0000, time 1548.00ms\n",
            "step 420: train loss 0.1166, val loss 0.5486\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 19639.46ms\n",
            "iter 421: loss 0.0000, time 1567.01ms\n",
            "iter 422: loss 0.0000, time 1606.16ms\n",
            "iter 423: loss 0.0000, time 1573.20ms\n",
            "iter 424: loss 0.0000, time 1634.11ms\n",
            "iter 425: loss 0.0000, time 1588.58ms\n",
            "iter 426: loss 0.0001, time 1546.94ms\n",
            "iter 427: loss 0.0000, time 1562.10ms\n",
            "iter 428: loss 0.0004, time 1586.99ms\n",
            "iter 429: loss 0.0000, time 1640.12ms\n",
            "iter 430: loss 0.0101, time 1674.60ms\n",
            "iter 431: loss 0.0000, time 1545.11ms\n",
            "iter 432: loss 0.0000, time 1580.09ms\n",
            "iter 433: loss 0.0000, time 1597.66ms\n",
            "iter 434: loss 0.0000, time 1567.30ms\n",
            "iter 435: loss 0.0000, time 1578.87ms\n",
            "iter 436: loss 0.0032, time 1648.36ms\n",
            "iter 437: loss 0.0654, time 1633.44ms\n",
            "iter 438: loss 0.0000, time 1569.59ms\n",
            "iter 439: loss 0.0000, time 1562.31ms\n",
            "step 440: train loss 0.1114, val loss 0.5382\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 19379.97ms\n",
            "iter 441: loss 0.0000, time 1558.42ms\n",
            "iter 442: loss 0.0000, time 1563.07ms\n",
            "iter 443: loss 0.0000, time 1680.40ms\n",
            "iter 444: loss 0.0000, time 1671.79ms\n",
            "iter 445: loss 0.0000, time 1573.35ms\n",
            "iter 446: loss 0.0000, time 1561.38ms\n",
            "iter 447: loss 0.0000, time 1569.39ms\n",
            "iter 448: loss 0.0000, time 1568.73ms\n",
            "iter 449: loss 0.0000, time 1636.42ms\n",
            "iter 450: loss 0.0001, time 1523.09ms\n",
            "iter 451: loss 0.1514, time 1543.74ms\n",
            "iter 452: loss 0.0000, time 1564.81ms\n",
            "iter 453: loss 0.0000, time 1542.21ms\n",
            "iter 454: loss 0.0000, time 1555.46ms\n",
            "iter 455: loss 0.0000, time 1548.53ms\n",
            "iter 456: loss 0.0000, time 1688.62ms\n",
            "iter 457: loss 0.0000, time 1615.81ms\n",
            "iter 458: loss 0.0000, time 1569.29ms\n",
            "iter 459: loss 0.0000, time 1595.21ms\n",
            "step 460: train loss 0.1066, val loss 0.5378\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 19643.08ms\n",
            "iter 461: loss 0.0028, time 1590.67ms\n",
            "iter 462: loss 0.0006, time 1615.42ms\n",
            "iter 463: loss 0.0000, time 1619.36ms\n",
            "iter 464: loss 0.0000, time 1664.01ms\n",
            "iter 465: loss 0.0000, time 1597.64ms\n",
            "iter 466: loss 0.0000, time 1569.70ms\n",
            "iter 467: loss 0.0000, time 1554.22ms\n",
            "iter 468: loss 0.0000, time 1575.56ms\n",
            "iter 469: loss 0.0000, time 1580.14ms\n",
            "iter 470: loss 0.0000, time 1646.46ms\n",
            "iter 471: loss 0.0000, time 1636.77ms\n",
            "iter 472: loss 0.0015, time 1574.88ms\n",
            "iter 473: loss 0.0000, time 1549.53ms\n",
            "iter 474: loss 0.0000, time 1568.58ms\n",
            "iter 475: loss 0.0000, time 1599.64ms\n",
            "iter 476: loss 0.0001, time 1549.86ms\n",
            "iter 477: loss 0.0000, time 1601.08ms\n",
            "iter 478: loss 0.0000, time 1567.52ms\n",
            "iter 479: loss 0.0000, time 1547.60ms\n",
            "step 480: train loss 0.1021, val loss 0.5221\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 19639.35ms\n",
            "iter 481: loss 0.0000, time 1611.19ms\n",
            "iter 482: loss 0.0000, time 1597.14ms\n",
            "iter 483: loss 0.0000, time 1607.31ms\n",
            "iter 484: loss 0.0000, time 1528.90ms\n",
            "iter 485: loss 0.0000, time 1590.37ms\n",
            "iter 486: loss 0.0000, time 1543.31ms\n",
            "iter 487: loss 0.0000, time 1550.99ms\n",
            "iter 488: loss 0.0000, time 1568.05ms\n",
            "iter 489: loss 0.0000, time 1564.55ms\n",
            "iter 490: loss 0.0000, time 1631.65ms\n",
            "iter 491: loss 0.0000, time 1635.66ms\n",
            "iter 492: loss 0.0000, time 1572.65ms\n",
            "iter 493: loss 0.0000, time 1593.91ms\n",
            "iter 494: loss 0.0000, time 1565.06ms\n",
            "iter 495: loss 0.0000, time 1543.80ms\n",
            "iter 496: loss 0.0000, time 1660.04ms\n",
            "iter 497: loss 0.0001, time 1668.01ms\n",
            "iter 498: loss 0.0000, time 1597.20ms\n",
            "iter 499: loss 0.0001, time 1554.07ms\n",
            "step 500: train loss 0.0981, val loss 0.5376\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7160\n",
            "Test recall 0.7000\n",
            "Test F1 0.6887\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  6  1]\n",
            " [ 0  5  5]]\n",
            "iter 500: loss 0.0000, time 20432.59ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▂▆███</td></tr><tr><td>Test_F1_Score</td><td>▁▂▄█▇█</td></tr><tr><td>Test_Precision</td><td>▁▂▃▆█▆</td></tr><tr><td>Test_Recall</td><td>▁▂▆███</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▆██████████████████████</td></tr><tr><td>val/loss</td><td> ▅▅█▆▄▃▃▂▁▁▅▅▅▄▄▄▅▄▄▄▄▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.7</td></tr><tr><td>Test_F1_Score</td><td>0.68866</td></tr><tr><td>Test_Precision</td><td>0.71601</td></tr><tr><td>Test_Recall</td><td>0.7</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.09808</td></tr><tr><td>val/acc</td><td>0.9838</td></tr><tr><td>val/loss</td><td>0.53764</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rose-sweep-11</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/gq5qzwms' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/gq5qzwms</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_042259-gq5qzwms\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a6njhdsm with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_044607-a6njhdsm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/a6njhdsm' target=\"_blank\">vibrant-sweep-12</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/a6njhdsm' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/a6njhdsm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.2322\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(2), np.int64(2), np.int64(0), np.int64(2), np.int64(0), np.int64(0), np.int64(2), np.int64(2), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5000\n",
            "Test precision 0.3397\n",
            "Test recall 0.5000\n",
            "Test F1 0.3974\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 0 1]\n",
            " [6 0 4]\n",
            " [4 0 6]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 8.1875, time 20020.88ms\n",
            "iter 1: loss 0.5195, time 1708.45ms\n",
            "iter 2: loss 0.3027, time 1684.33ms\n",
            "iter 3: loss 0.5547, time 1586.14ms\n",
            "iter 4: loss 0.5039, time 1613.80ms\n",
            "iter 5: loss 1.3047, time 1557.67ms\n",
            "iter 6: loss 0.5039, time 1590.18ms\n",
            "iter 7: loss 1.3906, time 1655.05ms\n",
            "iter 8: loss 0.2930, time 1656.97ms\n",
            "iter 9: loss 0.4688, time 1684.87ms\n",
            "iter 10: loss 0.4629, time 1614.40ms\n",
            "iter 11: loss 1.1563, time 1586.13ms\n",
            "iter 12: loss 0.3418, time 1568.93ms\n",
            "iter 13: loss 0.8281, time 1598.56ms\n",
            "iter 14: loss 1.2500, time 1592.97ms\n",
            "iter 15: loss 1.9141, time 1599.21ms\n",
            "iter 16: loss 0.2158, time 1580.90ms\n",
            "iter 17: loss 1.0781, time 1587.05ms\n",
            "iter 18: loss 0.6484, time 1589.45ms\n",
            "iter 19: loss 0.1055, time 1712.56ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 1.0907, val loss 0.8475\n",
            "step val accuracy 0.5130\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.1016, time 20120.63ms\n",
            "iter 21: loss 0.7070, time 1590.09ms\n",
            "iter 22: loss 0.4141, time 1597.67ms\n",
            "iter 23: loss 1.4219, time 1601.53ms\n",
            "iter 24: loss 0.2139, time 1580.62ms\n",
            "iter 25: loss 0.0410, time 1605.15ms\n",
            "iter 26: loss 0.3242, time 1637.61ms\n",
            "iter 27: loss 0.1226, time 1706.32ms\n",
            "iter 28: loss 1.1016, time 1670.60ms\n",
            "iter 29: loss 0.1367, time 1586.77ms\n",
            "iter 30: loss 0.2559, time 1575.97ms\n",
            "iter 31: loss 0.9883, time 1612.44ms\n",
            "iter 32: loss 0.0869, time 1619.05ms\n",
            "iter 33: loss 0.5586, time 1654.76ms\n",
            "iter 34: loss 0.0250, time 1703.85ms\n",
            "iter 35: loss 0.0447, time 1595.84ms\n",
            "iter 36: loss 0.0099, time 1642.64ms\n",
            "iter 37: loss 0.0310, time 1626.54ms\n",
            "iter 38: loss 0.0034, time 1628.75ms\n",
            "iter 39: loss 3.1406, time 1612.15ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.8129, val loss 0.6786\n",
            "step val accuracy 0.8942\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.1650, time 19573.87ms\n",
            "iter 41: loss 0.0359, time 1713.13ms\n",
            "iter 42: loss 0.5195, time 1642.33ms\n",
            "iter 43: loss 0.0179, time 1620.95ms\n",
            "iter 44: loss 0.0635, time 1609.13ms\n",
            "iter 45: loss 0.0138, time 1601.45ms\n",
            "iter 46: loss 1.6875, time 1561.22ms\n",
            "iter 47: loss 0.0276, time 1603.52ms\n",
            "iter 48: loss 0.0342, time 1614.05ms\n",
            "iter 49: loss 0.0254, time 1549.82ms\n",
            "iter 50: loss 0.3418, time 1618.80ms\n",
            "iter 51: loss 0.0806, time 1597.72ms\n",
            "iter 52: loss 0.0018, time 1638.91ms\n",
            "iter 53: loss 0.0359, time 1676.09ms\n",
            "iter 54: loss 0.0011, time 1675.98ms\n",
            "iter 55: loss 0.0007, time 1612.05ms\n",
            "iter 56: loss 0.1514, time 1623.84ms\n",
            "iter 57: loss 0.0776, time 1574.09ms\n",
            "iter 58: loss 0.8867, time 1577.73ms\n",
            "iter 59: loss 0.0002, time 1714.30ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.6361, val loss 0.5679\n",
            "step val accuracy 0.9244\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0116, time 19381.10ms\n",
            "iter 61: loss 0.1465, time 1731.64ms\n",
            "iter 62: loss 0.0004, time 1697.21ms\n",
            "iter 63: loss 0.0115, time 1597.33ms\n",
            "iter 64: loss 0.5625, time 1618.37ms\n",
            "iter 65: loss 0.0160, time 1609.00ms\n",
            "iter 66: loss 0.0289, time 1636.13ms\n",
            "iter 67: loss 0.0009, time 1703.95ms\n",
            "iter 68: loss 0.0344, time 1653.69ms\n",
            "iter 69: loss 0.0197, time 1598.73ms\n",
            "iter 70: loss 0.5273, time 1584.47ms\n",
            "iter 71: loss 0.0124, time 1644.51ms\n",
            "iter 72: loss 0.0376, time 1629.48ms\n",
            "iter 73: loss 0.0156, time 1552.52ms\n",
            "iter 74: loss 0.0008, time 1611.89ms\n",
            "iter 75: loss 0.0300, time 1594.60ms\n",
            "iter 76: loss 0.0085, time 1559.23ms\n",
            "iter 77: loss 0.0576, time 1579.32ms\n",
            "iter 78: loss 0.0064, time 1658.93ms\n",
            "iter 79: loss 0.0000, time 1684.39ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 80: train loss 0.5251, val loss 0.5180\n",
            "step val accuracy 0.9568\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0496, time 19592.51ms\n",
            "iter 81: loss 0.0116, time 1583.15ms\n",
            "iter 82: loss 0.2520, time 1619.93ms\n",
            "iter 83: loss 0.0060, time 1605.23ms\n",
            "iter 84: loss 0.0688, time 1585.16ms\n",
            "iter 85: loss 0.0608, time 1631.31ms\n",
            "iter 86: loss 0.0003, time 1652.35ms\n",
            "iter 87: loss 0.1245, time 1724.83ms\n",
            "iter 88: loss 0.0002, time 1645.46ms\n",
            "iter 89: loss 0.0947, time 1608.10ms\n",
            "iter 90: loss 0.0155, time 1586.00ms\n",
            "iter 91: loss 0.0000, time 1588.20ms\n",
            "iter 92: loss 0.0024, time 1660.33ms\n",
            "iter 93: loss 0.0021, time 1690.00ms\n",
            "iter 94: loss 0.0003, time 1646.81ms\n",
            "iter 95: loss 0.0442, time 1624.18ms\n",
            "iter 96: loss 0.0016, time 1567.11ms\n",
            "iter 97: loss 0.0001, time 1591.77ms\n",
            "iter 98: loss 0.0010, time 1606.66ms\n",
            "iter 99: loss 0.0096, time 1615.26ms\n",
            "step 100: train loss 0.4480, val loss 0.4746\n",
            "step val accuracy 0.9158\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.5333\n",
            "Test precision 0.3631\n",
            "Test recall 0.5333\n",
            "Test F1 0.4316\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 4  6  0]\n",
            " [ 0 10  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 100: loss 0.0181, time 20606.45ms\n",
            "iter 101: loss 0.7383, time 1651.36ms\n",
            "iter 102: loss 0.0000, time 1612.98ms\n",
            "iter 103: loss 3.8281, time 1633.77ms\n",
            "iter 104: loss 0.2754, time 1583.37ms\n",
            "iter 105: loss 0.0076, time 1633.88ms\n",
            "iter 106: loss 0.0002, time 1588.84ms\n",
            "iter 107: loss 0.1235, time 1605.66ms\n",
            "iter 108: loss 0.0041, time 1627.49ms\n",
            "iter 109: loss 0.0008, time 1633.58ms\n",
            "iter 110: loss 0.0008, time 1594.01ms\n",
            "iter 111: loss 0.0031, time 1624.28ms\n",
            "iter 112: loss 0.0000, time 1700.63ms\n",
            "iter 113: loss 0.0000, time 1663.07ms\n",
            "iter 114: loss 0.0000, time 1601.97ms\n",
            "iter 115: loss 0.1338, time 1594.75ms\n",
            "iter 116: loss 0.0000, time 1622.05ms\n",
            "iter 117: loss 0.0000, time 1631.36ms\n",
            "iter 118: loss 0.0613, time 1693.61ms\n",
            "iter 119: loss 0.0000, time 1679.26ms\n",
            "step 120: train loss 0.4050, val loss 0.4182\n",
            "step val accuracy 0.9654\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0000, time 19396.00ms\n",
            "iter 121: loss 0.0139, time 1620.13ms\n",
            "iter 122: loss 0.0001, time 1608.13ms\n",
            "iter 123: loss 0.0000, time 1573.16ms\n",
            "iter 124: loss 0.0012, time 1604.99ms\n",
            "iter 125: loss 0.0081, time 1666.00ms\n",
            "iter 126: loss 0.0006, time 1676.45ms\n",
            "iter 127: loss 0.0025, time 1616.43ms\n",
            "iter 128: loss 0.0023, time 1578.94ms\n",
            "iter 129: loss 0.0008, time 1558.83ms\n",
            "iter 130: loss 0.0000, time 1536.06ms\n",
            "iter 131: loss 0.0002, time 1571.96ms\n",
            "iter 132: loss 0.0000, time 1549.17ms\n",
            "iter 133: loss 0.0009, time 1552.31ms\n",
            "iter 134: loss 0.0000, time 1547.27ms\n",
            "iter 135: loss 0.0013, time 1699.73ms\n",
            "iter 136: loss 0.0000, time 1634.31ms\n",
            "iter 137: loss 0.0003, time 1596.70ms\n",
            "iter 138: loss 0.0005, time 1641.60ms\n",
            "iter 139: loss 0.0000, time 1777.61ms\n",
            "step 140: train loss 0.3522, val loss 0.4525\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0000, time 19969.93ms\n",
            "iter 141: loss 0.0003, time 1548.17ms\n",
            "iter 142: loss 0.0016, time 1599.99ms\n",
            "iter 143: loss 0.0000, time 1541.75ms\n",
            "iter 144: loss 0.0000, time 1618.67ms\n",
            "iter 145: loss 0.0005, time 1665.77ms\n",
            "iter 146: loss 0.0002, time 1681.86ms\n",
            "iter 147: loss 0.0000, time 1565.30ms\n",
            "iter 148: loss 0.0000, time 1591.69ms\n",
            "iter 149: loss 0.0000, time 1585.70ms\n",
            "iter 150: loss 0.0000, time 1539.91ms\n",
            "iter 151: loss 0.0028, time 1576.51ms\n",
            "iter 152: loss 0.0000, time 1584.15ms\n",
            "iter 153: loss 0.0001, time 1568.19ms\n",
            "iter 154: loss 0.0000, time 1581.31ms\n",
            "iter 155: loss 0.0002, time 1586.09ms\n",
            "iter 156: loss 1.2656, time 1580.33ms\n",
            "iter 157: loss 0.0000, time 1576.81ms\n",
            "iter 158: loss 0.0000, time 1695.95ms\n",
            "iter 159: loss 0.0000, time 1654.50ms\n",
            "step 160: train loss 0.3087, val loss 0.4385\n",
            "step val accuracy 0.9838\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 19574.64ms\n",
            "iter 161: loss 0.0000, time 1573.72ms\n",
            "iter 162: loss 0.0072, time 1590.56ms\n",
            "iter 163: loss 0.0000, time 1580.94ms\n",
            "iter 164: loss 0.0000, time 1572.88ms\n",
            "iter 165: loss 0.0000, time 1624.60ms\n",
            "iter 166: loss 0.0000, time 1681.35ms\n",
            "iter 167: loss 0.0000, time 1622.00ms\n",
            "iter 168: loss 0.0000, time 1577.56ms\n",
            "iter 169: loss 0.0000, time 1555.69ms\n",
            "iter 170: loss 0.0000, time 1601.26ms\n",
            "iter 171: loss 0.0000, time 1675.93ms\n",
            "iter 172: loss 0.0000, time 1676.15ms\n",
            "iter 173: loss 0.0013, time 1628.82ms\n",
            "iter 174: loss 0.0001, time 1594.73ms\n",
            "iter 175: loss 0.0003, time 1558.66ms\n",
            "iter 176: loss 0.0000, time 1563.91ms\n",
            "iter 177: loss 0.0000, time 1574.68ms\n",
            "iter 178: loss 0.0000, time 1564.65ms\n",
            "iter 179: loss 0.0000, time 1565.12ms\n",
            "step 180: train loss 0.2746, val loss 0.4414\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 19860.71ms\n",
            "iter 181: loss 0.0000, time 1569.02ms\n",
            "iter 182: loss 0.0000, time 1569.19ms\n",
            "iter 183: loss 0.0000, time 1600.77ms\n",
            "iter 184: loss 0.0000, time 1567.21ms\n",
            "iter 185: loss 0.0000, time 1597.82ms\n",
            "iter 186: loss 0.0003, time 1559.78ms\n",
            "iter 187: loss 0.0000, time 1610.20ms\n",
            "iter 188: loss 0.0000, time 1597.79ms\n",
            "iter 189: loss 0.0000, time 1583.68ms\n",
            "iter 190: loss 0.0000, time 1574.62ms\n",
            "iter 191: loss 0.0000, time 1611.09ms\n",
            "iter 192: loss 0.0000, time 1670.63ms\n",
            "iter 193: loss 0.0000, time 1644.37ms\n",
            "iter 194: loss 0.0000, time 1586.82ms\n",
            "iter 195: loss 0.0000, time 1579.87ms\n",
            "iter 196: loss 0.0031, time 1603.62ms\n",
            "iter 197: loss 0.0019, time 1607.83ms\n",
            "iter 198: loss 0.0000, time 1724.40ms\n",
            "iter 199: loss 0.0000, time 1662.14ms\n",
            "step 200: train loss 0.2473, val loss 0.4573\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6333\n",
            "Test precision 0.4921\n",
            "Test recall 0.6333\n",
            "Test F1 0.5308\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0 10  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 200: loss 0.0001, time 20158.37ms\n",
            "iter 201: loss 0.0000, time 1659.84ms\n",
            "iter 202: loss 0.0000, time 1595.33ms\n",
            "iter 203: loss 0.0000, time 1658.89ms\n",
            "iter 204: loss 0.0000, time 1638.67ms\n",
            "iter 205: loss 0.0000, time 1743.27ms\n",
            "iter 206: loss 0.0000, time 1712.02ms\n",
            "iter 207: loss 0.0000, time 1622.46ms\n",
            "iter 208: loss 0.0000, time 1628.70ms\n",
            "iter 209: loss 0.0000, time 1608.88ms\n",
            "iter 210: loss 0.0000, time 1619.82ms\n",
            "iter 211: loss 0.0005, time 1703.07ms\n",
            "iter 212: loss 0.0000, time 1916.75ms\n",
            "iter 213: loss 0.0000, time 1731.26ms\n",
            "iter 214: loss 0.0000, time 1624.32ms\n",
            "iter 215: loss 0.0000, time 1625.71ms\n",
            "iter 216: loss 0.0000, time 1573.03ms\n",
            "iter 217: loss 0.0000, time 1796.78ms\n",
            "iter 218: loss 0.0001, time 1716.95ms\n",
            "iter 219: loss 0.0000, time 1604.31ms\n",
            "step 220: train loss 0.2268, val loss 0.4900\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 19579.36ms\n",
            "iter 221: loss 0.0000, time 1697.60ms\n",
            "iter 222: loss 0.0014, time 1653.95ms\n",
            "iter 223: loss 0.0000, time 1617.29ms\n",
            "iter 224: loss 0.0005, time 1667.39ms\n",
            "iter 225: loss 0.0000, time 1750.74ms\n",
            "iter 226: loss 0.0000, time 1646.09ms\n",
            "iter 227: loss 0.0000, time 1617.54ms\n",
            "iter 228: loss 0.0000, time 1605.43ms\n",
            "iter 229: loss 0.0001, time 1575.75ms\n",
            "iter 230: loss 0.0000, time 1664.16ms\n",
            "iter 231: loss 0.0000, time 1705.86ms\n",
            "iter 232: loss 0.0000, time 1566.50ms\n",
            "iter 233: loss 0.0001, time 1596.88ms\n",
            "iter 234: loss 0.0000, time 1596.72ms\n",
            "iter 235: loss 0.0000, time 1555.47ms\n",
            "iter 236: loss 0.0000, time 1583.30ms\n",
            "iter 237: loss 0.0000, time 1600.98ms\n",
            "iter 238: loss 0.0000, time 1586.48ms\n",
            "iter 239: loss 0.0000, time 1589.06ms\n",
            "step 240: train loss 0.2082, val loss 0.4864\n",
            "step val accuracy 0.9827\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 19943.07ms\n",
            "iter 241: loss 0.0000, time 1559.17ms\n",
            "iter 242: loss 0.0032, time 1603.22ms\n",
            "iter 243: loss 0.0001, time 1556.53ms\n",
            "iter 244: loss 0.0000, time 1598.23ms\n",
            "iter 245: loss 2.4375, time 1586.16ms\n",
            "iter 246: loss 0.0001, time 1565.90ms\n",
            "iter 247: loss 0.0000, time 1575.39ms\n",
            "iter 248: loss 0.0000, time 1549.33ms\n",
            "iter 249: loss 0.0000, time 1573.04ms\n",
            "iter 250: loss 0.0000, time 1636.20ms\n",
            "iter 251: loss 0.0000, time 1688.46ms\n",
            "iter 252: loss 0.0000, time 1613.15ms\n",
            "iter 253: loss 0.0000, time 1578.04ms\n",
            "iter 254: loss 0.0000, time 1629.44ms\n",
            "iter 255: loss 0.0000, time 1600.18ms\n",
            "iter 256: loss 0.0000, time 1633.09ms\n",
            "iter 257: loss 0.0593, time 1673.70ms\n",
            "iter 258: loss 0.0000, time 1588.71ms\n",
            "iter 259: loss 0.0000, time 1600.13ms\n",
            "step 260: train loss 0.1926, val loss 0.4935\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 19701.93ms\n",
            "iter 261: loss 0.0000, time 1586.31ms\n",
            "iter 262: loss 0.0000, time 1619.74ms\n",
            "iter 263: loss 0.0000, time 1577.96ms\n",
            "iter 264: loss 0.0000, time 1699.45ms\n",
            "iter 265: loss 0.0000, time 1627.47ms\n",
            "iter 266: loss 0.0000, time 1559.59ms\n",
            "iter 267: loss 0.0000, time 1574.34ms\n",
            "iter 268: loss 0.0000, time 1565.67ms\n",
            "iter 269: loss 0.0000, time 1595.85ms\n",
            "iter 270: loss 0.0000, time 1572.55ms\n",
            "iter 271: loss 0.0000, time 1582.34ms\n",
            "iter 272: loss 0.0334, time 1571.72ms\n",
            "iter 273: loss 0.0000, time 1599.44ms\n",
            "iter 274: loss 0.0000, time 1557.49ms\n",
            "iter 275: loss 0.0000, time 1523.12ms\n",
            "iter 276: loss 0.0000, time 1675.42ms\n",
            "iter 277: loss 0.0000, time 1668.04ms\n",
            "iter 278: loss 0.0000, time 1596.17ms\n",
            "iter 279: loss 0.0000, time 1618.38ms\n",
            "step 280: train loss 0.1791, val loss 0.5053\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 19408.74ms\n",
            "iter 281: loss 0.0000, time 1583.17ms\n",
            "iter 282: loss 0.0000, time 1552.81ms\n",
            "iter 283: loss 0.0000, time 1599.06ms\n",
            "iter 284: loss 0.0000, time 1720.63ms\n",
            "iter 285: loss 0.0000, time 1671.35ms\n",
            "iter 286: loss 0.0000, time 1557.73ms\n",
            "iter 287: loss 0.0000, time 1573.93ms\n",
            "iter 288: loss 0.0000, time 1588.15ms\n",
            "iter 289: loss 0.0001, time 1590.19ms\n",
            "iter 290: loss 0.0000, time 1718.79ms\n",
            "iter 291: loss 0.0000, time 1684.55ms\n",
            "iter 292: loss 0.0000, time 1585.46ms\n",
            "iter 293: loss 0.0000, time 1589.00ms\n",
            "iter 294: loss 0.0000, time 1544.10ms\n",
            "iter 295: loss 0.0000, time 1597.45ms\n",
            "iter 296: loss 0.0000, time 1597.10ms\n",
            "iter 297: loss 0.0000, time 1539.32ms\n",
            "iter 298: loss 0.0000, time 1563.30ms\n",
            "iter 299: loss 0.0000, time 1598.58ms\n",
            "step 300: train loss 0.1673, val loss 0.4981\n",
            "step val accuracy 0.9881\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.8421\n",
            "Test recall 0.7000\n",
            "Test F1 0.6568\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  8  2]]\n",
            "iter 300: loss 0.0000, time 20566.25ms\n",
            "iter 301: loss 0.0000, time 1557.42ms\n",
            "iter 302: loss 0.0000, time 1576.39ms\n",
            "iter 303: loss 0.0000, time 1601.04ms\n",
            "iter 304: loss 0.0000, time 1582.81ms\n",
            "iter 305: loss 0.0000, time 1608.10ms\n",
            "iter 306: loss 0.0000, time 1597.04ms\n",
            "iter 307: loss 0.0000, time 1591.10ms\n",
            "iter 308: loss 0.0005, time 1565.45ms\n",
            "iter 309: loss 0.0000, time 1621.08ms\n",
            "iter 310: loss 0.0019, time 1697.05ms\n",
            "iter 311: loss 0.0000, time 1651.20ms\n",
            "iter 312: loss 0.0014, time 1589.07ms\n",
            "iter 313: loss 0.0000, time 1569.89ms\n",
            "iter 314: loss 0.0000, time 1591.25ms\n",
            "iter 315: loss 0.0000, time 1594.59ms\n",
            "iter 316: loss 0.0000, time 1569.04ms\n",
            "iter 317: loss 0.0001, time 1585.94ms\n",
            "iter 318: loss 0.0000, time 1572.85ms\n",
            "iter 319: loss 0.0000, time 1555.57ms\n",
            "step 320: train loss 0.1572, val loss 0.5042\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 19541.50ms\n",
            "iter 321: loss 0.0000, time 1597.47ms\n",
            "iter 322: loss 0.0000, time 1589.69ms\n",
            "iter 323: loss 0.0000, time 1608.97ms\n",
            "iter 324: loss 0.0000, time 1699.74ms\n",
            "iter 325: loss 0.0184, time 1656.76ms\n",
            "iter 326: loss 0.0004, time 1594.39ms\n",
            "iter 327: loss 0.0000, time 1541.96ms\n",
            "iter 328: loss 0.0000, time 1586.43ms\n",
            "iter 329: loss 0.0000, time 1578.21ms\n",
            "iter 330: loss 0.0000, time 1610.97ms\n",
            "iter 331: loss 0.0000, time 1602.68ms\n",
            "iter 332: loss 0.0000, time 1568.18ms\n",
            "iter 333: loss 0.0000, time 1600.13ms\n",
            "iter 334: loss 0.0000, time 1581.76ms\n",
            "iter 335: loss 0.0000, time 1568.03ms\n",
            "iter 336: loss 0.0001, time 1664.49ms\n",
            "iter 337: loss 0.0000, time 1698.67ms\n",
            "iter 338: loss 0.0000, time 1580.38ms\n",
            "iter 339: loss 0.0000, time 1575.35ms\n",
            "step 340: train loss 0.1485, val loss 0.5087\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 19426.52ms\n",
            "iter 341: loss 0.0000, time 1540.58ms\n",
            "iter 342: loss 0.0000, time 1638.85ms\n",
            "iter 343: loss 0.0049, time 1566.27ms\n",
            "iter 344: loss 0.0000, time 1696.01ms\n",
            "iter 345: loss 0.0000, time 1689.28ms\n",
            "iter 346: loss 0.0000, time 1550.15ms\n",
            "iter 347: loss 0.0000, time 1592.33ms\n",
            "iter 348: loss 0.0001, time 1608.31ms\n",
            "iter 349: loss 0.0000, time 1552.82ms\n",
            "iter 350: loss 0.0000, time 1663.91ms\n",
            "iter 351: loss 0.0000, time 1686.48ms\n",
            "iter 352: loss 0.0000, time 1590.57ms\n",
            "iter 353: loss 0.0000, time 1573.69ms\n",
            "iter 354: loss 0.0000, time 1577.42ms\n",
            "iter 355: loss 0.0000, time 1586.34ms\n",
            "iter 356: loss 0.0000, time 1565.22ms\n",
            "iter 357: loss 0.0000, time 1572.57ms\n",
            "iter 358: loss 0.0000, time 1585.00ms\n",
            "iter 359: loss 0.0000, time 1564.33ms\n",
            "step 360: train loss 0.1403, val loss 0.5622\n",
            "step val accuracy 0.9816\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 19672.64ms\n",
            "iter 361: loss 0.0000, time 1607.13ms\n",
            "iter 362: loss 0.0053, time 1564.41ms\n",
            "iter 363: loss 0.0001, time 1581.46ms\n",
            "iter 364: loss 0.0000, time 1576.05ms\n",
            "iter 365: loss 0.0000, time 1570.75ms\n",
            "iter 366: loss 0.0000, time 1567.49ms\n",
            "iter 367: loss 0.0000, time 1574.54ms\n",
            "iter 368: loss 0.0000, time 1567.62ms\n",
            "iter 369: loss 0.0000, time 1562.15ms\n",
            "iter 370: loss 0.0000, time 1657.90ms\n",
            "iter 371: loss 0.0000, time 1705.19ms\n",
            "iter 372: loss 0.0000, time 1597.51ms\n",
            "iter 373: loss 0.0000, time 1592.46ms\n",
            "iter 374: loss 0.0000, time 1635.65ms\n",
            "iter 375: loss 0.0000, time 1578.45ms\n",
            "iter 376: loss 0.8125, time 1645.14ms\n",
            "iter 377: loss 0.0000, time 1718.67ms\n",
            "iter 378: loss 0.0005, time 1603.47ms\n",
            "iter 379: loss 0.0000, time 1589.95ms\n",
            "step 380: train loss 0.1332, val loss 0.5768\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 19511.03ms\n",
            "iter 381: loss 0.0000, time 1596.90ms\n",
            "iter 382: loss 0.0000, time 1600.43ms\n",
            "iter 383: loss 0.0000, time 1572.75ms\n",
            "iter 384: loss 0.0000, time 1738.28ms\n",
            "iter 385: loss 0.0000, time 1651.96ms\n",
            "iter 386: loss 0.0000, time 1560.57ms\n",
            "iter 387: loss 0.0000, time 1577.46ms\n",
            "iter 388: loss 0.0000, time 1555.73ms\n",
            "iter 389: loss 0.0000, time 1568.57ms\n",
            "iter 390: loss 0.0000, time 1585.64ms\n",
            "iter 391: loss 0.0000, time 1554.24ms\n",
            "iter 392: loss 0.0000, time 1556.61ms\n",
            "iter 393: loss 0.0000, time 1615.32ms\n",
            "iter 394: loss 0.0000, time 1560.56ms\n",
            "iter 395: loss 0.0000, time 1576.54ms\n",
            "iter 396: loss 0.0110, time 1676.38ms\n",
            "iter 397: loss 0.0000, time 1684.36ms\n",
            "iter 398: loss 0.0000, time 1623.77ms\n",
            "iter 399: loss 0.0000, time 1597.87ms\n",
            "step 400: train loss 0.1270, val loss 0.5840\n",
            "step val accuracy 0.9806\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.7875\n",
            "Test recall 0.7333\n",
            "Test F1 0.7243\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [0 9 1]\n",
            " [0 6 4]]\n",
            "iter 400: loss 0.0000, time 20533.95ms\n",
            "iter 401: loss 0.0000, time 1552.40ms\n",
            "iter 402: loss 0.0000, time 1560.31ms\n",
            "iter 403: loss 0.0000, time 1664.87ms\n",
            "iter 404: loss 0.0000, time 1683.50ms\n",
            "iter 405: loss 0.0000, time 1635.52ms\n",
            "iter 406: loss 0.0000, time 1586.52ms\n",
            "iter 407: loss 0.0001, time 1631.75ms\n",
            "iter 408: loss 0.0000, time 1569.90ms\n",
            "iter 409: loss 0.0129, time 1616.71ms\n",
            "iter 410: loss 0.0000, time 1698.50ms\n",
            "iter 411: loss 0.0036, time 1623.97ms\n",
            "iter 412: loss 0.0000, time 1575.63ms\n",
            "iter 413: loss 0.0000, time 1582.16ms\n",
            "iter 414: loss 0.0008, time 1566.54ms\n",
            "iter 415: loss 0.0000, time 1586.66ms\n",
            "iter 416: loss 0.0000, time 1588.69ms\n",
            "iter 417: loss 0.0000, time 1594.91ms\n",
            "iter 418: loss 0.0001, time 1593.28ms\n",
            "iter 419: loss 0.0000, time 1601.74ms\n",
            "step 420: train loss 0.1218, val loss 0.5708\n",
            "step val accuracy 0.9698\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 19824.21ms\n",
            "iter 421: loss 0.0000, time 1545.49ms\n",
            "iter 422: loss 0.0000, time 1553.19ms\n",
            "iter 423: loss 0.0000, time 1576.99ms\n",
            "iter 424: loss 0.0000, time 1577.23ms\n",
            "iter 425: loss 0.0000, time 1550.44ms\n",
            "iter 426: loss 0.0000, time 1579.93ms\n",
            "iter 427: loss 0.0000, time 1571.85ms\n",
            "iter 428: loss 0.0000, time 1548.00ms\n",
            "iter 429: loss 0.0000, time 1593.97ms\n",
            "iter 430: loss 0.0000, time 1727.92ms\n",
            "iter 431: loss 0.0000, time 1656.85ms\n",
            "iter 432: loss 0.0000, time 1579.23ms\n",
            "iter 433: loss 0.0000, time 1602.11ms\n",
            "iter 434: loss 0.0000, time 1586.74ms\n",
            "iter 435: loss 0.0000, time 1550.12ms\n",
            "iter 436: loss 0.0000, time 1722.12ms\n",
            "iter 437: loss 0.0000, time 1685.77ms\n",
            "iter 438: loss 0.0000, time 1586.48ms\n",
            "iter 439: loss 0.0000, time 1554.65ms\n",
            "step 440: train loss 0.1167, val loss 0.5883\n",
            "step val accuracy 0.9741\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 19306.95ms\n",
            "iter 441: loss 0.0000, time 1610.86ms\n",
            "iter 442: loss 0.0000, time 1576.27ms\n",
            "iter 443: loss 0.0000, time 1580.15ms\n",
            "iter 444: loss 0.0000, time 1664.32ms\n",
            "iter 445: loss 0.0000, time 1652.47ms\n",
            "iter 446: loss 0.0000, time 1549.33ms\n",
            "iter 447: loss 0.0000, time 1554.26ms\n",
            "iter 448: loss 0.0000, time 1566.24ms\n",
            "iter 449: loss 0.0000, time 1555.80ms\n",
            "iter 450: loss 0.0000, time 1569.05ms\n",
            "iter 451: loss 0.0026, time 1572.78ms\n",
            "iter 452: loss 0.0000, time 1579.33ms\n",
            "iter 453: loss 0.0000, time 1601.71ms\n",
            "iter 454: loss 0.0000, time 1574.61ms\n",
            "iter 455: loss 0.0000, time 1566.52ms\n",
            "iter 456: loss 0.0610, time 1676.56ms\n",
            "iter 457: loss 0.0000, time 1673.58ms\n",
            "iter 458: loss 0.0000, time 1573.74ms\n",
            "iter 459: loss 0.0000, time 1579.34ms\n",
            "step 460: train loss 0.1118, val loss 0.5995\n",
            "step val accuracy 0.9741\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 19345.89ms\n",
            "iter 461: loss 0.0000, time 1603.29ms\n",
            "iter 462: loss 0.0000, time 1553.79ms\n",
            "iter 463: loss 0.0000, time 1548.20ms\n",
            "iter 464: loss 0.0000, time 1735.27ms\n",
            "iter 465: loss 0.0000, time 1665.51ms\n",
            "iter 466: loss 0.0000, time 1551.85ms\n",
            "iter 467: loss 0.0000, time 1603.54ms\n",
            "iter 468: loss 0.0000, time 1567.37ms\n",
            "iter 469: loss 0.0000, time 1571.90ms\n",
            "iter 470: loss 0.0000, time 1715.76ms\n",
            "iter 471: loss 0.0000, time 1731.11ms\n",
            "iter 472: loss 0.0000, time 1575.82ms\n",
            "iter 473: loss 0.0000, time 1584.09ms\n",
            "iter 474: loss 0.0000, time 1562.15ms\n",
            "iter 475: loss 0.0000, time 1585.79ms\n",
            "iter 476: loss 0.2109, time 1560.58ms\n",
            "iter 477: loss 0.0000, time 1569.78ms\n",
            "iter 478: loss 0.0000, time 1629.49ms\n",
            "iter 479: loss 0.0000, time 1580.46ms\n",
            "step 480: train loss 0.1084, val loss 0.5924\n",
            "step val accuracy 0.9600\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 19799.65ms\n",
            "iter 481: loss 0.0464, time 1653.68ms\n",
            "iter 482: loss 0.0000, time 1582.11ms\n",
            "iter 483: loss 0.0000, time 1606.61ms\n",
            "iter 484: loss 0.0001, time 1567.98ms\n",
            "iter 485: loss 0.0000, time 1565.10ms\n",
            "iter 486: loss 0.0000, time 1553.06ms\n",
            "iter 487: loss 0.0000, time 1596.52ms\n",
            "iter 488: loss 0.0000, time 1543.78ms\n",
            "iter 489: loss 0.0000, time 1552.62ms\n",
            "iter 490: loss 0.0000, time 1653.06ms\n",
            "iter 491: loss 0.0000, time 1725.31ms\n",
            "iter 492: loss 0.0000, time 1589.12ms\n",
            "iter 493: loss 0.0000, time 1589.24ms\n",
            "iter 494: loss 0.0000, time 1609.78ms\n",
            "iter 495: loss 0.0000, time 1559.01ms\n",
            "iter 496: loss 0.0001, time 1642.46ms\n",
            "iter 497: loss 0.0000, time 1711.11ms\n",
            "iter 498: loss 0.0000, time 1595.88ms\n",
            "iter 499: loss 0.0003, time 1556.30ms\n",
            "step 500: train loss 0.1044, val loss 0.5735\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8627\n",
            "Test recall 0.7667\n",
            "Test F1 0.7532\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  6  4]]\n",
            "iter 500: loss 0.0000, time 20924.46ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▂▅▆▇█</td></tr><tr><td>Test_F1_Score</td><td>▁▂▄▆▇█</td></tr><tr><td>Test_Precision</td><td>▁▁▃█▇█</td></tr><tr><td>Test_Recall</td><td>▁▂▅▆▇█</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▄▇▇█▇████████████████████</td></tr><tr><td>val/loss</td><td> █▅▃▃▂▁▂▁▁▂▂▂▂▂▂▂▂▃▄▄▃▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.76667</td></tr><tr><td>Test_F1_Score</td><td>0.75318</td></tr><tr><td>Test_Precision</td><td>0.86275</td></tr><tr><td>Test_Recall</td><td>0.76667</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.10436</td></tr><tr><td>val/acc</td><td>0.98488</td></tr><tr><td>val/loss</td><td>0.57346</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vibrant-sweep-12</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/a6njhdsm' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/a6njhdsm</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_044607-a6njhdsm\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: krjde92l with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_050921-krjde92l</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/krjde92l' target=\"_blank\">sparkling-sweep-13</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/krjde92l' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/krjde92l</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3445\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.5039, time 31205.92ms\n",
            "iter 1: loss 0.6523, time 1804.40ms\n",
            "iter 2: loss 1.7656, time 1710.59ms\n",
            "iter 3: loss 0.9141, time 1667.86ms\n",
            "iter 4: loss 1.2188, time 1677.60ms\n",
            "iter 5: loss 0.6094, time 1671.07ms\n",
            "iter 6: loss 0.9570, time 1734.22ms\n",
            "iter 7: loss 1.3281, time 1745.96ms\n",
            "iter 8: loss 0.6094, time 1684.41ms\n",
            "iter 9: loss 1.0391, time 1675.96ms\n",
            "iter 10: loss 1.1953, time 1673.94ms\n",
            "iter 11: loss 0.8398, time 1650.63ms\n",
            "iter 12: loss 0.7500, time 1688.87ms\n",
            "iter 13: loss 0.8711, time 1646.01ms\n",
            "iter 14: loss 0.8125, time 1682.33ms\n",
            "iter 15: loss 0.7305, time 1659.38ms\n",
            "iter 16: loss 0.5937, time 1635.84ms\n",
            "iter 17: loss 0.5625, time 1673.59ms\n",
            "iter 18: loss 0.5469, time 1758.70ms\n",
            "iter 19: loss 0.5547, time 1780.47ms\n",
            "step 20: train loss 1.1090, val loss 0.9975\n",
            "step val accuracy 0.8047\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.4531, time 16736.43ms\n",
            "iter 21: loss 0.4336, time 1675.51ms\n",
            "iter 22: loss 1.8516, time 1699.19ms\n",
            "iter 23: loss 0.4824, time 1668.38ms\n",
            "iter 24: loss 0.0659, time 1656.65ms\n",
            "iter 25: loss 0.0845, time 1680.25ms\n",
            "iter 26: loss 0.1387, time 1666.78ms\n",
            "iter 27: loss 0.2813, time 1735.76ms\n",
            "iter 28: loss 0.3730, time 1720.75ms\n",
            "iter 29: loss 0.2393, time 1670.49ms\n",
            "iter 30: loss 2.3906, time 1627.93ms\n",
            "iter 31: loss 0.2949, time 1629.38ms\n",
            "iter 32: loss 0.2949, time 1633.42ms\n",
            "iter 33: loss 0.0090, time 1694.19ms\n",
            "iter 34: loss 0.4551, time 1697.98ms\n",
            "iter 35: loss 0.4023, time 1640.96ms\n",
            "iter 36: loss 0.9688, time 1662.83ms\n",
            "iter 37: loss 0.1069, time 1617.71ms\n",
            "iter 38: loss 0.1099, time 1610.16ms\n",
            "iter 39: loss 0.1406, time 1689.13ms\n",
            "step 40: train loss 0.8141, val loss 0.8488\n",
            "step val accuracy 0.9068\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0767, time 16421.23ms\n",
            "iter 41: loss 0.1138, time 1642.77ms\n",
            "iter 42: loss 0.1040, time 1644.90ms\n",
            "iter 43: loss 0.2773, time 1723.90ms\n",
            "iter 44: loss 0.0144, time 1710.56ms\n",
            "iter 45: loss 0.1582, time 1610.11ms\n",
            "iter 46: loss 0.0420, time 1603.22ms\n",
            "iter 47: loss 0.0918, time 1630.41ms\n",
            "iter 48: loss 0.0203, time 1627.84ms\n",
            "iter 49: loss 0.1182, time 1615.93ms\n",
            "iter 50: loss 0.0240, time 1639.15ms\n",
            "iter 51: loss 0.5078, time 1643.26ms\n",
            "iter 52: loss 0.0532, time 1604.02ms\n",
            "iter 53: loss 0.0120, time 1642.17ms\n",
            "iter 54: loss 0.1025, time 1630.76ms\n",
            "iter 55: loss 0.0010, time 1757.75ms\n",
            "iter 56: loss 0.0012, time 1713.71ms\n",
            "iter 57: loss 0.0002, time 1663.11ms\n",
            "iter 58: loss 0.0038, time 1624.29ms\n",
            "iter 59: loss 0.0020, time 1653.53ms\n",
            "step 60: train loss 0.6070, val loss 0.7601\n",
            "step val accuracy 0.8476\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0013, time 16558.30ms\n",
            "iter 61: loss 0.0039, time 1665.32ms\n",
            "iter 62: loss 0.0008, time 1630.48ms\n",
            "iter 63: loss 0.0004, time 1640.77ms\n",
            "iter 64: loss 0.0045, time 1726.18ms\n",
            "iter 65: loss 0.3926, time 1756.50ms\n",
            "iter 66: loss 0.0684, time 1645.84ms\n",
            "iter 67: loss 0.1270, time 1651.10ms\n",
            "iter 68: loss 0.0003, time 1594.80ms\n",
            "iter 69: loss 0.0003, time 1626.78ms\n",
            "iter 70: loss 0.0001, time 1752.06ms\n",
            "iter 71: loss 0.0007, time 1679.79ms\n",
            "iter 72: loss 0.0004, time 1647.38ms\n",
            "iter 73: loss 0.0101, time 1713.99ms\n",
            "iter 74: loss 0.0002, time 1657.60ms\n",
            "iter 75: loss 0.0295, time 1612.26ms\n",
            "iter 76: loss 0.0177, time 1635.49ms\n",
            "iter 77: loss 0.0674, time 1677.41ms\n",
            "iter 78: loss 0.0144, time 1641.52ms\n",
            "iter 79: loss 0.0045, time 1647.65ms\n",
            "step 80: train loss 0.4875, val loss 0.6072\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0077, time 16852.89ms\n",
            "iter 81: loss 0.9688, time 1627.81ms\n",
            "iter 82: loss 0.0041, time 1666.62ms\n",
            "iter 83: loss 0.0015, time 1635.76ms\n",
            "iter 84: loss 0.0010, time 1612.77ms\n",
            "iter 85: loss 0.0039, time 1632.92ms\n",
            "iter 86: loss 0.0007, time 1644.98ms\n",
            "iter 87: loss 0.0048, time 1654.11ms\n",
            "iter 88: loss 0.0003, time 1605.53ms\n",
            "iter 89: loss 0.2676, time 1622.50ms\n",
            "iter 90: loss 0.0001, time 1621.72ms\n",
            "iter 91: loss 0.0400, time 1749.62ms\n",
            "iter 92: loss 0.0004, time 1730.37ms\n",
            "iter 93: loss 0.0008, time 1660.44ms\n",
            "iter 94: loss 0.0002, time 1670.34ms\n",
            "iter 95: loss 0.0471, time 1654.08ms\n",
            "iter 96: loss 0.0913, time 1616.82ms\n",
            "iter 97: loss 0.0000, time 1737.16ms\n",
            "iter 98: loss 0.0018, time 1768.06ms\n",
            "iter 99: loss 0.0000, time 1621.33ms\n",
            "step 100: train loss 0.4023, val loss 0.5302\n",
            "step val accuracy 0.9497\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8667\n",
            "Test precision 0.8778\n",
            "Test recall 0.8667\n",
            "Test F1 0.8660\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 2  8  0]\n",
            " [ 0  2  8]]\n",
            "iter 100: loss 0.0000, time 17294.86ms\n",
            "iter 101: loss 0.0032, time 1709.87ms\n",
            "iter 102: loss 0.0001, time 1642.49ms\n",
            "iter 103: loss 0.0014, time 1636.39ms\n",
            "iter 104: loss 0.0011, time 1649.12ms\n",
            "iter 105: loss 0.0009, time 1632.37ms\n",
            "iter 106: loss 0.0003, time 1762.21ms\n",
            "iter 107: loss 0.0001, time 1686.23ms\n",
            "iter 108: loss 0.0417, time 1652.92ms\n",
            "iter 109: loss 0.0082, time 1648.67ms\n",
            "iter 110: loss 0.0000, time 1635.81ms\n",
            "iter 111: loss 0.0001, time 1655.01ms\n",
            "iter 112: loss 0.0038, time 1679.68ms\n",
            "iter 113: loss 0.0015, time 1643.63ms\n",
            "iter 114: loss 0.0006, time 1612.79ms\n",
            "iter 115: loss 0.0002, time 1642.20ms\n",
            "iter 116: loss 0.0000, time 1624.02ms\n",
            "iter 117: loss 0.0034, time 1638.81ms\n",
            "iter 118: loss 0.0014, time 1731.87ms\n",
            "iter 119: loss 0.0013, time 1728.39ms\n",
            "step 120: train loss 0.3425, val loss 0.4796\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0002, time 16456.31ms\n",
            "iter 121: loss 0.0454, time 1629.26ms\n",
            "iter 122: loss 0.0007, time 1630.99ms\n",
            "iter 123: loss 0.0005, time 1614.67ms\n",
            "iter 124: loss 0.0000, time 1649.85ms\n",
            "iter 125: loss 0.0002, time 1623.29ms\n",
            "iter 126: loss 0.0000, time 1704.79ms\n",
            "iter 127: loss 0.0002, time 1764.75ms\n",
            "iter 128: loss 0.0002, time 1733.15ms\n",
            "iter 129: loss 0.0001, time 1636.89ms\n",
            "iter 130: loss 0.0002, time 1657.97ms\n",
            "iter 131: loss 0.0000, time 1658.66ms\n",
            "iter 132: loss 0.0001, time 1669.97ms\n",
            "iter 133: loss 0.0003, time 1691.07ms\n",
            "iter 134: loss 0.0002, time 1709.79ms\n",
            "iter 135: loss 0.0002, time 1644.73ms\n",
            "iter 136: loss 0.0000, time 1659.42ms\n",
            "iter 137: loss 0.0000, time 1619.97ms\n",
            "iter 138: loss 0.0000, time 1634.30ms\n",
            "iter 139: loss 0.0004, time 1611.93ms\n",
            "step 140: train loss 0.3004, val loss 0.4761\n",
            "step val accuracy 0.9601\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0002, time 16504.78ms\n",
            "iter 141: loss 0.0000, time 1630.13ms\n",
            "iter 142: loss 0.0017, time 1732.14ms\n",
            "iter 143: loss 0.0005, time 1746.15ms\n",
            "iter 144: loss 0.0002, time 1628.35ms\n",
            "iter 145: loss 0.0002, time 1645.03ms\n",
            "iter 146: loss 0.0000, time 1650.67ms\n",
            "iter 147: loss 0.0001, time 1623.11ms\n",
            "iter 148: loss 0.0001, time 1616.41ms\n",
            "iter 149: loss 0.0005, time 1648.68ms\n",
            "iter 150: loss 0.0003, time 1639.97ms\n",
            "iter 151: loss 0.0003, time 1636.34ms\n",
            "iter 152: loss 0.0002, time 1618.03ms\n",
            "iter 153: loss 0.0000, time 1641.85ms\n",
            "iter 154: loss 0.0003, time 1771.31ms\n",
            "iter 155: loss 0.0105, time 1748.18ms\n",
            "iter 156: loss 0.0001, time 1671.95ms\n",
            "iter 157: loss 0.0024, time 1610.47ms\n",
            "iter 158: loss 0.0000, time 1636.54ms\n",
            "iter 159: loss 0.0125, time 1643.54ms\n",
            "step 160: train loss 0.2649, val loss 0.4388\n",
            "step val accuracy 0.9896\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0001, time 16573.51ms\n",
            "iter 161: loss 0.0019, time 1612.91ms\n",
            "iter 162: loss 0.0005, time 1652.80ms\n",
            "iter 163: loss 0.0030, time 1713.62ms\n",
            "iter 164: loss 0.0000, time 1731.49ms\n",
            "iter 165: loss 0.0001, time 1662.67ms\n",
            "iter 166: loss 0.0000, time 1611.68ms\n",
            "iter 167: loss 0.0000, time 1666.57ms\n",
            "iter 168: loss 0.0000, time 1628.69ms\n",
            "iter 169: loss 0.0000, time 1667.01ms\n",
            "iter 170: loss 0.0000, time 1755.29ms\n",
            "iter 171: loss 0.0000, time 1657.46ms\n",
            "iter 172: loss 0.2715, time 1671.93ms\n",
            "iter 173: loss 0.0000, time 1636.50ms\n",
            "iter 174: loss 0.0002, time 1706.84ms\n",
            "iter 175: loss 0.0000, time 1683.45ms\n",
            "iter 176: loss 0.0000, time 1613.34ms\n",
            "iter 177: loss 0.0000, time 1634.56ms\n",
            "iter 178: loss 0.0013, time 1626.32ms\n",
            "iter 179: loss 0.0001, time 1655.30ms\n",
            "step 180: train loss 0.2371, val loss 0.3999\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0001, time 16796.90ms\n",
            "iter 181: loss 0.0001, time 1671.74ms\n",
            "iter 182: loss 0.0001, time 1647.18ms\n",
            "iter 183: loss 0.0003, time 1622.21ms\n",
            "iter 184: loss 0.0000, time 1635.05ms\n",
            "iter 185: loss 0.0000, time 1636.53ms\n",
            "iter 186: loss 0.0000, time 1662.46ms\n",
            "iter 187: loss 0.0000, time 1643.57ms\n",
            "iter 188: loss 0.0000, time 1621.95ms\n",
            "iter 189: loss 0.0000, time 1641.42ms\n",
            "iter 190: loss 0.0000, time 1685.41ms\n",
            "iter 191: loss 0.0000, time 1740.32ms\n",
            "iter 192: loss 0.0114, time 1693.77ms\n",
            "iter 193: loss 0.0000, time 1619.90ms\n",
            "iter 194: loss 0.0001, time 1628.74ms\n",
            "iter 195: loss 0.0000, time 1640.63ms\n",
            "iter 196: loss 0.0000, time 1664.16ms\n",
            "iter 197: loss 0.0000, time 1740.33ms\n",
            "iter 198: loss 0.0001, time 1693.56ms\n",
            "iter 199: loss 0.0031, time 1660.96ms\n",
            "step 200: train loss 0.2145, val loss 0.3687\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8535\n",
            "Test recall 0.8333\n",
            "Test F1 0.8315\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 2  8  0]\n",
            " [ 0  3  7]]\n",
            "iter 200: loss 0.0034, time 17598.96ms\n",
            "iter 201: loss 0.0003, time 1726.72ms\n",
            "iter 202: loss 0.0002, time 1769.10ms\n",
            "iter 203: loss 0.0034, time 1641.43ms\n",
            "iter 204: loss 0.0000, time 1674.27ms\n",
            "iter 205: loss 0.0001, time 1809.33ms\n",
            "iter 206: loss 0.0000, time 1681.77ms\n",
            "iter 207: loss 0.0003, time 1652.76ms\n",
            "iter 208: loss 0.0000, time 1634.40ms\n",
            "iter 209: loss 0.0000, time 1636.75ms\n",
            "iter 210: loss 0.0000, time 1613.49ms\n",
            "iter 211: loss 0.0000, time 1641.61ms\n",
            "iter 212: loss 0.0000, time 1658.36ms\n",
            "iter 213: loss 0.0000, time 1658.76ms\n",
            "iter 214: loss 0.0000, time 1636.31ms\n",
            "iter 215: loss 0.0000, time 1609.48ms\n",
            "iter 216: loss 0.0001, time 1648.92ms\n",
            "iter 217: loss 0.0000, time 1778.63ms\n",
            "iter 218: loss 0.0002, time 1721.61ms\n",
            "iter 219: loss 0.0001, time 1624.97ms\n",
            "step 220: train loss 0.1960, val loss 0.3652\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 16617.80ms\n",
            "iter 221: loss 0.0000, time 1634.04ms\n",
            "iter 222: loss 0.0000, time 1585.47ms\n",
            "iter 223: loss 0.0000, time 1611.24ms\n",
            "iter 224: loss 0.0000, time 1635.43ms\n",
            "iter 225: loss 0.0299, time 1622.52ms\n",
            "iter 226: loss 0.0000, time 1735.78ms\n",
            "iter 227: loss 0.0000, time 1913.83ms\n",
            "iter 228: loss 0.0000, time 1647.08ms\n",
            "iter 229: loss 0.0000, time 1643.68ms\n",
            "iter 230: loss 0.0000, time 1640.37ms\n",
            "iter 231: loss 0.0000, time 1613.57ms\n",
            "iter 232: loss 0.0000, time 1780.91ms\n",
            "iter 233: loss 0.0000, time 1716.73ms\n",
            "iter 234: loss 0.0001, time 1658.08ms\n",
            "iter 235: loss 0.0001, time 1621.72ms\n",
            "iter 236: loss 0.0000, time 1625.17ms\n",
            "iter 237: loss 0.0001, time 1638.62ms\n",
            "iter 238: loss 0.0002, time 1642.54ms\n",
            "iter 239: loss 0.0005, time 1647.25ms\n",
            "step 240: train loss 0.1826, val loss 0.3606\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0000, time 16526.77ms\n",
            "iter 241: loss 0.0000, time 1762.61ms\n",
            "iter 242: loss 0.0000, time 1760.14ms\n",
            "iter 243: loss 0.0003, time 1647.00ms\n",
            "iter 244: loss 0.0000, time 1644.00ms\n",
            "iter 245: loss 0.0000, time 1662.25ms\n",
            "iter 246: loss 0.0000, time 1632.49ms\n",
            "iter 247: loss 0.0000, time 1636.73ms\n",
            "iter 248: loss 0.0000, time 1641.02ms\n",
            "iter 249: loss 0.0001, time 1627.13ms\n",
            "iter 250: loss 0.0014, time 1612.39ms\n",
            "iter 251: loss 0.0000, time 1682.65ms\n",
            "iter 252: loss 0.0000, time 1705.75ms\n",
            "iter 253: loss 0.0000, time 1768.21ms\n",
            "iter 254: loss 0.0002, time 1728.62ms\n",
            "iter 255: loss 0.0000, time 1628.73ms\n",
            "iter 256: loss 0.0000, time 1698.70ms\n",
            "iter 257: loss 0.0000, time 1650.84ms\n",
            "iter 258: loss 0.0000, time 1615.43ms\n",
            "iter 259: loss 0.0000, time 1737.77ms\n",
            "step 260: train loss 0.1695, val loss 0.3564\n",
            "step val accuracy 0.9882\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 16379.55ms\n",
            "iter 261: loss 0.0059, time 1639.57ms\n",
            "iter 262: loss 0.0081, time 1732.93ms\n",
            "iter 263: loss 0.0000, time 1780.68ms\n",
            "iter 264: loss 0.0000, time 1667.40ms\n",
            "iter 265: loss 0.0000, time 1645.74ms\n",
            "iter 266: loss 0.0001, time 1618.65ms\n",
            "iter 267: loss 0.0000, time 1613.41ms\n",
            "iter 268: loss 0.0000, time 1690.58ms\n",
            "iter 269: loss 0.0000, time 1740.72ms\n",
            "iter 270: loss 0.0000, time 1691.54ms\n",
            "iter 271: loss 0.0000, time 1597.96ms\n",
            "iter 272: loss 0.0001, time 1761.44ms\n",
            "iter 273: loss 0.0000, time 1657.35ms\n",
            "iter 274: loss 0.0000, time 1658.79ms\n",
            "iter 275: loss 0.0000, time 1684.42ms\n",
            "iter 276: loss 0.0162, time 1720.68ms\n",
            "iter 277: loss 0.0000, time 1657.54ms\n",
            "iter 278: loss 0.0000, time 1656.94ms\n",
            "iter 279: loss 0.0000, time 1641.26ms\n",
            "step 280: train loss 0.1577, val loss 0.3395\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 16856.14ms\n",
            "iter 281: loss 0.0000, time 1657.49ms\n",
            "iter 282: loss 0.0000, time 1638.11ms\n",
            "iter 283: loss 0.0000, time 1672.70ms\n",
            "iter 284: loss 0.0000, time 1656.93ms\n",
            "iter 285: loss 0.0000, time 1654.11ms\n",
            "iter 286: loss 0.0001, time 1609.75ms\n",
            "iter 287: loss 0.0000, time 1611.35ms\n",
            "iter 288: loss 0.0000, time 1651.77ms\n",
            "iter 289: loss 0.0000, time 1723.81ms\n",
            "iter 290: loss 0.0001, time 1750.68ms\n",
            "iter 291: loss 0.0000, time 1654.58ms\n",
            "iter 292: loss 0.0000, time 1641.36ms\n",
            "iter 293: loss 0.0000, time 1657.20ms\n",
            "iter 294: loss 0.0000, time 1643.38ms\n",
            "iter 295: loss 0.0000, time 1735.27ms\n",
            "iter 296: loss 0.0000, time 1786.95ms\n",
            "iter 297: loss 0.0000, time 1658.73ms\n",
            "iter 298: loss 0.0000, time 1662.49ms\n",
            "iter 299: loss 0.0000, time 1624.81ms\n",
            "step 300: train loss 0.1472, val loss 0.3368\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8889\n",
            "Test recall 0.8333\n",
            "Test F1 0.8325\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  4  6]]\n",
            "iter 300: loss 0.0000, time 17388.35ms\n",
            "iter 301: loss 0.0000, time 1623.15ms\n",
            "iter 302: loss 0.0000, time 1624.22ms\n",
            "iter 303: loss 0.0000, time 1633.68ms\n",
            "iter 304: loss 0.0000, time 1731.07ms\n",
            "iter 305: loss 0.0001, time 1699.39ms\n",
            "iter 306: loss 0.0007, time 1721.50ms\n",
            "iter 307: loss 0.0000, time 1697.23ms\n",
            "iter 308: loss 0.0000, time 1581.91ms\n",
            "iter 309: loss 0.0000, time 1647.41ms\n",
            "iter 310: loss 0.0000, time 1642.92ms\n",
            "iter 311: loss 0.0000, time 1655.52ms\n",
            "iter 312: loss 0.0000, time 1604.41ms\n",
            "iter 313: loss 0.0007, time 1610.60ms\n",
            "iter 314: loss 0.0000, time 1634.08ms\n",
            "iter 315: loss 0.0000, time 1652.48ms\n",
            "iter 316: loss 0.0000, time 1704.33ms\n",
            "iter 317: loss 0.0000, time 1749.89ms\n",
            "iter 318: loss 0.0000, time 1628.03ms\n",
            "iter 319: loss 0.0000, time 1656.36ms\n",
            "step 320: train loss 0.1380, val loss 0.3381\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 16543.88ms\n",
            "iter 321: loss 0.0000, time 1625.96ms\n",
            "iter 322: loss 0.0000, time 1628.79ms\n",
            "iter 323: loss 0.0000, time 1621.38ms\n",
            "iter 324: loss 0.0001, time 1611.07ms\n",
            "iter 325: loss 0.0000, time 1762.09ms\n",
            "iter 326: loss 0.0000, time 1756.70ms\n",
            "iter 327: loss 0.0000, time 1632.91ms\n",
            "iter 328: loss 0.0000, time 1657.59ms\n",
            "iter 329: loss 0.0000, time 1657.29ms\n",
            "iter 330: loss 0.0000, time 1631.58ms\n",
            "iter 331: loss 0.0000, time 1711.29ms\n",
            "iter 332: loss 0.0000, time 1738.12ms\n",
            "iter 333: loss 0.0000, time 1652.12ms\n",
            "iter 334: loss 0.0000, time 1625.89ms\n",
            "iter 335: loss 0.0000, time 1630.93ms\n",
            "iter 336: loss 0.0000, time 1623.64ms\n",
            "iter 337: loss 0.0000, time 1640.42ms\n",
            "iter 338: loss 0.0000, time 1641.39ms\n",
            "iter 339: loss 0.0000, time 1624.63ms\n",
            "step 340: train loss 0.1299, val loss 0.3361\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 16678.44ms\n",
            "iter 341: loss 0.0000, time 1758.89ms\n",
            "iter 342: loss 0.0002, time 1710.88ms\n",
            "iter 343: loss 0.0001, time 1649.12ms\n",
            "iter 344: loss 0.0000, time 1603.29ms\n",
            "iter 345: loss 0.0000, time 1627.17ms\n",
            "iter 346: loss 0.0000, time 1660.40ms\n",
            "iter 347: loss 0.0000, time 1640.60ms\n",
            "iter 348: loss 0.0000, time 1592.51ms\n",
            "iter 349: loss 0.0000, time 1648.39ms\n",
            "iter 350: loss 0.0000, time 1638.37ms\n",
            "iter 351: loss 0.0000, time 1649.73ms\n",
            "iter 352: loss 0.0000, time 1687.08ms\n",
            "iter 353: loss 0.0000, time 1780.38ms\n",
            "iter 354: loss 0.0000, time 1642.86ms\n",
            "iter 355: loss 0.0000, time 1658.32ms\n",
            "iter 356: loss 0.0000, time 1603.12ms\n",
            "iter 357: loss 0.0000, time 1635.44ms\n",
            "iter 358: loss 0.0000, time 1687.95ms\n",
            "iter 359: loss 0.0000, time 1738.73ms\n",
            "step 360: train loss 0.1227, val loss 0.3526\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 16350.77ms\n",
            "iter 361: loss 0.0000, time 1672.16ms\n",
            "iter 362: loss 0.0000, time 1758.59ms\n",
            "iter 363: loss 0.0000, time 1673.02ms\n",
            "iter 364: loss 0.0000, time 1594.50ms\n",
            "iter 365: loss 0.0000, time 1658.56ms\n",
            "iter 366: loss 0.0000, time 1617.64ms\n",
            "iter 367: loss 0.0000, time 1649.36ms\n",
            "iter 368: loss 0.0000, time 1741.28ms\n",
            "iter 369: loss 0.0000, time 1721.85ms\n",
            "iter 370: loss 0.0000, time 1637.22ms\n",
            "iter 371: loss 0.0000, time 1616.95ms\n",
            "iter 372: loss 0.0000, time 1640.25ms\n",
            "iter 373: loss 0.0000, time 1639.00ms\n",
            "iter 374: loss 0.0000, time 1634.49ms\n",
            "iter 375: loss 0.0000, time 1629.45ms\n",
            "iter 376: loss 0.0000, time 1616.27ms\n",
            "iter 377: loss 0.0000, time 1637.55ms\n",
            "iter 378: loss 0.0000, time 1647.30ms\n",
            "iter 379: loss 0.0000, time 1648.89ms\n",
            "step 380: train loss 0.1162, val loss 0.3415\n",
            "step val accuracy 0.9882\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 16848.44ms\n",
            "iter 381: loss 0.0000, time 1642.33ms\n",
            "iter 382: loss 0.0000, time 1716.73ms\n",
            "iter 383: loss 0.0000, time 1664.92ms\n",
            "iter 384: loss 0.0000, time 1617.57ms\n",
            "iter 385: loss 0.0000, time 1765.56ms\n",
            "iter 386: loss 0.0000, time 1885.56ms\n",
            "iter 387: loss 0.0000, time 1747.07ms\n",
            "iter 388: loss 0.0000, time 1686.57ms\n",
            "iter 389: loss 0.0000, time 1753.88ms\n",
            "iter 390: loss 0.0000, time 1675.01ms\n",
            "iter 391: loss 0.0000, time 1748.60ms\n",
            "iter 392: loss 0.0000, time 1640.26ms\n",
            "iter 393: loss 0.0000, time 1622.63ms\n",
            "iter 394: loss 0.0000, time 1695.15ms\n",
            "iter 395: loss 0.0000, time 1743.31ms\n",
            "iter 396: loss 0.0000, time 1712.83ms\n",
            "iter 397: loss 0.0000, time 1615.20ms\n",
            "iter 398: loss 0.0000, time 1638.18ms\n",
            "iter 399: loss 0.0000, time 1625.60ms\n",
            "step 400: train loss 0.1104, val loss 0.3429\n",
            "step val accuracy 0.9882\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8476\n",
            "Test recall 0.8000\n",
            "Test F1 0.8000\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 4 6]]\n",
            "iter 400: loss 0.0000, time 18029.84ms\n",
            "iter 401: loss 0.0000, time 1672.61ms\n",
            "iter 402: loss 0.0000, time 1636.60ms\n",
            "iter 403: loss 0.0000, time 1731.66ms\n",
            "iter 404: loss 0.0000, time 1697.32ms\n",
            "iter 405: loss 0.0000, time 1640.40ms\n",
            "iter 406: loss 0.0001, time 1668.33ms\n",
            "iter 407: loss 0.0001, time 1651.77ms\n",
            "iter 408: loss 0.0000, time 1642.48ms\n",
            "iter 409: loss 0.0000, time 1658.03ms\n",
            "iter 410: loss 0.0000, time 1641.87ms\n",
            "iter 411: loss 0.0000, time 1592.37ms\n",
            "iter 412: loss 0.0000, time 1626.71ms\n",
            "iter 413: loss 0.0000, time 1649.62ms\n",
            "iter 414: loss 0.0000, time 1642.52ms\n",
            "iter 415: loss 0.0000, time 1738.20ms\n",
            "iter 416: loss 0.0000, time 1691.08ms\n",
            "iter 417: loss 0.0000, time 1652.42ms\n",
            "iter 418: loss 0.0000, time 1658.43ms\n",
            "iter 419: loss 0.0000, time 1625.16ms\n",
            "step 420: train loss 0.1051, val loss 0.3398\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 16547.98ms\n",
            "iter 421: loss 0.0000, time 1641.57ms\n",
            "iter 422: loss 0.0000, time 1619.47ms\n",
            "iter 423: loss 0.0000, time 1668.32ms\n",
            "iter 424: loss 0.0000, time 1780.67ms\n",
            "iter 425: loss 0.0000, time 1727.91ms\n",
            "iter 426: loss 0.0000, time 1655.28ms\n",
            "iter 427: loss 0.0000, time 1648.12ms\n",
            "iter 428: loss 0.0000, time 1631.65ms\n",
            "iter 429: loss 0.0000, time 1657.48ms\n",
            "iter 430: loss 0.0000, time 1709.38ms\n",
            "iter 431: loss 0.0000, time 1770.98ms\n",
            "iter 432: loss 0.0000, time 1631.91ms\n",
            "iter 433: loss 0.0000, time 1621.49ms\n",
            "iter 434: loss 0.0000, time 1625.62ms\n",
            "iter 435: loss 0.0000, time 1626.80ms\n",
            "iter 436: loss 0.0000, time 1638.57ms\n",
            "iter 437: loss 0.0000, time 1629.82ms\n",
            "iter 438: loss 0.0000, time 1645.97ms\n",
            "iter 439: loss 0.0000, time 1596.48ms\n",
            "step 440: train loss 0.1004, val loss 0.3371\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 17162.95ms\n",
            "iter 441: loss 0.0000, time 1651.36ms\n",
            "iter 442: loss 0.0000, time 1631.91ms\n",
            "iter 443: loss 0.0000, time 1632.70ms\n",
            "iter 444: loss 0.0000, time 1672.50ms\n",
            "iter 445: loss 0.0000, time 1661.13ms\n",
            "iter 446: loss 0.0000, time 1635.78ms\n",
            "iter 447: loss 0.0000, time 1643.91ms\n",
            "iter 448: loss 0.0000, time 1645.75ms\n",
            "iter 449: loss 0.0000, time 1658.51ms\n",
            "iter 450: loss 0.0000, time 1630.67ms\n",
            "iter 451: loss 0.0000, time 1835.40ms\n",
            "iter 452: loss 0.0000, time 1803.98ms\n",
            "iter 453: loss 0.0000, time 1645.41ms\n",
            "iter 454: loss 0.0000, time 1681.65ms\n",
            "iter 455: loss 0.0000, time 1644.68ms\n",
            "iter 456: loss 0.0000, time 1622.57ms\n",
            "iter 457: loss 0.0000, time 1789.91ms\n",
            "iter 458: loss 0.0000, time 1727.35ms\n",
            "iter 459: loss 0.0000, time 1658.35ms\n",
            "step 460: train loss 0.0960, val loss 0.3390\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 16503.72ms\n",
            "iter 461: loss 0.0000, time 1713.20ms\n",
            "iter 462: loss 0.0000, time 1607.42ms\n",
            "iter 463: loss 0.0000, time 1641.77ms\n",
            "iter 464: loss 0.0000, time 1596.83ms\n",
            "iter 465: loss 0.0000, time 1634.03ms\n",
            "iter 466: loss 0.0000, time 1776.22ms\n",
            "iter 467: loss 0.0000, time 1782.59ms\n",
            "iter 468: loss 0.0000, time 1644.20ms\n",
            "iter 469: loss 0.0000, time 1628.98ms\n",
            "iter 470: loss 0.0000, time 1649.74ms\n",
            "iter 471: loss 0.0000, time 1610.33ms\n",
            "iter 472: loss 0.0000, time 1648.49ms\n",
            "iter 473: loss 0.0000, time 1617.75ms\n",
            "iter 474: loss 0.0000, time 1630.18ms\n",
            "iter 475: loss 0.0000, time 1659.88ms\n",
            "iter 476: loss 0.0000, time 1651.18ms\n",
            "iter 477: loss 0.0000, time 1631.25ms\n",
            "iter 478: loss 0.0000, time 2164.88ms\n",
            "iter 479: loss 0.0000, time 1718.81ms\n",
            "step 480: train loss 0.0920, val loss 0.3405\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 16632.48ms\n",
            "iter 481: loss 0.0000, time 1634.80ms\n",
            "iter 482: loss 0.0000, time 1645.59ms\n",
            "iter 483: loss 0.0000, time 1623.74ms\n",
            "iter 484: loss 0.0000, time 1613.80ms\n",
            "iter 485: loss 0.0000, time 1634.67ms\n",
            "iter 486: loss 0.0000, time 1675.86ms\n",
            "iter 487: loss 0.0000, time 1763.23ms\n",
            "iter 488: loss 0.0000, time 1702.74ms\n",
            "iter 489: loss 0.0000, time 1665.72ms\n",
            "iter 490: loss 0.0000, time 1643.57ms\n",
            "iter 491: loss 0.0000, time 1666.77ms\n",
            "iter 492: loss 0.0000, time 1628.78ms\n",
            "iter 493: loss 0.0000, time 1739.95ms\n",
            "iter 494: loss 0.0000, time 1741.41ms\n",
            "iter 495: loss 0.0000, time 1654.87ms\n",
            "iter 496: loss 0.0000, time 1616.89ms\n",
            "iter 497: loss 0.0000, time 1643.18ms\n",
            "iter 498: loss 0.0000, time 1624.40ms\n",
            "iter 499: loss 0.0000, time 1732.06ms\n",
            "step 500: train loss 0.0883, val loss 0.3425\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8476\n",
            "Test recall 0.8000\n",
            "Test F1 0.8000\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 4 6]]\n",
            "iter 500: loss 0.0000, time 17737.94ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁███▇▇</td></tr><tr><td>Test_F1_Score</td><td>▁███▇▇</td></tr><tr><td>Test_Precision</td><td>▁█████</td></tr><tr><td>Test_Recall</td><td>▁███▇▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▆██████████████████████</td></tr><tr><td>val/loss</td><td> █▆▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.8</td></tr><tr><td>Test_F1_Score</td><td>0.8</td></tr><tr><td>Test_Precision</td><td>0.84762</td></tr><tr><td>Test_Recall</td><td>0.8</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.08831</td></tr><tr><td>val/acc</td><td>0.98669</td></tr><tr><td>val/loss</td><td>0.34254</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sparkling-sweep-13</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/krjde92l' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/krjde92l</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_050921-krjde92l\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 355zt8j9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_053017-355zt8j9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/355zt8j9' target=\"_blank\">clean-sweep-14</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/355zt8j9' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/355zt8j9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.3445\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 0.5039, time 31803.25ms\n",
            "iter 1: loss 1.5000, time 1691.57ms\n",
            "iter 2: loss 0.5977, time 1671.41ms\n",
            "iter 3: loss 0.8789, time 1697.14ms\n",
            "iter 4: loss 0.9180, time 1693.74ms\n",
            "iter 5: loss 3.5781, time 1770.87ms\n",
            "iter 6: loss 0.8320, time 1662.07ms\n",
            "iter 7: loss 1.0469, time 1658.68ms\n",
            "iter 8: loss 0.3828, time 1723.55ms\n",
            "iter 9: loss 2.0156, time 1671.65ms\n",
            "iter 10: loss 0.8750, time 1648.03ms\n",
            "iter 11: loss 0.8906, time 1828.14ms\n",
            "iter 12: loss 0.8516, time 1789.45ms\n",
            "iter 13: loss 1.0000, time 1701.40ms\n",
            "iter 14: loss 0.7891, time 1697.99ms\n",
            "iter 15: loss 0.6875, time 1663.02ms\n",
            "iter 16: loss 0.5469, time 1724.24ms\n",
            "iter 17: loss 0.4746, time 1864.29ms\n",
            "iter 18: loss 0.2090, time 1752.09ms\n",
            "iter 19: loss 0.4824, time 1688.91ms\n",
            "step 20: train loss 1.3189, val loss 1.4137\n",
            "step val accuracy 0.7870\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.4688, time 16498.87ms\n",
            "iter 21: loss 0.3672, time 1796.46ms\n",
            "iter 22: loss 3.1250, time 1714.81ms\n",
            "iter 23: loss 0.4004, time 1721.98ms\n",
            "iter 24: loss 0.0266, time 1670.77ms\n",
            "iter 25: loss 0.0308, time 1751.01ms\n",
            "iter 26: loss 0.0376, time 1858.66ms\n",
            "iter 27: loss 0.0283, time 1709.58ms\n",
            "iter 28: loss 0.2559, time 1650.36ms\n",
            "iter 29: loss 0.1396, time 1691.09ms\n",
            "iter 30: loss 1.7188, time 1666.29ms\n",
            "iter 31: loss 0.3984, time 1675.10ms\n",
            "iter 32: loss 0.4785, time 1638.35ms\n",
            "iter 33: loss 0.0047, time 1642.07ms\n",
            "iter 34: loss 0.2988, time 1674.21ms\n",
            "iter 35: loss 0.3750, time 1685.71ms\n",
            "iter 36: loss 0.5156, time 1654.26ms\n",
            "iter 37: loss 0.2285, time 1723.04ms\n",
            "iter 38: loss 0.0776, time 1819.74ms\n",
            "iter 39: loss 0.1836, time 1659.09ms\n",
            "step 40: train loss 0.9749, val loss 1.0529\n",
            "step val accuracy 0.9053\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.0181, time 16569.95ms\n",
            "iter 41: loss 0.0073, time 1652.75ms\n",
            "iter 42: loss 2.4531, time 1656.48ms\n",
            "iter 43: loss 1.2109, time 1653.05ms\n",
            "iter 44: loss 0.0007, time 1630.91ms\n",
            "iter 45: loss 0.0034, time 1685.29ms\n",
            "iter 46: loss 0.0006, time 1761.43ms\n",
            "iter 47: loss 0.0121, time 1791.90ms\n",
            "iter 48: loss 0.1011, time 1691.70ms\n",
            "iter 49: loss 0.0162, time 1653.43ms\n",
            "iter 50: loss 0.0104, time 1633.49ms\n",
            "iter 51: loss 0.1396, time 1656.69ms\n",
            "iter 52: loss 2.4688, time 1760.54ms\n",
            "iter 53: loss 0.0008, time 1806.86ms\n",
            "iter 54: loss 0.0009, time 1691.88ms\n",
            "iter 55: loss 0.0139, time 1694.02ms\n",
            "iter 56: loss 0.0344, time 1671.97ms\n",
            "iter 57: loss 0.0008, time 1650.60ms\n",
            "iter 58: loss 0.0015, time 1691.97ms\n",
            "iter 59: loss 0.0042, time 1660.80ms\n",
            "step 60: train loss 0.7427, val loss 0.8571\n",
            "step val accuracy 0.8728\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0125, time 16679.71ms\n",
            "iter 61: loss 0.1855, time 1767.43ms\n",
            "iter 62: loss 0.0011, time 1768.20ms\n",
            "iter 63: loss 0.0003, time 1646.14ms\n",
            "iter 64: loss 0.0015, time 1654.19ms\n",
            "iter 65: loss 0.0243, time 1663.19ms\n",
            "iter 66: loss 0.0082, time 1674.22ms\n",
            "iter 67: loss 0.0437, time 1646.74ms\n",
            "iter 68: loss 0.0033, time 1664.51ms\n",
            "iter 69: loss 0.0041, time 1649.16ms\n",
            "iter 70: loss 0.0149, time 1646.07ms\n",
            "iter 71: loss 0.0015, time 1661.98ms\n",
            "iter 72: loss 0.0017, time 1672.89ms\n",
            "iter 73: loss 0.0101, time 1828.75ms\n",
            "iter 74: loss 0.0013, time 1823.58ms\n",
            "iter 75: loss 0.0081, time 1653.35ms\n",
            "iter 76: loss 0.0024, time 1681.38ms\n",
            "iter 77: loss 0.0199, time 1634.54ms\n",
            "iter 78: loss 0.1108, time 1676.01ms\n",
            "iter 79: loss 0.0056, time 1764.20ms\n",
            "step 80: train loss 0.5918, val loss 0.7210\n",
            "step val accuracy 0.9645\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0001, time 16415.80ms\n",
            "iter 81: loss 3.5156, time 1644.84ms\n",
            "iter 82: loss 0.0001, time 1759.32ms\n",
            "iter 83: loss 0.0007, time 1969.82ms\n",
            "iter 84: loss 0.0000, time 1644.50ms\n",
            "iter 85: loss 0.0000, time 1666.50ms\n",
            "iter 86: loss 0.0008, time 1646.20ms\n",
            "iter 87: loss 0.0024, time 1702.49ms\n",
            "iter 88: loss 0.0003, time 1780.53ms\n",
            "iter 89: loss 0.0007, time 1743.75ms\n",
            "iter 90: loss 0.0000, time 1635.49ms\n",
            "iter 91: loss 0.0043, time 1665.05ms\n",
            "iter 92: loss 0.0001, time 1645.27ms\n",
            "iter 93: loss 0.0024, time 1671.73ms\n",
            "iter 94: loss 0.0000, time 1651.88ms\n",
            "iter 95: loss 0.0014, time 1660.02ms\n",
            "iter 96: loss 5.6250, time 1619.21ms\n",
            "iter 97: loss 0.0012, time 1672.28ms\n",
            "iter 98: loss 0.0322, time 1629.66ms\n",
            "iter 99: loss 0.0001, time 1705.87ms\n",
            "step 100: train loss 0.4976, val loss 0.6372\n",
            "step val accuracy 0.9512\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8019\n",
            "Test recall 0.7667\n",
            "Test F1 0.7621\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  7  0]\n",
            " [ 0  4  6]]\n",
            "iter 100: loss 0.0001, time 18078.36ms\n",
            "iter 101: loss 0.0043, time 1665.96ms\n",
            "iter 102: loss 0.0002, time 1644.49ms\n",
            "iter 103: loss 0.1523, time 1673.38ms\n",
            "iter 104: loss 0.0018, time 1650.46ms\n",
            "iter 105: loss 0.0002, time 1645.10ms\n",
            "iter 106: loss 0.0001, time 1651.85ms\n",
            "iter 107: loss 0.0004, time 1651.59ms\n",
            "iter 108: loss 0.8320, time 1806.21ms\n",
            "iter 109: loss 0.0047, time 1798.24ms\n",
            "iter 110: loss 0.0000, time 1642.60ms\n",
            "iter 111: loss 0.0003, time 1637.11ms\n",
            "iter 112: loss 0.0014, time 1690.21ms\n",
            "iter 113: loss 0.0063, time 1672.27ms\n",
            "iter 114: loss 0.0036, time 1789.09ms\n",
            "iter 115: loss 0.0002, time 1739.30ms\n",
            "iter 116: loss 0.0001, time 1656.17ms\n",
            "iter 117: loss 0.0176, time 1689.99ms\n",
            "iter 118: loss 0.0034, time 1676.27ms\n",
            "iter 119: loss 0.0410, time 1609.09ms\n",
            "step 120: train loss 0.4271, val loss 0.5560\n",
            "step val accuracy 0.9704\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0001, time 16581.33ms\n",
            "iter 121: loss 0.0171, time 1651.73ms\n",
            "iter 122: loss 0.0018, time 1695.83ms\n",
            "iter 123: loss 0.0010, time 1843.33ms\n",
            "iter 124: loss 0.0001, time 1952.89ms\n",
            "iter 125: loss 0.0000, time 1701.36ms\n",
            "iter 126: loss 0.0000, time 1644.01ms\n",
            "iter 127: loss 0.0002, time 1682.54ms\n",
            "iter 128: loss 0.0000, time 1699.78ms\n",
            "iter 129: loss 0.0003, time 1690.60ms\n",
            "iter 130: loss 0.0004, time 1643.65ms\n",
            "iter 131: loss 0.0000, time 1660.31ms\n",
            "iter 132: loss 0.0001, time 1646.84ms\n",
            "iter 133: loss 0.0002, time 1644.49ms\n",
            "iter 134: loss 0.0001, time 1668.22ms\n",
            "iter 135: loss 0.0011, time 1810.72ms\n",
            "iter 136: loss 0.0000, time 1720.60ms\n",
            "iter 137: loss 0.0000, time 1695.90ms\n",
            "iter 138: loss 0.0000, time 1666.76ms\n",
            "iter 139: loss 0.0009, time 1643.36ms\n",
            "step 140: train loss 0.3731, val loss 0.5279\n",
            "step val accuracy 0.9763\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0002, time 16631.23ms\n",
            "iter 141: loss 0.0000, time 1696.00ms\n",
            "iter 142: loss 0.0001, time 1664.97ms\n",
            "iter 143: loss 0.0001, time 1695.16ms\n",
            "iter 144: loss 0.0003, time 1800.62ms\n",
            "iter 145: loss 0.0000, time 1733.93ms\n",
            "iter 146: loss 0.0000, time 1679.13ms\n",
            "iter 147: loss 0.0000, time 1693.04ms\n",
            "iter 148: loss 0.0001, time 1648.78ms\n",
            "iter 149: loss 0.0000, time 1745.07ms\n",
            "iter 150: loss 0.0000, time 1794.30ms\n",
            "iter 151: loss 0.0005, time 1745.45ms\n",
            "iter 152: loss 0.0001, time 1655.03ms\n",
            "iter 153: loss 0.0000, time 1665.36ms\n",
            "iter 154: loss 0.0005, time 1668.52ms\n",
            "iter 155: loss 0.0006, time 1684.77ms\n",
            "iter 156: loss 0.0001, time 1654.70ms\n",
            "iter 157: loss 0.0000, time 1641.45ms\n",
            "iter 158: loss 0.0000, time 1662.36ms\n",
            "iter 159: loss 0.0001, time 1678.64ms\n",
            "step 160: train loss 0.3311, val loss 0.4829\n",
            "step val accuracy 0.9305\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 16803.77ms\n",
            "iter 161: loss 0.0000, time 1666.31ms\n",
            "iter 162: loss 0.0000, time 1726.23ms\n",
            "iter 163: loss 0.0013, time 1949.92ms\n",
            "iter 164: loss 0.0000, time 1653.86ms\n",
            "iter 165: loss 0.0000, time 1641.02ms\n",
            "iter 166: loss 0.0004, time 1642.66ms\n",
            "iter 167: loss 0.0000, time 1637.54ms\n",
            "iter 168: loss 0.0000, time 1653.18ms\n",
            "iter 169: loss 0.0000, time 1657.22ms\n",
            "iter 170: loss 0.0000, time 1762.61ms\n",
            "iter 171: loss 0.0000, time 1822.24ms\n",
            "iter 172: loss 0.0007, time 1658.18ms\n",
            "iter 173: loss 0.0000, time 1684.07ms\n",
            "iter 174: loss 0.0000, time 1624.88ms\n",
            "iter 175: loss 0.0000, time 1639.56ms\n",
            "iter 176: loss 0.0000, time 1793.54ms\n",
            "iter 177: loss 0.0000, time 1832.26ms\n",
            "iter 178: loss 0.0001, time 1630.15ms\n",
            "iter 179: loss 0.0000, time 1676.64ms\n",
            "step 180: train loss 0.2965, val loss 0.4862\n",
            "step val accuracy 0.9822\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 16616.70ms\n",
            "iter 181: loss 0.0001, time 1664.57ms\n",
            "iter 182: loss 0.0003, time 1622.02ms\n",
            "iter 183: loss 0.0009, time 1627.01ms\n",
            "iter 184: loss 0.0000, time 1667.74ms\n",
            "iter 185: loss 0.0000, time 1775.08ms\n",
            "iter 186: loss 0.0000, time 1753.07ms\n",
            "iter 187: loss 0.0000, time 1639.35ms\n",
            "iter 188: loss 0.0000, time 1663.62ms\n",
            "iter 189: loss 0.0007, time 1633.25ms\n",
            "iter 190: loss 0.0000, time 1686.15ms\n",
            "iter 191: loss 0.0000, time 1749.93ms\n",
            "iter 192: loss 0.0001, time 1750.55ms\n",
            "iter 193: loss 0.0000, time 1630.86ms\n",
            "iter 194: loss 0.0005, time 1688.26ms\n",
            "iter 195: loss 0.0000, time 1636.42ms\n",
            "iter 196: loss 0.0000, time 1681.23ms\n",
            "iter 197: loss 0.0000, time 1701.56ms\n",
            "iter 198: loss 0.0001, time 1631.20ms\n",
            "iter 199: loss 0.0000, time 1673.04ms\n",
            "step 200: train loss 0.2688, val loss 0.4502\n",
            "step val accuracy 0.9749\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8333\n",
            "Test recall 0.8000\n",
            "Test F1 0.7955\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 2  8  0]\n",
            " [ 0  4  6]]\n",
            "iter 200: loss 0.0016, time 18075.19ms\n",
            "iter 201: loss 0.0003, time 1625.65ms\n",
            "iter 202: loss 0.0003, time 1594.84ms\n",
            "iter 203: loss 0.0019, time 1691.41ms\n",
            "iter 204: loss 0.0000, time 1648.21ms\n",
            "iter 205: loss 0.0002, time 1681.14ms\n",
            "iter 206: loss 0.0000, time 1652.18ms\n",
            "iter 207: loss 0.0000, time 1653.15ms\n",
            "iter 208: loss 0.0001, time 1658.65ms\n",
            "iter 209: loss 0.0000, time 1613.23ms\n",
            "iter 210: loss 0.0049, time 1666.67ms\n",
            "iter 211: loss 0.0000, time 1737.88ms\n",
            "iter 212: loss 0.0000, time 1790.05ms\n",
            "iter 213: loss 0.0000, time 1691.80ms\n",
            "iter 214: loss 0.0000, time 1698.03ms\n",
            "iter 215: loss 0.0000, time 1632.35ms\n",
            "iter 216: loss 0.0001, time 1686.53ms\n",
            "iter 217: loss 0.0000, time 1757.22ms\n",
            "iter 218: loss 0.0003, time 1798.53ms\n",
            "iter 219: loss 0.0000, time 1634.80ms\n",
            "step 220: train loss 0.2452, val loss 0.4222\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 16438.14ms\n",
            "iter 221: loss 0.0000, time 1795.10ms\n",
            "iter 222: loss 0.0000, time 1642.19ms\n",
            "iter 223: loss 0.0000, time 1681.49ms\n",
            "iter 224: loss 0.0000, time 1689.19ms\n",
            "iter 225: loss 0.0000, time 1622.27ms\n",
            "iter 226: loss 0.0000, time 1757.59ms\n",
            "iter 227: loss 0.0000, time 1798.22ms\n",
            "iter 228: loss 0.0000, time 1713.70ms\n",
            "iter 229: loss 0.0000, time 1639.18ms\n",
            "iter 230: loss 0.0000, time 1653.40ms\n",
            "iter 231: loss 0.0000, time 1642.94ms\n",
            "iter 232: loss 0.0000, time 1704.96ms\n",
            "iter 233: loss 0.0000, time 1690.36ms\n",
            "iter 234: loss 0.0000, time 1636.47ms\n",
            "iter 235: loss 0.0000, time 1622.30ms\n",
            "iter 236: loss 0.0032, time 1642.71ms\n",
            "iter 237: loss 0.0000, time 1641.66ms\n",
            "iter 238: loss 0.0000, time 1752.34ms\n",
            "iter 239: loss 0.0001, time 1760.45ms\n",
            "step 240: train loss 0.2253, val loss 0.3987\n",
            "step val accuracy 0.9882\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0002, time 16553.20ms\n",
            "iter 241: loss 0.0000, time 1710.90ms\n",
            "iter 242: loss 0.0000, time 1634.20ms\n",
            "iter 243: loss 0.8906, time 1672.50ms\n",
            "iter 244: loss 0.0000, time 1653.26ms\n",
            "iter 245: loss 0.0000, time 1642.28ms\n",
            "iter 246: loss 0.0000, time 1622.40ms\n",
            "iter 247: loss 0.0001, time 1761.45ms\n",
            "iter 248: loss 0.0001, time 1891.20ms\n",
            "iter 249: loss 0.0000, time 1687.72ms\n",
            "iter 250: loss 0.0000, time 1640.30ms\n",
            "iter 251: loss 0.0000, time 1644.33ms\n",
            "iter 252: loss 0.0000, time 1673.56ms\n",
            "iter 253: loss 0.0000, time 1801.49ms\n",
            "iter 254: loss 0.0000, time 1748.91ms\n",
            "iter 255: loss 0.0000, time 1630.55ms\n",
            "iter 256: loss 0.0000, time 1629.46ms\n",
            "iter 257: loss 0.0062, time 1688.67ms\n",
            "iter 258: loss 0.0000, time 1667.66ms\n",
            "iter 259: loss 0.0000, time 1690.93ms\n",
            "step 260: train loss 0.2082, val loss 0.3744\n",
            "step val accuracy 0.9882\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 16662.44ms\n",
            "iter 261: loss 0.0008, time 1672.59ms\n",
            "iter 262: loss 0.0029, time 1795.13ms\n",
            "iter 263: loss 0.0000, time 1836.13ms\n",
            "iter 264: loss 0.0000, time 1626.36ms\n",
            "iter 265: loss 0.0000, time 1649.64ms\n",
            "iter 266: loss 0.0000, time 1645.15ms\n",
            "iter 267: loss 0.0000, time 1639.70ms\n",
            "iter 268: loss 0.0000, time 1659.31ms\n",
            "iter 269: loss 0.0000, time 1653.91ms\n",
            "iter 270: loss 0.0000, time 1697.90ms\n",
            "iter 271: loss 0.0017, time 1619.27ms\n",
            "iter 272: loss 0.0000, time 1661.44ms\n",
            "iter 273: loss 0.0000, time 1893.69ms\n",
            "iter 274: loss 0.0000, time 1889.75ms\n",
            "iter 275: loss 0.0000, time 1726.81ms\n",
            "iter 276: loss 2.7813, time 1650.40ms\n",
            "iter 277: loss 0.0000, time 1658.63ms\n",
            "iter 278: loss 0.0000, time 1673.88ms\n",
            "iter 279: loss 0.0000, time 1734.85ms\n",
            "step 280: train loss 0.1936, val loss 0.3841\n",
            "step val accuracy 0.9734\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 16900.05ms\n",
            "iter 281: loss 0.0000, time 1673.96ms\n",
            "iter 282: loss 0.0000, time 1726.30ms\n",
            "iter 283: loss 0.0000, time 1822.09ms\n",
            "iter 284: loss 0.0000, time 1676.57ms\n",
            "iter 285: loss 0.0000, time 1721.10ms\n",
            "iter 286: loss 0.0000, time 1682.43ms\n",
            "iter 287: loss 0.0000, time 1685.66ms\n",
            "iter 288: loss 0.0001, time 1781.97ms\n",
            "iter 289: loss 0.0007, time 1809.64ms\n",
            "iter 290: loss 0.0003, time 1622.72ms\n",
            "iter 291: loss 0.0000, time 1649.20ms\n",
            "iter 292: loss 0.0000, time 1714.50ms\n",
            "iter 293: loss 0.0000, time 1704.38ms\n",
            "iter 294: loss 0.0000, time 1682.43ms\n",
            "iter 295: loss 0.0000, time 1656.76ms\n",
            "iter 296: loss 0.0000, time 1662.93ms\n",
            "iter 297: loss 0.0000, time 1665.14ms\n",
            "iter 298: loss 0.0000, time 1666.93ms\n",
            "iter 299: loss 0.0000, time 1636.82ms\n",
            "step 300: train loss 0.1815, val loss 0.3651\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8333\n",
            "Test recall 0.8333\n",
            "Test F1 0.8293\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 2  7  1]\n",
            " [ 0  2  8]]\n",
            "iter 300: loss 0.0000, time 18451.78ms\n",
            "iter 301: loss 0.0000, time 1668.61ms\n",
            "iter 302: loss 0.0030, time 1686.76ms\n",
            "iter 303: loss 0.0000, time 1664.81ms\n",
            "iter 304: loss 0.0000, time 1644.07ms\n",
            "iter 305: loss 0.0000, time 1651.20ms\n",
            "iter 306: loss 0.0006, time 1626.42ms\n",
            "iter 307: loss 0.0000, time 1647.67ms\n",
            "iter 308: loss 0.0000, time 1755.82ms\n",
            "iter 309: loss 0.0000, time 1781.48ms\n",
            "iter 310: loss 0.0000, time 1651.86ms\n",
            "iter 311: loss 0.0000, time 1677.70ms\n",
            "iter 312: loss 0.0000, time 1629.98ms\n",
            "iter 313: loss 1.9375, time 1677.90ms\n",
            "iter 314: loss 0.0000, time 1750.60ms\n",
            "iter 315: loss 0.0000, time 1750.13ms\n",
            "iter 316: loss 0.0000, time 1641.23ms\n",
            "iter 317: loss 0.0000, time 1662.29ms\n",
            "iter 318: loss 0.0000, time 1677.62ms\n",
            "iter 319: loss 0.0000, time 1650.24ms\n",
            "step 320: train loss 0.1711, val loss 0.3426\n",
            "step val accuracy 0.9689\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 16644.61ms\n",
            "iter 321: loss 0.0000, time 1675.64ms\n",
            "iter 322: loss 0.0000, time 1641.98ms\n",
            "iter 323: loss 0.0000, time 1773.50ms\n",
            "iter 324: loss 0.0001, time 1783.73ms\n",
            "iter 325: loss 0.0000, time 1645.71ms\n",
            "iter 326: loss 0.0000, time 1636.54ms\n",
            "iter 327: loss 0.0000, time 1632.92ms\n",
            "iter 328: loss 0.0001, time 1672.57ms\n",
            "iter 329: loss 0.0000, time 1683.65ms\n",
            "iter 330: loss 0.0000, time 1623.47ms\n",
            "iter 331: loss 0.0001, time 1670.56ms\n",
            "iter 332: loss 0.0000, time 1630.32ms\n",
            "iter 333: loss 0.0008, time 1663.79ms\n",
            "iter 334: loss 0.0001, time 1689.23ms\n",
            "iter 335: loss 0.0000, time 1819.49ms\n",
            "iter 336: loss 0.0000, time 1772.31ms\n",
            "iter 337: loss 0.0000, time 1661.72ms\n",
            "iter 338: loss 0.0000, time 1641.15ms\n",
            "iter 339: loss 0.0000, time 1651.35ms\n",
            "step 340: train loss 0.1613, val loss 0.3228\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0001, time 16687.02ms\n",
            "iter 341: loss 0.0000, time 1641.55ms\n",
            "iter 342: loss 0.0000, time 1634.03ms\n",
            "iter 343: loss 0.0000, time 1619.81ms\n",
            "iter 344: loss 0.0000, time 1819.07ms\n",
            "iter 345: loss 0.0000, time 1771.59ms\n",
            "iter 346: loss 0.0002, time 1659.43ms\n",
            "iter 347: loss 0.0000, time 1629.99ms\n",
            "iter 348: loss 0.0000, time 1659.37ms\n",
            "iter 349: loss 0.0000, time 1681.94ms\n",
            "iter 350: loss 0.0000, time 1749.36ms\n",
            "iter 351: loss 0.0000, time 1796.02ms\n",
            "iter 352: loss 0.0000, time 1682.41ms\n",
            "iter 353: loss 0.0000, time 1667.60ms\n",
            "iter 354: loss 0.0000, time 1650.72ms\n",
            "iter 355: loss 0.0000, time 1696.51ms\n",
            "iter 356: loss 0.0000, time 1694.57ms\n",
            "iter 357: loss 0.0000, time 1681.51ms\n",
            "iter 358: loss 0.0001, time 1650.29ms\n",
            "iter 359: loss 0.0000, time 1663.14ms\n",
            "step 360: train loss 0.1523, val loss 0.3062\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 16876.47ms\n",
            "iter 361: loss 0.0000, time 1651.62ms\n",
            "iter 362: loss 0.0000, time 1642.39ms\n",
            "iter 363: loss 0.0000, time 1642.04ms\n",
            "iter 364: loss 0.0000, time 1673.99ms\n",
            "iter 365: loss 0.0000, time 1686.14ms\n",
            "iter 366: loss 0.0000, time 1664.70ms\n",
            "iter 367: loss 0.0000, time 1618.36ms\n",
            "iter 368: loss 0.0000, time 1638.54ms\n",
            "iter 369: loss 0.0000, time 1693.26ms\n",
            "iter 370: loss 0.0000, time 1674.27ms\n",
            "iter 371: loss 0.0000, time 1837.58ms\n",
            "iter 372: loss 0.0000, time 1804.75ms\n",
            "iter 373: loss 0.0000, time 1675.74ms\n",
            "iter 374: loss 0.0000, time 1688.51ms\n",
            "iter 375: loss 0.0000, time 1699.52ms\n",
            "iter 376: loss 0.0000, time 1705.95ms\n",
            "iter 377: loss 0.0000, time 1807.78ms\n",
            "iter 378: loss 0.0000, time 1684.90ms\n",
            "iter 379: loss 0.0000, time 1749.83ms\n",
            "step 380: train loss 0.1443, val loss 0.2921\n",
            "step val accuracy 0.9867\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 16514.19ms\n",
            "iter 381: loss 0.0000, time 1707.04ms\n",
            "iter 382: loss 0.0000, time 1679.33ms\n",
            "iter 383: loss 0.0000, time 1655.40ms\n",
            "iter 384: loss 0.0000, time 1656.58ms\n",
            "iter 385: loss 0.0000, time 1708.08ms\n",
            "iter 386: loss 0.0000, time 1795.72ms\n",
            "iter 387: loss 0.0016, time 1683.67ms\n",
            "iter 388: loss 0.0000, time 1655.84ms\n",
            "iter 389: loss 0.0000, time 1686.83ms\n",
            "iter 390: loss 0.0000, time 1657.56ms\n",
            "iter 391: loss 0.0000, time 1720.11ms\n",
            "iter 392: loss 0.0000, time 1638.04ms\n",
            "iter 393: loss 0.0000, time 1672.95ms\n",
            "iter 394: loss 0.0000, time 1642.06ms\n",
            "iter 395: loss 0.0000, time 1654.88ms\n",
            "iter 396: loss 0.0000, time 1639.87ms\n",
            "iter 397: loss 0.0000, time 1686.37ms\n",
            "iter 398: loss 0.0000, time 1773.46ms\n",
            "iter 399: loss 0.8828, time 1726.72ms\n",
            "step 400: train loss 0.1378, val loss 0.2831\n",
            "step val accuracy 0.9837\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8889\n",
            "Test recall 0.8333\n",
            "Test F1 0.8325\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[ 9  1  0]\n",
            " [ 0 10  0]\n",
            " [ 0  4  6]]\n",
            "iter 400: loss 0.0000, time 17501.43ms\n",
            "iter 401: loss 0.0000, time 1636.17ms\n",
            "iter 402: loss 0.0001, time 1604.82ms\n",
            "iter 403: loss 0.0000, time 1618.26ms\n",
            "iter 404: loss 0.0000, time 1690.69ms\n",
            "iter 405: loss 0.0000, time 1674.43ms\n",
            "iter 406: loss 0.0000, time 1786.16ms\n",
            "iter 407: loss 0.0014, time 1780.63ms\n",
            "iter 408: loss 0.0000, time 1623.09ms\n",
            "iter 409: loss 0.0000, time 1680.68ms\n",
            "iter 410: loss 0.0000, time 1671.97ms\n",
            "iter 411: loss 0.0000, time 1658.68ms\n",
            "iter 412: loss 0.0000, time 1787.60ms\n",
            "iter 413: loss 0.0000, time 1723.74ms\n",
            "iter 414: loss 0.0000, time 1642.17ms\n",
            "iter 415: loss 0.0000, time 1670.91ms\n",
            "iter 416: loss 0.0000, time 1654.66ms\n",
            "iter 417: loss 0.0004, time 1674.21ms\n",
            "iter 418: loss 0.0000, time 1687.67ms\n",
            "iter 419: loss 0.0000, time 1647.22ms\n",
            "step 420: train loss 0.1327, val loss 0.2782\n",
            "step val accuracy 0.9689\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 16547.62ms\n",
            "iter 421: loss 0.0000, time 1828.42ms\n",
            "iter 422: loss 0.0000, time 1733.71ms\n",
            "iter 423: loss 0.0000, time 1645.74ms\n",
            "iter 424: loss 0.0000, time 1692.08ms\n",
            "iter 425: loss 0.0000, time 1672.18ms\n",
            "iter 426: loss 0.0000, time 1692.86ms\n",
            "iter 427: loss 0.0000, time 1632.78ms\n",
            "iter 428: loss 0.0000, time 1642.34ms\n",
            "iter 429: loss 0.0000, time 1653.62ms\n",
            "iter 430: loss 0.0000, time 1649.51ms\n",
            "iter 431: loss 0.0000, time 1620.87ms\n",
            "iter 432: loss 0.0000, time 1635.82ms\n",
            "iter 433: loss 0.0000, time 1789.13ms\n",
            "iter 434: loss 0.0000, time 1763.00ms\n",
            "iter 435: loss 0.0000, time 1651.66ms\n",
            "iter 436: loss 0.0000, time 1660.02ms\n",
            "iter 437: loss 0.0000, time 1651.36ms\n",
            "iter 438: loss 0.0000, time 1703.23ms\n",
            "iter 439: loss 0.0000, time 1794.37ms\n",
            "step 440: train loss 0.1269, val loss 0.2781\n",
            "step val accuracy 0.9778\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 16452.35ms\n",
            "iter 441: loss 0.0000, time 1661.87ms\n",
            "iter 442: loss 0.0000, time 1817.39ms\n",
            "iter 443: loss 0.0000, time 1792.39ms\n",
            "iter 444: loss 0.0000, time 1675.58ms\n",
            "iter 445: loss 0.0000, time 1828.54ms\n",
            "iter 446: loss 0.0000, time 1876.11ms\n",
            "iter 447: loss 0.0000, time 1713.69ms\n",
            "iter 448: loss 0.0000, time 1783.71ms\n",
            "iter 449: loss 0.0000, time 1695.47ms\n",
            "iter 450: loss 0.0000, time 1640.55ms\n",
            "iter 451: loss 0.0000, time 1674.04ms\n",
            "iter 452: loss 5.4375, time 1676.90ms\n",
            "iter 453: loss 0.0000, time 1674.37ms\n",
            "iter 454: loss 0.0000, time 1670.08ms\n",
            "iter 455: loss 0.0000, time 1651.24ms\n",
            "iter 456: loss 0.0000, time 1637.54ms\n",
            "iter 457: loss 0.0000, time 1721.98ms\n",
            "iter 458: loss 0.0000, time 1644.24ms\n",
            "iter 459: loss 0.0000, time 1712.26ms\n",
            "step 460: train loss 0.1220, val loss 0.2706\n",
            "step val accuracy 0.9852\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 16939.67ms\n",
            "iter 461: loss 0.0000, time 1690.84ms\n",
            "iter 462: loss 0.0031, time 1625.90ms\n",
            "iter 463: loss 0.0005, time 1663.83ms\n",
            "iter 464: loss 0.0000, time 1630.53ms\n",
            "iter 465: loss 0.0000, time 1642.75ms\n",
            "iter 466: loss 0.0000, time 1621.38ms\n",
            "iter 467: loss 0.0000, time 1674.35ms\n",
            "iter 468: loss 0.0000, time 1706.74ms\n",
            "iter 469: loss 0.0000, time 1809.89ms\n",
            "iter 470: loss 0.0000, time 1686.80ms\n",
            "iter 471: loss 0.0000, time 1654.03ms\n",
            "iter 472: loss 0.0000, time 1641.76ms\n",
            "iter 473: loss 0.0000, time 1683.74ms\n",
            "iter 474: loss 0.0000, time 1745.16ms\n",
            "iter 475: loss 0.0000, time 1804.17ms\n",
            "iter 476: loss 0.0000, time 1646.23ms\n",
            "iter 477: loss 0.0000, time 1682.58ms\n",
            "iter 478: loss 0.0000, time 1618.04ms\n",
            "iter 479: loss 0.0000, time 1667.57ms\n",
            "step 480: train loss 0.1169, val loss 0.2804\n",
            "step val accuracy 0.9882\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 16700.09ms\n",
            "iter 481: loss 0.0000, time 1673.29ms\n",
            "iter 482: loss 0.0000, time 1638.01ms\n",
            "iter 483: loss 0.0000, time 1762.35ms\n",
            "iter 484: loss 0.0000, time 1775.28ms\n",
            "iter 485: loss 0.0000, time 1656.76ms\n",
            "iter 486: loss 0.0000, time 1651.33ms\n",
            "iter 487: loss 0.0000, time 1629.07ms\n",
            "iter 488: loss 0.0000, time 1701.04ms\n",
            "iter 489: loss 0.0000, time 1686.05ms\n",
            "iter 490: loss 0.0000, time 1661.60ms\n",
            "iter 491: loss 0.0000, time 1642.30ms\n",
            "iter 492: loss 0.0000, time 1669.23ms\n",
            "iter 493: loss 0.0000, time 1633.67ms\n",
            "iter 494: loss 0.0000, time 1674.59ms\n",
            "iter 495: loss 0.0000, time 1768.70ms\n",
            "iter 496: loss 0.0000, time 1822.98ms\n",
            "iter 497: loss 0.0000, time 1720.64ms\n",
            "iter 498: loss 0.0000, time 1659.56ms\n",
            "iter 499: loss 0.0000, time 1687.15ms\n",
            "step 500: train loss 0.1125, val loss 0.2717\n",
            "step val accuracy 0.9793\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.7714\n",
            "Test recall 0.7333\n",
            "Test F1 0.7278\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 4  6  0]\n",
            " [ 0  4  6]]\n",
            "iter 500: loss 0.0000, time 17739.03ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▇███▇</td></tr><tr><td>Test_F1_Score</td><td>▁▇███▇</td></tr><tr><td>Test_Precision</td><td>▁▇▇▇█▇</td></tr><tr><td>Test_Recall</td><td>▁▇███▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▇████▇█████████████████</td></tr><tr><td>val/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.73333</td></tr><tr><td>Test_F1_Score</td><td>0.72778</td></tr><tr><td>Test_Precision</td><td>0.77143</td></tr><tr><td>Test_Recall</td><td>0.73333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.11251</td></tr><tr><td>val/acc</td><td>0.97929</td></tr><tr><td>val/loss</td><td>0.27169</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clean-sweep-14</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/355zt8j9' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/355zt8j9</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_053017-355zt8j9\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sskt5yc5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_055131-sskt5yc5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/sskt5yc5' target=\"_blank\">tough-sweep-15</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/sskt5yc5' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/sskt5yc5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.4241\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 7.1250, time 21843.85ms\n",
            "iter 1: loss 0.5430, time 1741.20ms\n",
            "iter 2: loss 0.2061, time 1705.69ms\n",
            "iter 3: loss 0.6328, time 1733.75ms\n",
            "iter 4: loss 1.0938, time 1708.70ms\n",
            "iter 5: loss 0.3672, time 1862.87ms\n",
            "iter 6: loss 0.4004, time 1799.54ms\n",
            "iter 7: loss 0.3496, time 1701.05ms\n",
            "iter 8: loss 0.3340, time 1717.07ms\n",
            "iter 9: loss 0.2852, time 1697.41ms\n",
            "iter 10: loss 0.4180, time 1758.66ms\n",
            "iter 11: loss 1.4219, time 1678.42ms\n",
            "iter 12: loss 0.0864, time 1721.37ms\n",
            "iter 13: loss 1.3125, time 1693.26ms\n",
            "iter 14: loss 0.3574, time 1712.54ms\n",
            "iter 15: loss 0.4063, time 1687.23ms\n",
            "iter 16: loss 0.4473, time 1825.69ms\n",
            "iter 17: loss 0.0669, time 1882.51ms\n",
            "iter 18: loss 0.4746, time 1742.08ms\n",
            "iter 19: loss 0.0261, time 1727.81ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 0.8964, val loss 0.5325\n",
            "step val accuracy 0.8747\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.0747, time 21702.86ms\n",
            "iter 21: loss 0.0540, time 1703.56ms\n",
            "iter 22: loss 0.0026, time 1952.03ms\n",
            "iter 23: loss 1.7969, time 1835.36ms\n",
            "iter 24: loss 0.1914, time 1754.79ms\n",
            "iter 25: loss 0.0043, time 1729.52ms\n",
            "iter 26: loss 0.6719, time 1689.99ms\n",
            "iter 27: loss 0.0197, time 1705.03ms\n",
            "iter 28: loss 0.5273, time 1872.13ms\n",
            "iter 29: loss 0.0503, time 1804.76ms\n",
            "iter 30: loss 0.1748, time 1714.33ms\n",
            "iter 31: loss 0.8047, time 1733.80ms\n",
            "iter 32: loss 0.0339, time 1708.84ms\n",
            "iter 33: loss 0.6602, time 1729.67ms\n",
            "iter 34: loss 0.0028, time 1716.42ms\n",
            "iter 35: loss 0.0077, time 1704.67ms\n",
            "iter 36: loss 0.0603, time 1691.98ms\n",
            "iter 37: loss 0.0177, time 1742.78ms\n",
            "iter 38: loss 0.0151, time 1714.40ms\n",
            "iter 39: loss 1.3594, time 1830.22ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.6690, val loss 0.8249\n",
            "step val accuracy 0.8920\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.2598, time 21810.96ms\n",
            "iter 41: loss 0.0233, time 1697.50ms\n",
            "iter 42: loss 0.1777, time 1720.72ms\n",
            "iter 43: loss 0.0064, time 1713.16ms\n",
            "iter 44: loss 0.1641, time 1714.45ms\n",
            "iter 45: loss 0.0144, time 1795.74ms\n",
            "iter 46: loss 0.2227, time 1864.20ms\n",
            "iter 47: loss 0.0859, time 1748.12ms\n",
            "iter 48: loss 0.1748, time 1754.92ms\n",
            "iter 49: loss 0.0023, time 1696.58ms\n",
            "iter 50: loss 0.2187, time 1726.15ms\n",
            "iter 51: loss 0.0640, time 1833.74ms\n",
            "iter 52: loss 0.1045, time 1779.84ms\n",
            "iter 53: loss 0.0093, time 1722.89ms\n",
            "iter 54: loss 0.0435, time 1732.56ms\n",
            "iter 55: loss 0.0549, time 1737.66ms\n",
            "iter 56: loss 0.0356, time 1714.51ms\n",
            "iter 57: loss 0.0747, time 1695.19ms\n",
            "iter 58: loss 1.0391, time 1711.46ms\n",
            "iter 59: loss 0.0123, time 1709.61ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.5487, val loss 0.7820\n",
            "step val accuracy 0.9266\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.4219, time 21881.67ms\n",
            "iter 61: loss 0.1079, time 1738.96ms\n",
            "iter 62: loss 0.0142, time 1736.96ms\n",
            "iter 63: loss 0.0048, time 1715.23ms\n",
            "iter 64: loss 0.0408, time 1742.86ms\n",
            "iter 65: loss 0.0014, time 1700.20ms\n",
            "iter 66: loss 0.0143, time 1717.50ms\n",
            "iter 67: loss 0.0070, time 1686.52ms\n",
            "iter 68: loss 0.0015, time 1812.09ms\n",
            "iter 69: loss 0.0021, time 1836.33ms\n",
            "iter 70: loss 0.0500, time 1761.32ms\n",
            "iter 71: loss 0.0001, time 1685.58ms\n",
            "iter 72: loss 0.0126, time 1715.67ms\n",
            "iter 73: loss 0.0017, time 1691.21ms\n",
            "iter 74: loss 0.0115, time 1815.09ms\n",
            "iter 75: loss 0.0003, time 1833.14ms\n",
            "iter 76: loss 0.1328, time 1716.73ms\n",
            "iter 77: loss 0.0231, time 1711.66ms\n",
            "iter 78: loss 0.0023, time 1754.62ms\n",
            "iter 79: loss 0.0048, time 1712.26ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 80: train loss 0.4582, val loss 0.7488\n",
            "step val accuracy 0.9492\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0747, time 21689.45ms\n",
            "iter 81: loss 0.0237, time 1763.26ms\n",
            "iter 82: loss 0.0134, time 1703.05ms\n",
            "iter 83: loss 0.0094, time 1753.64ms\n",
            "iter 84: loss 0.0008, time 1760.93ms\n",
            "iter 85: loss 0.0012, time 1721.21ms\n",
            "iter 86: loss 0.0093, time 1705.63ms\n",
            "iter 87: loss 0.0002, time 1702.03ms\n",
            "iter 88: loss 0.0084, time 1719.32ms\n",
            "iter 89: loss 0.1099, time 1713.44ms\n",
            "iter 90: loss 0.0029, time 1739.28ms\n",
            "iter 91: loss 0.0001, time 1809.63ms\n",
            "iter 92: loss 0.1143, time 1907.23ms\n",
            "iter 93: loss 0.0079, time 1939.24ms\n",
            "iter 94: loss 0.0008, time 1735.90ms\n",
            "iter 95: loss 0.2734, time 1701.60ms\n",
            "iter 96: loss 0.0598, time 1729.84ms\n",
            "iter 97: loss 0.0055, time 1835.25ms\n",
            "iter 98: loss 0.0081, time 1809.51ms\n",
            "iter 99: loss 0.0413, time 1711.27ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 100: train loss 0.3874, val loss 0.6852\n",
            "step val accuracy 0.9611\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.6000\n",
            "Test precision 0.4133\n",
            "Test recall 0.6000\n",
            "Test F1 0.4874\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 2  8  0]\n",
            " [ 1  9  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 100: loss 0.0101, time 22587.95ms\n",
            "iter 101: loss 0.0025, time 1729.31ms\n",
            "iter 102: loss 0.0074, time 1859.22ms\n",
            "iter 103: loss 0.0027, time 1872.87ms\n",
            "iter 104: loss 0.0003, time 1717.54ms\n",
            "iter 105: loss 0.0018, time 1712.01ms\n",
            "iter 106: loss 0.0021, time 1698.81ms\n",
            "iter 107: loss 0.0025, time 1746.18ms\n",
            "iter 108: loss 0.0059, time 1755.27ms\n",
            "iter 109: loss 0.0001, time 1684.27ms\n",
            "iter 110: loss 0.0025, time 1686.43ms\n",
            "iter 111: loss 0.0026, time 1871.08ms\n",
            "iter 112: loss 0.0032, time 1761.30ms\n",
            "iter 113: loss 0.0014, time 1810.71ms\n",
            "iter 114: loss 0.0026, time 1882.78ms\n",
            "iter 115: loss 0.0002, time 1742.62ms\n",
            "iter 116: loss 0.0056, time 1754.80ms\n",
            "iter 117: loss 0.1167, time 1711.79ms\n",
            "iter 118: loss 0.0008, time 1725.77ms\n",
            "iter 119: loss 0.0225, time 1841.40ms\n",
            "step 120: train loss 0.3331, val loss 0.6427\n",
            "step val accuracy 0.9579\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0010, time 21619.37ms\n",
            "iter 121: loss 0.0081, time 1731.25ms\n",
            "iter 122: loss 0.0299, time 1751.90ms\n",
            "iter 123: loss 0.0003, time 1699.45ms\n",
            "iter 124: loss 0.0017, time 1749.04ms\n",
            "iter 125: loss 0.0679, time 1799.24ms\n",
            "iter 126: loss 0.0206, time 1810.14ms\n",
            "iter 127: loss 0.0002, time 1716.10ms\n",
            "iter 128: loss 0.0025, time 1743.20ms\n",
            "iter 129: loss 0.0015, time 1788.72ms\n",
            "iter 130: loss 0.0000, time 1726.65ms\n",
            "iter 131: loss 0.0018, time 1799.65ms\n",
            "iter 132: loss 0.0001, time 1729.58ms\n",
            "iter 133: loss 0.0040, time 1682.38ms\n",
            "iter 134: loss 0.0011, time 1645.02ms\n",
            "iter 135: loss 0.0114, time 1673.30ms\n",
            "iter 136: loss 0.0002, time 1688.85ms\n",
            "iter 137: loss 0.0001, time 1686.14ms\n",
            "iter 138: loss 0.0001, time 1690.43ms\n",
            "iter 139: loss 0.0000, time 1656.00ms\n",
            "step 140: train loss 0.2911, val loss 0.5682\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0005, time 21629.17ms\n",
            "iter 141: loss 0.0001, time 1668.25ms\n",
            "iter 142: loss 0.0011, time 1692.18ms\n",
            "iter 143: loss 0.0008, time 1687.27ms\n",
            "iter 144: loss 0.0001, time 1674.01ms\n",
            "iter 145: loss 0.0001, time 1654.97ms\n",
            "iter 146: loss 0.0045, time 1679.30ms\n",
            "iter 147: loss 0.0001, time 1661.75ms\n",
            "iter 148: loss 0.0002, time 1708.13ms\n",
            "iter 149: loss 0.0018, time 1832.23ms\n",
            "iter 150: loss 0.0013, time 1759.75ms\n",
            "iter 151: loss 0.0004, time 1692.96ms\n",
            "iter 152: loss 1.0234, time 1663.42ms\n",
            "iter 153: loss 0.0000, time 1704.19ms\n",
            "iter 154: loss 0.0001, time 1773.38ms\n",
            "iter 155: loss 0.0001, time 1813.84ms\n",
            "iter 156: loss 0.0003, time 1671.18ms\n",
            "iter 157: loss 0.0000, time 1659.72ms\n",
            "iter 158: loss 0.0026, time 1635.89ms\n",
            "iter 159: loss 0.0008, time 1674.29ms\n",
            "step 160: train loss 0.2581, val loss 0.5514\n",
            "step val accuracy 0.9870\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 21562.57ms\n",
            "iter 161: loss 0.0002, time 1836.10ms\n",
            "iter 162: loss 0.0063, time 1681.82ms\n",
            "iter 163: loss 0.0007, time 1708.39ms\n",
            "iter 164: loss 0.0014, time 1720.07ms\n",
            "iter 165: loss 0.0004, time 1662.03ms\n",
            "iter 166: loss 0.0000, time 1674.19ms\n",
            "iter 167: loss 0.0002, time 1671.19ms\n",
            "iter 168: loss 0.0006, time 1697.83ms\n",
            "iter 169: loss 0.0008, time 1644.16ms\n",
            "iter 170: loss 0.0002, time 1673.79ms\n",
            "iter 171: loss 0.0001, time 1671.11ms\n",
            "iter 172: loss 0.0001, time 1805.17ms\n",
            "iter 173: loss 0.0298, time 1806.47ms\n",
            "iter 174: loss 0.0014, time 1680.47ms\n",
            "iter 175: loss 0.0001, time 1686.83ms\n",
            "iter 176: loss 0.0002, time 1732.13ms\n",
            "iter 177: loss 0.0001, time 1698.28ms\n",
            "iter 178: loss 0.0000, time 1826.79ms\n",
            "iter 179: loss 0.0000, time 1760.29ms\n",
            "step 180: train loss 0.2307, val loss 0.5266\n",
            "step val accuracy 0.9330\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0002, time 21301.41ms\n",
            "iter 181: loss 0.0007, time 1682.86ms\n",
            "iter 182: loss 0.0002, time 1724.62ms\n",
            "iter 183: loss 0.0003, time 1685.43ms\n",
            "iter 184: loss 0.0008, time 1773.99ms\n",
            "iter 185: loss 0.0000, time 1793.53ms\n",
            "iter 186: loss 1.7500, time 1667.24ms\n",
            "iter 187: loss 0.0005, time 1654.17ms\n",
            "iter 188: loss 0.0001, time 1719.76ms\n",
            "iter 189: loss 0.0000, time 1648.46ms\n",
            "iter 190: loss 0.0000, time 1703.71ms\n",
            "iter 191: loss 0.0000, time 1693.93ms\n",
            "iter 192: loss 0.0000, time 1659.50ms\n",
            "iter 193: loss 0.0000, time 1643.18ms\n",
            "iter 194: loss 0.0000, time 1668.86ms\n",
            "iter 195: loss 0.0002, time 1705.00ms\n",
            "iter 196: loss 0.0117, time 1832.06ms\n",
            "iter 197: loss 0.0000, time 1769.04ms\n",
            "iter 198: loss 0.0000, time 1654.25ms\n",
            "iter 199: loss 0.0020, time 1696.15ms\n",
            "step 200: train loss 0.2104, val loss 0.5692\n",
            "step val accuracy 0.9676\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.8016\n",
            "Test recall 0.7333\n",
            "Test F1 0.7157\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 2  8  0]\n",
            " [ 0  6  4]]\n",
            "iter 200: loss 0.0001, time 22373.71ms\n",
            "iter 201: loss 0.0027, time 1748.01ms\n",
            "iter 202: loss 0.0000, time 1795.32ms\n",
            "iter 203: loss 0.0000, time 1690.79ms\n",
            "iter 204: loss 0.0000, time 1670.73ms\n",
            "iter 205: loss 0.0000, time 1665.79ms\n",
            "iter 206: loss 0.0000, time 1710.91ms\n",
            "iter 207: loss 0.0000, time 1799.33ms\n",
            "iter 208: loss 0.0019, time 1781.69ms\n",
            "iter 209: loss 0.0000, time 1677.53ms\n",
            "iter 210: loss 0.0001, time 1644.19ms\n",
            "iter 211: loss 0.0000, time 1660.72ms\n",
            "iter 212: loss 0.0000, time 1688.32ms\n",
            "iter 213: loss 0.0000, time 1666.65ms\n",
            "iter 214: loss 0.0000, time 1676.60ms\n",
            "iter 215: loss 0.0001, time 1698.60ms\n",
            "iter 216: loss 0.0006, time 1659.48ms\n",
            "iter 217: loss 0.0000, time 1664.75ms\n",
            "iter 218: loss 0.0001, time 1716.42ms\n",
            "iter 219: loss 0.0000, time 1855.74ms\n",
            "step 220: train loss 0.1926, val loss 0.5421\n",
            "step val accuracy 0.9903\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 21639.51ms\n",
            "iter 221: loss 0.0000, time 1669.16ms\n",
            "iter 222: loss 0.0000, time 1705.09ms\n",
            "iter 223: loss 0.0000, time 1657.61ms\n",
            "iter 224: loss 0.0001, time 1682.28ms\n",
            "iter 225: loss 0.0000, time 1872.34ms\n",
            "iter 226: loss 0.0001, time 1846.36ms\n",
            "iter 227: loss 0.0001, time 1668.31ms\n",
            "iter 228: loss 0.0001, time 1659.74ms\n",
            "iter 229: loss 0.0000, time 1720.84ms\n",
            "iter 230: loss 0.0002, time 1716.70ms\n",
            "iter 231: loss 0.0000, time 1807.36ms\n",
            "iter 232: loss 0.0000, time 1745.14ms\n",
            "iter 233: loss 0.0000, time 1671.33ms\n",
            "iter 234: loss 0.0001, time 1698.90ms\n",
            "iter 235: loss 0.0000, time 1690.08ms\n",
            "iter 236: loss 0.0000, time 1698.02ms\n",
            "iter 237: loss 0.0001, time 1707.54ms\n",
            "iter 238: loss 0.0000, time 1726.12ms\n",
            "iter 239: loss 0.0000, time 1688.66ms\n",
            "step 240: train loss 0.1771, val loss 0.4972\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0659, time 21768.56ms\n",
            "iter 241: loss 0.0001, time 1689.31ms\n",
            "iter 242: loss 0.0006, time 1630.74ms\n",
            "iter 243: loss 0.0018, time 1703.01ms\n",
            "iter 244: loss 0.0000, time 1698.77ms\n",
            "iter 245: loss 0.0002, time 1675.23ms\n",
            "iter 246: loss 0.0000, time 1618.35ms\n",
            "iter 247: loss 0.0001, time 1690.66ms\n",
            "iter 248: loss 0.0000, time 1771.01ms\n",
            "iter 249: loss 0.0000, time 1829.79ms\n",
            "iter 250: loss 0.0001, time 1712.07ms\n",
            "iter 251: loss 0.0422, time 1658.16ms\n",
            "iter 252: loss 0.0000, time 1715.02ms\n",
            "iter 253: loss 0.0002, time 1694.67ms\n",
            "iter 254: loss 0.0000, time 1823.82ms\n",
            "iter 255: loss 0.0000, time 1788.38ms\n",
            "iter 256: loss 0.0000, time 1711.16ms\n",
            "iter 257: loss 0.0123, time 1654.49ms\n",
            "iter 258: loss 0.0000, time 1675.44ms\n",
            "iter 259: loss 0.0000, time 1668.83ms\n",
            "step 260: train loss 0.1641, val loss 0.4959\n",
            "step val accuracy 0.9417\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 21566.63ms\n",
            "iter 261: loss 0.0000, time 1823.77ms\n",
            "iter 262: loss 0.0000, time 1711.31ms\n",
            "iter 263: loss 0.0000, time 1675.34ms\n",
            "iter 264: loss 0.0084, time 1641.07ms\n",
            "iter 265: loss 0.0069, time 1681.04ms\n",
            "iter 266: loss 0.0000, time 1691.35ms\n",
            "iter 267: loss 0.0001, time 1706.78ms\n",
            "iter 268: loss 0.0000, time 1734.04ms\n",
            "iter 269: loss 0.0000, time 1664.57ms\n",
            "iter 270: loss 0.0000, time 1657.20ms\n",
            "iter 271: loss 0.0000, time 1692.56ms\n",
            "iter 272: loss 0.0000, time 1826.28ms\n",
            "iter 273: loss 0.0000, time 1792.05ms\n",
            "iter 274: loss 0.0000, time 1685.02ms\n",
            "iter 275: loss 0.0000, time 1683.45ms\n",
            "iter 276: loss 0.0000, time 1705.40ms\n",
            "iter 277: loss 0.0005, time 1714.29ms\n",
            "iter 278: loss 0.0007, time 1841.85ms\n",
            "iter 279: loss 0.0000, time 1708.64ms\n",
            "step 280: train loss 0.1562, val loss 0.5064\n",
            "step val accuracy 0.9611\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0703, time 21385.68ms\n",
            "iter 281: loss 0.0000, time 1722.04ms\n",
            "iter 282: loss 0.0000, time 1684.91ms\n",
            "iter 283: loss 0.0000, time 1686.53ms\n",
            "iter 284: loss 0.0000, time 1844.60ms\n",
            "iter 285: loss 0.0000, time 1729.67ms\n",
            "iter 286: loss 0.0000, time 1666.11ms\n",
            "iter 287: loss 0.0029, time 1676.61ms\n",
            "iter 288: loss 0.0000, time 1650.78ms\n",
            "iter 289: loss 0.0042, time 1682.18ms\n",
            "iter 290: loss 0.0000, time 1666.70ms\n",
            "iter 291: loss 0.0000, time 1696.07ms\n",
            "iter 292: loss 0.0000, time 1660.10ms\n",
            "iter 293: loss 0.0000, time 1700.08ms\n",
            "iter 294: loss 0.0001, time 1672.06ms\n",
            "iter 295: loss 0.0000, time 1718.44ms\n",
            "iter 296: loss 0.0000, time 1786.86ms\n",
            "iter 297: loss 0.0000, time 1708.51ms\n",
            "iter 298: loss 0.0000, time 1672.79ms\n",
            "iter 299: loss 0.0000, time 1741.61ms\n",
            "step 300: train loss 0.1463, val loss 0.4777\n",
            "step val accuracy 0.9849\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8476\n",
            "Test recall 0.8000\n",
            "Test F1 0.8000\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [1 9 0]\n",
            " [0 4 6]]\n",
            "iter 300: loss 0.0000, time 22274.20ms\n",
            "iter 301: loss 0.0000, time 1815.59ms\n",
            "iter 302: loss 0.0002, time 1798.02ms\n",
            "iter 303: loss 0.0000, time 1663.33ms\n",
            "iter 304: loss 0.0000, time 1699.29ms\n",
            "iter 305: loss 0.0001, time 1704.23ms\n",
            "iter 306: loss 0.0000, time 1709.23ms\n",
            "iter 307: loss 0.0000, time 1802.07ms\n",
            "iter 308: loss 0.0006, time 1775.00ms\n",
            "iter 309: loss 0.0000, time 1679.15ms\n",
            "iter 310: loss 0.0012, time 1657.94ms\n",
            "iter 311: loss 0.0002, time 1690.82ms\n",
            "iter 312: loss 0.0000, time 1667.42ms\n",
            "iter 313: loss 0.0000, time 1658.78ms\n",
            "iter 314: loss 0.0000, time 1776.34ms\n",
            "iter 315: loss 0.0000, time 1768.63ms\n",
            "iter 316: loss 0.0000, time 1657.08ms\n",
            "iter 317: loss 0.0000, time 1671.17ms\n",
            "iter 318: loss 0.0000, time 1762.30ms\n",
            "iter 319: loss 0.0000, time 1822.44ms\n",
            "step 320: train loss 0.1374, val loss 0.4579\n",
            "step val accuracy 0.9881\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 21399.27ms\n",
            "iter 321: loss 0.0000, time 1636.19ms\n",
            "iter 322: loss 0.0000, time 1644.17ms\n",
            "iter 323: loss 0.0000, time 1673.45ms\n",
            "iter 324: loss 0.0000, time 1758.65ms\n",
            "iter 325: loss 0.0001, time 1836.80ms\n",
            "iter 326: loss 0.0000, time 1710.40ms\n",
            "iter 327: loss 0.0000, time 1691.70ms\n",
            "iter 328: loss 0.0000, time 1723.20ms\n",
            "iter 329: loss 0.0000, time 1676.05ms\n",
            "iter 330: loss 0.0000, time 1750.50ms\n",
            "iter 331: loss 0.0000, time 1823.57ms\n",
            "iter 332: loss 0.0000, time 1703.90ms\n",
            "iter 333: loss 0.0005, time 1697.26ms\n",
            "iter 334: loss 0.0000, time 1656.44ms\n",
            "iter 335: loss 0.0000, time 1652.25ms\n",
            "iter 336: loss 0.0000, time 1712.62ms\n",
            "iter 337: loss 0.0000, time 1719.70ms\n",
            "iter 338: loss 0.0001, time 1664.26ms\n",
            "iter 339: loss 0.0000, time 1672.16ms\n",
            "step 340: train loss 0.1293, val loss 0.4622\n",
            "step val accuracy 0.9903\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 21753.72ms\n",
            "iter 341: loss 0.0000, time 1655.62ms\n",
            "iter 342: loss 0.0001, time 1713.44ms\n",
            "iter 343: loss 0.0000, time 1683.45ms\n",
            "iter 344: loss 0.0000, time 1668.40ms\n",
            "iter 345: loss 0.0000, time 1681.07ms\n",
            "iter 346: loss 0.0000, time 1744.40ms\n",
            "iter 347: loss 0.0000, time 1664.86ms\n",
            "iter 348: loss 0.0000, time 1617.71ms\n",
            "iter 349: loss 0.0000, time 1778.33ms\n",
            "iter 350: loss 0.0000, time 1812.82ms\n",
            "iter 351: loss 0.0001, time 1680.71ms\n",
            "iter 352: loss 0.0000, time 1798.42ms\n",
            "iter 353: loss 0.0000, time 1683.27ms\n",
            "iter 354: loss 0.0000, time 1820.60ms\n",
            "iter 355: loss 0.0000, time 1795.09ms\n",
            "iter 356: loss 0.0000, time 1667.76ms\n",
            "iter 357: loss 0.0000, time 1638.00ms\n",
            "iter 358: loss 0.0000, time 1666.62ms\n",
            "iter 359: loss 0.0000, time 1670.05ms\n",
            "step 360: train loss 0.1221, val loss 0.4375\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 21528.60ms\n",
            "iter 361: loss 0.0000, time 1794.65ms\n",
            "iter 362: loss 0.0002, time 1675.42ms\n",
            "iter 363: loss 0.0000, time 1634.81ms\n",
            "iter 364: loss 0.0000, time 1716.67ms\n",
            "iter 365: loss 0.0000, time 1669.05ms\n",
            "iter 366: loss 0.0000, time 1672.33ms\n",
            "iter 367: loss 0.0000, time 1663.77ms\n",
            "iter 368: loss 0.0000, time 1662.96ms\n",
            "iter 369: loss 0.0000, time 1662.84ms\n",
            "iter 370: loss 0.0000, time 1658.65ms\n",
            "iter 371: loss 0.0000, time 1682.03ms\n",
            "iter 372: loss 0.0000, time 1842.56ms\n",
            "iter 373: loss 0.0000, time 1757.64ms\n",
            "iter 374: loss 0.0000, time 1706.12ms\n",
            "iter 375: loss 0.0001, time 1699.69ms\n",
            "iter 376: loss 0.0001, time 1661.22ms\n",
            "iter 377: loss 0.0000, time 1722.77ms\n",
            "iter 378: loss 0.0001, time 1826.01ms\n",
            "iter 379: loss 0.0000, time 1769.58ms\n",
            "step 380: train loss 0.1157, val loss 0.4171\n",
            "step val accuracy 0.9924\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 21880.98ms\n",
            "iter 381: loss 0.0000, time 1719.30ms\n",
            "iter 382: loss 0.0000, time 1688.59ms\n",
            "iter 383: loss 0.0000, time 1758.73ms\n",
            "iter 384: loss 0.0000, time 1838.51ms\n",
            "iter 385: loss 0.0000, time 1650.12ms\n",
            "iter 386: loss 0.0000, time 1664.51ms\n",
            "iter 387: loss 0.0000, time 1676.91ms\n",
            "iter 388: loss 0.0000, time 1673.64ms\n",
            "iter 389: loss 0.0000, time 1673.15ms\n",
            "iter 390: loss 0.0000, time 1657.97ms\n",
            "iter 391: loss 0.0000, time 1654.49ms\n",
            "iter 392: loss 0.0000, time 1681.10ms\n",
            "iter 393: loss 0.0000, time 1677.19ms\n",
            "iter 394: loss 0.0000, time 1672.26ms\n",
            "iter 395: loss 0.0000, time 1763.82ms\n",
            "iter 396: loss 0.0000, time 1818.01ms\n",
            "iter 397: loss 0.0000, time 1696.09ms\n",
            "iter 398: loss 0.0001, time 1693.91ms\n",
            "iter 399: loss 0.0000, time 1655.50ms\n",
            "step 400: train loss 0.1099, val loss 0.3972\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8000\n",
            "Test precision 0.8333\n",
            "Test recall 0.8000\n",
            "Test F1 0.7955\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 2  8  0]\n",
            " [ 0  4  6]]\n",
            "iter 400: loss 0.0000, time 22490.00ms\n",
            "iter 401: loss 0.0000, time 1893.82ms\n",
            "iter 402: loss 0.0000, time 1720.70ms\n",
            "iter 403: loss 0.0000, time 1679.56ms\n",
            "iter 404: loss 0.0000, time 1765.13ms\n",
            "iter 405: loss 0.0000, time 1666.14ms\n",
            "iter 406: loss 0.0000, time 1755.24ms\n",
            "iter 407: loss 0.0000, time 1837.99ms\n",
            "iter 408: loss 0.0000, time 1673.93ms\n",
            "iter 409: loss 0.0000, time 1675.12ms\n",
            "iter 410: loss 0.0000, time 1712.70ms\n",
            "iter 411: loss 0.0000, time 1651.92ms\n",
            "iter 412: loss 0.0000, time 1674.77ms\n",
            "iter 413: loss 0.0000, time 1713.20ms\n",
            "iter 414: loss 0.0000, time 1697.16ms\n",
            "iter 415: loss 0.0000, time 1634.28ms\n",
            "iter 416: loss 0.0000, time 1708.73ms\n",
            "iter 417: loss 0.0000, time 1641.55ms\n",
            "iter 418: loss 0.0000, time 1842.96ms\n",
            "iter 419: loss 0.0000, time 1834.99ms\n",
            "step 420: train loss 0.1047, val loss 0.3792\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 21495.01ms\n",
            "iter 421: loss 0.0000, time 1654.66ms\n",
            "iter 422: loss 0.0000, time 1673.60ms\n",
            "iter 423: loss 0.0000, time 1691.88ms\n",
            "iter 424: loss 0.0000, time 1795.92ms\n",
            "iter 425: loss 0.0000, time 1803.54ms\n",
            "iter 426: loss 0.0000, time 1648.16ms\n",
            "iter 427: loss 0.0000, time 1702.43ms\n",
            "iter 428: loss 0.0000, time 1712.47ms\n",
            "iter 429: loss 0.0000, time 1676.07ms\n",
            "iter 430: loss 0.0000, time 1819.21ms\n",
            "iter 431: loss 0.0000, time 1789.47ms\n",
            "iter 432: loss 0.0000, time 1673.35ms\n",
            "iter 433: loss 0.0000, time 1659.78ms\n",
            "iter 434: loss 0.0000, time 1670.29ms\n",
            "iter 435: loss 0.0000, time 1685.39ms\n",
            "iter 436: loss 0.0000, time 1666.20ms\n",
            "iter 437: loss 0.0000, time 1683.05ms\n",
            "iter 438: loss 0.0000, time 1619.66ms\n",
            "iter 439: loss 0.0000, time 1682.70ms\n",
            "step 440: train loss 0.0999, val loss 0.3627\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 22366.63ms\n",
            "iter 441: loss 0.0000, time 1697.39ms\n",
            "iter 442: loss 0.0000, time 1713.07ms\n",
            "iter 443: loss 0.0000, time 1701.39ms\n",
            "iter 444: loss 0.0000, time 1668.88ms\n",
            "iter 445: loss 0.0000, time 1669.92ms\n",
            "iter 446: loss 0.0000, time 1675.70ms\n",
            "iter 447: loss 0.0000, time 1764.50ms\n",
            "iter 448: loss 0.0000, time 1822.55ms\n",
            "iter 449: loss 0.0000, time 1660.54ms\n",
            "iter 450: loss 0.0000, time 1719.02ms\n",
            "iter 451: loss 0.0000, time 1672.58ms\n",
            "iter 452: loss 0.0000, time 1667.22ms\n",
            "iter 453: loss 0.0000, time 1753.13ms\n",
            "iter 454: loss 0.0000, time 1847.43ms\n",
            "iter 455: loss 0.0000, time 1708.05ms\n",
            "iter 456: loss 0.0000, time 1678.05ms\n",
            "iter 457: loss 0.0000, time 1690.10ms\n",
            "iter 458: loss 0.0000, time 1642.75ms\n",
            "iter 459: loss 0.0000, time 1756.61ms\n",
            "step 460: train loss 0.0956, val loss 0.3476\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 21847.98ms\n",
            "iter 461: loss 0.0000, time 1688.97ms\n",
            "iter 462: loss 0.0000, time 1666.53ms\n",
            "iter 463: loss 0.0000, time 1654.29ms\n",
            "iter 464: loss 0.0000, time 1675.15ms\n",
            "iter 465: loss 0.0000, time 1704.91ms\n",
            "iter 466: loss 0.0000, time 1657.18ms\n",
            "iter 467: loss 0.0000, time 1692.02ms\n",
            "iter 468: loss 0.0000, time 1666.02ms\n",
            "iter 469: loss 0.0000, time 1690.25ms\n",
            "iter 470: loss 0.0000, time 1704.11ms\n",
            "iter 471: loss 0.0000, time 1844.65ms\n",
            "iter 472: loss 0.0000, time 1758.29ms\n",
            "iter 473: loss 0.0000, time 1696.44ms\n",
            "iter 474: loss 0.0000, time 1723.88ms\n",
            "iter 475: loss 0.0000, time 1658.88ms\n",
            "iter 476: loss 0.0000, time 1690.28ms\n",
            "iter 477: loss 0.0000, time 1793.28ms\n",
            "iter 478: loss 0.0000, time 1782.17ms\n",
            "iter 479: loss 0.0000, time 1659.09ms\n",
            "step 480: train loss 0.0916, val loss 0.3336\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 21424.18ms\n",
            "iter 481: loss 0.0000, time 1668.89ms\n",
            "iter 482: loss 0.0000, time 1728.28ms\n",
            "iter 483: loss 0.0000, time 1856.87ms\n",
            "iter 484: loss 0.0000, time 1752.25ms\n",
            "iter 485: loss 0.0000, time 1666.56ms\n",
            "iter 486: loss 0.0000, time 1635.99ms\n",
            "iter 487: loss 0.0000, time 1689.52ms\n",
            "iter 488: loss 0.0000, time 1682.14ms\n",
            "iter 489: loss 0.0000, time 1701.06ms\n",
            "iter 490: loss 0.0000, time 1647.27ms\n",
            "iter 491: loss 0.0000, time 1685.27ms\n",
            "iter 492: loss 0.0000, time 1659.19ms\n",
            "iter 493: loss 0.0000, time 1681.43ms\n",
            "iter 494: loss 0.0000, time 1740.00ms\n",
            "iter 495: loss 0.0000, time 1828.77ms\n",
            "iter 496: loss 0.0000, time 1727.80ms\n",
            "iter 497: loss 0.0000, time 1708.20ms\n",
            "iter 498: loss 0.0000, time 1724.08ms\n",
            "iter 499: loss 0.0000, time 1649.74ms\n",
            "step 500: train loss 0.0879, val loss 0.3206\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.8333\n",
            "Test precision 0.8671\n",
            "Test recall 0.8333\n",
            "Test F1 0.8283\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 1  9  0]\n",
            " [ 0  4  6]]\n",
            "iter 500: loss 0.0000, time 22330.90ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁▅▇███</td></tr><tr><td>Test_F1_Score</td><td>▁▄▇███</td></tr><tr><td>Test_Precision</td><td>▁▄▇███</td></tr><tr><td>Test_Recall</td><td>▁▅▇███</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▇▇▇▇████▇███▇████████████</td></tr><tr><td>val/loss</td><td> ▄█▇▇▆▅▄▄▄▄▄▃▃▄▃▃▃▃▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.83333</td></tr><tr><td>Test_F1_Score</td><td>0.82833</td></tr><tr><td>Test_Precision</td><td>0.86713</td></tr><tr><td>Test_Recall</td><td>0.83333</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.08795</td></tr><tr><td>val/acc</td><td>0.99136</td></tr><tr><td>val/loss</td><td>0.3206</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">tough-sweep-15</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/sskt5yc5' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/sskt5yc5</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 12 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_055131-sskt5yc5\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7lmcs0qz with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_preprocess: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tif_smote: False\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Ali\\Desktop\\DI725_e2231355_asg1\\DI725\\assignment_1\\wandb\\run-20250402_061447-7lmcs0qz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/7lmcs0qz' target=\"_blank\">comfy-sweep-16</a></strong> to <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/sweeps/9urflm9m</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/7lmcs0qz' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/7lmcs0qz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 40,960\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_15488\\2720948829.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 51, with 124,320,768 parameters\n",
            "num non-decayed parameter tensors: 99, with 121,347 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss nan, val loss nan\n",
            "step val accuracy 0.4241\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.3333\n",
            "Test precision 0.1111\n",
            "Test recall 0.3333\n",
            "Test F1 0.1667\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [10  0  0]\n",
            " [10  0  0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter 0: loss 7.1250, time 21852.55ms\n",
            "iter 1: loss 1.4766, time 1771.72ms\n",
            "iter 2: loss 2.9531, time 1674.53ms\n",
            "iter 3: loss 0.5430, time 1752.54ms\n",
            "iter 4: loss 4.2188, time 1853.49ms\n",
            "iter 5: loss 0.0591, time 1782.07ms\n",
            "iter 6: loss 0.1494, time 1698.75ms\n",
            "iter 7: loss 0.1104, time 1686.77ms\n",
            "iter 8: loss 2.2656, time 1696.01ms\n",
            "iter 9: loss 1.1328, time 1747.85ms\n",
            "iter 10: loss 1.4609, time 1707.61ms\n",
            "iter 11: loss 0.0703, time 1707.67ms\n",
            "iter 12: loss 0.3887, time 1680.17ms\n",
            "iter 13: loss 0.4980, time 1748.39ms\n",
            "iter 14: loss 0.1934, time 1718.49ms\n",
            "iter 15: loss 0.6523, time 1812.89ms\n",
            "iter 16: loss 0.6953, time 1867.48ms\n",
            "iter 17: loss 0.5820, time 1705.51ms\n",
            "iter 18: loss 0.1865, time 1721.05ms\n",
            "iter 19: loss 0.1953, time 1752.81ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 20: train loss 1.1280, val loss 0.8286\n",
            "step val accuracy 0.8672\n",
            "saving checkpoint to out\n",
            "iter 20: loss 0.1270, time 21604.94ms\n",
            "iter 21: loss 0.3457, time 1990.87ms\n",
            "iter 22: loss 0.0033, time 1807.90ms\n",
            "iter 23: loss 2.7813, time 1699.25ms\n",
            "iter 24: loss 0.1050, time 1748.02ms\n",
            "iter 25: loss 0.0525, time 1731.42ms\n",
            "iter 26: loss 2.2031, time 1719.19ms\n",
            "iter 27: loss 0.0102, time 1884.22ms\n",
            "iter 28: loss 2.0781, time 1773.03ms\n",
            "iter 29: loss 0.0206, time 1703.62ms\n",
            "iter 30: loss 0.0815, time 1702.40ms\n",
            "iter 31: loss 1.0312, time 1707.25ms\n",
            "iter 32: loss 0.0078, time 1688.85ms\n",
            "iter 33: loss 0.8086, time 1731.86ms\n",
            "iter 34: loss 0.0020, time 1693.87ms\n",
            "iter 35: loss 0.0051, time 1704.51ms\n",
            "iter 36: loss 0.0381, time 1718.20ms\n",
            "iter 37: loss 0.0410, time 1707.33ms\n",
            "iter 38: loss 0.0060, time 1759.24ms\n",
            "iter 39: loss 1.5156, time 1844.47ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 40: train loss 0.8462, val loss 0.9722\n",
            "step val accuracy 0.8888\n",
            "saving checkpoint to out\n",
            "iter 40: loss 0.2871, time 21631.10ms\n",
            "iter 41: loss 0.0471, time 1714.39ms\n",
            "iter 42: loss 0.1055, time 1702.03ms\n",
            "iter 43: loss 0.0371, time 1701.18ms\n",
            "iter 44: loss 0.1543, time 1810.28ms\n",
            "iter 45: loss 0.0121, time 1849.90ms\n",
            "iter 46: loss 0.3477, time 1686.87ms\n",
            "iter 47: loss 0.0282, time 1720.38ms\n",
            "iter 48: loss 0.1572, time 1736.47ms\n",
            "iter 49: loss 0.0195, time 1699.92ms\n",
            "iter 50: loss 0.5117, time 1813.01ms\n",
            "iter 51: loss 0.1523, time 1830.92ms\n",
            "iter 52: loss 0.1582, time 1668.92ms\n",
            "iter 53: loss 0.0038, time 1720.72ms\n",
            "iter 54: loss 0.0189, time 1713.49ms\n",
            "iter 55: loss 0.0108, time 1704.76ms\n",
            "iter 56: loss 0.0933, time 1729.98ms\n",
            "iter 57: loss 0.2988, time 1730.47ms\n",
            "iter 58: loss 1.2188, time 1714.33ms\n",
            "iter 59: loss 0.0046, time 1733.26ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 60: train loss 0.6509, val loss 0.8194\n",
            "step val accuracy 0.9309\n",
            "saving checkpoint to out\n",
            "iter 60: loss 0.0171, time 21875.73ms\n",
            "iter 61: loss 0.1729, time 1734.23ms\n",
            "iter 62: loss 0.0017, time 1720.80ms\n",
            "iter 63: loss 0.0093, time 1727.67ms\n",
            "iter 64: loss 0.0135, time 1797.81ms\n",
            "iter 65: loss 0.0019, time 1721.65ms\n",
            "iter 66: loss 0.0095, time 1747.37ms\n",
            "iter 67: loss 0.0007, time 1751.80ms\n",
            "iter 68: loss 0.0012, time 1857.15ms\n",
            "iter 69: loss 0.0105, time 1687.40ms\n",
            "iter 70: loss 0.0198, time 1758.93ms\n",
            "iter 71: loss 0.0000, time 1690.55ms\n",
            "iter 72: loss 0.0334, time 1694.39ms\n",
            "iter 73: loss 0.0009, time 1779.85ms\n",
            "iter 74: loss 0.0012, time 1792.24ms\n",
            "iter 75: loss 0.0004, time 1707.98ms\n",
            "iter 76: loss 0.0305, time 1690.50ms\n",
            "iter 77: loss 0.0454, time 1651.22ms\n",
            "iter 78: loss 0.0046, time 1737.61ms\n",
            "iter 79: loss 0.0006, time 1708.28ms\n",
            "step 80: train loss 0.5176, val loss 0.7300\n",
            "step val accuracy 0.9773\n",
            "saving checkpoint to out\n",
            "iter 80: loss 0.0386, time 21758.94ms\n",
            "iter 81: loss 0.0214, time 1700.59ms\n",
            "iter 82: loss 0.0006, time 1687.32ms\n",
            "iter 83: loss 0.0068, time 1711.63ms\n",
            "iter 84: loss 0.0004, time 1710.44ms\n",
            "iter 85: loss 0.0020, time 1825.22ms\n",
            "iter 86: loss 0.0042, time 1676.63ms\n",
            "iter 87: loss 0.0001, time 1674.29ms\n",
            "iter 88: loss 0.0013, time 1655.30ms\n",
            "iter 89: loss 0.2393, time 1673.74ms\n",
            "iter 90: loss 0.0011, time 1688.72ms\n",
            "iter 91: loss 0.0000, time 1808.37ms\n",
            "iter 92: loss 0.0019, time 1748.12ms\n",
            "iter 93: loss 0.0008, time 1721.14ms\n",
            "iter 94: loss 0.0002, time 1706.68ms\n",
            "iter 95: loss 0.0032, time 1664.44ms\n",
            "iter 96: loss 0.0018, time 1710.37ms\n",
            "iter 97: loss 0.0004, time 1828.92ms\n",
            "iter 98: loss 0.0004, time 1689.47ms\n",
            "iter 99: loss 0.0008, time 1689.31ms\n",
            "step 100: train loss 0.4263, val loss 0.7000\n",
            "step val accuracy 0.9330\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7667\n",
            "Test precision 0.8019\n",
            "Test recall 0.7667\n",
            "Test F1 0.7621\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  7  0]\n",
            " [ 0  4  6]]\n",
            "iter 100: loss 0.0166, time 22838.91ms\n",
            "iter 101: loss 0.0004, time 1720.01ms\n",
            "iter 102: loss 0.0000, time 1667.97ms\n",
            "iter 103: loss 0.0002, time 1697.10ms\n",
            "iter 104: loss 0.0001, time 1663.03ms\n",
            "iter 105: loss 0.0000, time 1712.17ms\n",
            "iter 106: loss 0.0008, time 1672.43ms\n",
            "iter 107: loss 0.0042, time 1767.35ms\n",
            "iter 108: loss 0.0029, time 1683.04ms\n",
            "iter 109: loss 0.0000, time 1688.31ms\n",
            "iter 110: loss 0.0001, time 1665.15ms\n",
            "iter 111: loss 0.0000, time 1693.74ms\n",
            "iter 112: loss 0.0000, time 1637.36ms\n",
            "iter 113: loss 0.0000, time 1753.39ms\n",
            "iter 114: loss 0.0000, time 1783.91ms\n",
            "iter 115: loss 0.0023, time 1715.18ms\n",
            "iter 116: loss 0.0004, time 1670.24ms\n",
            "iter 117: loss 0.0021, time 1702.59ms\n",
            "iter 118: loss 0.0001, time 1674.15ms\n",
            "iter 119: loss 0.0010, time 1756.10ms\n",
            "step 120: train loss 0.3652, val loss 0.6255\n",
            "step val accuracy 0.9762\n",
            "saving checkpoint to out\n",
            "iter 120: loss 0.0000, time 21534.13ms\n",
            "iter 121: loss 0.0327, time 1704.58ms\n",
            "iter 122: loss 0.0004, time 1723.70ms\n",
            "iter 123: loss 0.0001, time 1664.69ms\n",
            "iter 124: loss 0.0001, time 1663.17ms\n",
            "iter 125: loss 0.0016, time 1881.48ms\n",
            "iter 126: loss 0.0005, time 1787.40ms\n",
            "iter 127: loss 0.0000, time 1695.35ms\n",
            "iter 128: loss 0.0000, time 1700.17ms\n",
            "iter 129: loss 0.0002, time 1650.75ms\n",
            "iter 130: loss 0.0000, time 1655.41ms\n",
            "iter 131: loss 0.0000, time 1714.70ms\n",
            "iter 132: loss 0.0000, time 1736.47ms\n",
            "iter 133: loss 0.2832, time 1673.84ms\n",
            "iter 134: loss 0.0000, time 1670.33ms\n",
            "iter 135: loss 0.0002, time 1655.83ms\n",
            "iter 136: loss 0.0000, time 1674.35ms\n",
            "iter 137: loss 0.0000, time 1805.31ms\n",
            "iter 138: loss 0.0000, time 1814.78ms\n",
            "iter 139: loss 0.0000, time 1672.92ms\n",
            "step 140: train loss 0.3180, val loss 0.6191\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 140: loss 0.0004, time 21413.93ms\n",
            "iter 141: loss 0.0012, time 1678.72ms\n",
            "iter 142: loss 0.0001, time 1674.52ms\n",
            "iter 143: loss 0.0000, time 1811.04ms\n",
            "iter 144: loss 0.0000, time 1834.52ms\n",
            "iter 145: loss 0.0000, time 1745.34ms\n",
            "iter 146: loss 0.0000, time 1707.47ms\n",
            "iter 147: loss 0.0003, time 1713.97ms\n",
            "iter 148: loss 0.0000, time 1688.23ms\n",
            "iter 149: loss 0.0000, time 1825.64ms\n",
            "iter 150: loss 0.0016, time 1737.19ms\n",
            "iter 151: loss 0.0004, time 1681.45ms\n",
            "iter 152: loss 0.0000, time 1689.89ms\n",
            "iter 153: loss 0.0001, time 1676.00ms\n",
            "iter 154: loss 0.0000, time 1634.35ms\n",
            "iter 155: loss 0.0000, time 1721.87ms\n",
            "iter 156: loss 0.0046, time 1691.46ms\n",
            "iter 157: loss 0.0005, time 1718.87ms\n",
            "iter 158: loss 0.0002, time 1643.61ms\n",
            "iter 159: loss 0.0000, time 1700.05ms\n",
            "step 160: train loss 0.2803, val loss 0.6045\n",
            "step val accuracy 0.9795\n",
            "saving checkpoint to out\n",
            "iter 160: loss 0.0000, time 21683.73ms\n",
            "iter 161: loss 0.0000, time 1677.98ms\n",
            "iter 162: loss 0.0000, time 1696.94ms\n",
            "iter 163: loss 0.0000, time 1710.05ms\n",
            "iter 164: loss 0.0000, time 1683.69ms\n",
            "iter 165: loss 0.0000, time 1652.63ms\n",
            "iter 166: loss 0.0001, time 1699.58ms\n",
            "iter 167: loss 0.0010, time 1801.20ms\n",
            "iter 168: loss 0.0000, time 1718.46ms\n",
            "iter 169: loss 0.0000, time 1722.05ms\n",
            "iter 170: loss 0.0000, time 1700.94ms\n",
            "iter 171: loss 0.0000, time 1642.46ms\n",
            "iter 172: loss 0.0000, time 1728.84ms\n",
            "iter 173: loss 0.1992, time 1801.82ms\n",
            "iter 174: loss 0.0001, time 1756.01ms\n",
            "iter 175: loss 0.0302, time 1637.92ms\n",
            "iter 176: loss 0.0000, time 1649.97ms\n",
            "iter 177: loss 0.0000, time 1716.35ms\n",
            "iter 178: loss 0.0000, time 1689.87ms\n",
            "iter 179: loss 0.0000, time 1665.84ms\n",
            "step 180: train loss 0.2522, val loss 0.6208\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 180: loss 0.0000, time 21646.94ms\n",
            "iter 181: loss 0.0030, time 1627.05ms\n",
            "iter 182: loss 0.0001, time 1704.55ms\n",
            "iter 183: loss 0.0001, time 1677.95ms\n",
            "iter 184: loss 0.0003, time 1804.96ms\n",
            "iter 185: loss 0.0005, time 1667.55ms\n",
            "iter 186: loss 0.0047, time 1683.77ms\n",
            "iter 187: loss 0.0000, time 1686.52ms\n",
            "iter 188: loss 0.0000, time 1669.17ms\n",
            "iter 189: loss 0.0000, time 1663.82ms\n",
            "iter 190: loss 0.0000, time 1797.43ms\n",
            "iter 191: loss 0.0000, time 1825.36ms\n",
            "iter 192: loss 0.0004, time 1708.12ms\n",
            "iter 193: loss 0.0000, time 1676.17ms\n",
            "iter 194: loss 0.0000, time 1665.69ms\n",
            "iter 195: loss 0.0001, time 1690.32ms\n",
            "iter 196: loss 0.0059, time 1819.20ms\n",
            "iter 197: loss 0.0001, time 1788.88ms\n",
            "iter 198: loss 0.0001, time 1677.87ms\n",
            "iter 199: loss 0.0001, time 1647.38ms\n",
            "step 200: train loss 0.2275, val loss 0.6587\n",
            "step val accuracy 0.9903\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1), np.int64(2), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7333\n",
            "Test precision 0.7529\n",
            "Test recall 0.7333\n",
            "Test F1 0.7331\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[9 1 0]\n",
            " [2 7 1]\n",
            " [0 4 6]]\n",
            "iter 200: loss 0.0001, time 22476.53ms\n",
            "iter 201: loss 0.0005, time 1717.49ms\n",
            "iter 202: loss 0.0000, time 1829.37ms\n",
            "iter 203: loss 0.0245, time 1691.70ms\n",
            "iter 204: loss 0.0000, time 1680.19ms\n",
            "iter 205: loss 0.0000, time 1671.24ms\n",
            "iter 206: loss 0.0000, time 1660.52ms\n",
            "iter 207: loss 0.0000, time 1683.51ms\n",
            "iter 208: loss 0.0000, time 1729.01ms\n",
            "iter 209: loss 0.0000, time 1651.20ms\n",
            "iter 210: loss 0.0000, time 1651.26ms\n",
            "iter 211: loss 0.0000, time 1669.35ms\n",
            "iter 212: loss 0.0835, time 1674.30ms\n",
            "iter 213: loss 0.0000, time 1776.95ms\n",
            "iter 214: loss 0.0000, time 1815.79ms\n",
            "iter 215: loss 0.0000, time 1685.63ms\n",
            "iter 216: loss 3.2500, time 1686.28ms\n",
            "iter 217: loss 0.0000, time 1656.91ms\n",
            "iter 218: loss 0.0000, time 1708.42ms\n",
            "iter 219: loss 0.0000, time 1746.70ms\n",
            "step 220: train loss 0.2073, val loss 0.6607\n",
            "step val accuracy 0.9881\n",
            "saving checkpoint to out\n",
            "iter 220: loss 0.0000, time 21422.46ms\n",
            "iter 221: loss 0.0000, time 1752.37ms\n",
            "iter 222: loss 0.0000, time 1692.69ms\n",
            "iter 223: loss 0.0000, time 1701.64ms\n",
            "iter 224: loss 0.0000, time 1704.16ms\n",
            "iter 225: loss 0.0000, time 1814.72ms\n",
            "iter 226: loss 0.0000, time 1819.20ms\n",
            "iter 227: loss 0.0002, time 1678.03ms\n",
            "iter 228: loss 0.0000, time 1635.45ms\n",
            "iter 229: loss 0.0000, time 1713.46ms\n",
            "iter 230: loss 0.0079, time 1687.10ms\n",
            "iter 231: loss 0.0000, time 1677.45ms\n",
            "iter 232: loss 0.0000, time 1647.89ms\n",
            "iter 233: loss 0.0000, time 1706.35ms\n",
            "iter 234: loss 0.0000, time 1662.04ms\n",
            "iter 235: loss 0.0000, time 1683.03ms\n",
            "iter 236: loss 0.0000, time 1688.66ms\n",
            "iter 237: loss 0.0000, time 1813.78ms\n",
            "iter 238: loss 0.0005, time 1748.46ms\n",
            "iter 239: loss 0.0000, time 1720.36ms\n",
            "step 240: train loss 0.1905, val loss 0.6501\n",
            "step val accuracy 0.9860\n",
            "saving checkpoint to out\n",
            "iter 240: loss 0.0001, time 21474.58ms\n",
            "iter 241: loss 0.0000, time 1692.29ms\n",
            "iter 242: loss 0.0001, time 1644.45ms\n",
            "iter 243: loss 0.0000, time 1798.29ms\n",
            "iter 244: loss 0.0000, time 1819.19ms\n",
            "iter 245: loss 0.0001, time 1688.55ms\n",
            "iter 246: loss 0.0000, time 1670.98ms\n",
            "iter 247: loss 0.0014, time 1649.20ms\n",
            "iter 248: loss 0.0000, time 1724.60ms\n",
            "iter 249: loss 0.0000, time 1785.74ms\n",
            "iter 250: loss 0.0000, time 1744.28ms\n",
            "iter 251: loss 0.0000, time 1658.78ms\n",
            "iter 252: loss 0.0001, time 1695.30ms\n",
            "iter 253: loss 0.0000, time 1644.14ms\n",
            "iter 254: loss 0.0000, time 1690.51ms\n",
            "iter 255: loss 0.0000, time 1650.17ms\n",
            "iter 256: loss 0.0000, time 1656.88ms\n",
            "iter 257: loss 0.0000, time 1659.47ms\n",
            "iter 258: loss 0.0000, time 1657.22ms\n",
            "iter 259: loss 0.0000, time 1631.83ms\n",
            "step 260: train loss 0.1759, val loss 0.6558\n",
            "step val accuracy 0.9924\n",
            "saving checkpoint to out\n",
            "iter 260: loss 0.0000, time 21834.16ms\n",
            "iter 261: loss 0.0000, time 1668.44ms\n",
            "iter 262: loss 0.0000, time 1663.40ms\n",
            "iter 263: loss 0.0000, time 1685.34ms\n",
            "iter 264: loss 0.0000, time 1646.10ms\n",
            "iter 265: loss 0.0000, time 1665.06ms\n",
            "iter 266: loss 0.0000, time 1711.84ms\n",
            "iter 267: loss 0.0000, time 1842.61ms\n",
            "iter 268: loss 0.0000, time 1764.04ms\n",
            "iter 269: loss 0.0000, time 1629.67ms\n",
            "iter 270: loss 0.0000, time 1653.37ms\n",
            "iter 271: loss 0.0000, time 1658.48ms\n",
            "iter 272: loss 0.0000, time 1715.04ms\n",
            "iter 273: loss 0.0000, time 1782.01ms\n",
            "iter 274: loss 0.0000, time 1718.82ms\n",
            "iter 275: loss 0.0000, time 1692.52ms\n",
            "iter 276: loss 0.0000, time 1661.99ms\n",
            "iter 277: loss 0.0000, time 1680.37ms\n",
            "iter 278: loss 0.0000, time 1642.66ms\n",
            "iter 279: loss 0.0000, time 1711.11ms\n",
            "step 280: train loss 0.1633, val loss 0.6350\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 280: loss 0.0000, time 21825.87ms\n",
            "iter 281: loss 0.0000, time 1660.60ms\n",
            "iter 282: loss 0.0000, time 1669.78ms\n",
            "iter 283: loss 0.0000, time 1633.12ms\n",
            "iter 284: loss 0.0000, time 1702.36ms\n",
            "iter 285: loss 0.0000, time 1665.56ms\n",
            "iter 286: loss 0.0000, time 1648.75ms\n",
            "iter 287: loss 0.0000, time 1636.55ms\n",
            "iter 288: loss 0.0000, time 1678.66ms\n",
            "iter 289: loss 0.0000, time 1701.15ms\n",
            "iter 290: loss 0.0000, time 1679.19ms\n",
            "iter 291: loss 0.0000, time 1786.39ms\n",
            "iter 292: loss 0.0000, time 1716.38ms\n",
            "iter 293: loss 0.0000, time 1675.94ms\n",
            "iter 294: loss 0.0000, time 1686.60ms\n",
            "iter 295: loss 0.0000, time 1683.79ms\n",
            "iter 296: loss 0.0000, time 1689.94ms\n",
            "iter 297: loss 0.0000, time 1805.36ms\n",
            "iter 298: loss 0.0000, time 1670.89ms\n",
            "iter 299: loss 0.0000, time 1712.59ms\n",
            "step 300: train loss 0.1524, val loss 0.6243\n",
            "step val accuracy 0.9892\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7692\n",
            "Test recall 0.7000\n",
            "Test F1 0.6832\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  7  0]\n",
            " [ 0  6  4]]\n",
            "iter 300: loss 0.0000, time 22443.46ms\n",
            "iter 301: loss 0.0000, time 1708.20ms\n",
            "iter 302: loss 0.0000, time 1782.00ms\n",
            "iter 303: loss 0.0001, time 1738.70ms\n",
            "iter 304: loss 0.0000, time 1688.25ms\n",
            "iter 305: loss 0.0000, time 1677.28ms\n",
            "iter 306: loss 0.0000, time 1667.42ms\n",
            "iter 307: loss 0.0000, time 1895.62ms\n",
            "iter 308: loss 0.0000, time 1788.30ms\n",
            "iter 309: loss 0.0000, time 1707.99ms\n",
            "iter 310: loss 0.0000, time 1665.76ms\n",
            "iter 311: loss 0.0000, time 1693.60ms\n",
            "iter 312: loss 0.0000, time 1670.96ms\n",
            "iter 313: loss 0.0000, time 1825.30ms\n",
            "iter 314: loss 0.0000, time 1815.16ms\n",
            "iter 315: loss 0.0000, time 1721.23ms\n",
            "iter 316: loss 0.0000, time 1683.46ms\n",
            "iter 317: loss 0.0000, time 1657.38ms\n",
            "iter 318: loss 0.0000, time 1643.75ms\n",
            "iter 319: loss 0.0000, time 1782.40ms\n",
            "step 320: train loss 0.1429, val loss 0.6500\n",
            "step val accuracy 0.9892\n",
            "saving checkpoint to out\n",
            "iter 320: loss 0.0000, time 21376.88ms\n",
            "iter 321: loss 0.0001, time 1704.87ms\n",
            "iter 322: loss 0.0000, time 1750.53ms\n",
            "iter 323: loss 0.0000, time 1672.24ms\n",
            "iter 324: loss 0.0000, time 1662.08ms\n",
            "iter 325: loss 0.0000, time 1785.97ms\n",
            "iter 326: loss 0.0000, time 1807.12ms\n",
            "iter 327: loss 0.0000, time 1672.95ms\n",
            "iter 328: loss 0.0000, time 1688.74ms\n",
            "iter 329: loss 0.0000, time 1689.82ms\n",
            "iter 330: loss 0.0000, time 1669.97ms\n",
            "iter 331: loss 0.0000, time 1704.53ms\n",
            "iter 332: loss 0.0000, time 1667.56ms\n",
            "iter 333: loss 0.0001, time 1659.13ms\n",
            "iter 334: loss 0.0000, time 1672.47ms\n",
            "iter 335: loss 0.0000, time 1669.90ms\n",
            "iter 336: loss 0.0000, time 1693.62ms\n",
            "iter 337: loss 0.0000, time 1784.52ms\n",
            "iter 338: loss 0.0000, time 1806.08ms\n",
            "iter 339: loss 0.0000, time 1669.31ms\n",
            "step 340: train loss 0.1345, val loss 0.6705\n",
            "step val accuracy 0.9892\n",
            "saving checkpoint to out\n",
            "iter 340: loss 0.0000, time 21357.04ms\n",
            "iter 341: loss 0.0000, time 1684.79ms\n",
            "iter 342: loss 0.0000, time 1677.18ms\n",
            "iter 343: loss 0.0000, time 1811.31ms\n",
            "iter 344: loss 0.0000, time 1809.57ms\n",
            "iter 345: loss 0.0000, time 1686.88ms\n",
            "iter 346: loss 0.0000, time 1669.85ms\n",
            "iter 347: loss 0.0000, time 1708.79ms\n",
            "iter 348: loss 0.0000, time 1637.83ms\n",
            "iter 349: loss 0.0000, time 1827.04ms\n",
            "iter 350: loss 0.0000, time 1804.60ms\n",
            "iter 351: loss 0.0000, time 1687.73ms\n",
            "iter 352: loss 0.0000, time 1670.41ms\n",
            "iter 353: loss 0.0000, time 1645.63ms\n",
            "iter 354: loss 0.0000, time 1720.19ms\n",
            "iter 355: loss 0.0000, time 1697.92ms\n",
            "iter 356: loss 0.0000, time 1740.29ms\n",
            "iter 357: loss 0.0000, time 1662.14ms\n",
            "iter 358: loss 0.0000, time 1640.48ms\n",
            "iter 359: loss 0.0000, time 1665.64ms\n",
            "step 360: train loss 0.1270, val loss 0.6932\n",
            "step val accuracy 0.9903\n",
            "saving checkpoint to out\n",
            "iter 360: loss 0.0000, time 21793.37ms\n",
            "iter 361: loss 0.0000, time 1811.65ms\n",
            "iter 362: loss 0.0000, time 1729.53ms\n",
            "iter 363: loss 0.0000, time 1654.40ms\n",
            "iter 364: loss 0.0000, time 1683.76ms\n",
            "iter 365: loss 0.0000, time 1653.61ms\n",
            "iter 366: loss 0.0000, time 1662.82ms\n",
            "iter 367: loss 0.0000, time 1654.60ms\n",
            "iter 368: loss 0.0000, time 1736.73ms\n",
            "iter 369: loss 0.0000, time 1672.26ms\n",
            "iter 370: loss 0.0000, time 1676.45ms\n",
            "iter 371: loss 0.0000, time 1635.57ms\n",
            "iter 372: loss 0.0000, time 1744.31ms\n",
            "iter 373: loss 0.0000, time 1870.24ms\n",
            "iter 374: loss 0.0000, time 1759.32ms\n",
            "iter 375: loss 0.0000, time 1628.06ms\n",
            "iter 376: loss 0.0000, time 1683.62ms\n",
            "iter 377: loss 0.0000, time 1670.47ms\n",
            "iter 378: loss 0.0000, time 1759.20ms\n",
            "iter 379: loss 0.0000, time 1850.05ms\n",
            "step 380: train loss 0.1203, val loss 0.7147\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 380: loss 0.0000, time 21263.57ms\n",
            "iter 381: loss 0.0000, time 1688.32ms\n",
            "iter 382: loss 0.0000, time 1678.74ms\n",
            "iter 383: loss 0.0001, time 1683.79ms\n",
            "iter 384: loss 0.0000, time 1745.26ms\n",
            "iter 385: loss 0.0000, time 1854.62ms\n",
            "iter 386: loss 0.0000, time 1711.99ms\n",
            "iter 387: loss 0.0000, time 1685.36ms\n",
            "iter 388: loss 0.0000, time 1675.54ms\n",
            "iter 389: loss 0.0000, time 1660.67ms\n",
            "iter 390: loss 0.0000, time 1668.60ms\n",
            "iter 391: loss 0.0000, time 1716.23ms\n",
            "iter 392: loss 0.0000, time 1695.19ms\n",
            "iter 393: loss 0.0000, time 1697.60ms\n",
            "iter 394: loss 0.0000, time 1774.67ms\n",
            "iter 395: loss 0.0000, time 1844.63ms\n",
            "iter 396: loss 0.0000, time 1785.31ms\n",
            "iter 397: loss 0.0000, time 1824.56ms\n",
            "iter 398: loss 0.0000, time 1657.30ms\n",
            "iter 399: loss 0.0000, time 1671.59ms\n",
            "step 400: train loss 0.1143, val loss 0.7340\n",
            "step val accuracy 0.9903\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7692\n",
            "Test recall 0.7000\n",
            "Test F1 0.6832\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  7  0]\n",
            " [ 0  6  4]]\n",
            "iter 400: loss 0.0000, time 22475.29ms\n",
            "iter 401: loss 0.0000, time 1717.58ms\n",
            "iter 402: loss 0.0000, time 1823.60ms\n",
            "iter 403: loss 0.0000, time 1727.62ms\n",
            "iter 404: loss 0.0000, time 1664.82ms\n",
            "iter 405: loss 0.0000, time 1644.64ms\n",
            "iter 406: loss 0.0000, time 1672.49ms\n",
            "iter 407: loss 0.0000, time 1773.90ms\n",
            "iter 408: loss 0.0000, time 1836.09ms\n",
            "iter 409: loss 0.0000, time 1684.27ms\n",
            "iter 410: loss 0.0000, time 1658.16ms\n",
            "iter 411: loss 0.0000, time 1712.02ms\n",
            "iter 412: loss 0.0000, time 1665.78ms\n",
            "iter 413: loss 0.0000, time 1670.40ms\n",
            "iter 414: loss 0.0000, time 1690.84ms\n",
            "iter 415: loss 0.0000, time 1714.24ms\n",
            "iter 416: loss 0.0000, time 1656.69ms\n",
            "iter 417: loss 0.0000, time 1697.31ms\n",
            "iter 418: loss 0.0000, time 1644.19ms\n",
            "iter 419: loss 0.0000, time 1748.75ms\n",
            "step 420: train loss 0.1089, val loss 0.7524\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 420: loss 0.0000, time 21625.94ms\n",
            "iter 421: loss 0.0000, time 1673.35ms\n",
            "iter 422: loss 0.0000, time 1672.41ms\n",
            "iter 423: loss 0.0000, time 1660.51ms\n",
            "iter 424: loss 0.0000, time 1714.97ms\n",
            "iter 425: loss 0.0000, time 1811.11ms\n",
            "iter 426: loss 0.0000, time 1814.39ms\n",
            "iter 427: loss 0.0000, time 1665.67ms\n",
            "iter 428: loss 0.0000, time 1646.59ms\n",
            "iter 429: loss 0.0000, time 1656.32ms\n",
            "iter 430: loss 0.0000, time 1689.35ms\n",
            "iter 431: loss 0.0000, time 1826.47ms\n",
            "iter 432: loss 0.0000, time 1750.12ms\n",
            "iter 433: loss 0.0000, time 1645.74ms\n",
            "iter 434: loss 0.0000, time 1679.44ms\n",
            "iter 435: loss 0.0000, time 1661.49ms\n",
            "iter 436: loss 0.0000, time 1720.74ms\n",
            "iter 437: loss 0.0000, time 1665.95ms\n",
            "iter 438: loss 0.0000, time 1733.04ms\n",
            "iter 439: loss 0.0000, time 1675.79ms\n",
            "step 440: train loss 0.1039, val loss 0.7700\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 440: loss 0.0000, time 21690.75ms\n",
            "iter 441: loss 0.0000, time 1659.23ms\n",
            "iter 442: loss 0.0000, time 1659.73ms\n",
            "iter 443: loss 0.0000, time 1678.35ms\n",
            "iter 444: loss 0.0000, time 1684.24ms\n",
            "iter 445: loss 0.0000, time 1658.23ms\n",
            "iter 446: loss 0.0000, time 1644.68ms\n",
            "iter 447: loss 0.0000, time 1664.65ms\n",
            "iter 448: loss 0.0000, time 1643.61ms\n",
            "iter 449: loss 0.0000, time 1814.03ms\n",
            "iter 450: loss 0.0000, time 1741.41ms\n",
            "iter 451: loss 0.0000, time 1654.40ms\n",
            "iter 452: loss 0.0000, time 1661.42ms\n",
            "iter 453: loss 0.0000, time 1682.02ms\n",
            "iter 454: loss 0.0000, time 1727.46ms\n",
            "iter 455: loss 0.0000, time 1813.82ms\n",
            "iter 456: loss 0.0000, time 1721.39ms\n",
            "iter 457: loss 0.0000, time 1681.75ms\n",
            "iter 458: loss 0.0000, time 1656.79ms\n",
            "iter 459: loss 0.0000, time 1660.39ms\n",
            "step 460: train loss 0.0994, val loss 0.7853\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 460: loss 0.0000, time 21485.15ms\n",
            "iter 461: loss 0.0000, time 1812.66ms\n",
            "iter 462: loss 0.0000, time 1818.93ms\n",
            "iter 463: loss 0.0000, time 1633.19ms\n",
            "iter 464: loss 0.0000, time 1655.77ms\n",
            "iter 465: loss 0.0000, time 1682.16ms\n",
            "iter 466: loss 0.0000, time 1682.55ms\n",
            "iter 467: loss 0.0000, time 1721.57ms\n",
            "iter 468: loss 0.0000, time 1671.48ms\n",
            "iter 469: loss 0.0000, time 1694.13ms\n",
            "iter 470: loss 0.0000, time 1800.25ms\n",
            "iter 471: loss 0.0000, time 1675.98ms\n",
            "iter 472: loss 0.0000, time 1712.88ms\n",
            "iter 473: loss 0.0000, time 1834.50ms\n",
            "iter 474: loss 0.0000, time 1740.17ms\n",
            "iter 475: loss 0.0000, time 1651.77ms\n",
            "iter 476: loss 0.0000, time 1681.48ms\n",
            "iter 477: loss 0.0000, time 1636.73ms\n",
            "iter 478: loss 0.0000, time 1776.37ms\n",
            "iter 479: loss 0.0000, time 1852.93ms\n",
            "step 480: train loss 0.0953, val loss 0.8011\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "iter 480: loss 0.0000, time 21259.97ms\n",
            "iter 481: loss 0.0000, time 1675.81ms\n",
            "iter 482: loss 0.0000, time 1630.84ms\n",
            "iter 483: loss 0.0000, time 1693.88ms\n",
            "iter 484: loss 0.0000, time 1809.29ms\n",
            "iter 485: loss 0.0000, time 1812.75ms\n",
            "iter 486: loss 0.0000, time 1754.37ms\n",
            "iter 487: loss 0.0000, time 1669.95ms\n",
            "iter 488: loss 0.0000, time 1654.65ms\n",
            "iter 489: loss 0.0000, time 1660.86ms\n",
            "iter 490: loss 0.0000, time 1645.93ms\n",
            "iter 491: loss 0.0000, time 1686.90ms\n",
            "iter 492: loss 0.0000, time 1655.73ms\n",
            "iter 493: loss 0.0000, time 1681.68ms\n",
            "iter 494: loss 0.0000, time 1685.41ms\n",
            "iter 495: loss 0.0000, time 1625.47ms\n",
            "iter 496: loss 0.0000, time 1739.11ms\n",
            "iter 497: loss 0.0000, time 1833.69ms\n",
            "iter 498: loss 0.0000, time 1677.67ms\n",
            "iter 499: loss 0.0000, time 1678.49ms\n",
            "step 500: train loss 0.0915, val loss 0.8155\n",
            "step val accuracy 0.9914\n",
            "saving checkpoint to out\n",
            "Number of Samples Test =  30\n",
            "Labels and Preds\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(1)]\n",
            "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2), np.int64(2)]\n",
            "Test accuracy 0.7000\n",
            "Test precision 0.7692\n",
            "Test recall 0.7000\n",
            "Test F1 0.6832\n",
            "Confusion Matrix\n",
            "------------------\n",
            "[[10  0  0]\n",
            " [ 3  7  0]\n",
            " [ 0  6  4]]\n",
            "iter 500: loss 0.0000, time 22506.86ms\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>▁█▇▇▇▇</td></tr><tr><td>Test_F1_Score</td><td>▁██▇▇▇</td></tr><tr><td>Test_Precision</td><td>▁█████</td></tr><tr><td>Test_Recall</td><td>▁█▇▇▇▇</td></tr><tr><td>iter</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td> █▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▆▇▇█▇████████████████████</td></tr><tr><td>val/loss</td><td> ▅█▅▃▃▁▁▁▁▂▂▂▂▂▁▂▂▃▃▃▄▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_Accuracy</td><td>0.7</td></tr><tr><td>Test_F1_Score</td><td>0.68323</td></tr><tr><td>Test_Precision</td><td>0.76923</td></tr><tr><td>Test_Recall</td><td>0.7</td></tr><tr><td>iter</td><td>500</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.09145</td></tr><tr><td>val/acc</td><td>0.99136</td></tr><tr><td>val/loss</td><td>0.81546</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comfy-sweep-16</strong> at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/7lmcs0qz' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune/runs/7lmcs0qz</a><br> View project at: <a href='https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune' target=\"_blank\">https://wandb.ai/aliyigitbasaran-/DI725-HW1-HyperParamTune</a><br>Synced 5 W&B file(s), 6 media file(s), 8 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20250402_061447-7lmcs0qz\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter Tuning Script\n",
        "\"\"\"\n",
        "This training script can be run both on a single gpu in debug mode,\n",
        "and also in a larger training run with distributed data parallel (ddp).\n",
        "\n",
        "To run on a single GPU, example:\n",
        "$ python train.py --batch_size=32 --compile=False\n",
        "\n",
        "To run with DDP on 4 gpus on 1 node, example:\n",
        "$ torchrun --standalone --nproc_per_node=4 train.py\n",
        "\n",
        "To run with DDP on 4 gpus across 2 nodes, example:\n",
        "- Run on the first (master) node with example IP 123.456.123.456:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "- Run on the worker node:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "from model import GPTConfig, GPT\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import numpy as np\n",
        "from torchmetrics import MeanMetric\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from transformers import GPT2ForSequenceClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import wandb\n",
        "\n",
        "\n",
        "def configure_optimizers(model, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in model.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "\n",
        "def train():\n",
        "  with wandb.init() as run:\n",
        "    wandb_config = wandb.config\n",
        "    out_dir = 'out'\n",
        "    eval_interval = 20\n",
        "    eval_test_interval = 100\n",
        "    log_interval = 1\n",
        "    eval_iters = 200\n",
        "    if_SMOTE = wandb_config[\"if_smote\"]\n",
        "    eval_only = False # if True, script exits right after the first eval\n",
        "    always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "    #init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "    init_from=\"gpt2\"\n",
        "    #init_from = 'scratch'\n",
        "    # wandb logging\n",
        "    wandb_log = True # disabled by default\n",
        "    # data\n",
        "    #dataset = 'openwebtext'\n",
        "    gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "    batch_size = 1 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "    block_size = 1024\n",
        "    # model\n",
        "    n_layer = 12\n",
        "    n_head = 12\n",
        "    n_embd = 768\n",
        "    dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "    bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "    # adamw optimizer\n",
        "    #learning_rate = 6e-4 # max learning rate\n",
        "    max_iters = 500 # total number of training iterations\n",
        "    weight_decay = 1e-1\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.95\n",
        "    grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "    # learning rate decay settings\n",
        "    learning_rate=wandb_config[\"learning_rate\"]\n",
        "    decay_lr = False # whether to decay the learning rate\n",
        "    warmup_iters = 2000 # how many steps to warm up for\n",
        "    lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "    min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "    # DDP settings\n",
        "    backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "    # system\n",
        "    #device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "    device = \"cuda\"\n",
        "    dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "    compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "    # -----------------------------------------------------------------------------\n",
        "    config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "    #exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "    config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "    # -----------------------------------------------------------------------------\n",
        "\n",
        "    # various inits, derived attributes, I/O setup\n",
        "    ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "    if ddp:\n",
        "        init_process_group(backend=backend)\n",
        "        ddp_rank = int(os.environ['RANK'])\n",
        "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "        device = f'cuda:{ddp_local_rank}'\n",
        "        torch.cuda.set_device(device)\n",
        "        master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "        seed_offset = ddp_rank # each process gets a different seed\n",
        "        # world_size number of processes will be training simultaneously, so we can scale\n",
        "        # down the desired gradient accumulation iterations per process proportionally\n",
        "        assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "        gradient_accumulation_steps //= ddp_world_size\n",
        "    else:\n",
        "        # if not ddp, we are running on a single gpu, and one process\n",
        "        master_process = True\n",
        "        seed_offset = 0\n",
        "        ddp_world_size = 1\n",
        "    tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "    print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "    if master_process:\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "    torch.manual_seed(1337 + seed_offset)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "    torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "    device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "    # note: float16 data type will automatically use a GradScaler\n",
        "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "    # poor man's data loader\n",
        "    data_dir = os.path.join('data', \"random\")\n",
        "\n",
        "\n",
        "    # init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "    iter_num = 0\n",
        "    best_val_loss = 1e9\n",
        "\n",
        "\n",
        "\n",
        "    # attempt to derive vocab_size from the dataset\n",
        "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "    meta_vocab_size = None\n",
        "    if os.path.exists(meta_path):\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f)\n",
        "        meta_vocab_size = meta['vocab_size']\n",
        "        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "    # model init\n",
        "    model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                      bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "    if init_from == 'scratch':\n",
        "        # init a new model from scratch\n",
        "        print(\"Initializing a new model from scratch\")\n",
        "        # determine the vocab size we'll use for from-scratch training\n",
        "        if meta_vocab_size is None:\n",
        "            print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "        model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model_gpt = GPT(gptconf)\n",
        "        model = GPTSentiment(gptconf)\n",
        "    elif init_from == 'resume':\n",
        "        print(f\"Resuming training from {out_dir}\")\n",
        "        # resume training from a checkpoint.\n",
        "        ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "        checkpoint_model_args = checkpoint['model_args']\n",
        "        # force these config attributes to be equal otherwise we can't even resume training\n",
        "        # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "            model_args[k] = checkpoint_model_args[k]\n",
        "        # create the model\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model_gpt = GPT(gptconf)\n",
        "        state_dict = checkpoint['model']\n",
        "        # fix the keys of the state dictionary :(\n",
        "        # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "        unwanted_prefix = '_orig_mod.'\n",
        "        for k,v in list(state_dict.items()):\n",
        "            if k.startswith(unwanted_prefix):\n",
        "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "        model_gpt.load_state_dict(state_dict)\n",
        "        model = GPTSentiment(model_gpt)\n",
        "        iter_num = checkpoint['iter_num']\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "    elif init_from.startswith('gpt2'):\n",
        "        print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "        # initialize from OpenAI GPT-2 weights\n",
        "        override_args = dict(dropout=dropout)\n",
        "        model_gpt = GPT.from_pretrained(init_from, override_args)\n",
        "        # read off the created config params, so we can store them into checkpoint correctly\n",
        "        for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "            model_args[k] = getattr(model_gpt.config, k)\n",
        "        gptconf = GPTConfig(**model_args)\n",
        "        model = GPTSentiment(gptconf)\n",
        "        model.load_state_dict(model_gpt.state_dict(),strict=False)\n",
        "\n",
        "\n",
        "\n",
        "    # crop down the model block size if desired, using model surgery\n",
        "    #if block_size < model.config.block_size:\n",
        "    #    model.crop_block_size(block_size)\n",
        "    #    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "    model.to(device)\n",
        "\n",
        "    # initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = configure_optimizers(model,weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "    if init_from == 'resume':\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    checkpoint = None # free up memory\n",
        "\n",
        "    # compile the model\n",
        "    if compile:\n",
        "        print(\"compiling the model... (takes a ~minute)\")\n",
        "        unoptimized_model = model\n",
        "        model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "    # wrap model into DDP container\n",
        "    if ddp:\n",
        "        model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "    # helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "\n",
        "\n",
        "    # learning rate decay scheduler (cosine with warmup)\n",
        "    def get_lr(it):\n",
        "        # 1) linear warmup for warmup_iters steps\n",
        "        if it < warmup_iters:\n",
        "            return learning_rate * it / warmup_iters\n",
        "        # 2) if it > lr_decay_iters, return min learning rate\n",
        "        if it > lr_decay_iters:\n",
        "            return min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "        return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "    # logging\n",
        "    #if wandb_log and master_process:\n",
        "    #    import wandb\n",
        "    #    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "    # training loop\n",
        "\n",
        "\n",
        "    # Assuming 'filepath' is the path to your CSV file\n",
        "    train_df = pd.read_csv('./data/customer_service/train.csv')\n",
        "    test_df = pd.read_csv('./data/customer_service/test.csv')\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize tokenizers, datasets, and dataloaders\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    if if_SMOTE:\n",
        "        sm = RandomOverSampler(random_state=42)\n",
        "        X_train_smote, y_train_smote = sm.fit_resample(train_df[[\"conversation\"]],train_df[\"customer_sentiment\"])\n",
        "        train_df_smoted = pd.concat([X_train_smote, y_train_smote.reset_index(drop=True)], axis=1)\n",
        "        train_dataset = SentimentDataset(train_df_smoted, tokenizer, max_length=512,padding=False,augment=wandb_config[\"augment\"],take_only_customer=False,if_preprocess=wandb_config[\"if_preprocess\"])\n",
        "    else:\n",
        "        train_dataset = SentimentDataset(train_df, tokenizer, max_length=512,padding=False,augment=wandb_config[\"augment\"],take_only_customer=False,if_preprocess=wandb_config[\"if_preprocess\"])\n",
        "    val_dataset = SentimentDataset(val_df, tokenizer, max_length=512,padding=False,take_only_customer=False,if_preprocess=wandb_config[\"if_preprocess\"])\n",
        "    test_dataset = SentimentDataset(test_df, tokenizer, max_length=512,padding=False,take_only_customer=False,if_preprocess=wandb_config[\"if_preprocess\"])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "    t0 = time.time()\n",
        "    local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "    raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "    running_mfu = -1.0\n",
        "\n",
        "\n",
        "    def plot_confusion_matrix(conf_matrix, x_labels, y_labels):\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        cax = ax.matshow(conf_matrix, cmap=plt.cm.Blues)\n",
        "        plt.title('Confusion Matrix')\n",
        "        fig.colorbar(cax)\n",
        "        ax.set_xticklabels([''] + x_labels)\n",
        "        ax.set_yticklabels([''] + y_labels)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.xticks(rotation=90)\n",
        "\n",
        "        # Adding text to each cell\n",
        "        for (i, j), val in np.ndenumerate(conf_matrix):\n",
        "            ax.text(j, i, f'{val}', ha='center', va='center', color='red')\n",
        "\n",
        "        plt.close(fig)  # Prevents the figure from being displayed in the notebook\n",
        "\n",
        "        # Convert to PIL Image\n",
        "        buf = io.BytesIO()\n",
        "        fig.savefig(buf, format='png')\n",
        "        buf.seek(0)\n",
        "        img = Image.open(buf)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "    def estimate_loss(train_loader_iter,val_loader_iter,num_train,num_val):\n",
        "        out = {\"train\":{},\"val\":{}}\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for split in ['train', 'val']:\n",
        "\n",
        "\n",
        "\n",
        "            if split==\"train\":\n",
        "              if train_loader.batch_size == 1:\n",
        "                  iter_num= num_train\n",
        "              else:\n",
        "                  iter_num = min( num_train // train_loader.batch_size + 1  , eval_iters // train_loader.batch_size + 1)\n",
        "            else:\n",
        "              if val_loader.batch_size == 1:\n",
        "                  iter_num = num_val\n",
        "              else:\n",
        "                  iter_num = min( num_val // val_loader.batch_size + 1  , eval_iters // val_loader.batch_size + 1)\n",
        "\n",
        "            losses = torch.zeros(iter_num)\n",
        "            for k in range(iter_num):\n",
        "                if split == \"train\":\n",
        "                    try:\n",
        "                        batch = next(train_loader_iter)\n",
        "                        X = batch['input_ids'].to(device)\n",
        "                        attention_mask = batch['attention_mask'].to(device)\n",
        "                        Y = batch['labels'].to(device)\n",
        "                    except:\n",
        "                      continue\n",
        "                else:\n",
        "                    try:\n",
        "                        batch = next(val_loader_iter)\n",
        "                        X = batch['input_ids'].to(device)\n",
        "                        attention_mask = batch['attention_mask'].to(device)\n",
        "                        Y = batch['labels'].to(device)\n",
        "                    except:\n",
        "                      continue\n",
        "                with ctx:\n",
        "                    logits, loss = model(X,Y)\n",
        "                    preds = torch.argmax(logits, dim=-1)\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_labels.extend(Y.cpu().numpy())\n",
        "                losses[k] = loss.item()\n",
        "            accuracy = accuracy_score(all_labels, all_preds)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "            conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "            out[split][\"loss\"] = losses.mean()\n",
        "            out[split][\"accuracy\"] = accuracy\n",
        "            out[split][\"precision\"] = precision\n",
        "            out[split][\"recall\"] = recall\n",
        "            out[split][\"f1\"] = f1\n",
        "            out[split][\"conf_matrix\"] = conf_matrix\n",
        "\n",
        "        model.train()\n",
        "        return out\n",
        "\n",
        "\n",
        "    def estimate_loss_test(test_loader,num_samples):\n",
        "        out = {\"test\":{}}\n",
        "        test_loader_iter = iter(test_loader)\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for split in ['test']:\n",
        "            if test_loader.batch_size == 1:\n",
        "              iter_num = num_samples\n",
        "            else:\n",
        "              iter_num = min( num_samples // test_loader.batch_size + 1  , eval_iters // test_loader.batch_size + 1)\n",
        "            losses = torch.zeros(iter_num)\n",
        "            print(\"Number of Samples Test = \",iter_num)\n",
        "            for k in range(iter_num):\n",
        "                if split == \"test\":\n",
        "                    try:\n",
        "                      batch = next(test_loader_iter)\n",
        "                      X = batch['input_ids'].to(device)\n",
        "                      attention_mask = batch['attention_mask'].to(device)\n",
        "                      Y = batch['labels'].to(device)\n",
        "                    except:\n",
        "                      continue\n",
        "                with ctx:\n",
        "                    logits,loss = model(X,Y)\n",
        "                    preds = torch.argmax(logits, dim=-1)\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_labels.extend(Y.cpu().numpy())\n",
        "                losses[k] = loss.item()\n",
        "            print(\"Labels and Preds\")\n",
        "            print(all_preds)\n",
        "            print(all_labels)\n",
        "            accuracy = accuracy_score(all_labels, all_preds)\n",
        "            #print(\"All Labels\",all_labels)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "            conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "            out[split][\"loss\"] = losses.mean()\n",
        "            out[split][\"accuracy\"] = accuracy\n",
        "            out[split][\"precision\"] = precision\n",
        "            out[split][\"recall\"] = recall\n",
        "            out[split][\"f1\"] = f1\n",
        "            out[split][\"conf_matrix\"] = conf_matrix\n",
        "\n",
        "        model.train()\n",
        "        return out,all_labels,all_preds\n",
        "\n",
        "    train_loader_iter = iter(train_loader)\n",
        "    test_loader_iter = iter(test_loader)\n",
        "    val_loader_iter = iter(val_loader)\n",
        "    batch = next(train_loader_iter)\n",
        "    X = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    Y = batch['labels'].to(device)\n",
        "\n",
        "    train_loss_metric = MeanMetric()\n",
        "    val_loss_metric = MeanMetric()\n",
        "    while True:\n",
        "\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # evaluate the loss on train/val sets and write checkpoints\n",
        "        if iter_num % eval_interval == 0 and master_process:\n",
        "            out_res = estimate_loss(train_loader_iter,val_loader_iter,len(train_dataset), len(val_dataset))\n",
        "            print(f\"step {iter_num}: train loss {train_loss_metric.compute():.4f}, val loss {val_loss_metric.compute():.4f}\")\n",
        "            print(f\"step val accuracy {out_res['val']['accuracy']:.4f}\")\n",
        "\n",
        "            if wandb_log:\n",
        "                wandb.log({\n",
        "                    \"iter\": iter_num,\n",
        "                    \"train/loss\": train_loss_metric.compute(),\n",
        "                    \"val/loss\": val_loss_metric.compute(),\n",
        "                    \"lr\": lr,\n",
        "                    \"val/acc\":out_res['val']['accuracy'] ,\n",
        "                    #\"mfu\": running_mfu*100, # convert to percentage\n",
        "                })\n",
        "\n",
        "            if out_res['val'][\"loss\"] < best_val_loss or always_save_checkpoint:\n",
        "                best_val_loss = out_res['val'][\"loss\"]\n",
        "                if iter_num > 0:\n",
        "                    checkpoint = {\n",
        "                        'model': raw_model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'model_args': model_args,\n",
        "                        'iter_num': iter_num,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'config': config,\n",
        "                    }\n",
        "                    print(f\"saving checkpoint to {out_dir}\")\n",
        "                    torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "        if iter_num % eval_test_interval == 0:\n",
        "            out_res,all_labels,all_preds = estimate_loss_test(test_loader,len(test_dataset))\n",
        "            test_acc = out_res['test']['accuracy']\n",
        "            test_precision = out_res['test']['precision']\n",
        "            test_recall = out_res['test']['recall']\n",
        "            test_f1 = out_res['test']['f1']\n",
        "            print(f\"Test accuracy {test_acc:.4f}\")\n",
        "            print(f\"Test precision {test_precision:.4f}\")\n",
        "            print(f\"Test recall {test_recall:.4f}\")\n",
        "            print(f\"Test F1 {test_f1:.4f}\")\n",
        "            print(\"Confusion Matrix\")\n",
        "            print(\"------------------\")\n",
        "            print(out_res['test']['conf_matrix'])\n",
        "\n",
        "            if wandb_log:\n",
        "                wandb.log({\"Test_Accuracy\": test_acc, \"Test_Precision\": test_precision, \"Test_Recall\": test_recall, \"Test_F1_Score\": test_f1})\n",
        "                x_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "                y_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "                #wandb.log({\"Confusion Matrix\": wandb.plots.HeatMap(\n",
        "                #x_labels, y_labels, out_res['test']['conf_matrix'], show_text=True\n",
        "                #)})\n",
        "                #conf_matrix_img = plot_confusion_matrix(out_res['test']['conf_matrix'], x_labels, y_labels)\n",
        "                #print(\"All labels and all preds\", all_labels, all_preds)\n",
        "                wandb.log({\"Confusion Matrix\":wandb.plot.confusion_matrix(probs=None,\n",
        "                                                y_true=all_labels, preds=all_preds,\n",
        "                                                class_names=[\"negative\", \"neutral\", \"positive\"])})\n",
        "\n",
        "        if iter_num == 0 and eval_only:\n",
        "            break\n",
        "\n",
        "        # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "        # and using the GradScaler if data type is float16\n",
        "        for micro_step in range(gradient_accumulation_steps):\n",
        "            if ddp:\n",
        "                # in DDP training we only need to sync gradients at the last micro step.\n",
        "                # the official way to do this is with model.no_sync() context manager, but\n",
        "                # I really dislike that this bloats the code and forces us to repeat code\n",
        "                # looking at the source of that context manager, it just toggles this variable\n",
        "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "            with ctx:\n",
        "                logits, loss = model(X,Y)\n",
        "                #print(logits)\n",
        "                #print(Y)\n",
        "                #print(loss)\n",
        "                train_loss_metric.update(loss.item())\n",
        "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "            try:\n",
        "              batch = next(train_loader_iter)\n",
        "            except:\n",
        "              train_loader_iter = iter(train_loader)\n",
        "              batch = next(train_loader_iter)\n",
        "            X = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            Y = batch['labels'].to(device)\n",
        "            # backward pass, with gradient scaling if training in fp16\n",
        "            scaler.scale(loss).backward()\n",
        "        # clip the gradient\n",
        "        if grad_clip != 0.0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        # step the optimizer and scaler if training in fp16\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # flush the gradients as soon as we can, no need for this memory anymore\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "              batch_val = next(val_loader_iter)\n",
        "            except:\n",
        "              val_loader_iter= iter(val_loader)\n",
        "              batch_val = next(val_loader_iter)\n",
        "\n",
        "            X_val = batch_val['input_ids'].to(device)\n",
        "            attention_mask = batch_val['attention_mask'].to(device)\n",
        "            Y_val = batch_val['labels'].to(device)\n",
        "            logits_val , loss_val = model(X_val,Y_val)\n",
        "            val_loss_metric.update(loss_val.item())\n",
        "        model.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # timing and logging\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        t0 = t1\n",
        "        if iter_num % log_interval == 0 and master_process:\n",
        "            # get loss as float. note: this is a CPU-GPU sync point\n",
        "            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "            lossf = loss.item() * gradient_accumulation_steps\n",
        "            if local_iter_num >= 5: # let the training loop settle a bit\n",
        "                #mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "                mfu = 0\n",
        "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "        iter_num += 1\n",
        "        local_iter_num += 1\n",
        "\n",
        "        # termination conditions\n",
        "        if iter_num > max_iters:\n",
        "            break\n",
        "\n",
        "    if ddp:\n",
        "        destroy_process_group()\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'grid',  # Can be grid, random, or bayesian\n",
        "    'metric': {\n",
        "        'name': 'Test_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'if_smote': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'if_preprocess': {\n",
        "            'values': [True, False]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [5e-5, 1e-4]\n",
        "        },\n",
        "        'augment': {\n",
        "            'values': [True, False]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "wandb_project = 'DI725-HW1-HyperParamTune'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=wandb_project)\n",
        "wandb.agent(sweep_id, train)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
